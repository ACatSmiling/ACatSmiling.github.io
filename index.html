<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
<meta property="og:type" content="website">
<meta property="og:title" content="XiSun的博客">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="XiSun的博客">
<meta property="og:description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="XiSun">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>XiSun的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">XiSun的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Learning is endless</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/29/kafka-consumer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/29/kafka-consumer/" class="post-title-link" itemprop="url">Kafka Consumer 消费数据时要考虑的问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-29 15:10:10" itemprop="dateCreated datePublished" datetime="2020-10-29T15:10:10+08:00">2020-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-30 14:26:59" itemprop="dateModified" datetime="2020-10-30T14:26:59+08:00">2020-10-30</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="如何消费数据"><a href="#如何消费数据" class="headerlink" title="如何消费数据"></a>如何消费数据</h2><p>在<a target="_blank" rel="noopener" href="https://acatsmiling.github.io/2020/10/26/kafka-producer/">上一篇文章</a>中，介绍了如何发送数据到 Kafka，既然有数据发送，那么肯定就有数据消费，消费者也是 Kafka 整个体系中不可缺少的一环。</p>
<p>下面是一段创建 <code>KafkaConsumer</code> 的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 必须设置的属性</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.239.131:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;group1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 可选设置的属性</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;earliest &quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;client.id&quot;</span>, <span class="string">&quot;zy_client_id&quot;</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">&quot;test&quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">            records.forEach(record -&gt; System.out.printf(<span class="string">&quot;topic = %s, partition = %d, offset = %d, key = %s, value = %s%n&quot;</span>,</span><br><span class="line">                    record.topic(), record.partition(), record.offset(), record.key(), record.value()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="push-还是-pull"><a href="#push-还是-pull" class="headerlink" title="push 还是 pull"></a>push 还是 pull</h3><p>Kafka Consumer 采用的是主动拉取 broker 数据进行消费的。</p>
<p>一般消息中间件存在推送 (Server 推送数据给 Consumer) 和拉取 (Consumer 主动取服务器取数据) 两种方式，这两种方式各有优劣。</p>
<p>如果是选择推送的方式，最大的阻碍就是服务器不清楚 Consumer 的消费速度，如果 Consumer 中执行的操作又是比较耗时的，那么 Consumer 可能会不堪重负，甚至会导致系统挂掉。</p>
<p>而采用拉取的方式则可以解决这种情况，Consumer 根据自己的状态来拉取数据，可以对服务器的数据进行延迟处理。但是这种方式也有一个劣势就是服务器没有数据的时候可能会一直轮询，不过还好 <code>KafkaConsumer</code> 的 <code>poll()</code> 方法有参数允许消费者请求在 “长轮询” 中阻塞，等待数据到达 (并且可选地等待直到给定数量的字节可用以确保传输大小)。</p>
<h3 id="必须设置的属性"><a href="#必须设置的属性" class="headerlink" title="必须设置的属性"></a>必须设置的属性</h3><p><code>KafkaConsumer</code> 的必须设置的属性有4个：</p>
<p>1.<code>bootstrap.servers</code>：连接 Kafka 集群的地址，多个地址以逗号分隔</p>
<p>2.<code>key.deserializer</code>：消息中 key 反序列化类，需要和 <code>KafkaProducer</code> 中 key 序列化类相对应</p>
<p>3.<code>value.deserializer</code>：消息中 value 的反序列化类，需要和 <code>KafkaProducer</code> 中 Value 序列化类相对应</p>
<p>4.<code>group.id</code>：消费者所属消费组的唯一标识</p>
<p>这里着重说一下 <code>group.id</code> 这个属性，<code>KafkaConsumer</code> 和 <code>KafkaProducer</code> 不一样，<code>KafkaConsumer</code> 中有一个 Consumer group (消费者组)，由它来决定同一个 Consumer group 中的消费者具体拉取哪个 partition 的数据，所以这里必须指定 <code>group.id</code> 属性。</p>
<h3 id="订阅-取消主题"><a href="#订阅-取消主题" class="headerlink" title="订阅/取消主题"></a>订阅/取消主题</h3><p>1.使用 <code>subscribe()</code> 方法订阅主题</p>
<p>2.使用 <code>assign()</code> 方法订阅指定主题和分区</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitionInfoList = kafkaConsumer.partitionsFor(<span class="string">&quot;test&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (partitionInfoList != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (PartitionInfo partitionInfo : partitionInfoList) &#123;</span><br><span class="line">        kafkaConsumer.assign(Collections.singletonList(<span class="keyword">new</span> TopicPartition(partitionInfo.topic(), partitionInfo.partition())));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3.取消主题</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafkaConsumer.unsubscribe();</span><br><span class="line">kafkaConsumer.subscribe(<span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line">kafkaConsumer.assign(<span class="keyword">new</span> ArrayList&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure>

<p>上面的三行代码作用相同，都是取消订阅，其中 <code>unsubscribe ()</code> 方法即可以取消通过 <code>subscribe ()</code> 方式实现的订阅，也可以取消通过 <code>assign ()</code> 方式实现的订阅。</p>
<h2 id="如何更好的消费数据"><a href="#如何更好的消费数据" class="headerlink" title="如何更好的消费数据"></a>如何更好的消费数据</h2><p>文章开头处的代码展示了我们是如何消费数据的，但是代码未免过于简单，我们测试的时候这样写没有问题，但是实际开发过程中我们并不会这样写，我们会选择更加高效的方式，这里提供两种方式供大家参考。</p>
<p>1.一个 Consumer group，多个 consumer，数量小于等于 partition 的数量</p>
<p><img src="/2020/10/29/kafka-consumer/Kafka_multi_consumer.png" alt="Kafka_multi_consumer"></p>
<p>2.一个 consumer，多线程处理事件</p>
<p><img src="/2020/10/29/kafka-consumer/Kafka_multi_event_handler.png" alt="Kafka_multi_event_handler"></p>
<p>第一种方式每个 consumer 都要维护一个独立的 TCP 连接，如果分区数和创建 consumer 线程的数量过多，会造成不小的系统开销。但是如果处理消息足够快速，消费性能也会提升，如果慢的话就会导致消费性能降低。</p>
<p>第二种方式是采用一个 consumer，多个消息处理线程来处理消息，其实在生产中，瓶颈一般是集中在消息处理上 (可能会插入数据到数据库，或者请求第三方 API)，所以我们采用多个线程来处理这些消息。</p>
<p>当然可以结合第一和第二两种方式，采用多 consumer + 多个消息处理线程来消费 Kafka 中的数据，核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; consumerNum; i++) &#123;</span><br><span class="line">  <span class="comment">// 根据属性创建Consumer</span></span><br><span class="line">  <span class="keyword">final</span> Consumer&lt;String, <span class="keyword">byte</span>[]&gt; consumer = consumerFactory.getConsumer(getServers(), groupId);</span><br><span class="line">  consumerList.add(consumer);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 订阅主题</span></span><br><span class="line">  consumer.subscribe(Arrays.asList(<span class="keyword">this</span>.getTopic()));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// consumer.poll()拉取数据</span></span><br><span class="line">  BufferedConsumerRecords bufferedConsumerRecords = <span class="keyword">new</span> BufferedConsumerRecords(consumer);</span><br><span class="line"></span><br><span class="line">  getExecutor().scheduleWithFixedDelay(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">long</span> startTime = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 进行消息处理</span></span><br><span class="line">      consumeEvents(bufferedConsumerRecords);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">long</span> sleepTime = intervalMillis - (System.currentTimeMillis() - startTime);</span><br><span class="line">      <span class="keyword">if</span> (sleepTime &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        Thread.sleep(sleepTime);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;, <span class="number">0</span>, <span class="number">1000</span>, TimeUnit.MILLISECONDS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>不过这种方式不能顺序处理数据，如果你的业务是顺序处理，那么第一种方式可能更适合你。所以实际生产中请根据业务选择最适合自己的方式。</p>
<h2 id="消费数据考虑哪些问题？"><a href="#消费数据考虑哪些问题？" class="headerlink" title="消费数据考虑哪些问题？"></a>消费数据考虑哪些问题？</h2><p>在 Kafka 中无论是 <code>KafkarPoducer</code> 往 topic 中写数据，还是 <code>KafkaConsumer</code> 从 topic 中读数据，都避免不了和 offset 打交道，关于 offset 主要有以下几个概念。</p>
<p><img src="/2020/10/29/kafka-consumer/kafka-partition-offset.png" alt="Kafka Offset"></p>
<ul>
<li><p>Last Committed Offset：Consumer group 最新一次 commit 的 offset，表示这个 group 已经把 Last Committed Offset 之前的数据都消费成功了。</p>
</li>
<li><p>Current Position：Consumer group 当前消费数据的 offset，也就是说，Last Committed Offset 到 Current Position 之间的数据已经拉取成功，可能正在处理，但是还未 commit。</p>
</li>
<li><p>Log End Offset (LEO)：记录底层日志 (log) 中的下一条消息的 offset。对 <code>KafkaProducer</code> 来说，就是即将插入下一条消息的 offset。</p>
</li>
<li><p>High Watermark (HW)：已经成功备份到其他 replicas 中的最新一条数据的 offset，也就是说 Log End Offset 与 High Watermark 之间的数据已经写入到该 partition 的 leader 中，但是还未完全备份到其他的 replicas 中，consumer 是无法消费这部分消息 (未提交消息)。</p>
</li>
</ul>
<p>每个 Kafka 副本对象都有两个重要的属性：LEO 和 HW。注意是所有的副本，而不只是 leader 副本。关于这两者更详细解释，参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/huxi2b/p/7453543.html"><a target="_blank" rel="noopener" href="https://www.cnblogs.com/huxi2b/p/7453543.html">Kafka 的 High Watermark 与 leader epoch 的讨论</a></a></p>
<p>对于消费者而言，我们更多时候关注的是消费完成之后如何和服务器进行消费确认，告诉服务器这部分数据我已经消费过了。</p>
<p>这里就涉及到了 2 个 offset，一个是 Current Position, 一个是处理完毕向服务器确认的 Last Committed Offset。显然，异步模式下 Last Committed Offset 是落后于 Current Position 的。如果 consumer 挂掉了，那么下一次消费数据又只会从 Last Committed Offset 的位置拉取数据，就会导致数据被重复消费。</p>
<h3 id="提交策略如何选择？"><a href="#提交策略如何选择？" class="headerlink" title="提交策略如何选择？"></a>提交策略如何选择？</h3><p>Kafka 提供了三种提交 offset 的方式。</p>
<p>1.自动提交</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自动提交,默认true</span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"><span class="comment">// 设置自动每1s提交一次</span></span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>2.手动同步提交</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafkaConsumer.commitSync();</span><br></pre></td></tr></table></figure>

<p>3.手动异步提交</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafkaConsumer.commitAsync();</span><br></pre></td></tr></table></figure>

<p>上面说了，既然异步提交 offset 可能会重复消费，那么我使用同步提交是否就可以解决数据重复消费的问题呢？我只能说 too young, too sample。且看如下代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">  records.forEach(record -&gt; &#123;</span><br><span class="line">      insertIntoDB(record);</span><br><span class="line">      kafkaConsumer.commitSync();</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>很明显不行，因为 <code>insertIntoDB()</code> 和 <code>kafkaConsumer.commitSync ()</code> 两个方法做不到原子操作，如果 <code>insertIntoDB ()</code> 成功了，但是提交 offset 的时候 <code>KafkaConsumer</code> 挂掉了，然后服务器重启，仍然会导致重复消费问题。</p>
<h3 id="是否需要做到不重复消费？"><a href="#是否需要做到不重复消费？" class="headerlink" title="是否需要做到不重复消费？"></a>是否需要做到不重复消费？</h3><p>只要保证处理消息和提交 offset 的操作是原子操作，就可以做到不重复消费。我们可以自己管理 committed offset，而不让 Kafka 来进行管理。</p>
<p>比如如下使用方式：</p>
<p>1.如果消费的数据刚好需要存储在数据库，那么可以把 offset 也存在数据库，就可以在一个事物中提交这两个结果，保证原子操作。</p>
<p>2.借助搜索引擎，把 offset 和数据一起放到索引里面，比如 Elasticsearch。</p>
<p>每条记录都有自己的 offset，所以如果要管理自己的 offset 还得要做下面事情：</p>
<p>1.设置 <code>enable.auto.commit</code> 为 <code>false</code></p>
<p>2.使用每个 <code>ConsumerRecord</code> 提供的 offset 来保存消费的位置</p>
<p>3.在重新启动时使用 <code>seek (TopicPartition partition, long offset)</code> 恢复上次消费的位置</p>
<p>通过上面的方式就可以在消费端实现 ”Exactly Once” 的语义，即保证只消费一次。但是是否真的需要保证不重复消费呢？这个得看具体业务，如果重复消费数据对整体有什么影响，然后再来决定是否需要做到不重复消费。</p>
<h3 id="再均衡-reblance-怎么办？"><a href="#再均衡-reblance-怎么办？" class="headerlink" title="再均衡 (reblance) 怎么办？"></a>再均衡 (reblance) 怎么办？</h3><p>再均衡是指分区的所属权从一个消费者转移到另一个消费者的行为，再均衡期间，消费者组内的消费者无法读取消息。为了更精确的控制消息的消费，我们可以在订阅主题的时候，通过指定监听器的方式来设定发生再均衡动作前后的一些准备或者收尾的动作。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kafkaConsumer.subscribe(Collections.singletonList(<span class="string">&quot;test&quot;</span>), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 再均衡之前和消费者停止读取消息之后被调用</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 重新分配分区之后和消费者开始消费之前被调用</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>具体如何操作，得根据具体的业务逻辑来实现，如果消息比较重要，你可以在再均衡的时候处理 offset，如果不够重要，你可以什么都不做。</p>
<h3 id="无法消费的数据怎么办？"><a href="#无法消费的数据怎么办？" class="headerlink" title="无法消费的数据怎么办？"></a>无法消费的数据怎么办？</h3><p>可能由于你的业务逻辑有些数据没法消费，这个时候怎么办？同样的还是的看你认为这个数据有多重要或者多不重要，如果重要可以记录日志，把它存入文件或者数据库，以便于稍候进行重试或者定向分析。如果不重要就当做什么事情都没有发生好了。</p>
<h2 id="实际开发中我的处理方式"><a href="#实际开发中我的处理方式" class="headerlink" title="实际开发中我的处理方式"></a>实际开发中我的处理方式</h2><p>我开发的项目中，用到 Kafka 的其中一个地方是消息通知 (谁给你发了消息，点赞，评论等)，大概的流程就是用户在 client 端做了某些操作，就会发送数据到 Kafka，然后把这些数据进行一定的处理之后插入到 HBase 中。</p>
<p>其中采用了 N consumer thread + N Event Handler 的方式来消费数据，并采用自动提交 offset。对于无法消费的数据往往只是简单处理下，打印下日志以及消息体 (无法消费的情况非常非常少)。</p>
<p>得益于 HBase 的多 version 控制，即使是重复消费了数据也无关紧要。这样做没有去避免重复消费的问题主要是基于以下几点考虑：</p>
<p>1.重复消费的概率较低，服务器整体性能稳定</p>
<p>2.即便是重复消费了数据，入库了 HBase，获取数据也是只有一条，不影响结果的正确性</p>
<p>3.有更高的吞吐量</p>
<p>4.编程简单，不用单独去处理以及保存 offset</p>
<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="http://generalthink.github.io/2019/05/06/kafka-consumer-use/">http://generalthink.github.io/2019/05/06/kafka-consumer-use/</a></p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:&#x77;&#100;&#115;&#104;&#x66;&#117;&#x74;&#x40;&#x31;&#x36;&#x33;&#x2e;&#x63;&#x6f;&#109;">&#x77;&#100;&#115;&#104;&#x66;&#117;&#x74;&#x40;&#x31;&#x36;&#x33;&#x2e;&#x63;&#x6f;&#109;</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/26/kafka-producer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/26/kafka-producer/" class="post-title-link" itemprop="url">Kafka Producer 部分源码解析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-26 14:15:52" itemprop="dateCreated datePublished" datetime="2020-10-26T14:15:52+08:00">2020-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-30 14:25:14" itemprop="dateModified" datetime="2020-10-30T14:25:14+08:00">2020-10-30</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>17 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>先来看一段创建 <code>KafkaProducer</code> 的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// bootstrap.servers 必须设置</span></span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.239.131:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key.serializer    必须设置</span></span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// value.serializer  必须设置</span></span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// client.id</span></span><br><span class="line">        props.put(ProducerConfig.CLIENT_ID_CONFIG, <span class="string">&quot;client-0&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// retries</span></span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// acks</span></span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">&quot;all&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// max.in.flight.requests.per.connection</span></span><br><span class="line">        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// linger.ms</span></span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// batch.size</span></span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">10240</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// buffer.memory</span></span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">10240</span>);</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定topic，key，value</span></span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;test1&quot;</span>, <span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 异步发送</span></span><br><span class="line">        kafkaProducer.send(record, (recordMetadata, exception) -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 发送失败的处理逻辑</span></span><br><span class="line">                exception.printStackTrace();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 发送成功的处理逻辑</span></span><br><span class="line">                System.out.println(recordMetadata.topic());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 同步发送</span></span><br><span class="line">        <span class="comment">// kafkaProducer.send(record).get();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭Producer</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="主要流程图"><a href="#主要流程图" class="headerlink" title="主要流程图"></a>主要流程图</h2><p><img src="/2020/10/26/kafka-producer/image-20201026150837966.png"></p>
<p>简要说明：</p>
<p>1.<code>new KafkaProducer()</code> 后，创建一个后台线程 <code>KafkaThread</code> (实际运行线程是 <code>Sender</code>，<code>KafkaThread</code> 是对 <code>Sender</code> 的封装) 扫描 <code>RecordAccumulator</code> 中是否有消息；</p>
<p>2.调用 <code>KafkaProducer.send()</code> 发送消息，实际是将消息保存到 <code>RecordAccumulator</code> 中，实际上就是保存到一个Map中 (<code>ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;</code>)，这条消息会被记录到同一个记录批次(相同主题相同分区算同一个批次)里面，这个批次的所有消息会被发送到相同的主题和分区上；</p>
<p>3.后台的独立线程扫描到 <code>RecordAccumulator</code> 中有消息后，会将消息发送到Kafka集群中 (不是一有消息就发送，而是要看消息是否 ready)；</p>
<p>4.如果发送成功 (消息成功写入 Kafka)，就返回一个 <code>RecordMetaData</code> 对象，它包换了主题和分区信息，以及记录在分区里的偏移量等信息；</p>
<p>5.如果写入失败，就会返回一个错误，生产者在收到错误之后会尝试重新发送消息 (如果允许的话，此时会将消息在保存到 <code>RecordAccumulator</code> 中)，达到重试次数之后如果还是失败就返回错误消息。</p>
<h2 id="缓存器的创建"><a href="#缓存器的创建" class="headerlink" title="缓存器的创建"></a>缓存器的创建</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.accumulator = <span class="keyword">new</span> RecordAccumulator(logContext,</span><br><span class="line">                    config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),</span><br><span class="line">                    <span class="keyword">this</span>.compressionType,</span><br><span class="line">                    lingerMs(config),</span><br><span class="line">                    retryBackoffMs,</span><br><span class="line">                    deliveryTimeoutMs,</span><br><span class="line">                    metrics,</span><br><span class="line">                    PRODUCER_METRIC_GROUP_NAME,</span><br><span class="line">                    time,</span><br><span class="line">                    apiVersions,</span><br><span class="line">                    transactionManager,</span><br><span class="line">                    <span class="keyword">new</span> BufferPool(<span class="keyword">this</span>.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));</span><br></pre></td></tr></table></figure>

<h2 id="后台线程的创建"><a href="#后台线程的创建" class="headerlink" title="后台线程的创建"></a>后台线程的创建</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.sender = newSender(logContext, kafkaClient, <span class="keyword">this</span>.metadata);</span><br><span class="line">String ioThreadName = NETWORK_THREAD_PREFIX + <span class="string">&quot; | &quot;</span> + clientId;</span><br><span class="line"><span class="keyword">this</span>.ioThread = <span class="keyword">new</span> KafkaThread(ioThreadName, <span class="keyword">this</span>.sender, <span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">this</span>.ioThread.start();</span><br><span class="line"></span><br><span class="line">KafkaClient client = kafkaClient != <span class="keyword">null</span> ? kafkaClient : <span class="keyword">new</span> NetworkClient(</span><br><span class="line">                <span class="keyword">new</span> Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),</span><br><span class="line">                        <span class="keyword">this</span>.metrics, time, <span class="string">&quot;producer&quot;</span>, channelBuilder, logContext),</span><br><span class="line">                metadata,</span><br><span class="line">                clientId,</span><br><span class="line">                maxInflightRequests,</span><br><span class="line">                producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),</span><br><span class="line">                producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),</span><br><span class="line">                producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),</span><br><span class="line">                producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),</span><br><span class="line">                requestTimeoutMs,</span><br><span class="line">                ClientDnsLookup.forConfig(producerConfig.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)),</span><br><span class="line">                time,</span><br><span class="line">                <span class="keyword">true</span>,</span><br><span class="line">                apiVersions,</span><br><span class="line">                throttleTimeSensor,</span><br><span class="line">                logContext);</span><br></pre></td></tr></table></figure>

<p>上述代码中，构造了一个 <code>KafkaClient</code> 负责和 broker 通信，同时构造一个 <code>Sender</code> 并启动一个异步线程，这个线程会被命名为：<code>kafka-producer-network-thread | $&#123;clientId&#125;</code>，如果你在创建 producer 的时候指定 <code>client.id</code> 的值为 myclient，那么线程名称就是 kafka-producer-network-thread | myclient。</p>
<h2 id="发送消息-缓存消息"><a href="#发送消息-缓存消息" class="headerlink" title="发送消息 (缓存消息)"></a>发送消息 (缓存消息)</h2><p>发送消息有同步发送以及异步发送两种方式，我们一般不使用同步发送，毕竟太过于耗时，使用异步发送的时候可以指定回调函数，当消息发送完成的时候(成功或者失败)会通过回调通知生产者。</p>
<p>同步 send：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> send(record, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>异步 send：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// intercept the record, which can be potentially modified; this method does not throw exceptions</span></span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptedRecord = <span class="keyword">this</span>.interceptors.onSend(record);</span><br><span class="line">    <span class="keyword">return</span> doSend(interceptedRecord, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，同步和异步实际上调用的是同一个方法，同步发送时，设置回调函数为 null。</p>
<p>消息发送之前，会先对 key 和 value 进行序列化：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] serializedKey;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">&quot;Can&#x27;t convert key of class &quot;</span> + record.key().getClass().getName() +</span><br><span class="line">                                     <span class="string">&quot; to class &quot;</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                                     <span class="string">&quot; specified in key.serializer&quot;</span>, cce);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">byte</span>[] serializedValue;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">&quot;Can&#x27;t convert value of class &quot;</span> + record.value().getClass().getName() +</span><br><span class="line">                                     <span class="string">&quot; to class &quot;</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                                     <span class="string">&quot; specified in value.serializer&quot;</span>, cce);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>计算分区：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br></pre></td></tr></table></figure>

<p>发送消息，实际上是将消息缓存起来，核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                    serializedValue, headers, interceptCallback, remainingWaitMs);</span><br></pre></td></tr></table></figure>

<p><code>RecordAccumulator</code> 的核心数据结构是 <code>ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;</code>，会将相同 topic 相同 partition 的数据放到一个 Deque (双向队列) 中，这也是我们之前提到的同一个记录批次里面的消息会发送到同一个主题和分区的意思。<code>append()</code> 方法的核心源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从batchs(ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)中，根据主题分区获取对应的队列，如果没有则new ArrayDeque&lt;&gt;返回</span></span><br><span class="line">Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算同一个记录批次占用空间大小，batchSize根据batch.size参数决定</span></span><br><span class="line"><span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(</span><br><span class="line">    maxUsableMagic, compression, key, value, headers));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为同一个topic，partition分配buffer，如果同一个记录批次的内存不足，那么会阻塞maxTimeToBlock(max.block.ms参数)这么长时间</span></span><br><span class="line">ByteBuffer buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line"></span><br><span class="line"><span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">  <span class="comment">// 创建MemoryRecordBuilder，通过buffer初始化appendStream(DataOutputStream)属性</span></span><br><span class="line">  MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">  ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将key，value写入到MemoryRecordsBuilder中的appendStream(DataOutputStream)中</span></span><br><span class="line">  FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将需要发送的消息放入到队列中</span></span><br><span class="line">  dq.addLast(batch);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="发送消息到-Kafka"><a href="#发送消息到-Kafka" class="headerlink" title="发送消息到 Kafka"></a>发送消息到 Kafka</h2><p>上面已经将消息存储 <code>RecordAccumulator</code> 中去了，现在看看怎么发送消息。前面提到创建 <code>KafkaProducer</code> 的时候，会启动一个异步线程去从 <code>RecordAccumulator</code> 中取得消息然后发送到 Kafka，发送消息的核心代码在 <code>Sender.java</code> 中，它实现了 Runnable 接口并在后台一直运行处理发送请求并将消息发送到合适的节点，直到 <code>KafkaProducer</code> 被关闭。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata</span></span><br><span class="line"><span class="comment"> * requests to renew its view of the cluster and then sends produce requests to the appropriate nodes.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sender</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The main run loop for the sender thread</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    	<span class="comment">// main loop, runs until close is called</span></span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                runOnce();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;Uncaught error in kafka producer I/O thread: &quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// okay we stopped accepting requests but there may still be</span></span><br><span class="line">        <span class="comment">// requests in the transaction manager, accumulator or waiting for acknowledgment,</span></span><br><span class="line">        <span class="comment">// wait until these are completed.</span></span><br><span class="line">        <span class="keyword">while</span> (!forceClose &amp;&amp; ((<span class="keyword">this</span>.accumulator.hasUndrained() || <span class="keyword">this</span>.client.inFlightRequestCount() &gt; <span class="number">0</span>) || hasPendingTransactionalRequests())) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                runOnce();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;Uncaught error in kafka producer I/O thread: &quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Abort the transaction if any commit or abort didn&#x27;t go through the transaction manager&#x27;s queue</span></span><br><span class="line">        <span class="keyword">while</span> (!forceClose &amp;&amp; transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.hasOngoingTransaction()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!transactionManager.isCompleting()) &#123;</span><br><span class="line">                log.info(<span class="string">&quot;Aborting incomplete transaction due to shutdown&quot;</span>);</span><br><span class="line">                transactionManager.beginAbort();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                runOnce();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;Uncaught error in kafka producer I/O thread: &quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (forceClose) &#123;</span><br><span class="line">            <span class="comment">// We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on</span></span><br><span class="line">            <span class="comment">// the futures.</span></span><br><span class="line">            <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;Aborting incomplete transactional requests due to forced shutdown&quot;</span>);</span><br><span class="line">                transactionManager.close();</span><br><span class="line">            &#125;</span><br><span class="line">            log.debug(<span class="string">&quot;Aborting incomplete batches due to forced shutdown&quot;</span>);</span><br><span class="line">            <span class="keyword">this</span>.accumulator.abortIncompleteBatches();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>KafkaProducer</code> 的关闭方法有2个：<code>close()</code> 以及 <code>close(Duration timeout)</code>，<code>close(long timeout, TimeUnit timUnit)</code> 已被弃用，其中 timeout 参数的意思是等待生产者完成任何待处理请求的最长时间，第一种方式的 timeout 为 <code>Long.MAX_VALUE</code> 毫秒，如果采用第二种方式关闭，当 timeout = 0 的时候则表示强制关闭，直接关闭 Sender (设置 running = false)。</p>
<p><code>Send.java</code> 中，<code>runOnce()</code> 方法，跳过对 <code>transactionManager</code> 的处理，查看发送消息的主要流程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> currentTimeMs = time.milliseconds();</span><br><span class="line"><span class="comment">// 将记录批次转移到每个节点的生产请求列表中</span></span><br><span class="line"><span class="keyword">long</span> pollTimeout = sendProducerData(currentTimeMs);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 轮询进行消息发送</span></span><br><span class="line">client.poll(pollTimeout, currentTimeMs);</span><br></pre></td></tr></table></figure>

<p>首先，查看 <code>sendProducerData(currentTimeMs)</code> 方法，它的核心逻辑在 <code>sendProduceRequest(batches, now)</code> 方法中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (ProducerBatch batch : batches) &#123;</span><br><span class="line">    TopicPartition tp = batch.topicPartition;</span><br><span class="line">    <span class="comment">// 将ProducerBatch中MemoryRecordsBuilder转换为MemoryRecords(发送的数据就在这里面)</span></span><br><span class="line">    MemoryRecords records = batch.records();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// down convert if necessary to the minimum magic used. In general, there can be a delay between the time</span></span><br><span class="line">    <span class="comment">// that the producer starts building the batch and the time that we send the request, and we may have</span></span><br><span class="line">    <span class="comment">// chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use</span></span><br><span class="line">    <span class="comment">// the new message format, but found that the broker didn&#x27;t support it, so we need to down-convert on the</span></span><br><span class="line">    <span class="comment">// client before sending. This is intended to handle edge cases around cluster upgrades where brokers may</span></span><br><span class="line">    <span class="comment">// not all support the same message format version. For example, if a partition migrates from a broker</span></span><br><span class="line">    <span class="comment">// which is supporting the new magic version to one which doesn&#x27;t, then we will need to convert.</span></span><br><span class="line">    <span class="keyword">if</span> (!records.hasMatchingMagic(minUsedMagic))</span><br><span class="line">        records = batch.records().downConvert(minUsedMagic, <span class="number">0</span>, time).records();</span><br><span class="line">    produceRecordsByPartition.put(tp, records);</span><br><span class="line">    recordsByPartition.put(tp, batch);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout,</span><br><span class="line">                produceRecordsByPartition, transactionalId);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 消息发送完成时的回调(消息发送失败后，在handleProduceResponse中处理)</span></span><br><span class="line">RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>&#123;</span><br><span class="line">        handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据参数构造ClientRequest，此时需要发送的消息在requestBuilder中</span></span><br><span class="line">ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>,</span><br><span class="line">                requestTimeoutMs, callback);</span><br><span class="line"><span class="comment">// 将clientRequest转换成Send对象(Send.java，包含了需要发送数据的buffer)，给KafkaChannel设置该对象，记住这里还没有发送数据</span></span><br><span class="line">client.send(clientRequest, now);</span><br></pre></td></tr></table></figure>

<p>在没有指定 kafkaclient 时，<code>client.send(clientRequest, now)</code> 方法，实际就是 <code>NetworkClient.send(ClientRequest request, long now)</code> 方法，所有的请求 (无论是 producer 发送消息的请求，还是获取 metadata 的请求) 都是通过该方法设置对应的 Send 对象：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Send send = request.toSend(destination, header);</span><br></pre></td></tr></table></figure>

<p>需要知道的是，上面只是设置了发送消息所需要准备的内容。</p>
<p>接下来，查看 <code>client.poll(pollTimeout, currentTimeMs)</code> 方法，进入到发送消息的主流程，发送消息的核心代码最终可以定位到 <code>Selector.java</code> 的 <code>pollSelectionKeys(Set&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos)</code> 方法中，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* if channel is ready write to any sockets that have space in their buffer and for which we have data */</span></span><br><span class="line"><span class="keyword">if</span> (channel.ready() &amp;&amp; key.isWritable() &amp;&amp; !channel.maybeBeginClientReauthentication(</span><br><span class="line">    () -&gt; channelStartTimeNanos != <span class="number">0</span> ? channelStartTimeNanos : currentTimeNanos)) &#123;</span><br><span class="line">    Send send;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 底层实际调用的是java8 GatheringByteChannel的write方法</span></span><br><span class="line">        send = channel.write();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        sendFailed = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (send != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>.completedSends.add(send);</span><br><span class="line">        <span class="keyword">this</span>.sensors.recordBytesSent(channel.id(), send.size());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>就这样，我们的消息就发送到了 broker 中了，发送流程分析完毕，注意，这个是完美发送的情况。但是总会有发送失败的时候 (消息过大或者没有可用的 leader 等)，那么发送失败后重发又是在哪里完成的呢？还记得上面的回调函数吗，没错，就是在回调函数这里设置的，先来看下回调函数源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Handle a produce response</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleProduceResponse</span><span class="params">(ClientResponse response, Map&lt;TopicPartition, ProducerBatch&gt; batches, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    RequestHeader requestHeader = response.requestHeader();</span><br><span class="line">    <span class="keyword">long</span> receivedTimeMs = response.receivedTimeMs();</span><br><span class="line">    <span class="keyword">int</span> correlationId = requestHeader.correlationId();</span><br><span class="line">    <span class="keyword">if</span> (response.wasDisconnected()) &#123;</span><br><span class="line">        <span class="comment">// 如果是网络断开则构造Errors.NETWORK_EXCEPTION的响应</span></span><br><span class="line">        <span class="keyword">for</span> (ProducerBatch batch : batches.values())</span><br><span class="line">            completeBatch(batch, <span class="keyword">new</span> ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION), correlationId, now, <span class="number">0L</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (response.versionMismatch() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果是版本不匹配，则构造Errors.UNSUPPORTED_VERSION的响应</span></span><br><span class="line">        <span class="keyword">for</span> (ProducerBatch batch : batches.values())</span><br><span class="line">            completeBatch(batch, <span class="keyword">new</span> ProduceResponse.PartitionResponse(Errors.UNSUPPORTED_VERSION), correlationId, now, <span class="number">0L</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// if we have a response, parse it(如果存在response就返回正常的response)</span></span><br><span class="line">        <span class="keyword">if</span> (response.hasResponse()) &#123;</span><br><span class="line">            ProduceResponse produceResponse = (ProduceResponse) response.responseBody();</span><br><span class="line">            <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, ProduceResponse.PartitionResponse&gt; entry : produceResponse.responses().entrySet()) &#123;</span><br><span class="line">                TopicPartition tp = entry.getKey();</span><br><span class="line">                ProduceResponse.PartitionResponse partResp = entry.getValue();</span><br><span class="line">                ProducerBatch batch = batches.get(tp);</span><br><span class="line">                completeBatch(batch, partResp, correlationId, now, receivedTimeMs + produceResponse.throttleTimeMs());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.sensors.recordLatency(response.destination(), response.requestLatencyMs());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// this is the acks = 0 case, just complete all requests(如果acks=0，那么则构造Errors.NONE的响应，因为这种情况只需要发送不需要响应结果)</span></span><br><span class="line">            <span class="keyword">for</span> (ProducerBatch batch : batches.values()) &#123;</span><br><span class="line">                completeBatch(batch, <span class="keyword">new</span> ProduceResponse.PartitionResponse(Errors.NONE), correlationId, now, <span class="number">0L</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 <code>completeBatch()</code> 方法中我们主要关注失败的逻辑处理，核心源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Complete or retry the given batch of records.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> batch The record batch</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> response The produce response</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> correlationId The correlation id for the request</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> now The current POSIX timestamp in milliseconds</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">completeBatch</span><span class="params">(ProducerBatch batch, ProduceResponse.PartitionResponse response, <span class="keyword">long</span> correlationId,</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="keyword">long</span> now, <span class="keyword">long</span> throttleUntilTimeMs)</span> </span>&#123;</span><br><span class="line">    Errors error = response.error;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (error == Errors.MESSAGE_TOO_LARGE &amp;&amp; batch.recordCount &gt; <span class="number">1</span> &amp;&amp; !batch.isDone() &amp;&amp;</span><br><span class="line">        (batch.magic() &gt;= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) &#123;</span><br><span class="line">        <span class="comment">// If the batch is too large, we split the batch and send the split batches again. We do not decrement</span></span><br><span class="line">        <span class="comment">// the retry attempts in this case.(如果发送的消息太大，需要重新进行分割发送)</span></span><br><span class="line">        <span class="keyword">this</span>.accumulator.splitAndReenqueue(batch);</span><br><span class="line">        maybeRemoveAndDeallocateBatch(batch);</span><br><span class="line">        <span class="keyword">this</span>.sensors.recordBatchSplit();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error != Errors.NONE) &#123;</span><br><span class="line">        <span class="comment">// 发生了错误，如果此时可以retry(retry次数未达到限制以及产生的异常是RetriableException)</span></span><br><span class="line">        <span class="keyword">if</span> (canRetry(batch, response, now)) &#123;</span><br><span class="line">            <span class="keyword">if</span> (transactionManager == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 把需要重试的消息放入队列中，等待重试，实际就是调用deque.addFirst(batch)</span></span><br><span class="line">                reenqueueBatch(batch, now);</span><br><span class="line">            &#125; </span><br><span class="line">            ...</span><br><span class="line">        &#125; </span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>以上，就是 <code>KafkaProducer</code> 发送消息的流程。</p>
<h2 id="补充：分区算法"><a href="#补充：分区算法" class="headerlink" title="补充：分区算法"></a>补充：分区算法</h2><p>在发送消息前，调用的计算分区方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * computes partition for given record.</span></span><br><span class="line"><span class="comment"> * if the record has partition returns the value otherwise</span></span><br><span class="line"><span class="comment"> * calls configured partitioner class to compute the partition.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">        partition :</span><br><span class="line">    partitioner.partition(</span><br><span class="line">        record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果在创建 <code>ProducerRecord</code> 的时候，指定了 partition，则使用指定的，否则调用配置的 partitioner 类来计算分区。</p>
<p>如果没有配置自定义的分区器，Kafka 默认使用 <code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code>，源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The default partitioning strategy:</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If a partition is specified in the record, use it</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic The topic name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 如果key为null，则使用Round Robin算法</span></span><br><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// hash the keyBytes to choose a partition(根据key进行散列，使用murmur2算法)</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123;</span><br><span class="line">            counter = <span class="keyword">new</span> AtomicInteger(ThreadLocalRandom.current().nextInt());</span><br><span class="line">            AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">            <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">                counter = currentCounter;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>DefaultPartitioner </code>中对于分区的算法有两种情况：</p>
<p>1.如果键值为 null，那么记录键随机地发送到主题内各个可用的分区上。分区器使用轮询 (Round Robin) 算法键消息均衡地分布到各个分区上。</p>
<p>2.如果键不为 null，那么 Kafka 会对键进行散列 (使用 Kafka 自己的散列算法，即使升级 java 版本，散列值也不会发生变化) ，然后根据散列值把消息映射到特定的分区上。应该注意的是，同一个键总是被映射到同一个分区上 (如果分区数量发生了变化则不能保证)，映射的时候会使用主题所有的分区，而不仅仅是可用分区，所以如果写入数据分区是不可用的，那么就会发生错误，当然这种情况很少发生。</p>
<p>当然，如果你想要实现自定义分区，那么只需要实现 <code>Partitioner</code> 接口即可：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将key的hash值，对分区总数取余，以确定消息发送到哪个分区</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        Integer numPartitions = cluster.partitionCountForTopic(topic);</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> InvalidRecordException(<span class="string">&quot;key can not be null&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后，使用 <code>partitioner.class</code> 参数，指定你自定义的分区器的路径：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">&quot;partitioner.class&quot;</span>, <span class="string">&quot;cn.xisun.partitioner.KeyPartitioner&quot;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/">https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/</a></p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:&#119;&#x64;&#115;&#104;&#102;&#117;&#x74;&#64;&#49;&#54;&#x33;&#46;&#x63;&#111;&#109;">&#119;&#x64;&#115;&#104;&#102;&#117;&#x74;&#64;&#49;&#54;&#x33;&#46;&#x63;&#111;&#109;</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/23/kafka-introduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/23/kafka-introduce/" class="post-title-link" itemprop="url">什么是 Kafka</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-23 15:58:39" itemprop="dateCreated datePublished" datetime="2020-10-23T15:58:39+08:00">2020-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-30 14:34:29" itemprop="dateModified" datetime="2020-10-30T14:34:29+08:00">2020-10-30</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h2><p>分布式系统由多个运行的计算机系统组成，所有这些计算机在一个集群中一起工作，对终端用户来讲只是一个单一节点。</p>
<p><img src="/2020/10/23/kafka-introduce/distributed_system.png" alt="分布式系统"></p>
<p>Kafka 也是分布式的，因为它在不同的节点 (又被称为 broker) 上存储，接受以及发送消息，这样做的好处是具有很高的可扩展性和容错性。</p>
<h2 id="水平可扩展性"><a href="#水平可扩展性" class="headerlink" title="水平可扩展性"></a>水平可扩展性</h2><p>在这之前，先看看什么是垂直可扩展，比如你有一个传统的数据库服务器，它开始过度负载，解决这个问题的办法就是给服务器加配置 (cpu，内存，SSD)，这就叫做垂直扩展。但是这种方式存在两个巨大的劣势：</p>
<p>1.硬件存在限制，不可能无限的添加机器配置</p>
<p>2.它需要停机时间，通常这是很多公司无法容忍的</p>
<p>水平可扩展就是通过添加更多的机器来解决同样的问题，添加新机器不需要停机，而且集群中也不会对机器的数量有任何的限制。但问题在于并非所有系统都支持水平可伸缩性，因为它们不是设计用于集群中 (在集群中工作会更加复杂)。</p>
<h2 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h2><p>非分布式系统中容易最致命的问题就是单点失败，如果你唯一的服务器挂掉了，那么我相信你会很崩溃。</p>
<p>而分布式系统的设计方式就是可以以配置的方式来容许失败。比如在5个节点的 Kafka 集群中，即使其中两个节点挂掉了，你仍然可以继续工作。</p>
<p>需要注意的是，容错与性能直接相关，你的系统容错程度越高，性能就越差。</p>
<h2 id="提交日志-commit-log"><a href="#提交日志-commit-log" class="headerlink" title="提交日志 (commit log)"></a>提交日志 (commit log)</h2><p>提交日志 (也被称为预写日志或者事物日志) 是仅支持附加的持久有序数据结构，你无法修改或者删除记录，它从左往右读并且保证日志的顺序。</p>
<p><img src="/2020/10/23/kafka-introduce/commint_log.png" alt="commit log"></p>
<p>是不是觉得 Kafka 的数据结构如此简单?</p>
<p>是的，从很多方面来讲，这个数据结构就是 Kafka 的核心。这个数据结构的记录是有序的，而有序的数据可以确保我们的处理流程。这两个在分布式系统中都是极其重要的问题。</p>
<p>Kafka 实际上将所有消息存储到磁盘并在数据结构中对它们进行排序，以便利用顺序磁盘读取。</p>
<p>1.读取和写入都是常量时间 O(1) (当确定了 record id)，与磁盘上其他结构的 O(log N)操作相比是一个巨大的优势，因为每个磁盘搜索都很耗时。</p>
<p>2.读取和写入不会相互影响，写不会锁住读，反之亦然。</p>
<p>这两点有着巨大的优势， 因为数据大小与性能完全分离。无论你的服务器上有100 KB 还是100 TB 的数据，Kafka 都具有相同的性能。</p>
<h2 id="如何工作"><a href="#如何工作" class="headerlink" title="如何工作"></a>如何工作</h2><p>生产者消费者模式：生产者 (producer) 发送消息 (record) 到 Kafka 服务器 (broker)，这些消息存储在主题 (topic) 中，然后消费者 (consumer) 订阅该主题，接受新消息后并进行处理。</p>
<p><img src="/2020/10/23/kafka-introduce/work_model.png" alt="工作模式"></p>
<p>随着消息的越来越多，topic 也会越来越大，为了获得更好的性能和可伸缩性，可以在 topic 下建立多个更小的分区 (partition)，在发送消息时，可以根据实际情况，对消息进行分类，同一类的消息发送到同一个 partition (比如存储不同用户发送的消息，可以根据用户名的首字母进行分区匹配)。Kafka 保证 partition 内的所有消息都按照它们的顺序排序，区分特定消息的方式是通过其偏移量 (offset)，你可以将其视为普通数组索引，即为分区中的每个新消息递增的序列号。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_partition.png" alt="分区"></p>
<p>Kafka 遵守着愚蠢的 broker 和聪明的 consumer 的准则。这意味着 Kafka 不会跟踪消费者读取了哪些记录并删除它们，而是会将它们存储一定的时间 (比如1天，以 log.retention 开头的来决定日志保留时间)，直到达到某个阈值。消费者自己轮询 Kafka 的新消息并且告诉它自己想要读取哪些记录，这允许它们按照自己的意愿递增/递减它们所处的偏移量，从而能够重放和重新处理事件。</p>
<p>需要注意的是消费者是属于消费者组的 (在创建 consumer 时，必须指定其所属的消费者组的 <code>group.id</code>)，消费者组有一个或多个消费者。为了避免两个进程读取同样的消息两次，每个 partition 只能被一个消费者组中的一个消费者访问。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_consum_data.png" alt="kafka消费数据"></p>
<h2 id="持久化到硬盘"><a href="#持久化到硬盘" class="headerlink" title="持久化到硬盘"></a>持久化到硬盘</h2><p>正如之前提到的，Kafka 实际上是将所有记录存储到硬盘而不在 RAM 中保存任何内容，这背后有很多优化使得这个方案可行。</p>
<p>1.Kafka 有一个将消息分组的协议，这允许网络请求将消息组合在一起并减少网络开销，服务器反过来一次性保留大量消息，消费者一次获取大量线性块。</p>
<p>2.磁盘上线性读写非常快，现代磁盘非常慢的原因是由于大量磁盘寻址，但是在大量的线性操作中不是问题。</p>
<p>3.操作系统对线性操作进行了大量优化，通过预读(预取大块多次)和后写 (将小型逻辑写入组成大型物理写入) 技术。</p>
<p>4.操作系统将磁盘文件缓存在空闲 RAM 中。这称为 page cache，而 Kafka 的读写都大量使用了 page cache：</p>
<p>​    ① 写消息的时候消息先从 java 到 page cache，然后异步线程刷盘，消息从 page cache 刷入磁盘；</p>
<p>​    ② 读消息的时候先从 page cache 找，有就直接转入 socket，没有就先从磁盘 load 到 page cache，然后直接从 socket 发出去。</p>
<p>5.由于 Kafka 在整个流程 (producer → broker → consumer) 中以未经修改的标准化二进制格式存储消息，因此它可以使用零拷贝优化。那时操作系统将数据从 page cache 直接复制到 socket，有效地完全绕过了 Kafka broker。</p>
<p>所有这些优化都使 Kafka 能够以接近网络的速度传递消息。</p>
<h2 id="数据分发和复制"><a href="#数据分发和复制" class="headerlink" title="数据分发和复制"></a>数据分发和复制</h2><p>下面来谈谈 Kafka 如何实现容错以及它如何在节点之间分配数据。</p>
<p>为了使得一个 broker 挂掉的时候，数据还能得以保留，分区 (partition) 数据在多个 broker 中复制。</p>
<p>在任何时候，一个 broker 拥有一个 partition，应用程序读取/写入都要通过这个节点，这个节点叫做 partition leader。它将收到的数据复制到 N 个其他 broker，这些接收数据的 broker 叫做 follower，follower 也存储数据，一旦 leader 节点死掉的时候，它们就准备竞争上岗成为 leader。</p>
<p>这可以保证你成功发布的消息不会丢失，通过选择更改副本因子，你可以根据数据的重要性来交换性能以获得更强的持久性保证。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_data_backup.png" alt="4个Kafka broker,副本因子是3"></p>
<p>这样如果 leader 挂掉了，那么其中一个 follower 就会接替它称为 leader。包括 leader 在内的总副本数就是副本因子 (创建 topic 时，使用 <code>--replication-factor</code> 参数指定)，上图有1个 leader，2个 follower，所以副本因子就是3。</p>
<p>但是你可能会问：producer 或者 consumer 怎么知道 partition leader 是谁？</p>
<p>对生产者/消费者对分区的写/读请求，它们需要知道分区的 leader 是哪一个，对吧？这个信息肯定是可以获取到的，Kafka 使用 ZooKeeper 来存储这些元数据。</p>
<h2 id="什么是-ZooKeeper"><a href="#什么是-ZooKeeper" class="headerlink" title="什么是 ZooKeeper"></a>什么是 ZooKeeper</h2><p>ZooKeeper 是一个分布式键值存储。它针对读取进行了高度优化，但写入速度较慢。它最常用于存储元数据和处理集群的机制 (心跳，分发更新/配置等)。</p>
<p>它允许服务的客户 (Kafka broker) 订阅并在发生变更后发送给他们，这就是 Kafka 如何知道何时切换分区领导者。ZooKeeper 本身维护了一个集群，所以它就有很高的容错性，当然它也应该具有，毕竟 Kafka 很大程度上是依赖于它的。</p>
<p>ZooKeeper 用于存储所有的元数据信息，包括但不限于如下几项：</p>
<ul>
<li>消费者组每个分区的偏移量 (现在客户端在单独的 Kafka topic 上存储偏移量)</li>
<li>ACL —— 权限控制</li>
<li>生产者/消费者的流量控制——每秒生产/消费的数据大小。参考：<a target="_blank" rel="noopener" href="https://shiyueqi.github.io/2017/04/27/Kafka-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6Quota%E5%8A%9F%E8%83%BD/"><em>Kafka-流量控制Quota功能</em></a></li>
<li>partition leader 以及它们的健康信息</li>
</ul>
<p>那么 producer/consumer 是如何知道谁是 partition leader 的呢？</p>
<p>生产者和消费者以前常常直接连接 ZooKeeper 来获取这些信息，但是 Kafka 从0.8和0.9版本开始移除了这种强耦合关系。客户端直接从 Kafka broker 获取这些元数据，而让 Kafka broker 从 ZooKeeper 那里获取这些元数据。</p>
<p><img src="/2020/10/23/kafka-introduce/get_leader_info.png" alt="获取leader"></p>
<p>更多 ZooKeeper 的讲解参考：<a target="_blank" rel="noopener" href="https://juejin.im/post/6844903608685707271"><em>漫画：什么是 ZooKeeper？</em></a></p>
<h2 id="流式处理-Streaming"><a href="#流式处理-Streaming" class="headerlink" title="流式处理 (Streaming)"></a>流式处理 (Streaming)</h2><p>在 Kafka 中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理并生成数据流以输出到其他主题 (或者外部服务，数据库，容器等等)。</p>
<p>什么是数据流呢？首先，数据流是无边界数据集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的，是因为随着时间推移，新的记录会不断加入进来。比如信用卡交易，股票交易等事件都可以用来表示数据流。</p>
<p>我们可以使用 producer/consumer的API 直接进行简单处理，但是对于更加复杂的转换，比如将流连接到一起，Kafka 提供了集成 <a target="_blank" rel="noopener" href="https://kafka.apache.org/documentation/streams/"><em>Stream API</em></a> 库。</p>
<p>这个 API 是在你自己的代码中使用的，它并不是运行在 broker 上，它的工作原理和 consumer API 类似，可帮助你在多个应用程序 (类似于消费者组) 上扩展流处理工作。</p>
<h3 id="无状态处理"><a href="#无状态处理" class="headerlink" title="无状态处理"></a>无状态处理</h3><p>流的无状态处理是确定性处理，其不依赖于任何外部条件，对于任何给定的数据，将始终生成与其他任何内容无关的相同输出。举个例子，我们要做一个简单的数据转换—“zhangsan” → “Hello,zhangsan”</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_stream_process.png" alt="Kafka流处理"></p>
<h3 id="流-表二义性"><a href="#流-表二义性" class="headerlink" title="流-表二义性"></a>流-表二义性</h3><p>重要的是要认识到流和表实质上是一样的，流可以被解释称为表，表也可以被解释称为流。</p>
<h4 id="流作为表"><a href="#流作为表" class="headerlink" title="流作为表"></a>流作为表</h4><p>流可以解释为数据的一系列更新，聚合后的结果就是表的最终结果，这项技术被称为事件溯源 (<a target="_blank" rel="noopener" href="https://martinfowler.com/eaaDev/EventSourcing.html"><em>Event Sourcing</em></a>)。</p>
<p>如果你了解数据库备份同步，你就会知道它们的技术实现被称为流式复制—将对表的每个更改都发送报副本服务器。比如 redis 中的 AOF 以及 Mysql 中的 binlog。</p>
<p>Kafka 流可以用相同的方式解释 - 当累积形成最终状态时的事件。此类流聚合保存在本地 RocksDB 中 (默认情况下)，被称为 KTable。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_stream_table.png" alt="Kafka流转换为表"></p>
<h4 id="表作为流"><a href="#表作为流" class="headerlink" title="表作为流"></a>表作为流</h4><p>可以将表视为流中每个键的最新值的快照。与流记录可以生成表一样，表更新可以生成更改日志流。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_table_stream.png" alt="Kafka表转换为流"></p>
<h3 id="有状态处理"><a href="#有状态处理" class="headerlink" title="有状态处理"></a>有状态处理</h3><p>我们在 java 中常用的一些操作比如 <code>map()</code> 或者 <code>filter()</code> 是没有状态的，它不会要求你保留任何原始数据。但是现实中，大多数的操作都是有状态的 (比如 <code>count()</code>)，因为这需要你存储当前累计的状态。</p>
<p>在流处理器上维护状态的问题是流处理器可能会失败！你需要在哪里保持这种状态才能容错？</p>
<p>一种简单的方法是简单地将所有状态存储在远程数据库中，并通过网络连接到该存储，这样做的问题是大量的网络带宽会使得你的应用程序变慢。一个更微妙但重要的问题是你的流处理作业的正常运行时间将与远程数据库紧密耦合，并且作业将不是自包含的 (其他 team 更改数据库可能会破坏你的处理)。</p>
<p>那么什么是更好的办法呢？</p>
<p>回想一下表和流的二元性。这允许我们将流转换为与我们的处理位于同一位置的表。它还为我们提供了一种处理容错的机制—通过将流存储在 Kafka broker 中。</p>
<p>流处理器可以将其状态保持在本地表 (例如 RocksDB)中，该表将从输入流(可能在某些任意转换之后)更新。当进程失败时，它可以通过重放流来恢复其数据。</p>
<p>你甚至可以将远程数据库作为流的生产者，有效地广播用于在本地重建表的更改日志。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_deal_statful_data.png" alt="Kafka处理有状态数据"></p>
<h3 id="KSQL"><a href="#KSQL" class="headerlink" title="KSQL"></a>KSQL</h3><p>通常，我们不得不使用 JVM 语言编写流处理，因为这是唯一的官方 Kafka Streams API 客户端。<br>2018年4月，KSQL 作为一项新特性被发布，它允许你使用熟悉的类似 SQL 的语言编写简单的 stream jobs。你安装了 KSQL 服务器并通过 CLI 以交互方式查询以及管理。它使用相同的抽象 (KStream 和 KTable)，保证了 Streams API 的相同优点 (可伸缩性，容错性)，并大大简化了流的工作。</p>
<p>这听起来可能不是很多，但在实践中对于测试内容更有用，甚至允许开发之外的人(例如产品所有者)使用流处理，可以看看 Confluent 提供的这篇关于<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=A45uRzJiv7I&t=2m13s"> <em>ksql 的使用</em></a>。</p>
<h2 id="什么时候使用-kafka"><a href="#什么时候使用-kafka" class="headerlink" title="什么时候使用 kafka"></a>什么时候使用 kafka</h2><p>正如我们已经介绍的那样，Kafka 允许你通过集中式介质获取大量消息并存储它们，而不必担心性能或数据丢失等问题。</p>
<p>这意味着它非常适合用作系统架构的核心，充当连接不同应用程序的集中式媒体。Kafka 可以成为事件驱动架构的中心部分，使你可以真正地将应用程序彼此分离。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_when_use.png" alt="什么时候使用Kafka"></p>
<p>Kafka 允许你轻松地分离不同 (微) 服务之间的通信。使用 Streams API，现在可以比以往更轻松地编写业务逻辑，从而丰富 Kafka 主题数据以供服务使用。可能性很大，我恳请你探讨公司如何使用 Kafka。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Apache Kafka 是一个分布式流媒体平台，每天可处理数万亿个事件。Kafka 提供低延迟，高吞吐量，容错的发布和订阅管道，并能够处理事件流。我们回顾了它的基本语义 (生产者，代理，消费者，主题)，了解了它的一些优化 (page cache)，通过复制数据了解了它的容错能力，并介绍了它不断增长的强大流媒体功能。Kafka 已经在全球数千家公司中大量采用，其中包括财富500强企业中的三分之一。随着 Kafka 的积极开发和最近发布的第一个主要版本1.0 (2017年11月1日)，有预测这个流媒体平台将会与关系数据库一样，是数据平台的重要核心。我希望这篇介绍能帮助你熟悉 Apache Kafka。</p>
<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="http://generalthink.github.io/2019/02/27/introduction-of-kafka/">http://generalthink.github.io/2019/02/27/introduction-of-kafka/</a></p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:&#119;&#100;&#115;&#x68;&#102;&#117;&#x74;&#64;&#x31;&#x36;&#x33;&#x2e;&#x63;&#x6f;&#x6d;">&#119;&#100;&#115;&#x68;&#102;&#117;&#x74;&#64;&#x31;&#x36;&#x33;&#x2e;&#x63;&#x6f;&#x6d;</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/23/hexo-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/23/hexo-blog/" class="post-title-link" itemprop="url">使用 hexo 搭建 github 博客</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-23 11:33:51" itemprop="dateCreated datePublished" datetime="2020-10-23T11:33:51+08:00">2020-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-30 14:24:20" itemprop="dateModified" datetime="2020-10-30T14:24:20+08:00">2020-10-30</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="使用工具版本"><a href="#使用工具版本" class="headerlink" title="使用工具版本"></a>使用工具版本</h2><p>默认已经安装node.js和git。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git version: git version 2.27.0.windows.1</span><br><span class="line">npm version: 6.14.7</span><br><span class="line">hexo version: 4.2.0</span><br></pre></td></tr></table></figure>

<h2 id="git客户端与github建立SSH连接"><a href="#git客户端与github建立SSH连接" class="headerlink" title="git客户端与github建立SSH连接"></a>git客户端与github建立SSH连接</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Please make sure you have the correct access rights and the repository exists.</span><br></pre></td></tr></table></figure>

<p>当git客服端出现以上提示时，说明SSH连接过期，需要重新建立连接。参考如下方式：</p>
<p>1.先查看下name和email</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看user的name和email</span></span><br><span class="line">$ git config user.name</span><br><span class="line">$ git config user.email</span><br><span class="line"><span class="comment"># 如果没设置，按如下命令设置</span></span><br><span class="line">$ git config --global user.name &#123;<span class="variable">$yourname</span>&#125;</span><br><span class="line">$ git config --global user.email &#123;<span class="variable">$youremail</span>&#125;</span><br></pre></td></tr></table></figure>

<p>2.删除.ssh文件夹下的known_hosts，路径为：C:\Users\{$userrname}\.ssh</p>
<p>3.git bash输入命令</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C &#123;<span class="variable">$youremail</span>&#125;</span><br></pre></td></tr></table></figure>

<p>一直按回车，等结束后，.ssh文件夹下会生成两个文件：id_rsa和id_rsa.pub，将id_rsa.pub的内容全部复制。</p>
<p>4.登录个人github账户，进入Settings → SSH and GPG keys，点击New SSH key，将复制的内容粘贴到Key里，点击Add SSH key</p>
<p>5.git bash输入命令</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure>

<p>在弹出的确定对话框输入：yes</p>
<h2 id="hexo安装"><a href="#hexo安装" class="headerlink" title="hexo安装"></a>hexo安装</h2><p>在git bash中输入以下命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-cli -g</span><br><span class="line">$ <span class="built_in">cd</span> f: <span class="comment"># 可以是任何路径</span></span><br><span class="line">$ hexo init blog</span><br><span class="line">$ <span class="built_in">cd</span> blog <span class="comment"># 进入blog目录</span></span><br><span class="line">$ npm install</span><br><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<p>命令执行完成后，会在F:\目录下，多一个blog文件夹。</p>
<h2 id="修改-config-yml文件"><a href="#修改-config-yml文件" class="headerlink" title="修改_config.yml文件"></a>修改_config.yml文件</h2><p>修改blog根目录下的_config.yml文件，将deploy节点修改为如下内容：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: git@github.com:&#123;<span class="variable">$yourname</span>&#125;/&#123;<span class="variable">$yourname</span>&#125;.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>

<p>说明：_config.yml文件的配置均为[key: value]形式，value前面必须要有一个空格。</p>
<p>然后在git bash中输入以下命令，发布博客：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<h2 id="访问自己的博客"><a href="#访问自己的博客" class="headerlink" title="访问自己的博客"></a>访问自己的博客</h2><p>博客地址：https://{$yourname}.github.io/</p>
<h2 id="写一个自己的博客"><a href="#写一个自己的博客" class="headerlink" title="写一个自己的博客"></a>写一个自己的博客</h2><p>hexo的项目结构是在网站根目录的source/_posts目录下存放你的博客文档，以.md文档格式存储，默认已存在一个hello-world.md文章。</p>
<p>1.新建文章</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new &lt;title&gt;</span><br></pre></td></tr></table></figure>

<p>会在blog的source\_posts目录下，新建一个名叫&lt;title&gt;.md文章，如：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO  Validating config</span><br><span class="line">INFO  Created: F:\blog\<span class="built_in">source</span>\_posts\tesss.md</span><br></pre></td></tr></table></figure>

<p>之后，在文章中添加自己的内容即可，建议使用Typora编辑，参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/hider/p/11614688.html"><em>Typora入门</em></a></p>
<p>2.发布文章</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean <span class="comment"># 清楚缓存</span></span><br><span class="line">$ hexo generate <span class="comment"># 生成静态页面</span></span><br><span class="line">$ hexo server <span class="comment"># 本地发布，浏览器输入localhost:4000即可访问博客</span></span><br><span class="line">$ hexo deploy <span class="comment"># 将public中的静态页面复制到.deploy_git文件夹中，并提交到github</span></span><br></pre></td></tr></table></figure>

<p>至此，你的第一个自己的博客发布完成。</p>
<p>说明：以上hexo的命令，都要在f:\blog目录下执行。</p>
<h2 id="修改博客的themes"><a href="#修改博客的themes" class="headerlink" title="修改博客的themes"></a>修改博客的themes</h2><p>如果向修改自己博客的themes，可以下载好想要的，然后拷贝到blog的themes目录下，然后修改_config.yml文件，将theme节点的值，修改为你下载好的themes的名称，如：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure>

<p>之后，再按照你下载的themes的使用说明，做相应修改即可。</p>
<p>参考：<a target="_blank" rel="noopener" href="http://theme-next.iissnan.com/getting-started.html"><em>NexT的使用</em></a></p>
<h3 id="NexT中tags的使用"><a href="#NexT中tags的使用" class="headerlink" title="NexT中tags的使用"></a>NexT中tags的使用</h3><p>1.修改NexT目录下的_config.yml文件，取消<code>menu</code>菜单下<code>tags</code>字段的注释</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br></pre></td></tr></table></figure>

<p>2.在blog根目录的source目录下，新建tags目录</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new page <span class="string">&quot;tags&quot;</span></span><br></pre></td></tr></table></figure>

<p>3.修改tags目录下的index.md文件</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">title: tags</span><br><span class="line">date: 2020-10-27 16:35:56</span><br><span class="line"><span class="built_in">type</span>: tags</span><br><span class="line">layout: <span class="string">&quot;tags&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="NexT中添加字数统计、阅读时长"><a href="#NexT中添加字数统计、阅读时长" class="headerlink" title="NexT中添加字数统计、阅读时长"></a>NexT中添加字数统计、阅读时长</h3><p>1.安装hexo-symbols-count-time插件</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-symbols-count-time</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yarn add hexo-symbols-count-time</span><br></pre></td></tr></table></figure>

<p>2.hexo配置，根目录下的_config.yaml文件，添加<code>symbols_count_time</code>节点</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line">symbols_count_time:</span><br><span class="line">  symbols: <span class="literal">true</span> <span class="comment"># 文章字数</span></span><br><span class="line">  time: <span class="literal">true</span> <span class="comment"># 阅读时长</span></span><br><span class="line">  total_symbols: <span class="literal">true</span> <span class="comment"># 所有文章总字数</span></span><br><span class="line">  total_time: <span class="literal">true</span> <span class="comment"># 所有文章阅读中时长</span></span><br></pre></td></tr></table></figure>

<p>3.NexT配置，themes目录下的_config.yml文件，<code>symbols_count_time</code>节点</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/hexo-symbols-count-time</span></span><br><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: <span class="literal">true</span> <span class="comment"># 是否换行显示 字数统计 及 阅读时长</span></span><br><span class="line">  item_text_post: <span class="literal">true</span> <span class="comment"># 文章 字数统计 阅读时长 使用图标 还是 文本表示</span></span><br><span class="line">  item_text_total: <span class="literal">false</span> <span class="comment"># 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示</span></span><br></pre></td></tr></table></figure>

<h2 id="在博客中添加图片"><a href="#在博客中添加图片" class="headerlink" title="在博客中添加图片"></a>在博客中添加图片</h2><p>md文件中插入图片的语法为：![]()</p>
<p>其中，方括号是图片描述，圆括号是图片路径。一般来说有三种图片路径，分别是相对路径，绝对路径和网络路径。</p>
<p>相对而言，使用相对路径会更加方便，设置如下：</p>
<p>1.安装hexo-renderer-marked插件</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-renderer-marked</span><br></pre></td></tr></table></figure>

<p>2.修改config.yaml配置</p>
<p>将：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>修改为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: <span class="literal">true</span></span><br><span class="line">marked:</span><br><span class="line">  prependRoot: <span class="literal">true</span></span><br><span class="line">  postAsset: <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>3.设置Typora</p>
<p>点击文件 → 偏好设置，设置如下：<br><img src="/2020/10/23/hexo-blog/image-20201026100024754.png"></p>
<p>这样，在粘贴图片到文件中时，会自动将图片复制到source\_posts目录下，与.md文件同名的目录中。</p>
<p>之后，在将博客发布时，会自动上传到文章所生成的静态网页同级目录之下。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="XiSun"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">XiSun</p>
  <div class="site-description" itemprop="description">心如止水者，虽世间繁华之红尘纷扰，已然空无一物</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XiSun</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">34k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">31 分钟</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
