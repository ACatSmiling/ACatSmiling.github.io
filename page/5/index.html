<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
<meta property="og:type" content="website">
<meta property="og:title" content="XiSun的博客">
<meta property="og:url" content="http://example.com/page/5/index.html">
<meta property="og:site_name" content="XiSun的博客">
<meta property="og:description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="XiSun">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>XiSun的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">XiSun的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Learning is endless</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/10/kafka-consumer-offsetandfetcher/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/10/kafka-consumer-offsetandfetcher/" class="post-title-link" itemprop="url">KafkaConsumer 源码之 consumer 如何拉取 offset 和数据</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-10 11:32:06" itemprop="dateCreated datePublished" datetime="2020-11-10T11:32:06+08:00">2020-11-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-05 15:32:53" itemprop="dateModified" datetime="2021-01-05T15:32:53+08:00">2021-01-05</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>27k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>25 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://acatsmiling.github.io/2020/11/04/kafka-consumer-group/">上一篇文章</a>讲了 consumer 如何加入 consumer group，现在加入 group 成功之后，就要准备开始消费。</p>
<p><code>kafkaConsumer.poll ()</code> 方法的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Timer timer, <span class="keyword">final</span> <span class="keyword">boolean</span> includeMetadataInTimeout)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭</span></span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// poll for new data until the timeout expires</span></span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            client.maybeTriggerWakeup();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (includeMetadataInTimeout) &#123;</span><br><span class="line">                <span class="comment">// Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息</span></span><br><span class="line">                <span class="keyword">if</span> (!updateAssignmentMetadataIfNeeded(timer)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">while</span> (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123;</span><br><span class="line">                    log.warn(<span class="string">&quot;Still waiting for metadata&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Step3:拉取数据，核心步骤</span></span><br><span class="line">            <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer);</span><br><span class="line">            <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">// before returning the fetched records, we can send off the next round of fetches</span></span><br><span class="line">                <span class="comment">// and avoid block waiting for their responses to enable pipelining while the user</span></span><br><span class="line">                <span class="comment">// is handling the fetched records.</span></span><br><span class="line">                <span class="comment">//</span></span><br><span class="line">                <span class="comment">// <span class="doctag">NOTE:</span> since the consumed position has already been updated, we must not allow</span></span><br><span class="line">                <span class="comment">// wakeups or any other errors to be triggered prior to returning the fetched records.</span></span><br><span class="line">                <span class="comment">// 在返回数据之前，发送下次的fetch请求，避免用户在下次获取数据时线程block</span></span><br><span class="line">                <span class="keyword">if</span> (fetcher.sendFetches() &gt; <span class="number">0</span> || client.hasPendingRequests()) &#123;</span><br><span class="line">                    client.pollNoWakeup();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">this</span>.interceptors.onConsume(<span class="keyword">new</span> ConsumerRecords&lt;&gt;(records));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">while</span> (timer.notExpired());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>紧跟上一篇文章，我们继续分析 consumer 加入 group 后的行为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Visible for testing</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">updateAssignmentMetadataIfNeeded</span><span class="params">(<span class="keyword">final</span> Timer timer)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1.上一篇主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group)</span></span><br><span class="line">    <span class="keyword">if</span> (coordinator != <span class="keyword">null</span> &amp;&amp; !coordinator.poll(timer)) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.本篇文章从updateFetchPositions(timer)方法开始继续分析(主要功能是:consumer获得partition的offset)</span></span><br><span class="line">    <span class="keyword">return</span> updateFetchPositions(timer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="KafkaConsumer-的消费策略"><a href="#KafkaConsumer-的消费策略" class="headerlink" title="KafkaConsumer 的消费策略"></a>KafkaConsumer 的消费策略</h2><p>首先，我们应该知道，KafkaConsumer 关于如何消费的 2 种策略：</p>
<ul>
<li><p><strong>手动指定</strong>：调用 <code>consumer.seek(TopicPartition, offset)</code>，然后开始 <code>poll ()</code>。</p>
</li>
<li><p><strong>自动指定</strong>：<code>poll ()</code> 之前给集群发送请求，让集群告知客户端，当前该 TopicPartition 的 offset 是多少，这也是我们此次分析的重点。</p>
</li>
</ul>
<p>在讲如何拉取 offset 之前，先认识下下面这个类 (SubscriptionState 的内部类)：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TopicPartitionState</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> FetchState fetchState;</span><br><span class="line">    <span class="keyword">private</span> FetchPosition position; <span class="comment">// last consumed position</span></span><br><span class="line">    <span class="keyword">private</span> Long highWatermark; <span class="comment">// the high watermark from last fetch</span></span><br><span class="line">    <span class="keyword">private</span> Long logStartOffset; <span class="comment">// the log start offset</span></span><br><span class="line">    <span class="keyword">private</span> Long lastStableOffset;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> paused;  <span class="comment">// whether this partition has been paused by the user</span></span><br><span class="line">    <span class="keyword">private</span> OffsetResetStrategy resetStrategy;  <span class="comment">// the strategy to use if the offset needs resetting</span></span><br><span class="line">    <span class="keyword">private</span> Long nextRetryTimeMs;</span><br><span class="line">    <span class="keyword">private</span> Integer preferredReadReplica;</span><br><span class="line">    <span class="keyword">private</span> Long preferredReadReplicaExpireTimeMs;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>consumer 实例订阅的每个 topic-partition 都会有一个对应的 TopicPartitionState 对象，在这个对象中会记录上面内容，最需要关注的就是 <strong>position 这个属性，它表示上一次消费的位置</strong>。通过 <code>consumer.seek ()</code> 方式指定消费 offset 的时候，其实设置的就是这个 position 值。</p>
<h2 id="updateFetchPositions-拉取-offset"><a href="#updateFetchPositions-拉取-offset" class="headerlink" title="updateFetchPositions - 拉取 offset"></a>updateFetchPositions - 拉取 offset</h2><p>在 consumer 成功加入 group 并开始消费之前，我们还需要知道 consumer 是从 offset 为多少的位置开始消费。consumer 加入 group 之后，就得去获取 offset 了，下面的方法，就是开始更新 position (offset)：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set the fetch position to the committed position (if there is one)</span></span><br><span class="line"><span class="comment"> * or reset it using the offset reset policy the user has configured.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> org.apache.kafka.common.errors.AuthenticationException if authentication fails. See the exception for more details</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NoOffsetForPartitionException If no offset is stored for a given partition and no offset reset policy is</span></span><br><span class="line"><span class="comment"> *             defined</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true iff the operation completed without timing out</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">updateFetchPositions</span><span class="params">(<span class="keyword">final</span> Timer timer)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// If any partitions have been truncated due to a leader change, we need to validate the offsets</span></span><br><span class="line">    fetcher.validateOffsetsIfNeeded();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step1:查看TopicPartitionState的position是否为空，第一次消费肯定为空</span></span><br><span class="line">    cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions();</span><br><span class="line">    <span class="keyword">if</span> (cachedSubscriptionHashAllFetchPositions) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If there are any partitions which do not have a valid position and are not</span></span><br><span class="line">    <span class="comment">// awaiting reset, then we need to fetch committed offsets. We will only do a</span></span><br><span class="line">    <span class="comment">// coordinator lookup if there are partitions which have missing positions, so</span></span><br><span class="line">    <span class="comment">// a consumer with manually assigned partitions can avoid a coordinator dependence</span></span><br><span class="line">    <span class="comment">// by always ensuring that assigned partitions have an initial position.</span></span><br><span class="line">    <span class="comment">// Step2:如果没有有效的offset，那么需要从GroupCoordinator中获取</span></span><br><span class="line">    <span class="keyword">if</span> (coordinator != <span class="keyword">null</span> &amp;&amp; !coordinator.refreshCommittedOffsetsIfNeeded(timer)) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If there are partitions still needing a position and a reset policy is defined,</span></span><br><span class="line">    <span class="comment">// request reset using the default policy. If no reset strategy is defined and there</span></span><br><span class="line">    <span class="comment">// are partitions with a missing position, then we will raise an exception.</span></span><br><span class="line">    <span class="comment">// Step3:如果还存在partition不知道position，并且设置了offsetreset策略，那么就等待重置，不然就抛出异常</span></span><br><span class="line">    subscriptions.resetMissingPositions();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Finally send an asynchronous request to lookup and update the positions of any</span></span><br><span class="line">    <span class="comment">// partitions which are awaiting reset.</span></span><br><span class="line">    <span class="comment">// Step4:向PartitionLeader(GroupCoordinator所在机器)发送ListOffsetRequest重置position</span></span><br><span class="line">    fetcher.resetOffsetsIfNeeded();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的代码主要分为 4 个步骤，具体如下：</p>
<ol>
<li>首先，查看当前 TopicPartition 的 position 是否为空，如果不为空，表示知道下次 fetch position (即拉取数据时从哪个位置开始拉取)，但如果是第一次消费，这个 <code>TopicPartitionState.position</code> 肯定为空。</li>
<li>然后，通过 GroupCoordinator 为缺少 fetch position 的 partition 拉取 position (即 last committed offset)。</li>
<li>继而，仍不知道 partition 的 position (_consumer_offsets 中未保存位移信息)，且设置了 offsetreset 策略，那么就等待重置，如果没有设置重置策略，就抛出 <code>NoOffsetForPartitionException</code> 异常。</li>
<li>最后，为那些需要重置 fetch position 的 partition 发送 ListOffsetRequest 重置 position (<code>consumer.beginningOffsets ()</code>，<code>consumer.endOffsets ()</code>，<code>consumer.offsetsForTimes ()</code>，<code>consumer.seek ()</code> 都会发送 ListOffRequest 请求)。</li>
</ol>
<blockquote>
<p>上面说的几个方法相当于都是用户自己自定义消费的 offset，所以可能出现越界 (消费位置无法在实际分区中查到) 的情况，所以也是会发送 ListOffsetRequest 请求的，即触发 <code>auto.offset.reset</code> 参数的执行。<br>比如现在某个 partition 的可拉取 offset 最大值为 100，如果你指定消费 offset=200 的位置，那肯定拉取不到，此时就会根据 <code>auto.offset.reset</code> 策略将拉取位置重置为 100 (默认的 <code>auto.offset.reset</code> 为 latest)。</p>
</blockquote>
<h3 id="refreshCommittedOffsetsIfNeeded"><a href="#refreshCommittedOffsetsIfNeeded" class="headerlink" title="refreshCommittedOffsetsIfNeeded"></a>refreshCommittedOffsetsIfNeeded</h3><p>我们先看下 Setp 2 中 GroupCoordinator 是如何 fetch position 的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Refresh the committed offsets for provided partitions.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timer Timer bounding how long this method can block</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true iff the operation completed within the timeout</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">refreshCommittedOffsetsIfNeeded</span><span class="params">(Timer timer)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Set&lt;TopicPartition&gt; missingFetchPositions = subscriptions.missingFetchPositions();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.发送获取offset的请求，核心步骤</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = fetchCommittedOffsets(missingFetchPositions, timer);</span><br><span class="line">    <span class="keyword">if</span> (offsets == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">final</span> Map.Entry&lt;TopicPartition, OffsetAndMetadata&gt; entry : offsets.entrySet()) &#123;</span><br><span class="line">        <span class="keyword">final</span> TopicPartition tp = entry.getKey();</span><br><span class="line">        <span class="comment">// 2.获取response中的offset</span></span><br><span class="line">        <span class="keyword">final</span> OffsetAndMetadata offsetAndMetadata = entry.getValue();</span><br><span class="line">        <span class="keyword">final</span> ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.leaderAndEpoch(tp);</span><br><span class="line">        <span class="keyword">final</span> SubscriptionState.FetchPosition position = <span class="keyword">new</span> SubscriptionState.FetchPosition(</span><br><span class="line">                offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(),</span><br><span class="line">                leaderAndEpoch);</span><br><span class="line"></span><br><span class="line">        log.info(<span class="string">&quot;Setting offset for partition &#123;&#125; to the committed offset &#123;&#125;&quot;</span>, tp, position);</span><br><span class="line">        entry.getValue().leaderEpoch().ifPresent(epoch -&gt; <span class="keyword">this</span>.metadata.updateLastSeenEpochIfNewer(entry.getKey(), epoch));</span><br><span class="line">        <span class="comment">// 3.实际就是设置SubscriptionState的position值</span></span><br><span class="line">        <span class="keyword">this</span>.subscriptions.seekUnvalidated(tp, position);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>fetchCommittedOffsets ()</code> 方法的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Fetch the current committed offsets from the coordinator for a set of partitions.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> partitions The partitions to fetch offsets for</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> A map from partition to the committed offset or null if the operation timed out</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; <span class="title">fetchCommittedOffsets</span><span class="params">(<span class="keyword">final</span> Set&lt;TopicPartition&gt; partitions,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                                    <span class="keyword">final</span> Timer timer)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (partitions.isEmpty()) <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Generation generation = generation();</span><br><span class="line">    <span class="keyword">if</span> (pendingCommittedOffsetRequest != <span class="keyword">null</span> &amp;&amp; !pendingCommittedOffsetRequest.sameRequest(partitions, generation)) &#123;</span><br><span class="line">        <span class="comment">// if we were waiting for a different request, then just clear it.</span></span><br><span class="line">        pendingCommittedOffsetRequest = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (!ensureCoordinatorReady(timer)) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// contact coordinator to fetch committed offsets</span></span><br><span class="line">        <span class="keyword">final</span> RequestFuture&lt;Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt; future;</span><br><span class="line">        <span class="keyword">if</span> (pendingCommittedOffsetRequest != <span class="keyword">null</span>) &#123;</span><br><span class="line">            future = pendingCommittedOffsetRequest.response;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 1.封装FetchRequest请求</span></span><br><span class="line">            future = sendOffsetFetchRequest(partitions);</span><br><span class="line">            pendingCommittedOffsetRequest = <span class="keyword">new</span> PendingCommittedOffsetRequest(partitions, generation, future);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2.通过KafkaClient发送请求</span></span><br><span class="line">        client.poll(future, timer);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (future.isDone()) &#123;</span><br><span class="line">            pendingCommittedOffsetRequest = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (future.succeeded()) &#123;</span><br><span class="line">                <span class="comment">// 3.请求成功，获取请求的响应数据</span></span><br><span class="line">                <span class="keyword">return</span> future.value();</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!future.isRetriable()) &#123;</span><br><span class="line">                <span class="keyword">throw</span> future.exception();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                timer.sleep(retryBackoffMs);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">while</span> (timer.notExpired());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的步骤和我们之前提到的发送其他请求毫无区别，基本就是这三个套路。</p>
<p>在获取到响应之后，会通过 <code>subscriptions.seekUnvalidated ()</code> 方法为每个 TopicPartition 设置 position 值后，就知道从哪里开始消费订阅 topic 下的 partition 了。</p>
<h3 id="resetMissingPositions"><a href="#resetMissingPositions" class="headerlink" title="resetMissingPositions"></a>resetMissingPositions</h3><p>在 Step 3 中，什么时候发起 FetchRequest 拿不到 position 呢？</p>
<p>我们知道消费位移 (consume offset) 是保存在 _consumer_offsets 这个 topic 里面的，当我们进行消费的时候需要知道上次消费到了什么位置。那么就会发起请求去看上次消费到了 topic 的 partition 的哪个位置，但是这个消费位移是有保存时长的，默认为 7 天 (broker 端通过 <code>offsets.retention.minutes</code> 设置)。</p>
<p>当隔了一段时间再进行消费，如果这个间隔时间超过了参数的配置值，那么原先的位移信息就会丢失，最后只能通过客户端参数 <code>auto.offset.reset</code> 来确定开始消费的位置。</p>
<p>如果我们第一次消费 topic，那么在 _consumer_offsets 中也是找不到消费位移的，所以就会执行第四个步骤，发起 ListOffsetRequest 请求根据配置的 reset 策略 (即 <code>auto.offset.reset</code>) 来决定开始消费的位置。</p>
<h3 id="resetOffsetsIfNeeded"><a href="#resetOffsetsIfNeeded" class="headerlink" title="resetOffsetsIfNeeded"></a>resetOffsetsIfNeeded</h3><p>在 Step 4 中，发起 ListOffsetRequest 请求和处理 response 的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Reset offsets for all assigned partitions that require it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> org.apache.kafka.clients.consumer.NoOffsetForPartitionException If no offset reset strategy is defined</span></span><br><span class="line"><span class="comment"> *   and one or more partitions aren&#x27;t awaiting a seekToBeginning() or seekToEnd().</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">resetOffsetsIfNeeded</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Raise exception from previous offset fetch if there is one</span></span><br><span class="line">    RuntimeException exception = cachedListOffsetsException.getAndSet(<span class="keyword">null</span>);</span><br><span class="line">    <span class="keyword">if</span> (exception != <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> exception;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.需要执行reset策略的partition</span></span><br><span class="line">    Set&lt;TopicPartition&gt; partitions = subscriptions.partitionsNeedingReset(time.milliseconds());</span><br><span class="line">    <span class="keyword">if</span> (partitions.isEmpty())</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, Long&gt; offsetResetTimestamps = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">final</span> TopicPartition partition : partitions) &#123;</span><br><span class="line">        Long timestamp = offsetResetStrategyTimestamp(partition);</span><br><span class="line">        <span class="keyword">if</span> (timestamp != <span class="keyword">null</span>)</span><br><span class="line">            offsetResetTimestamps.put(partition, timestamp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.执行reset策略</span></span><br><span class="line">    resetOffsetsAsync(offsetResetTimestamps);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">resetOffsetsAsync</span><span class="params">(Map&lt;TopicPartition, Long&gt; partitionResetTimestamps)</span> </span>&#123;</span><br><span class="line">    Map&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; timestampsToSearchByNode =</span><br><span class="line">            groupListOffsetRequests(partitionResetTimestamps, <span class="keyword">new</span> HashSet&lt;&gt;());</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; entry : timestampsToSearchByNode.entrySet()) &#123;</span><br><span class="line">        Node node = entry.getKey();</span><br><span class="line">        <span class="keyword">final</span> Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; resetTimestamps = entry.getValue();</span><br><span class="line">        subscriptions.setNextAllowedRetry(resetTimestamps.keySet(), time.milliseconds() + requestTimeoutMs);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.发送ListOffsetRequest请求</span></span><br><span class="line">        RequestFuture&lt;ListOffsetResult&gt; future = sendListOffsetRequest(node, resetTimestamps, <span class="keyword">false</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2.为ListOffsetRequest请求添加监听器</span></span><br><span class="line">        future.addListener(<span class="keyword">new</span> RequestFutureListener&lt;ListOffsetResult&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ListOffsetResult result)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (!result.partitionsToRetry.isEmpty()) &#123;</span><br><span class="line">                    subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs);</span><br><span class="line">                    metadata.requestUpdate();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, ListOffsetData&gt; fetchedOffset : result.fetchedOffsets.entrySet()) &#123;</span><br><span class="line">                    TopicPartition partition = fetchedOffset.getKey();</span><br><span class="line">                    ListOffsetData offsetData = fetchedOffset.getValue();</span><br><span class="line">                    ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition);</span><br><span class="line">                    <span class="comment">// 3.发送ListOffsetRequest请求成功，对结果reset，如果reset策略设置的是latest，那么requestedReset.timestamp = -1，如果是earliest，requestedReset.timestamp = -2</span></span><br><span class="line">                    resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</span><br><span class="line">                subscriptions.requestFailed(resetTimestamps.keySet(), time.milliseconds() + retryBackoffMs);</span><br><span class="line">                metadata.requestUpdate();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (!(e <span class="keyword">instanceof</span> RetriableException) &amp;&amp; !cachedListOffsetsException.compareAndSet(<span class="keyword">null</span>, e))</span><br><span class="line">                    log.error(<span class="string">&quot;Discarding error in ListOffsetResponse because another error is pending&quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>sendListOffsetRequest ()</code> 方法的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Send the ListOffsetRequest to a specific broker for the partitions and target timestamps.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> node The node to send the ListOffsetRequest to.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timestampsToSearch The mapping from partitions to the target timestamps.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> requireTimestamp  True if we require a timestamp in the response.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> A response which can be polled to obtain the corresponding timestamps and offsets.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;ListOffsetResult&gt; <span class="title">sendListOffsetRequest</span><span class="params">(<span class="keyword">final</span> Node node,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                              <span class="keyword">final</span> Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; timestampsToSearch,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                              <span class="keyword">boolean</span> requireTimestamp)</span> </span>&#123;</span><br><span class="line">    ListOffsetRequest.Builder builder = ListOffsetRequest.Builder</span><br><span class="line">            .forConsumer(requireTimestamp, isolationLevel)</span><br><span class="line">            .setTargetTimes(timestampsToSearch);</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">&quot;Sending ListOffsetRequest &#123;&#125; to broker &#123;&#125;&quot;</span>, builder, node);</span><br><span class="line">    <span class="keyword">return</span> client.send(node, builder)</span><br><span class="line">            .compose(<span class="keyword">new</span> RequestFutureAdapter&lt;ClientResponse, ListOffsetResult&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse response, RequestFuture&lt;ListOffsetResult&gt; future)</span> </span>&#123;</span><br><span class="line">                    ListOffsetResponse lor = (ListOffsetResponse) response.responseBody();</span><br><span class="line">                    log.trace(<span class="string">&quot;Received ListOffsetResponse &#123;&#125; from broker &#123;&#125;&quot;</span>, lor, node);</span><br><span class="line">                    handleListOffsetResponse(timestampsToSearch, lor, future);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>resetOffsetIfNeeded ()</code> 方法的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">resetOffsetIfNeeded</span><span class="params">(TopicPartition partition, OffsetResetStrategy requestedResetStrategy, ListOffsetData offsetData)</span> </span>&#123;</span><br><span class="line">    SubscriptionState.FetchPosition position = <span class="keyword">new</span> SubscriptionState.FetchPosition(</span><br><span class="line">            offsetData.offset, offsetData.leaderEpoch, metadata.leaderAndEpoch(partition));</span><br><span class="line">    offsetData.leaderEpoch.ifPresent(epoch -&gt; metadata.updateLastSeenEpochIfNewer(partition, epoch));</span><br><span class="line">    <span class="comment">// reset对应的TopicPartition fetch的position</span></span><br><span class="line">    subscriptions.maybeSeekUnvalidated(partition, position.offset, requestedResetStrategy);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里解释下 <code>auto.offset.reset</code> 的两个值 (latest 和 earliest) 的区别：</p>
<p>假设我们现在要消费 MyConsumerTopic 的数据，它有 3 个分区，生产者往这个 topic 发送了 10 条数据，然后分区数据按照 MyConsumerTopic-0 (3 条数据)，MyConsumerTopic-1 (3 条数据)，MyConsumerTopic-2 (4 条数据) 这样分配。</p>
<p>当设置为 latest 的时候，返回的 offset 具体到每个 partition 就是 HW 值 (partition 0 是 3，partition 1 是 3，partition 2 是 4)。</p>
<p>当设置为 earliest 的时候，就会从起始处 (即 LogStartOffset，注意不是 LSO) 开始消费，这里就是从 0 开始。</p>
<p><img src="/2020/11/10/kafka-consumer-offsetandfetcher/kafka-partition-analysis.png"></p>
<ul>
<li><p><strong>Log Start Offset</strong>：表示 partition 的起始位置，初始值为 0，由于消息的增加以及日志清除策略影响，这个值会阶段性增大。尤其注意这个不能缩写为 LSO，LSO 代表的是 LastStableOffset，和事务有关。</p>
</li>
<li><p><strong>Consumer Offset</strong>：消费位移，表示 partition 的某个消费者消费到的位移位置。</p>
</li>
<li><p><strong>High Watermark</strong>：简称 HW，代表消费端能看到的 partition 的最高日志位移，HW 大于等于 ConsumerOffset 的值。</p>
</li>
<li><p><strong>Log End Offset</strong>：简称 LEO，代表 partition 的最高日志位移，对消费者不可见，HW 到 LEO 这之间的数据未被 follwer 完全同步。</p>
</li>
</ul>
<p>至此，我们成功的知道 consumer 消费的 partition 的 offset 位置在哪里，下面就开始拉取 partition 里的数据。</p>
<h2 id="pollForFetches-拉取数据"><a href="#pollForFetches-拉取数据" class="headerlink" title="pollForFetches - 拉取数据"></a>pollForFetches - 拉取数据</h2><p>现在万事俱备只欠东风了，consumer 成功加入 group，也确定了需要拉取的 topic partition 的 offset，那么现在就应该去拉取数据了，其核心源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) &#123;</span><br><span class="line">    <span class="keyword">long</span> pollTimeout = coordinator == <span class="keyword">null</span> ? timer.remainingMs() :</span><br><span class="line">            Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if data is available already, return it immediately</span></span><br><span class="line">    <span class="comment">// 1.获取fetcher已经拉取到的数据</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</span><br><span class="line">    <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> records;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 到此，说明上次fetch到的数据已经全部拉取了，需要再次发送fetch请求，从broker拉取新的数据</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// send any new fetches (won&#x27;t resend pending fetches)</span></span><br><span class="line">    <span class="comment">// 2.发送fetch请求，会从多个topic-partition拉取数据(只要对应的topic-partition没有未完成的请求)</span></span><br><span class="line">    fetcher.sendFetches();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We do not want to be stuck blocking in poll if we are missing some positions</span></span><br><span class="line">    <span class="comment">// since the offset lookup may be backing off after a failure</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> the use of cachedSubscriptionHashAllFetchPositions means we MUST call</span></span><br><span class="line">    <span class="comment">// updateAssignmentMetadataIfNeeded before this method.</span></span><br><span class="line">    <span class="keyword">if</span> (!cachedSubscriptionHashAllFetchPositions &amp;&amp; pollTimeout &gt; retryBackoffMs) &#123;</span><br><span class="line">        pollTimeout = retryBackoffMs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Timer pollTimer = time.timer(pollTimeout);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 3.真正开始发送，底层同样使用NIO</span></span><br><span class="line">    client.poll(pollTimer, () -&gt; &#123;</span><br><span class="line">        <span class="comment">// since a fetch might be completed by the background thread, we need this poll condition</span></span><br><span class="line">        <span class="comment">// to ensure that we do not block unnecessarily in poll()</span></span><br><span class="line">        <span class="keyword">return</span> !fetcher.hasCompletedFetches();</span><br><span class="line">    &#125;);</span><br><span class="line">    timer.update(pollTimer.currentTimeMs());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// after the long poll, we should check whether the group needs to rebalance</span></span><br><span class="line">    <span class="comment">// prior to returning data so that the group can stabilize faster</span></span><br><span class="line">    <span class="comment">// 4.如果group需要rebalance，直接返回空数据，这样更快地让group进入稳定状态</span></span><br><span class="line">    <span class="keyword">if</span> (coordinator != <span class="keyword">null</span> &amp;&amp; coordinator.rejoinNeededOrPending()) &#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">     <span class="comment">// 5.返回拉取到的新数据</span></span><br><span class="line">    <span class="keyword">return</span> fetcher.fetchedRecords();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="fetcher-sendFetches"><a href="#fetcher-sendFetches" class="headerlink" title="fetcher.sendFetches"></a>fetcher.sendFetches</h3><p>这里需要注意的是 <code>fetcher.sendFetches ()</code> 方法，在发送请求的同时会注册回调函数，当有 response 的时候，会解析 response，将返回的数据放到 Fetcher 的成员变量中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set-up a fetch request for any node that we have assigned partitions for which doesn&#x27;t already have</span></span><br><span class="line"><span class="comment"> * an in-flight fetch or pending fetch data.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> number of fetches sent</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">sendFetches</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Update metrics in case there was an assignment change</span></span><br><span class="line">    sensors.maybeUpdateAssignment(subscriptions);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.创建FetchRequest</span></span><br><span class="line">    Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; fetchRequestMap = prepareFetchRequests();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData&gt; entry : fetchRequestMap.entrySet()) &#123;</span><br><span class="line">        <span class="keyword">final</span> Node fetchTarget = entry.getKey();</span><br><span class="line">        <span class="keyword">final</span> FetchSessionHandler.FetchRequestData data = entry.getValue();</span><br><span class="line">        <span class="keyword">final</span> FetchRequest.Builder request = FetchRequest.Builder</span><br><span class="line">                .forConsumer(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, data.toSend())</span><br><span class="line">                .isolationLevel(isolationLevel)</span><br><span class="line">                .setMaxBytes(<span class="keyword">this</span>.maxBytes)</span><br><span class="line">                .metadata(data.metadata())</span><br><span class="line">                .toForget(data.toForget())</span><br><span class="line">                .rackId(clientRackId);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (log.isDebugEnabled()) &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;Sending &#123;&#125; &#123;&#125; to broker &#123;&#125;&quot;</span>, isolationLevel, data.toString(), fetchTarget);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2.发送FetchRequest</span></span><br><span class="line">        RequestFuture&lt;ClientResponse&gt; future = client.send(fetchTarget, request);</span><br><span class="line">        <span class="comment">// We add the node to the set of nodes with pending fetch requests before adding the</span></span><br><span class="line">        <span class="comment">// listener because the future may have been fulfilled on another thread (e.g. during a</span></span><br><span class="line">        <span class="comment">// disconnection being handled by the heartbeat thread) which will mean the listener</span></span><br><span class="line">        <span class="comment">// will be invoked synchronously.</span></span><br><span class="line">        <span class="keyword">this</span>.nodesWithPendingFetchRequests.add(entry.getKey().id());</span><br><span class="line">        future.addListener(<span class="keyword">new</span> RequestFutureListener&lt;ClientResponse&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">synchronized</span> (Fetcher.<span class="keyword">this</span>) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">                        FetchResponse&lt;Records&gt; response = (FetchResponse&lt;Records&gt;) resp.responseBody();</span><br><span class="line">                        FetchSessionHandler handler = sessionHandler(fetchTarget.id());</span><br><span class="line">                        <span class="keyword">if</span> (handler == <span class="keyword">null</span>) &#123;</span><br><span class="line">                            log.error(<span class="string">&quot;Unable to find FetchSessionHandler for node &#123;&#125;. Ignoring fetch response.&quot;</span>,</span><br><span class="line">                                    fetchTarget.id());</span><br><span class="line">                            <span class="keyword">return</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">if</span> (!handler.handleResponse(response)) &#123;</span><br><span class="line">                            <span class="keyword">return</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        Set&lt;TopicPartition&gt; partitions = <span class="keyword">new</span> HashSet&lt;&gt;(response.responseData().keySet());</span><br><span class="line">                        FetchResponseMetricAggregator metricAggregator = <span class="keyword">new</span> FetchResponseMetricAggregator(sensors, partitions);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&lt;Records&gt;&gt; entry : response.responseData().entrySet()) &#123;</span><br><span class="line">                            TopicPartition partition = entry.getKey();</span><br><span class="line">                            FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition);</span><br><span class="line">                            <span class="keyword">if</span> (requestData == <span class="keyword">null</span>) &#123;</span><br><span class="line">                                String message;</span><br><span class="line">                                <span class="keyword">if</span> (data.metadata().isFull()) &#123;</span><br><span class="line">                                    message = MessageFormatter.arrayFormat(</span><br><span class="line">                                            <span class="string">&quot;Response for missing full request partition: partition=&#123;&#125;; metadata=&#123;&#125;&quot;</span>,</span><br><span class="line">                                            <span class="keyword">new</span> Object[]&#123;partition, data.metadata()&#125;).getMessage();</span><br><span class="line">                                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                    message = MessageFormatter.arrayFormat(</span><br><span class="line">                                            <span class="string">&quot;Response for missing session request partition: partition=&#123;&#125;; metadata=&#123;&#125;; toSend=&#123;&#125;; toForget=&#123;&#125;&quot;</span>,</span><br><span class="line">                                            <span class="keyword">new</span> Object[]&#123;partition, data.metadata(), data.toSend(), data.toForget()&#125;).getMessage();</span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// Received fetch response for missing session partition</span></span><br><span class="line">                                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(message);</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                <span class="keyword">long</span> fetchOffset = requestData.fetchOffset;</span><br><span class="line">                                FetchResponse.PartitionData&lt;Records&gt; fetchData = entry.getValue();</span><br><span class="line"></span><br><span class="line">                                log.debug(<span class="string">&quot;Fetch &#123;&#125; at offset &#123;&#125; for partition &#123;&#125; returned fetch data &#123;&#125;&quot;</span>,</span><br><span class="line">                                        isolationLevel, fetchOffset, partition, fetchData);</span><br><span class="line">                                <span class="comment">// 3.发送FetchRequest请求成功，将返回的数据放到ConcurrentLinkedQueue&lt;CompletedFetch&gt;中</span></span><br><span class="line">                                completedFetches.add(<span class="keyword">new</span> CompletedFetch(partition, fetchOffset, fetchData, metricAggregator,</span><br><span class="line">                                        resp.requestHeader().apiVersion()));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        sensors.fetchLatency.record(resp.requestLatencyMs());</span><br><span class="line">                    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                        nodesWithPendingFetchRequests.remove(fetchTarget.id());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">synchronized</span> (Fetcher.<span class="keyword">this</span>) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        FetchSessionHandler handler = sessionHandler(fetchTarget.id());</span><br><span class="line">                        <span class="keyword">if</span> (handler != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            handler.handleError(e);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                        nodesWithPendingFetchRequests.remove(fetchTarget.id());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fetchRequestMap.size();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该方法主要分为以下两步：</p>
<ol>
<li><code>prepareFetchRequests ()</code>：为订阅的所有 topic-partition list 创建 fetch 请求 (只要该 topic-partition 没有还在处理的请求)，创建的 fetch 请求依然是按照 node 级别创建的；</li>
<li><code>client.send ()</code>：发送 fetch 请求，并设置相应的 Listener，请求处理成功的话，就加入到 completedFetches 中，在加入这个 completedFetches 队列时，是按照 topic-partition 级别去加入，这样也就方便了后续的处理。</li>
</ol>
<p>从这里可以看出，在每次发送 fetch 请求时，都会向所有可发送的 topic-partition 发送 fetch 请求，调用一次 <code>fetcher.sendFetches</code>，拉取到的数据，可能需要多次 pollForFetches 循环才能处理完，因为 Fetcher 线程是在后台运行，这也保证了尽可能少地阻塞用户的处理线程，因为如果 Fetcher 中没有可处理的数据，用户的线程是会阻塞在 poll 方法中的。</p>
<h3 id="fetcher-fetchedRecords"><a href="#fetcher-fetchedRecords" class="headerlink" title="fetcher.fetchedRecords"></a>fetcher.fetchedRecords</h3><p>这个方法的作用就是获取已经从 server 拉取到的 Records，其核心源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return the fetched records, empty the record buffer and update the consumed position.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">NOTE:</span> returning empty records guarantees the consumed position are NOT updated.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The fetched records per partition</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> OffsetOutOfRangeException If there is OffsetOutOfRange error in fetchResponse and</span></span><br><span class="line"><span class="comment"> *         the defaultResetPolicy is NONE</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> TopicAuthorizationException If there is TopicAuthorization error in fetchResponse.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords() &#123;</span><br><span class="line">    Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetched = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="comment">// 在max.poll.records中设置单词最大的拉取条数，默认500条</span></span><br><span class="line">    <span class="keyword">int</span> recordsRemaining = maxPollRecords;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (recordsRemaining &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nextInLineRecords == <span class="keyword">null</span> || nextInLineRecords.isFetched) &#123;<span class="comment">// nextInLineRecords为空时</span></span><br><span class="line">                <span class="comment">// Step1:当一个nextInLineRecords处理完，就从completedFetches处理下一个完成的Fetch请求</span></span><br><span class="line">                CompletedFetch completedFetch = completedFetches.peek();</span><br><span class="line">                <span class="keyword">if</span> (completedFetch == <span class="keyword">null</span>) <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// Step2:获取下一个要处理的nextInLineRecords</span></span><br><span class="line">                    nextInLineRecords = parseCompletedFetch(completedFetch);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    <span class="comment">// Remove a completedFetch upon a parse with exception if (1) it contains no records, and</span></span><br><span class="line">                    <span class="comment">// (2) there are no fetched records with actual content preceding this exception.</span></span><br><span class="line">                    <span class="comment">// The first condition ensures that the completedFetches is not stuck with the same completedFetch</span></span><br><span class="line">                    <span class="comment">// in cases such as the TopicAuthorizationException, and the second condition ensures that no</span></span><br><span class="line">                    <span class="comment">// potential data loss due to an exception in a following record.</span></span><br><span class="line">                    FetchResponse.PartitionData partition = completedFetch.partitionData;</span><br><span class="line">                    <span class="keyword">if</span> (fetched.isEmpty() &amp;&amp; (partition.records == <span class="keyword">null</span> || partition.records.sizeInBytes() == <span class="number">0</span>)) &#123;</span><br><span class="line">                        completedFetches.poll();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">throw</span> e;</span><br><span class="line">                &#125;</span><br><span class="line">                completedFetches.poll();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// Step3:拉取records，更新position</span></span><br><span class="line">                List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = fetchRecords(nextInLineRecords, recordsRemaining);</span><br><span class="line">                TopicPartition partition = nextInLineRecords.partition;</span><br><span class="line">                <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">                    List&lt;ConsumerRecord&lt;K, V&gt;&gt; currentRecords = fetched.get(partition);</span><br><span class="line">                    <span class="keyword">if</span> (currentRecords == <span class="keyword">null</span>) &#123;<span class="comment">// 正常情况下，一个node只会发送一个request，一般只会有一个</span></span><br><span class="line">                        fetched.put(partition, records);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// this case shouldn&#x27;t usually happen because we only send one fetch at a time per partition,</span></span><br><span class="line">                        <span class="comment">// but it might conceivably happen in some rare cases (such as partition leader changes).</span></span><br><span class="line">                        <span class="comment">// we have to copy to a new list because the old one may be immutable</span></span><br><span class="line">                        List&lt;ConsumerRecord&lt;K, V&gt;&gt; newRecords = <span class="keyword">new</span> ArrayList&lt;&gt;(records.size() + currentRecords.size());</span><br><span class="line">                        newRecords.addAll(currentRecords);</span><br><span class="line">                        newRecords.addAll(records);</span><br><span class="line">                        fetched.put(partition, newRecords);</span><br><span class="line">                    &#125;</span><br><span class="line">                    recordsRemaining -= records.size();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">        <span class="keyword">if</span> (fetched.isEmpty())</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Step4:返回相应的Records数据</span></span><br><span class="line">    <span class="keyword">return</span> fetched;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> List&lt;ConsumerRecord&lt;K, V&gt;&gt; fetchRecords(PartitionRecords partitionRecords, <span class="keyword">int</span> maxRecords) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!subscriptions.isAssigned(partitionRecords.partition)) &#123;</span><br><span class="line">        <span class="comment">// this can happen when a rebalance happened before fetched records are returned to the consumer&#x27;s poll call</span></span><br><span class="line">        log.debug(<span class="string">&quot;Not returning fetched records for partition &#123;&#125; since it is no longer assigned&quot;</span>,</span><br><span class="line">                partitionRecords.partition);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!subscriptions.isFetchable(partitionRecords.partition)) &#123;</span><br><span class="line">        <span class="comment">// this can happen when a partition is paused before fetched records are returned to the consumer&#x27;s</span></span><br><span class="line">        <span class="comment">// poll call or if the offset is being reset</span></span><br><span class="line">        <span class="comment">// 这个topic-partition不能被消费了，比如调用了pause</span></span><br><span class="line">        log.debug(<span class="string">&quot;Not returning fetched records for assigned partition &#123;&#125; since it is no longer fetchable&quot;</span>,</span><br><span class="line">                partitionRecords.partition);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        SubscriptionState.FetchPosition position = subscriptions.position(partitionRecords.partition);</span><br><span class="line">        <span class="keyword">if</span> (partitionRecords.nextFetchOffset == position.offset) &#123;<span class="comment">// offset对的上，也就是拉取是按顺序拉的</span></span><br><span class="line">            <span class="comment">// 获取该topic-partition对应的records，并更新partitionRecords的fetchOffset(用于判断是否顺序)</span></span><br><span class="line">            List&lt;ConsumerRecord&lt;K, V&gt;&gt; partRecords = partitionRecords.fetchRecords(maxRecords);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (partitionRecords.nextFetchOffset &gt; position.offset) &#123;</span><br><span class="line">                SubscriptionState.FetchPosition nextPosition = <span class="keyword">new</span> SubscriptionState.FetchPosition(</span><br><span class="line">                        partitionRecords.nextFetchOffset,</span><br><span class="line">                        partitionRecords.lastEpoch,</span><br><span class="line">                        position.currentLeader);</span><br><span class="line">                log.trace(<span class="string">&quot;Returning fetched records at offset &#123;&#125; for assigned partition &#123;&#125; and update &quot;</span> +</span><br><span class="line">                        <span class="string">&quot;position to &#123;&#125;&quot;</span>, position, partitionRecords.partition, nextPosition);</span><br><span class="line">                <span class="comment">// 更新消费到的offset(the fetch position)</span></span><br><span class="line">                subscriptions.position(partitionRecords.partition, nextPosition);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取Lag(即position与hw之间差值)，hw为null时，才返回null</span></span><br><span class="line">            Long partitionLag = subscriptions.partitionLag(partitionRecords.partition, isolationLevel);</span><br><span class="line">            <span class="keyword">if</span> (partitionLag != <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">this</span>.sensors.recordPartitionLag(partitionRecords.partition, partitionLag);</span><br><span class="line"></span><br><span class="line">            Long lead = subscriptions.partitionLead(partitionRecords.partition);</span><br><span class="line">            <span class="keyword">if</span> (lead != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">this</span>.sensors.recordPartitionLead(partitionRecords.partition, lead);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> partRecords;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// these records aren&#x27;t next in line based on the last consumed position, ignore them</span></span><br><span class="line">            <span class="comment">// they must be from an obsolete request</span></span><br><span class="line">            log.debug(<span class="string">&quot;Ignoring fetched records for &#123;&#125; at offset &#123;&#125; since the current position is &#123;&#125;&quot;</span>,</span><br><span class="line">                    partitionRecords.partition, partitionRecords.nextFetchOffset, position);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    partitionRecords.drain();</span><br><span class="line">    <span class="keyword">return</span> emptyList();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>consumer 的 Fetcher 处理从 server 获取的 fetch response 大致分为以下几个过程：</p>
<ol>
<li>通过 <code>completedFetches.peek()</code> 获取已经成功的 fetch response (在 <code>fetcher.sendFetches ()</code> 方法中会把发送FetchRequest请求成功后的结果放在这个集合中，是拆分为 topic-partition 的粒度放进去的)；</li>
<li><code>parseCompletedFetch()</code> 处理上面获取的 completedFetch，构造成 PartitionRecords 类型；</li>
<li>通过 <code>fetchRecords()</code> 方法处理 PartitionRecords 对象，在这个里面会去验证 fetchOffset 是否能对得上，只有 fetchOffset 是一致的情况下才会去处理相应的数据，并更新 the fetch offset 的信息，如果 fetchOffset 不一致，这里就不会处理，the fetch offset 就不会更新，下次 fetch 请求时是会接着 the fetch offset 的位置去请求相应的数据；</li>
<li>返回相应的 Records 数据。</li>
</ol>
<p>至此，KafkaConsumer 如何拉取消息的整体流程也分析完毕。</p>
<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="http://generalthink.github.io/2019/05/31/kafka-consumer-offset/">http://generalthink.github.io/2019/05/31/kafka-consumer-offset/</a></p>
<p><a target="_blank" rel="noopener" href="https://matt33.com/2017/11/11/consumer-pollonce/">https://matt33.com/2017/11/11/consumer-pollonce/</a></p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:wdshfut@163.com">wdshfut@163.com</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/10/ouyangxiu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/10/ouyangxiu/" class="post-title-link" itemprop="url">欧阳修</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-10 08:51:06" itemprop="dateCreated datePublished" datetime="2020-11-10T08:51:06+08:00">2020-11-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-23 16:48:11" itemprop="dateModified" datetime="2020-11-23T16:48:11+08:00">2020-11-23</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="伐树记"><a href="#伐树记" class="headerlink" title="伐树记"></a>伐树记</h2><p>署之东园，久茀不治。修至始辟之，粪瘠溉枯，为蔬圃十数畦，又植花果桐竹凡百本。</p>
<p>春阳既浮，萌者将动。园之守启曰：“园有樗焉，其根壮而叶大。根壮则梗地脉，耗阳气，而新植者不得滋；叶大则阴翳蒙碍，而新植者不得畅以茂。又其材拳曲臃肿，疏轻而不坚，不足养，是宜伐。”因尽薪之。明日，圃之守又曰：“圃之南有杏焉，凡其根庇之广可六七尺，其下之地最壤腴，以杏故，特不得蔬，是亦宜薪。”修曰：“噫！今杏方春且华，将待其实，若独不能损数畦之广为杏地邪？“因勿伐。</p>
<p>既而悟且叹曰：“吁！庄周之说曰：樗、栎以不材终其天年，桂、漆以有用而见伤夭。今樗诚不材矣，然一旦悉翦弃；杏之体最坚密，美泽可用，反见存。岂才不才各遭其时之可否邪？”</p>
<p>他日，客有过修者。仆夫曳薪过堂下，因指而语客以所疑。客曰： “是何怪邪？夫以无用处无用，庄周之贵也。以无用而贼有用，乌能免哉！彼杏之有华实也，以有生之具而庇其根，幸矣。若桂、漆之不能逃乎斤斧者，盖有利之者在死，势不得以生也，与乎杏实异矣。今樗之臃肿不材，而以壮大害物，其见伐，诚宜尔。与夫‘才者死、不才者生’之说，又异矣。凡物幸之与不幸，视其处之而已。”客既去，修善其言而记之。</p>
<h2 id="非非堂记"><a href="#非非堂记" class="headerlink" title="非非堂记"></a>非非堂记</h2><p>权衡之平物，动则轻重差，其于静也，锱铢不失。水之鉴物，动则不能有睹，其于静也，毫发可辨。在乎人，耳司听，目司视，动则乱于聪明，其于静也，闻见必审。处身者不为外物眩晃而动，则其心静，心静则智识明，是是非非，无所施而不中。夫是是近乎谄，非非近乎讪，不幸而过，宁讪无谄。是者，君子之常，是之何加？一以视之，未若非非之为正也。</p>
<p>予居洛之明年，既新厅事，有文纪于壁末。营其西偏作堂，户北向，植丛竹，辟户于其南，纳日月之光。设一几一榻，架书数百卷，朝夕居其中。以其静也，闭目澄心，览今照古，思虑无所不至焉。故其堂以非非为名云。</p>
<h2 id="浪淘沙-·-把酒祝东风"><a href="#浪淘沙-·-把酒祝东风" class="headerlink" title="浪淘沙 · 把酒祝东风"></a>浪淘沙 · 把酒祝东风</h2><p>把酒祝东风，且共从容。垂杨紫陌洛城东。总是当时携手处，游遍芳丛。</p>
<p>聚散苦匆匆，此恨无穷。今年花胜去年红。可惜明年花更好，知与谁同？</p>
<h2 id="玉楼春-·-尊前拟把归期说"><a href="#玉楼春-·-尊前拟把归期说" class="headerlink" title="玉楼春 · 尊前拟把归期说"></a>玉楼春 · 尊前拟把归期说</h2><p>尊前拟把归期说，欲语春容先惨咽。人生自是有情痴，此恨不关风与月。</p>
<p>离歌且莫翻新阕，一曲能教肠寸结。直须看尽洛城花，始共春风容易别。</p>
<h2 id="生查子-·-元夕"><a href="#生查子-·-元夕" class="headerlink" title="生查子 · 元夕"></a>生查子 · 元夕</h2><p>去年元夜时，花市灯如昼。</p>
<p>月上柳梢头，人约黄昏后。</p>
<p>今年元夜时，月与灯依旧。</p>
<p>不见去年人，泪满春衫袖。</p>
<h2 id="读李翱文"><a href="#读李翱文" class="headerlink" title="读李翱文"></a>读李翱文</h2><p>予始读翱《复性书》三篇，曰：此《中庸》之义疏尔。智者诚其性，当读《中庸》；愚者虽读此不晓也，不作可焉。又读《与韩侍郎荐贤书》，以谓翱特穷时愤世无荐己者，故丁宁如此；使其得志，亦未必。然以韩为秦汉间好侠行义之一豪俊，亦善论人者也。最后读《幽怀赋》，然后置书而叹，叹已复读，不自休。恨翱不生于今，不得与之交；又恨予不得生翱时，与翱上下其论也。</p>
<p>凡昔翱一时人，有道而能文者，莫若韩愈。愈尝有赋矣，不过羡二鸟之光荣，叹一饱之无时尔。此其心使光荣而饱，则不复云矣。若翱独不然，其赋曰：“众嚣嚣而杂处兮，咸叹老而嗟卑；视予心之不然兮，虑行道之犹非。”又怪神尧以一旅取天下，后世子孙不能以天下取河北，以为忧。呜呼！使当时君子皆易其叹老嗟卑之心为翱所忧之心，则唐之天下岂有乱与亡哉！</p>
<p>然翱幸不生今时，见今之事，则其忧又甚矣。奈何今之人不忧也？余行天下，见人多矣，脱有一人能如翱忧者，又皆贱远，与翱无异；其余光荣而饱者，一闻忧世之言，不以为狂人，则以为病痴子，不怒则笑之矣。呜呼，在位而不肯自忧，又禁他人使皆不得忧，可叹也夫!</p>
<p>景祐三年十月十七日，欧阳修书。</p>
<h2 id="答吴充秀才书"><a href="#答吴充秀才书" class="headerlink" title="答吴充秀才书"></a>答吴充秀才书</h2><p>修顿首白，先辈吴君足下。前辱示书及文三篇，发而读之，浩乎若千万言之多，及少定而视焉，才数百言尔。非夫辞丰意雄，沛然有不可御之势，何以至此！然犹自患伥伥莫有开之使前者，此好学之谦言也。</p>
<p>修材不足用于时，仕不足荣于世，其毁誉不足轻重，气力不足动人。世之欲假誉以为重，借力而后进者，奚取于修焉？先辈学精文雄，其施于时，又非待修誉而为重，力而后进者也。然而惠然见临，若有所责，得非急于谋道，不择其人而问焉者欤？</p>
<p>夫学者未始不为道，而至者鲜焉；非道之于人远也，学者有所溺焉尔。盖文之为言，难工而可喜，易悦而自足。世之学者往往溺之，一有工焉，则曰：“吾学足矣。“甚者至弃百事不关于心，曰：“吾文士也，职于文而已。”此其所以至之鲜也。</p>
<p>昔孔子老而归鲁，六经之作，数年之顷尔。然读《易》者如无《春秋》，读《书》者如无《诗》，何其用功少而至于至也！圣人之文虽不可及，然大抵道胜者，文不难而自至也。故孟子皇皇不暇著书，荀卿盖亦晚而有作。若子云、仲淹，方勉焉以模言语，此道未足而强言者也。后之惑者，徒见前世之文传，以为学者文而已，故愈力愈勤而愈不至。此足下所谓”终日不出于轩序，不能纵横高下皆如意“者，道未足也。若道之充焉，虽行乎天地，入于渊泉，无不之也。</p>
<p>先辈之文浩乎沛然，可谓善矣。而又志于为道，犹自以为未广。若不止焉，孟、荀可至而不难也。修，学道而不至者，然幸不甘于所悦而溺于所止。因吾子之能不自止，又以励修之少进焉。幸甚！幸甚！修白。</p>
<h2 id="答祖择之书"><a href="#答祖择之书" class="headerlink" title="答祖择之书"></a>答祖择之书</h2><p>修启。秀才人至，蒙示书一通，并诗赋杂文两策，谕之曰：“一览以为如何？”某既陋，不足以辱好学者之问；又其少贱而长穷，其素所为未有足称以取信于人。亦尝有人问者，以不足问之愚，而未尝答人之问。足下卒然及之，是以愧惧不知所言。虽然，不远数百里走使者以及门，意厚礼勤，何敢不报。</p>
<p>某闻古之学者必严其师，师严然后道尊，道尊然后笃敬，笃敬然后能自守，能自守然后果于用，果于用然后不畏而不迁。三代之衰，学校废。至两汉，师道尚存，故其学者各守其经以自用。是以汉之政理文章与其当时之事，后世莫及者，其所从来深矣。后世师，法渐坏，而今世无师，则学者不尊严，故自轻其道。轻之则不能至，不至则不能笃信，信不笃则不知所守，守不固则有所畏而物可移。是故学者惟俯仰徇时，以希禄利为急，至于忘本趋末，流而不返。夫以不信不固之心，守不至之学，虽欲果于自用，而莫知其所以用之之道，又况有禄利之诱、刑祸之惧以迁之哉！此足下所谓志古知道之士世所鲜，而未有合者，由此也。</p>
<p>足下所为文，用意甚高，卓然有不顾世俗之心，直欲自到于古人。今世之人用心如足下者有几？是则乡曲之中能为足下之师者谓谁，交游之间能发足下之议论者谓谁？学不师则守不一，议论不博则无所发明而究其深。足下之言高趣远，甚善，然所守未一而议论未精，此其病也。窃惟足下之交游能为足下称才誉美者不少，今皆舍之，远而见及，乃知足下是欲求其不至。此古君子之用心也，是以言之不敢隐。</p>
<p>夫世无师矣，学者当师经，师经必先求其意，意得则心定，心定则道纯，道纯则充于中者实，中充实则发为文者辉光，施于世者果致。三代、两汉之学，不过此也。足下患世未有合者，而不弃其愚，将某以为合，故敢道此。未知足下之意合否？</p>
<h2 id="与荆南乐秀才书"><a href="#与荆南乐秀才书" class="headerlink" title="与荆南乐秀才书"></a>与荆南乐秀才书</h2><p>修顿首白秀才足下。前者舟行往来，屡辱见过。又辱以所业一编，先之启事，及门而贽。田秀才西来，辱书；其后予家奴自府还县，比又辱书。仆有罪之人，人所共弃，而足下见礼如此，何以当之？当之未暇答，宜遂绝，而再辱书；再而未答，益宜绝，而又辱之。何其勤之甚也！如修者，天下穷贱之人尔，安能使足下之切切如是邪？盖足下力学好问，急于自为谋而然也。然蒙索仆所为文字者，此似有所过听也。</p>
<p>仆少从进士举于有司，学为诗赋，以备程试，凡三举而得第。与士君子相识者多，故往往能道仆名字，而又以游从相爱之私，或过称其文字。故使足下闻仆虚名，而欲见其所为者，由此也。</p>
<p>仆少孤贫，贪禄仕以养亲，不暇就师穷经，以学圣人之遗业。而涉猎书史，姑随世俗作所谓时文者，皆穿蠹经传，移此俪彼，以为浮薄，惟恐不悦于时人，非有卓然自立之言如古人者。然有司过采，屡以先多士。及得第已来，自以前所为不足以称有司之举而当长者之知，始大改其为，庶几有立。然言出而罪至，学成而身辱，为彼则获誉，为此则受祸，此明效也。</p>
<p>夫时文虽曰浮巧，然其为功，亦不易也。仆天姿不好而强为之，故比时人之为者尤不工，然已足以取禄仕而窃名誉者，顺时故也。先辈少年志盛，方欲取荣誉于世，则莫若顺时。天圣中，天子下诏书，敕学者去浮华，其后风俗大变。今时之士大夫所为，彬彬有两汉之风矣。先辈往学之，非徒足以顺时取誉而已，如其至之，是直齐肩于两汉之士也。若仆者，其前所为既不足学，其后所为慎不可学，是以徘徊不敢出其所为者，为此也。</p>
<p>在《易》之《困》曰：“有言不信。”谓夫人方困时，其言不为人所信也。今可谓困矣，安足为足下所取信哉？辱书既多且切，不敢不答。幸察。</p>
<h2 id="答李诩第一书"><a href="#答李诩第一书" class="headerlink" title="答李诩第一书"></a>答李诩第一书</h2><p>修白。人至，辱书及《性诠》三篇，曰以质其果是。夫自信笃者，无所待于人；有质于人者，自疑者也。今吾子自谓“夫子与孟、荀、扬、韩复生，不能夺吾言”，其可谓自信不疑者矣。而返以质于修。使修有过于夫子者，乃可为吾子辩，况修未及孟、荀、扬、韩之一二也。修非知道者，好学而未至者也。世无师久矣，尚赖朋友切磋之益，苟不自满而中止，庶几终身而有成。固常乐与学者论议往来，非敢以益于人，盖求益于人者也。况如吾子之文章论议，岂易得哉？固乐为吾子辩也。苟尚有所疑，敢不尽其所学以告，既吾子自信如是，虽夫子不能夺，使修何所说焉？人还索书，未知所答，惭惕惭惕。修再拜。</p>
<h2 id="答李诩第二书"><a href="#答李诩第二书" class="headerlink" title="答李诩第二书"></a>答李诩第二书</h2><p>修白。前辱示书及《性诠》三篇，见吾子好学善辩，而文能尽其意之详。令世之言性者多矣，有所不及也，故思与吾子卒其说。</p>
<p>修患世之学者多言性，故常为说曰“夫性，非学者之所急，而圣人之所罕言也。《易》六十四卦不言性，其言者动静得失吉凶之常理也；《春秋》二百四十二年不言性，其言者善恶是非之实录也；《诗》三百五篇不言性，其言者政教兴衰之美刺也；《书》五十九篇不言性，其言者尧、舜、三代之治乱也；《礼》、《乐》之书虽不完，而杂出于诸儒之记，然其大要，治国修身之法也。六经之所载，皆人事之切于世者，是以言之甚详。至于性也，百不一二言之，或因言而及焉，非为性而言也，故虽言而不究。</p>
<p>予之所谓不言者，非谓绝而无言，盖其言者鲜，而又不主于性而言也。《论语》所载七十二子之问于孔子者，问孝、问忠、问仁义、问礼乐、问修身、问为政、问朋友、问鬼神者有矣，未尝有问性者。孔子之告其弟子者，凡数千言，其及于性者一言而已。予故曰：非学者之所急，而圣人之罕言也。</p>
<p>《书》曰“习与性成”，《语》曰“性相近，习相远”者，戒人慎所习而言也。《中庸》曰“天命之谓性，率性之谓道”者，明性无常，必有以率之也。《乐记》亦曰“感物而动，性之欲”者，明物之感人无不至也。然终不言性果善果恶，但戒人慎所习与所感，而勤其所以率之者尔。予故曰“因言以及之，而不究也。</p>
<p>修少好学，知学之难。凡所谓六经之所载，七十二子之所问者，学之终身，有不能达者矣；于其所达，行之终身，有不能至者矣。以予之汲汲于此而不暇乎其他，因以知七十二子亦以是汲汲而不暇也，又以知圣人所以教人垂世，亦皇皇而不暇也。今之学者于古圣贤所皇皇汲汲者，学之行之，或未至其一二，而好为性说，以穷圣贤之所罕言而不究者，执后儒之偏说，事无用之空言，此予之所不暇也。</p>
<p>或有问曰：性果不足学乎？予曰：性者，与身俱生而人之所皆有也。为君子者，修身治人而已，性之善恶不必究也。使性果善邪，身不可以不修，人不可以不治；使性果恶邪，身不可以不修，人不可以不治。不修其身，虽君子而为小人，《书》曰“惟圣罔念作狂”是也；能修其身，虽小人而为君子，《书》曰“惟狂克念作圣”是也。治道备，人斯为善矣，《书》曰“黎民于变时雍”是也；治道失，人斯为恶矣，《书》曰“殷顽民”，又曰“旧染污俗”是也。故为君子者，以修身治人为急，而不穷性以为言。夫七十二子之不问，六经之不主言，或虽言而不究，岂略之哉，盖有意也。</p>
<p>或又问曰：然则三子言性，过欤？曰：不过也。其不同何也？曰：始异而终同也。使孟子曰人性善矣，遂怠而不教，则是过也；使荀子曰人性恶矣，遂弃而不教，则是过也；使扬子曰人性混矣，遂肆而不教，则是过也。然三子者，或身奔走诸侯以行其道，或著书累千万言以告于后世，未尝不区区以仁义礼乐为急。盖其意以谓善者一日不教，则失而入于恶；恶者勤而教之，则可使至于善；混者驱而率之，则可使去恶而就善也。其说与《书》之“习与性成”，《语》之“性近习远”，《中庸》之“有以率之”，《乐记》之“慎物所感”皆合。夫三子者，推其言则殊，察其用心则一，故予以为推其言不过始异而终同也。凡论三子者，以予言而一之，则譊譊者可以息矣。</p>
<p>予之所说如此，吾子其择焉。</p>
<h2 id="醉翁亭记"><a href="#醉翁亭记" class="headerlink" title="醉翁亭记"></a>醉翁亭记</h2><p>环滁皆山也。其西南诸峰，林壑尤美，望之蔚然而深秀者，琅琊也。山行六七里，渐闻水声潺潺，而泻出于两峰之间者，酿泉也。峰回路转，有亭翼然临于泉上者，醉翁亭也。作亭者谁？山之僧智仙也。名之者谁？太守自谓也。太守与客来饮于此，饮少辄醉，而年又最高，故自号曰醉翁也。醉翁之意不在酒，在乎山水之间也。山水之乐，得之心而寓之酒也。</p>
<p>若夫日出而林霏开，云归而岩穴暝，晦明变化者，山间之朝暮也。野芳发而幽香，佳木秀而繁阴，风霜高洁，水落而石出者，山间之四时也。朝而往，暮而归，四时之景不同，而乐亦无穷也。</p>
<p>至于负者歌于途，行者休于树，前者呼，后者应，伛偻提携，往来而不绝者，滁人游也。临溪而渔，溪深而鱼肥。酿泉为酒，泉香而酒洌；山肴野蔌，杂然而前陈者，太守宴也。宴酣之乐，非丝非竹，射者中，弈者胜，觥筹交错，起坐而喧哗者，众宾欢也。苍颜白发，颓然乎其间者，太守醉也。</p>
<p>已而夕阳在山，人影散乱，太守归而宾客从也。树林阴翳，鸣声上下，游人去而禽鸟乐也。然而禽鸟知山林之乐，而不知人之乐；人知从太守游而乐，而不知太守之乐其乐也。醉能同其乐，醒能述以文者，太守也。太守谓谁？庐陵欧阳修也。</p>
<h2 id="朝中措-·-送刘仲原甫出守维扬"><a href="#朝中措-·-送刘仲原甫出守维扬" class="headerlink" title="朝中措 · 送刘仲原甫出守维扬"></a>朝中措 · 送刘仲原甫出守维扬</h2><p>平山阑槛倚晴空，山色有无中。手种堂前垂柳，别来几度春风？</p>
<p>文章太守，挥毫万字，一饮千钟。行乐直须年少，尊前看取衰翁。</p>
<h2 id="夜行船-·-忆昔西都欢纵"><a href="#夜行船-·-忆昔西都欢纵" class="headerlink" title="夜行船 · 忆昔西都欢纵"></a>夜行船 · 忆昔西都欢纵</h2><p>忆昔西都欢纵。自别后、有谁能共。伊川山水洛川花，细寻思、旧游如梦。</p>
<p>今日相逢情愈重。愁闻唱、画楼钟动。白发天涯逢此景，倒金尊、殢谁相送。</p>
<h2 id="伶官传序"><a href="#伶官传序" class="headerlink" title="伶官传序"></a>伶官传序</h2><p>呜呼！盛衰之理，虽曰天命，岂非人事哉！原庄宗之所以得天下，与其所以失之者，可以知之矣。</p>
<p>世言晋王之将终也，以三矢赐庄宗而告之曰：“梁，吾仇也；燕王，吾所立，契丹，与吾约为兄弟，而皆背晋以归梁。此三者，吾遗恨也。与尔三矢，尔其无忘乃父之志！”庄宗受而藏之于庙。其后用兵，则遣从事以一少牢告庙，请其矢，盛以锦囊，负而前驱，及凯旋而纳之。</p>
<p>方其系燕父子以组，函梁君臣之首，入于太庙，还矢先王，而告以成功，其意气之盛，可谓壮哉！及仇雠已灭，天下已定，一夫夜呼，乱者四应，仓皇东出，未及见贼而士卒离散，君臣相顾，不知所归。至于誓天断发，泣下沾襟，何其衰也！岂得之难而失之易欤？抑本其成败之迹，而皆自于人欤？</p>
<p>《书》曰：“满招损，谦得益。”忧劳可以兴国，逸豫可以亡身，自然之理也。故方其盛也，举天下之豪杰莫能与之争；及其衰也，数十伶人困之，而身死国灭，为天下笑。夫祸患常积于忽微，而智勇多困于所溺，岂独伶人也哉！作《伶官传》。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/04/kafka-consumer-group/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/04/kafka-consumer-group/" class="post-title-link" itemprop="url">KafkaConsumer 源码之 consumer 如何加入 consumer group</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-04 09:35:04" itemprop="dateCreated datePublished" datetime="2020-11-04T09:35:04+08:00">2020-11-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-05 15:32:45" itemprop="dateModified" datetime="2021-01-05T15:32:45+08:00">2021-01-05</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>37k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>33 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Kafka 的 consumer 比 producer 要复杂许多，producer 没有 group 的概念，也不需要关注 offset，而 consumer 不一样，它有组织 (consumer group)，有纪律 (offset)。这些对 consumer 的要求就会很高，这篇文章就主要介绍 consumer 是如何加入 consumer group 的。</p>
<p>在这之前，我们需要先了解一下什么是 GroupCoordinator。简单地说，<strong>GroupCoordinator 是运行在服务器上的一个服务，Kafka 集群上的每一个 broker 节点启动的时候，都会启动一个 GroupCoordinator 服务，其功能是负责进行 consumer 的 group 成员与 offset 管理 (但每个 GroupCoordinator 只是管理一部分的 consumer group member 和 offset 信息)。</strong></p>
<p>consumer group 对应的 GroupCoordinator 节点的确定，会通过如下方式：</p>
<p>将 consumer group 的 <code>group.id</code> 进行 hash，把得到的值的绝对值，对 _consumer_offsets 的 partition 总数取余，然后得到其对应的 partition 值，该 partition 的 leader 所在的 broker 即为该 consumer group 所对应的 GroupCoordinator 节点，GroupCoordinator 会存储与该 consumer group 相关的所有的 Meta 信息。</p>
<blockquote>
<p>1._consumer_offsets 这个 topic 是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有 50 个 partition，每个 partition 默认有三个副本。</p>
<p>2.partition 计算方式：<code>abs(GroupId.hashCode()) % NumPartitions</code>，其中，NumPartitions 是 _consumer_offsets 的 partition 数，默认是 50 个。</p>
<p>3.比如，现在通过计算 <code>abs(GroupId.hashCode()) % NumPartitions</code> 的值为 35，然后就找第 35 个 partition 的 leader 在哪个 broker 上 (假设在 192.168.1.12)，那么 GroupCoordinator 节点就在这个 broker 上。</p>
</blockquote>
<p><strong>同时，这个 consumer group 所提交的消费 offset 信息也会发送给这个 partition 的 leader 所对应的 broker 节点，因此，这个节点不仅是 GroupCoordinator，而且还保存分区分配方案和组内消费者 offset 信息。</strong></p>
<p>更多关于 GroupCoordinator 的解析，参考：<a target="_blank" rel="noopener" href="https://matt33.com/2018/01/28/server-group-coordinator/">Kafka 源码解析之 GroupCoordinator 详解</a>。</p>
<h2 id="KafkaConsumer-消费消息的主体流程"><a href="#KafkaConsumer-消费消息的主体流程" class="headerlink" title="KafkaConsumer 消费消息的主体流程"></a>KafkaConsumer 消费消息的主体流程</h2><p>接下来，我们回顾下 KafkaConsumer 消费消息的主体流程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建消费者</span></span><br><span class="line">KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 订阅主题</span></span><br><span class="line">kafkaConsumer.subscribe(Collections.singletonList(<span class="string">&quot;consumerCodeTopic&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从服务器拉取数据</span></span><br><span class="line">ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br></pre></td></tr></table></figure>

<h3 id="创建-KafkaConsumer"><a href="#创建-KafkaConsumer" class="headerlink" title="创建 KafkaConsumer"></a>创建 KafkaConsumer</h3><p>创建 KafkaConsumer 的时候，会创建一个 ConsumerCoordinator 服务，由它来负责和 GroupCoordinator 通信：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// no coordinator will be constructed for the default (null) group id</span></span><br><span class="line"><span class="keyword">this</span>.coordinator = groupId == <span class="keyword">null</span> ? <span class="keyword">null</span> :</span><br><span class="line">	<span class="keyword">new</span> ConsumerCoordinator(logContext,</span><br><span class="line">                        <span class="keyword">this</span>.client,</span><br><span class="line">                        groupId,</span><br><span class="line">                        <span class="keyword">this</span>.groupInstanceId,</span><br><span class="line">                        maxPollIntervalMs,</span><br><span class="line">                        sessionTimeoutMs,</span><br><span class="line">                        <span class="keyword">new</span> Heartbeat(time, sessionTimeoutMs, heartbeatIntervalMs, maxPollIntervalMs, retryBackoffMs),</span><br><span class="line">                        assignors,</span><br><span class="line">                        <span class="keyword">this</span>.metadata,</span><br><span class="line">                        <span class="keyword">this</span>.subscriptions,</span><br><span class="line">                        metrics,</span><br><span class="line">                        metricGrpPrefix,</span><br><span class="line">                        <span class="keyword">this</span>.time,</span><br><span class="line">                        retryBackoffMs,</span><br><span class="line">                        enableAutoCommit,</span><br><span class="line">                        config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG),</span><br><span class="line">                        <span class="keyword">this</span>.interceptors,</span><br><span class="line">                        config.getBoolean(ConsumerConfig.LEAVE_GROUP_ON_CLOSE_CONFIG));</span><br></pre></td></tr></table></figure>

<h3 id="订阅-topic"><a href="#订阅-topic" class="headerlink" title="订阅 topic"></a>订阅 topic</h3><p>KafkaConsumer 订阅 topic 的方式有好几种，这在<a target="_blank" rel="noopener" href="https://acatsmiling.github.io/2020/10/29/kafka-consumer/">前面的文章</a>有提到过。订阅的时候，会根据订阅的方式，设置其对应的订阅类型，默认存在四种订阅类型：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">enum</span> <span class="title">SubscriptionType</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 默认</span></span><br><span class="line">    NONE,</span><br><span class="line">    <span class="comment">// subscribe方式订阅</span></span><br><span class="line">    AUTO_TOPICS,</span><br><span class="line">    <span class="comment">// subscribe方式订阅</span></span><br><span class="line">    AUTO_PATTERN,</span><br><span class="line">    <span class="comment">// assign方式订阅</span></span><br><span class="line">    USER_ASSIGNED</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>比如，采用 <code>kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;))</code> 方式订阅 topic 时，会将订阅类型设置为 <code>SubscriptionType.AUTO_TOPICS</code>，其核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Subscribe to the given list of topics to get dynamically assigned partitions.</span></span><br><span class="line"><span class="comment"> * &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current</span></span><br><span class="line"><span class="comment"> * assignment (if there is one).&lt;/b&gt; It is not possible to combine topic subscription with group management</span></span><br><span class="line"><span class="comment"> * with manual partition assignment through &#123;<span class="doctag">@link</span> #assign(Collection)&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * If the given list of topics is empty, it is treated the same as &#123;<span class="doctag">@link</span> #unsubscribe()&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * This is a short-hand for &#123;<span class="doctag">@link</span> #subscribe(Collection, ConsumerRebalanceListener)&#125;, which</span></span><br><span class="line"><span class="comment"> * uses a no-op listener. If you need the ability to seek to particular offsets, you should prefer</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> #subscribe(Collection, ConsumerRebalanceListener)&#125;, since group rebalances will cause partition offsets</span></span><br><span class="line"><span class="comment"> * to be reset. You should also provide your own listener if you are doing your own offset</span></span><br><span class="line"><span class="comment"> * management since the listener gives you an opportunity to commit offsets before a rebalance finishes.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics The list of topics to subscribe to</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IllegalArgumentException If topics is null or contains null or empty elements</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IllegalStateException If &#123;<span class="doctag">@code</span> subscribe()&#125; is called previously with pattern, or assign is called</span></span><br><span class="line"><span class="comment"> *                               previously (without a subsequent call to &#123;<span class="doctag">@link</span> #unsubscribe()&#125;), or if not</span></span><br><span class="line"><span class="comment"> *                               configured at-least one partition assignment strategy</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span> </span>&#123;</span><br><span class="line">    subscribe(topics, <span class="keyword">new</span> NoOpConsumerRebalanceListener());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Subscribe to the given list of topics to get dynamically</span></span><br><span class="line"><span class="comment"> * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current</span></span><br><span class="line"><span class="comment"> * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management</span></span><br><span class="line"><span class="comment"> * with manual partition assignment through &#123;<span class="doctag">@link</span> #assign(Collection)&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * If the given list of topics is empty, it is treated the same as &#123;<span class="doctag">@link</span> #unsubscribe()&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * As part of group management, the consumer will keep track of the list of consumers that belong to a particular</span></span><br><span class="line"><span class="comment"> * group and will trigger a rebalance operation if any one of the following events are triggered:</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;Number of partitions change for any of the subscribed topics</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;A subscribed topic is created or deleted</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;An existing member of the consumer group is shutdown or fails</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;A new member is added to the consumer group</span></span><br><span class="line"><span class="comment"> * &lt;/ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * When any of these events are triggered, the provided listener will be invoked first to indicate that</span></span><br><span class="line"><span class="comment"> * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received.</span></span><br><span class="line"><span class="comment"> * Note that rebalances will only occur during an active call to &#123;<span class="doctag">@link</span> #poll(Duration)&#125;, so callbacks will</span></span><br><span class="line"><span class="comment"> * also only be invoked during that time.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The provided listener will immediately override any listener set in a previous call to subscribe.</span></span><br><span class="line"><span class="comment"> * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics</span></span><br><span class="line"><span class="comment"> * subscribed in this call. See &#123;<span class="doctag">@link</span> ConsumerRebalanceListener&#125; for more details.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics The list of topics to subscribe to</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> listener Non-null listener instance to get notifications on partition assignment/revocation for the</span></span><br><span class="line"><span class="comment"> *                 subscribed topics</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IllegalStateException If &#123;<span class="doctag">@code</span> subscribe()&#125; is called previously with pattern, or assign is called</span></span><br><span class="line"><span class="comment"> *                               previously (without a subsequent call to &#123;<span class="doctag">@link</span> #unsubscribe()&#125;), or if not</span></span><br><span class="line"><span class="comment"> *                               configured at-least one partition assignment strategy</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span>&#123;</span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        maybeThrowInvalidGroupIdException();</span><br><span class="line">        <span class="keyword">if</span> (topics == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Topic collection to subscribe to cannot be null&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (topics.isEmpty()) &#123;</span><br><span class="line">            <span class="comment">// treat subscribing to empty topic list as the same as unsubscribing</span></span><br><span class="line">            <span class="keyword">this</span>.unsubscribe();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (String topic : topics) &#123;</span><br><span class="line">                <span class="keyword">if</span> (topic == <span class="keyword">null</span> || topic.trim().isEmpty())</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            throwIfNoAssignorsConfigured();</span><br><span class="line">            fetcher.clearBufferedDataForUnassignedTopics(topics);</span><br><span class="line">            log.info(<span class="string">&quot;Subscribed to topic(s): &#123;&#125;&quot;</span>, Utils.join(topics, <span class="string">&quot;, &quot;</span>));</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.subscribe(<span class="keyword">new</span> HashSet&lt;&gt;(topics), listener))</span><br><span class="line">                metadata.requestUpdateForNewTopics();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">subscribe</span><span class="params">(Set&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span>&#123;</span><br><span class="line">    registerRebalanceListener(listener);</span><br><span class="line">    setSubscriptionType(SubscriptionType.AUTO_TOPICS);</span><br><span class="line">    <span class="keyword">return</span> changeSubscription(topics);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * This method sets the subscription type if it is not already set (i.e. when it is NONE),</span></span><br><span class="line"><span class="comment"> * or verifies that the subscription type is equal to the give type when it is set (i.e.</span></span><br><span class="line"><span class="comment"> * when it is not NONE)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> type The given subscription type</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setSubscriptionType</span><span class="params">(SubscriptionType type)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptionType == SubscriptionType.NONE)</span><br><span class="line">        <span class="keyword">this</span>.subscriptionType = type;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptionType != type)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="从服务器拉取数据"><a href="#从服务器拉取数据" class="headerlink" title="从服务器拉取数据"></a>从服务器拉取数据</h3><p>订阅完成后，就可以从服务器拉取数据了，应该注意的是，KafkaConsumer 没有后台线程默默的拉取数据，它的所有行为都集中在 <code>poll ()</code> 方法中，<strong>KafkaConsumer 是线程不安全的，同时只能允许一个线程运行。</strong></p>
<p><code>kafkaConsumer.poll ()</code> 方法的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Timer timer, <span class="keyword">final</span> <span class="keyword">boolean</span> includeMetadataInTimeout)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭</span></span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// poll for new data until the timeout expires</span></span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            client.maybeTriggerWakeup();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (includeMetadataInTimeout) &#123;</span><br><span class="line">                <span class="comment">// Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息</span></span><br><span class="line">                <span class="keyword">if</span> (!updateAssignmentMetadataIfNeeded(timer)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">while</span> (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123;</span><br><span class="line">                    log.warn(<span class="string">&quot;Still waiting for metadata&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Step3:拉取数据</span></span><br><span class="line">            <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer);</span><br><span class="line">            <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">// before returning the fetched records, we can send off the next round of fetches</span></span><br><span class="line">                <span class="comment">// and avoid block waiting for their responses to enable pipelining while the user</span></span><br><span class="line">                <span class="comment">// is handling the fetched records.</span></span><br><span class="line">                <span class="comment">//</span></span><br><span class="line">                <span class="comment">// <span class="doctag">NOTE:</span> since the consumed position has already been updated, we must not allow</span></span><br><span class="line">                <span class="comment">// wakeups or any other errors to be triggered prior to returning the fetched records.</span></span><br><span class="line">                <span class="keyword">if</span> (fetcher.sendFetches() &gt; <span class="number">0</span> || client.hasPendingRequests()) &#123;</span><br><span class="line">                    client.pollNoWakeup();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">this</span>.interceptors.onConsume(<span class="keyword">new</span> ConsumerRecords&lt;&gt;(records));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">while</span> (timer.notExpired());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，在 Step 1 阶段， <code>poll ()</code> 方法会先进行判定，如果有多个线程同时使用一个 KafkaConsumer 则会抛出异常：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Acquire the light lock and ensure that the consumer hasn&#x27;t been closed.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IllegalStateException If the consumer has been closed</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">acquireAndEnsureOpen</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    acquire();</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.closed) &#123;</span><br><span class="line">        release();</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;This consumer has already been closed.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Acquire the light lock protecting this consumer from multi-threaded access. Instead of blocking</span></span><br><span class="line"><span class="comment"> * when the lock is not available, however, we just throw an exception (since multi-threaded usage is not</span></span><br><span class="line"><span class="comment"> * supported).</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> ConcurrentModificationException if another thread already has the lock</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> threadId = Thread.currentThread().getId();</span><br><span class="line">    <span class="keyword">if</span> (threadId != currentThread.get() &amp;&amp; !currentThread.compareAndSet(NO_CURRENT_THREAD, threadId))</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException(<span class="string">&quot;KafkaConsumer is not safe for multi-threaded access&quot;</span>);</span><br><span class="line">    refcount.incrementAndGet();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="KafkaConsumer-如何加入-consumer-group"><a href="#KafkaConsumer-如何加入-consumer-group" class="headerlink" title="KafkaConsumer 如何加入 consumer group"></a>KafkaConsumer 如何加入 consumer group</h2><p><strong>一个 KafkaConsumer 实例消费数据的前提是能够加入一个 consumer group 成功，并获取其要订阅的 tp（topic-partition）列表，因此首先要做的就是和 GroupCoordinator 建立连接，加入组织。</strong></p>
<blockquote>
<p>consumer 加入 group 的过程，也就是 reblance 的过程。如果出现了频繁 reblance 的问题，可能和 <code>max.poll.interval.ms</code> 和 <code>max.poll.records</code> 两个参数有关。</p>
</blockquote>
<p>因此，我们先把目光集中在 ConsumerCoordinator 上，这个过程主要发生在 Step 2 阶段：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Visible for testing</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">updateAssignmentMetadataIfNeeded</span><span class="params">(<span class="keyword">final</span> Timer timer)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1.本篇文章的内容主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group)</span></span><br><span class="line">    <span class="keyword">if</span> (coordinator != <span class="keyword">null</span> &amp;&amp; !coordinator.poll(timer)) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.updateFetchPositions(timer)方法留待下一篇文章分析(主要功能是:consumer获得partition的offset)</span></span><br><span class="line">    <span class="keyword">return</span> updateFetchPositions(timer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>关于对 ConsumerCoordinator 的处理都集中在 <code>coordinator.poll ()</code> 方法中。其主要逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Poll for coordinator events. This ensures that the coordinator is known and that the consumer</span></span><br><span class="line"><span class="comment"> * has joined the group (if it is using group management). This also handles periodic offset commits</span></span><br><span class="line"><span class="comment"> * if they are enabled.</span></span><br><span class="line"><span class="comment"> * (确保group的coordinator是已知的，并且这个consumer是已经加入到了group中，也用于offset周期性的commit)</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * Returns early if the timeout expires</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timer Timer bounding how long this method can block</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true iff the operation succeeded</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">poll</span><span class="params">(Timer timer)</span> </span>&#123;</span><br><span class="line">    maybeUpdateSubscriptionMetadata();</span><br><span class="line"></span><br><span class="line">    invokeCompletedOffsetCommitCallbacks();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果是subscribe方式订阅的topic</span></span><br><span class="line">    <span class="keyword">if</span> (subscriptions.partitionsAutoAssigned()) &#123;</span><br><span class="line">        <span class="comment">// Always update the heartbeat last poll time so that the heartbeat thread does not leave the</span></span><br><span class="line">        <span class="comment">// group proactively due to application inactivity even if (say) the coordinator cannot be found.</span></span><br><span class="line">        <span class="comment">// 1.检查心跳线程运行是否正常，如果心跳线程失败则抛出异常，反之则更新poll调用的时间</span></span><br><span class="line">        pollHeartbeat(timer.currentTimeMs());</span><br><span class="line">        <span class="comment">// 2.如果coordinator未知，则初始化ConsumeCoordinator</span></span><br><span class="line">        <span class="keyword">if</span> (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是否需要重新加入group，如果订阅的partition变化或者分配的partition变化，都可能需要重新加入group</span></span><br><span class="line">        <span class="keyword">if</span> (rejoinNeededOrPending()) &#123;</span><br><span class="line">            <span class="comment">// due to a race condition between the initial metadata fetch and the initial rebalance,</span></span><br><span class="line">            <span class="comment">// we need to ensure that the metadata is fresh before joining initially. This ensures</span></span><br><span class="line">            <span class="comment">// that we have matched the pattern against the cluster&#x27;s topics at least once before joining.</span></span><br><span class="line">            <span class="keyword">if</span> (subscriptions.hasPatternSubscription()) &#123;</span><br><span class="line">                <span class="comment">// For consumer group that uses pattern-based subscription, after a topic is created,</span></span><br><span class="line">                <span class="comment">// any consumer that discovers the topic after metadata refresh can trigger rebalance</span></span><br><span class="line">                <span class="comment">// across the entire consumer group. Multiple rebalances can be triggered after one topic</span></span><br><span class="line">                <span class="comment">// creation if consumers refresh metadata at vastly different times. We can significantly</span></span><br><span class="line">                <span class="comment">// reduce the number of rebalances caused by single topic creation by asking consumer to</span></span><br><span class="line">                <span class="comment">// refresh metadata before re-joining the group as long as the refresh backoff time has</span></span><br><span class="line">                <span class="comment">// passed.</span></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.metadata.timeToAllowUpdate(timer.currentTimeMs()) == <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (!client.ensureFreshMetadata(timer)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                maybeUpdateSubscriptionMetadata();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3.确保group是active的，重新加入group，分配订阅的partition</span></span><br><span class="line">            <span class="keyword">if</span> (!ensureActiveGroup(timer)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// For manually assigned partitions, if there are no ready nodes, await metadata.</span></span><br><span class="line">        <span class="comment">// If connections to all nodes fail, wakeups triggered while attempting to send fetch</span></span><br><span class="line">        <span class="comment">// requests result in polls returning immediately, causing a tight loop of polls. Without</span></span><br><span class="line">        <span class="comment">// the wakeup, poll() with no channels would block for the timeout, delaying re-connection.</span></span><br><span class="line">        <span class="comment">// awaitMetadataUpdate() initiates new connections with configured backoff and avoids the busy loop.</span></span><br><span class="line">        <span class="comment">// When group management is used, metadata wait is already performed for this scenario as</span></span><br><span class="line">        <span class="comment">// coordinator is unknown, hence this check is not required.</span></span><br><span class="line">        <span class="keyword">if</span> (metadata.updateRequested() &amp;&amp; !client.hasReadyNodes(timer.currentTimeMs())) &#123;</span><br><span class="line">            client.awaitMetadataUpdate(timer);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4.如果设置的是自动commit,如果定时达到则自动commit</span></span><br><span class="line">    maybeAutoCommitOffsetsAsync(timer.currentTimeMs());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>coordinator.poll ()</code> 方法中，具体实现可以分为四个步骤：</p>
<ol>
<li><code>pollHeartbeat ()</code>：检测心跳线程运行是否正常，需要定时向 GroupCoordinator 发送心跳，如果超时未发送心跳，consumer 会离开 consumer group。</li>
<li><code>ensureCoordinatorReady ()</code>：当通过 <code>subscribe ()</code> 方法订阅 topic 时，如果 coordinator 未知，则初始化 ConsumerCoordinator (在 <code>ensureCoordinatorReady ()</code> 中实现，该方法主要的作用是发送 FindCoordinatorRequest 请求，并建立连接)。</li>
<li><code>ensureActiveGroup ()</code>：判断是否需要重新加入 group，如果订阅的 partition 变化或者分配的 partition 变化时，需要 rejoin，则通过 <code>ensureActiveGroup ()</code> 发送 join-group、sync-group 请求，加入 group 并获取其 assign 的 TopicPartition list。</li>
<li><code>maybeAutoCommitOffsetsAsync ()</code>：如果设置的是自动 commit，并且达到了发送时限则自动 commit offset。</li>
</ol>
<p>关于 rejoin，<strong>下列几种情况会触发再均衡 (reblance) 操作</strong>：</p>
<ul>
<li><p>订阅的 topic 列表变化</p>
</li>
<li><p>topic 被创建或删除</p>
</li>
<li><p>新的消费者加入消费者组 (第一次进行消费也属于这种情况)</p>
</li>
<li><p>消费者宕机下线 (长时间未发送心跳包)</p>
</li>
<li><p>消费者主动退出消费组，比如调用 <code>unsubscrible ()</code> 方法取消对主题的订阅</p>
</li>
<li><p>消费者组对应的 GroupCoorinator 节点发生了变化</p>
</li>
<li><p>消费者组内所订阅的任一主题或者主题的分区数量发生了变化</p>
</li>
</ul>
<blockquote>
<p>取消  topic 订阅，consumer 心跳线程超时以及在  Server 端给定的时间内未收到心跳请求，这三个都是触发的  LEAVE_GROUP 请求。</p>
</blockquote>
<p>下面重点介绍下第二步中的 <code>ensureCoordinatorReady ()</code> 方法和第三步中的 <code>ensureActiveGroup ()</code> 方法。</p>
<h3 id="ensureCoordinatorReady"><a href="#ensureCoordinatorReady" class="headerlink" title="ensureCoordinatorReady"></a>ensureCoordinatorReady</h3><p><code>ensureCoordinatorReady ()</code>这个方法主要作用：<strong>选择一个连接数最少的 broker (还未响应请求最少的 broker)，发送 FindCoordinator 请求，找到 GroupCoordinator 后，建立对应的 TCP 连接。</strong></p>
<ul>
<li>方法调用流程是 <code>ensureCoordinatorReady ()</code> → <code>lookupCoordinator ()</code> → <code>sendFindCoordinatorRequest ()</code>。</li>
<li>如果 client 收到 server response，那么就与 GroupCoordinator 建立连接。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Visible for testing.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Ensure that the coordinator is ready to receive requests.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timer Timer bounding how long this method can block</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true If coordinator discovery and initial connection succeeded, false otherwise</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">ensureCoordinatorReady</span><span class="params">(<span class="keyword">final</span> Timer timer)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!coordinatorUnknown())</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        <span class="comment">// 找到GroupCoordinator，并建立连接</span></span><br><span class="line">        <span class="keyword">final</span> RequestFuture&lt;Void&gt; future = lookupCoordinator();</span><br><span class="line">        client.poll(future, timer);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!future.isDone()) &#123;</span><br><span class="line">            <span class="comment">// ran out of time</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (future.failed()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (future.isRetriable()) &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;Coordinator discovery failed, refreshing metadata&quot;</span>);</span><br><span class="line">                client.awaitMetadataUpdate(timer);</span><br><span class="line">            &#125; <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">throw</span> future.exception();</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (coordinator != <span class="keyword">null</span> &amp;&amp; client.isUnavailable(coordinator)) &#123;</span><br><span class="line">            <span class="comment">// we found the coordinator, but the connection has failed, so mark</span></span><br><span class="line">            <span class="comment">// it dead and backoff before retrying discovery</span></span><br><span class="line">            markCoordinatorUnknown();</span><br><span class="line">            timer.sleep(retryBackoffMs);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">while</span> (coordinatorUnknown() &amp;&amp; timer.notExpired());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> !coordinatorUnknown();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> RequestFuture&lt;Void&gt; <span class="title">lookupCoordinator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (findCoordinatorFuture == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// find a node to ask about the coordinator(找一个最少连接的broker，此处对应的应该就是文章开头处确定GroupCoordinator节点的发发)</span></span><br><span class="line">        Node node = <span class="keyword">this</span>.client.leastLoadedNode();</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;No broker available to send FindCoordinator request&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> RequestFuture.noBrokersAvailable();</span><br><span class="line">        &#125; <span class="keyword">else</span></span><br><span class="line">            <span class="comment">// 对找到的broker发送FindCoordinator请求，并对response进行处理</span></span><br><span class="line">            findCoordinatorFuture = sendFindCoordinatorRequest(node);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> findCoordinatorFuture;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Discover the current coordinator for the group. Sends a GroupMetadata request to</span></span><br><span class="line"><span class="comment"> * one of the brokers. The returned future should be polled to get the result of the request.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> A request future which indicates the completion of the metadata request</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;Void&gt; <span class="title">sendFindCoordinatorRequest</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// initiate the group metadata request</span></span><br><span class="line">    log.debug(<span class="string">&quot;Sending FindCoordinator request to broker &#123;&#125;&quot;</span>, node);</span><br><span class="line">    FindCoordinatorRequest.Builder requestBuilder =</span><br><span class="line">            <span class="keyword">new</span> FindCoordinatorRequest.Builder(</span><br><span class="line">                    <span class="keyword">new</span> FindCoordinatorRequestData()</span><br><span class="line">                        .setKeyType(CoordinatorType.GROUP.id())</span><br><span class="line">                        .setKey(<span class="keyword">this</span>.groupId));</span><br><span class="line">    <span class="comment">// 发送请求，并将response转换为RequestFuture</span></span><br><span class="line">    <span class="comment">// compose的作用是将FindCoordinatorResponseHandler类转换为RequestFuture</span></span><br><span class="line">    <span class="comment">// 实际上就是为返回的Future类重置onSuccess()和onFailure()方法</span></span><br><span class="line">    <span class="keyword">return</span> client.send(node, requestBuilder)</span><br><span class="line">            .compose(<span class="keyword">new</span> FindCoordinatorResponseHandler());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 根据response返回的ip以及端口信息，和该broke上开启的GroupCoordinator建立连接</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">FindCoordinatorResponseHandler</span> <span class="keyword">extends</span> <span class="title">RequestFutureAdapter</span>&lt;<span class="title">ClientResponse</span>, <span class="title">Void</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp, RequestFuture&lt;Void&gt; future)</span> </span>&#123;</span><br><span class="line">        log.debug(<span class="string">&quot;Received FindCoordinator response &#123;&#125;&quot;</span>, resp);</span><br><span class="line">        clearFindCoordinatorFuture();</span><br><span class="line"></span><br><span class="line">        FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) resp.responseBody();</span><br><span class="line">        Errors error = findCoordinatorResponse.error();</span><br><span class="line">        <span class="keyword">if</span> (error == Errors.NONE) &#123;</span><br><span class="line">            <span class="comment">// 如果正确获取broker上的GroupCoordinator，建立连接，并更新心跳时间</span></span><br><span class="line">            <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="comment">// use MAX_VALUE - node.id as the coordinator id to allow separate connections</span></span><br><span class="line">                <span class="comment">// for the coordinator in the underlying network client layer</span></span><br><span class="line">                <span class="keyword">int</span> coordinatorConnectionId = Integer.MAX_VALUE - findCoordinatorResponse.data().nodeId();</span><br><span class="line"></span><br><span class="line">                AbstractCoordinator.<span class="keyword">this</span>.coordinator = <span class="keyword">new</span> Node(</span><br><span class="line">                        coordinatorConnectionId,</span><br><span class="line">                        findCoordinatorResponse.data().host(),</span><br><span class="line">                        findCoordinatorResponse.data().port());</span><br><span class="line">                log.info(<span class="string">&quot;Discovered group coordinator &#123;&#125;&quot;</span>, coordinator);</span><br><span class="line">                <span class="comment">// 初始化tcp连接</span></span><br><span class="line">                client.tryConnect(coordinator);</span><br><span class="line">                <span class="comment">// 更新心跳时间</span></span><br><span class="line">                heartbeat.resetSessionTimeout();</span><br><span class="line">            &#125;</span><br><span class="line">            future.complete(<span class="keyword">null</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</span><br><span class="line">            future.raise(<span class="keyword">new</span> GroupAuthorizationException(groupId));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;Group coordinator lookup failed: &#123;&#125;&quot;</span>, findCoordinatorResponse.data().errorMessage());</span><br><span class="line">            future.raise(error);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e, RequestFuture&lt;Void&gt; future)</span> </span>&#123;</span><br><span class="line">        clearFindCoordinatorFuture();</span><br><span class="line">        <span class="keyword">super</span>.onFailure(e, future);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面代码主要作用就是：往一个负载最小的 broker 节点发起 FindCoordinator 请求，Kafka 在走到这个请求后会根据 group_id 查找对应的 GroupCoordinator 节点 (文章开头处介绍的方法)，如果找到对应的 GroupCoordinator 则会返回其对应的 node_id，host 和 port 信息，并建立连接。</p>
<blockquote>
<p>这里的 GroupCoordinator 节点的确定在文章开头提到过，是通过 <code>group.id</code> 和 partitionCount 来确定的。</p>
</blockquote>
<h3 id="ensureActiveGroup"><a href="#ensureActiveGroup" class="headerlink" title="ensureActiveGroup"></a>ensureActiveGroup</h3><p>现在已经知道了 GroupCoordinator 节点，并建立了连接。<code>ensureActiveGroup ()</code> 这个方法的主要作用：<strong>向 GroupCoordinator 发送 join-group、sync-group 请求，获取 assign 的 tp list。</strong></p>
<ul>
<li>调用过程是 <code>ensureActiveGroup ()</code> → <code>ensureCoordinatorReady ()</code> → <code>startHeartbeatThreadIfNeeded ()</code> → <code>joinGroupIfNeeded ()</code>。</li>
<li><code>joinGroupIfNeeded ()</code> 方法中最重要的方法是 <code>initiateJoinGroup ()</code>，它的的调用流程是 <code>disableHeartbeatThread ()</code> → <code>sendJoinGroupRequest ()</code> → <code>JoinGroupResponseHandler::handle ()</code> → <code>onJoinLeader ()</code>，<code>onJoinFollower ()</code> → <code>sendSyncGroupRequest ()</code>。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Ensure the group is active (i.e., joined and synced)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timer Timer bounding how long this method can block</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true iff the group is active</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">ensureActiveGroup</span><span class="params">(<span class="keyword">final</span> Timer timer)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// always ensure that the coordinator is ready because we may have been disconnected</span></span><br><span class="line">    <span class="comment">// when sending heartbeats and does not necessarily require us to rejoin the group.</span></span><br><span class="line">    <span class="comment">// 1.确保GroupCoordinator已经连接</span></span><br><span class="line">    <span class="keyword">if</span> (!ensureCoordinatorReady(timer)) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.启动心跳线程，但是并不一定发送心跳，满足条件后才会发送心跳</span></span><br><span class="line">    startHeartbeatThreadIfNeeded();</span><br><span class="line">    <span class="comment">// 3.发送joinGroup请求，并对返回的信息进行处理，核心步骤</span></span><br><span class="line">    <span class="keyword">return</span> joinGroupIfNeeded(timer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>心跳线程就是在这里启动的，但是并不一定马上发送心跳包，会在满足条件之后才会开始发送。后面最主要的逻辑就集中在 <code>joinGroupIfNeeded ()</code> 方法，它的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Joins the group without starting the heartbeat thread.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Visible for testing.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timer Timer bounding how long this method can block</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true iff the operation succeeded</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">joinGroupIfNeeded</span><span class="params">(<span class="keyword">final</span> Timer timer)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (rejoinNeededOrPending()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!ensureCoordinatorReady(timer)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// call onJoinPrepare if needed. We set a flag to make sure that we do not call it a second</span></span><br><span class="line">        <span class="comment">// time if the client is woken up before a pending rebalance completes. This must be called</span></span><br><span class="line">        <span class="comment">// on each iteration of the loop because an event requiring a rebalance (such as a metadata</span></span><br><span class="line">        <span class="comment">// refresh which changes the matched subscription set) can occur while another rebalance is</span></span><br><span class="line">        <span class="comment">// still in progress.</span></span><br><span class="line">        <span class="comment">// 触发onJoinPrepare，包括offset commit和rebalance listener</span></span><br><span class="line">        <span class="keyword">if</span> (needsJoinPrepare) &#123;</span><br><span class="line">            <span class="comment">// 如果是自动提交，则要开始提交offset以及在join group之前回调reblance listener接口</span></span><br><span class="line">            onJoinPrepare(generation.generationId, generation.memberId);</span><br><span class="line">            needsJoinPrepare = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化joinGroup请求，并发送joinGroup请求，核心步骤</span></span><br><span class="line">        <span class="keyword">final</span> RequestFuture&lt;ByteBuffer&gt; future = initiateJoinGroup();</span><br><span class="line">        client.poll(future, timer);</span><br><span class="line">        <span class="keyword">if</span> (!future.isDone()) &#123;</span><br><span class="line">            <span class="comment">// we ran out of time</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// join succeed，这一步时，时间上sync-group已经成功了</span></span><br><span class="line">        <span class="keyword">if</span> (future.succeeded()) &#123;</span><br><span class="line">            <span class="comment">// Duplicate the buffer in case `onJoinComplete` does not complete and needs to be retried.</span></span><br><span class="line">            ByteBuffer memberAssignment = future.value().duplicate();</span><br><span class="line">            <span class="comment">// 发送完成，consumer加入group成功，触发onJoinComplete()方法</span></span><br><span class="line">            onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// We reset the join group future only after the completion callback returns. This ensures</span></span><br><span class="line">            <span class="comment">// that if the callback is woken up, we will retry it on the next joinGroupIfNeeded.</span></span><br><span class="line">            <span class="comment">// 重置joinFuture为空</span></span><br><span class="line">            resetJoinGroupFuture();</span><br><span class="line">            needsJoinPrepare = <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            resetJoinGroupFuture();</span><br><span class="line">            <span class="keyword">final</span> RuntimeException exception = future.exception();</span><br><span class="line">            <span class="keyword">if</span> (exception <span class="keyword">instanceof</span> UnknownMemberIdException ||</span><br><span class="line">                    exception <span class="keyword">instanceof</span> RebalanceInProgressException ||</span><br><span class="line">                    exception <span class="keyword">instanceof</span> IllegalGenerationException ||</span><br><span class="line">                    exception <span class="keyword">instanceof</span> MemberIdRequiredException)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (!future.isRetriable())</span><br><span class="line">                <span class="keyword">throw</span> exception;</span><br><span class="line"></span><br><span class="line">            timer.sleep(retryBackoffMs);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>initiateJoinGroup ()</code> 方法的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">initiateJoinGroup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// we store the join future in case we are woken up by the user after beginning the</span></span><br><span class="line">    <span class="comment">// rebalance in the call to poll below. This ensures that we do not mistakenly attempt</span></span><br><span class="line">    <span class="comment">// to rejoin before the pending rebalance has completed.</span></span><br><span class="line">    <span class="keyword">if</span> (joinFuture == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// fence off the heartbeat thread explicitly so that it cannot interfere with the join group.</span></span><br><span class="line">        <span class="comment">// Note that this must come after the call to onJoinPrepare since we must be able to continue</span></span><br><span class="line">        <span class="comment">// sending heartbeats if that callback takes some time.</span></span><br><span class="line">        <span class="comment">// Step1:rebalance期间，心跳线程停止运行</span></span><br><span class="line">        disableHeartbeatThread();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置当前状态为rebalance</span></span><br><span class="line">        state = MemberState.REBALANCING;</span><br><span class="line">        <span class="comment">// Step2:发送joinGroup请求，核心步骤</span></span><br><span class="line">        joinFuture = sendJoinGroupRequest();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Step3:为joinGroup请求添加监听器，监听joinGroup请求的结果并做相应的处理</span></span><br><span class="line">        joinFuture.addListener(<span class="keyword">new</span> RequestFutureListener&lt;ByteBuffer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ByteBuffer value)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// handle join completion in the callback so that the callback will be invoked</span></span><br><span class="line">                <span class="comment">// even if the consumer is woken up before finishing the rebalance</span></span><br><span class="line">                <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</span><br><span class="line">                    log.info(<span class="string">&quot;Successfully joined group with generation &#123;&#125;&quot;</span>, generation.generationId);</span><br><span class="line">                    <span class="comment">// 如果joinGroup成功，设置状态为stable</span></span><br><span class="line">                    state = MemberState.STABLE;</span><br><span class="line">                    rejoinNeeded = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (heartbeatThread != <span class="keyword">null</span>)</span><br><span class="line">                        <span class="comment">// Step4:允许心跳线程继续运行</span></span><br><span class="line">                        heartbeatThread.enable();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// we handle failures below after the request finishes. if the join completes</span></span><br><span class="line">                <span class="comment">// after having been woken up, the exception is ignored and we will rejoin</span></span><br><span class="line">                <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</span><br><span class="line">                    <span class="comment">// 如果joinGroup失败，设置状态为unjoined</span></span><br><span class="line">                    state = MemberState.UNJOINED;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> joinFuture;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到在 joinGroup 之前会让心跳线程暂时停下来，此时会将 ConsumerCoordinator 的状态设置为 rebalance 状态，当 joinGroup 成功之后会将状态设置为 stable 状态，同时让之前停下来的心跳线程继续运行。</p>
<p><code>sendJoinGroupRequest ()</code> 方法的核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Join the group and return the assignment for the next generation. This function handles both</span></span><br><span class="line"><span class="comment"> * JoinGroup and SyncGroup, delegating to &#123;<span class="doctag">@link</span> #performAssignment(String, String, List)&#125; if</span></span><br><span class="line"><span class="comment"> * elected leader by the coordinator.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">NOTE:</span> This is visible only for testing</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> A request future which wraps the assignment returned from the group leader</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 发送joinGroup请求</span></span><br><span class="line"><span class="function">RequestFuture&lt;ByteBuffer&gt; <span class="title">sendJoinGroupRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (coordinatorUnknown())</span><br><span class="line">        <span class="keyword">return</span> RequestFuture.coordinatorNotAvailable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// send a join group request to the coordinator</span></span><br><span class="line">    log.info(<span class="string">&quot;(Re-)joining group&quot;</span>);</span><br><span class="line">    JoinGroupRequest.Builder requestBuilder = <span class="keyword">new</span> JoinGroupRequest.Builder(</span><br><span class="line">            <span class="keyword">new</span> JoinGroupRequestData()</span><br><span class="line">                    .setGroupId(groupId)</span><br><span class="line">                    .setSessionTimeoutMs(<span class="keyword">this</span>.sessionTimeoutMs)</span><br><span class="line">                    .setMemberId(<span class="keyword">this</span>.generation.memberId)</span><br><span class="line">                    .setGroupInstanceId(<span class="keyword">this</span>.groupInstanceId.orElse(<span class="keyword">null</span>))</span><br><span class="line">                    .setProtocolType(protocolType())</span><br><span class="line">                    .setProtocols(metadata())</span><br><span class="line">                    .setRebalanceTimeoutMs(<span class="keyword">this</span>.rebalanceTimeoutMs)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">&quot;Sending JoinGroup (&#123;&#125;) to coordinator &#123;&#125;&quot;</span>, requestBuilder, <span class="keyword">this</span>.coordinator);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Note that we override the request timeout using the rebalance timeout since that is the</span></span><br><span class="line">    <span class="comment">// maximum time that it may block on the coordinator. We add an extra 5 seconds for small delays.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> joinGroupTimeoutMs = Math.max(rebalanceTimeoutMs, rebalanceTimeoutMs + <span class="number">5000</span>);</span><br><span class="line">    <span class="keyword">return</span> client.send(coordinator, requestBuilder, joinGroupTimeoutMs)</span><br><span class="line">            .compose(<span class="keyword">new</span> JoinGroupResponseHandler());<span class="comment">// Step5:处理joinGroup请求后的response</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在发送 joinGroup 请求之后，会收到来自服务器的响应，然后针对这个响应再做一些重要的事情：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 处理发送joinGroup请求后的response的handler(同步group信息)</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinGroupResponseHandler</span> <span class="keyword">extends</span> <span class="title">CoordinatorResponseHandler</span>&lt;<span class="title">JoinGroupResponse</span>, <span class="title">ByteBuffer</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(JoinGroupResponse joinResponse, RequestFuture&lt;ByteBuffer&gt; future)</span> </span>&#123;</span><br><span class="line">        Errors error = joinResponse.error();</span><br><span class="line">        <span class="keyword">if</span> (error == Errors.NONE) &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;Received successful JoinGroup response: &#123;&#125;&quot;</span>, joinResponse);</span><br><span class="line">            sensors.joinLatency.record(response.requestLatencyMs());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (state != MemberState.REBALANCING) &#123;</span><br><span class="line">                    <span class="comment">// if the consumer was woken up before a rebalance completes, we may have already left</span></span><br><span class="line">                    <span class="comment">// the group. In this case, we do not want to continue with the sync group.</span></span><br><span class="line">                    future.raise(<span class="keyword">new</span> UnjoinedGroupException());</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    AbstractCoordinator.<span class="keyword">this</span>.generation = <span class="keyword">new</span> Generation(joinResponse.data().generationId(),</span><br><span class="line">                            joinResponse.data().memberId(), joinResponse.data().protocolName());</span><br><span class="line">                    <span class="comment">// Step6:joinGroup成功，下面需要进行sync-group，获取分配的tp列表</span></span><br><span class="line">                    <span class="keyword">if</span> (joinResponse.isLeader()) &#123;</span><br><span class="line">                        <span class="comment">// 当前consumer是leader</span></span><br><span class="line">                        onJoinLeader(joinResponse).chain(future);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 当前consumer是follower</span></span><br><span class="line">                        onJoinFollower().chain(future);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_LOAD_IN_PROGRESS) &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;Attempt to join group rejected since coordinator &#123;&#125; is loading the group.&quot;</span>, coordinator());</span><br><span class="line">            <span class="comment">// backoff and retry</span></span><br><span class="line">            future.raise(error);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.UNKNOWN_MEMBER_ID) &#123;</span><br><span class="line">            <span class="comment">// reset the member id and retry immediately</span></span><br><span class="line">            resetGeneration();</span><br><span class="line">            log.debug(<span class="string">&quot;Attempt to join group failed due to unknown member id.&quot;</span>);</span><br><span class="line">            future.raise(Errors.UNKNOWN_MEMBER_ID);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_NOT_AVAILABLE</span><br><span class="line">                || error == Errors.NOT_COORDINATOR) &#123;</span><br><span class="line">            <span class="comment">// re-discover the coordinator and retry with backoff</span></span><br><span class="line">            markCoordinatorUnknown();</span><br><span class="line">            log.debug(<span class="string">&quot;Attempt to join group failed due to obsolete coordinator information: &#123;&#125;&quot;</span>, error.message());</span><br><span class="line">            future.raise(error);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.FENCED_INSTANCE_ID) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;Received fatal exception: group.instance.id gets fenced&quot;</span>);</span><br><span class="line">            future.raise(error);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.INCONSISTENT_GROUP_PROTOCOL</span><br><span class="line">                || error == Errors.INVALID_SESSION_TIMEOUT</span><br><span class="line">                || error == Errors.INVALID_GROUP_ID</span><br><span class="line">                || error == Errors.GROUP_AUTHORIZATION_FAILED</span><br><span class="line">                || error == Errors.GROUP_MAX_SIZE_REACHED) &#123;</span><br><span class="line">            <span class="comment">// log the error and re-throw the exception</span></span><br><span class="line">            log.error(<span class="string">&quot;Attempt to join group failed due to fatal error: &#123;&#125;&quot;</span>, error.message());</span><br><span class="line">            <span class="keyword">if</span> (error == Errors.GROUP_MAX_SIZE_REACHED) &#123;</span><br><span class="line">                future.raise(<span class="keyword">new</span> GroupMaxSizeReachedException(groupId));</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</span><br><span class="line">                future.raise(<span class="keyword">new</span> GroupAuthorizationException(groupId));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                future.raise(error);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.UNSUPPORTED_VERSION) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;Attempt to join group failed due to unsupported version error. Please unset field group.instance.id and retry&quot;</span> +</span><br><span class="line">                    <span class="string">&quot;to see if the problem resolves&quot;</span>);</span><br><span class="line">            future.raise(error);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.MEMBER_ID_REQUIRED) &#123;</span><br><span class="line">            <span class="comment">// Broker requires a concrete member id to be allowed to join the group. Update member id</span></span><br><span class="line">            <span class="comment">// and send another join group request in next cycle.</span></span><br><span class="line">            <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</span><br><span class="line">                AbstractCoordinator.<span class="keyword">this</span>.generation = <span class="keyword">new</span> Generation(OffsetCommitRequest.DEFAULT_GENERATION_ID,</span><br><span class="line">                        joinResponse.data().memberId(), <span class="keyword">null</span>);</span><br><span class="line">                AbstractCoordinator.<span class="keyword">this</span>.rejoinNeeded = <span class="keyword">true</span>;</span><br><span class="line">                AbstractCoordinator.<span class="keyword">this</span>.state = MemberState.UNJOINED;</span><br><span class="line">            &#125;</span><br><span class="line">            future.raise(Errors.MEMBER_ID_REQUIRED);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// unexpected error, throw the exception</span></span><br><span class="line">            log.error(<span class="string">&quot;Attempt to join group failed due to unexpected error: &#123;&#125;&quot;</span>, error.message());</span><br><span class="line">            future.raise(<span class="keyword">new</span> KafkaException(<span class="string">&quot;Unexpected error in join group response: &quot;</span> + error.message()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面代码的主要过程如下：</p>
<ol>
<li>如果 group 是新的 <code>group.id</code>，那么此时 group 初始化的状态为 <strong>Empty</strong>；</li>
<li>当 GroupCoordinator 接收到 consumer 的 join-group 请求后，由于此时这个 group 的 member 列表还是空 (group 是新建的，每个 consumer 实例被称为这个 group 的一个 member)，<strong>第一个加入的 member 将被选为 leader</strong>，也就是说，对于一个新的 consumer group 而言，当第一个 consumer 实例加入后将会被选为 leader。如果后面 leader 挂了，会从其他 member 里面随机选择一个 member 成为新的 leader；</li>
<li>如果 GroupCoordinator 接收到 leader 发送 join-group 请求，将会触发 rebalance，group 的状态变为 <strong>PreparingRebalance</strong>；</li>
<li>此时，GroupCoordinator 将会等待一定的时间，如果在一定时间内，接收到 join-group 请求的 consumer 将被认为是依然存活的，此时 group 会变为 <strong>AwaitSync</strong> 状态，并且 GroupCoordinator 会向这个 group 的所有 member 返回其 response；</li>
<li>consumer 在接收到 GroupCoordinator 的 response 后，如果这个 consumer 是 group 的 leader，那么这个 consumer 将会负责为整个 group assign partition 订阅安排（默认是按 range 的策略，目前也可选 roundrobin），然后 leader 将分配后的信息以 <code>sendSyncGroupRequest ()</code> 请求的方式发给 GroupCoordinator，而作为 follower 的 consumer 实例会发送一个空列表；</li>
<li>GroupCoordinator 在接收到 leader 发来的请求后，会将 assign 的结果返回给所有已经发送 sync-group 请求的 consumer 实例，并且 group 的状态将会转变为 <strong>Stable</strong>，如果后续再收到 sync-group 请求，由于 group 的状态已经是 Stable，将会直接返回其分配结果。</li>
</ol>
<p>sync-group 发送请求核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 当consumer为follower时，从GroupCoordinator拉取分配结果</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">onJoinFollower</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// send follower&#x27;s sync group with an empty assignment</span></span><br><span class="line">    SyncGroupRequest.Builder requestBuilder =</span><br><span class="line">            <span class="keyword">new</span> SyncGroupRequest.Builder(</span><br><span class="line">                    <span class="keyword">new</span> SyncGroupRequestData()</span><br><span class="line">                            .setGroupId(groupId)</span><br><span class="line">                            .setMemberId(generation.memberId)</span><br><span class="line">                            .setGroupInstanceId(<span class="keyword">this</span>.groupInstanceId.orElse(<span class="keyword">null</span>))</span><br><span class="line">                            .setGenerationId(generation.generationId)</span><br><span class="line">                            .setAssignments(Collections.emptyList())</span><br><span class="line">            );</span><br><span class="line">    log.debug(<span class="string">&quot;Sending follower SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;</span>, <span class="keyword">this</span>.coordinator, requestBuilder);</span><br><span class="line">    <span class="comment">// 发送sync-group请求</span></span><br><span class="line">    <span class="keyword">return</span> sendSyncGroupRequest(requestBuilder);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当consumer客户端为leader时，对group下的所有实例进行分配，将assign的结果发送到GroupCoordinator</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">onJoinLeader</span><span class="params">(JoinGroupResponse joinResponse)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// perform the leader synchronization and send back the assignment for the group(assign 操作)</span></span><br><span class="line">        Map&lt;String, ByteBuffer&gt; groupAssignment = performAssignment(joinResponse.data().leader(), joinResponse.data().protocolName(),</span><br><span class="line">                joinResponse.data().members());</span><br><span class="line"></span><br><span class="line">        List&lt;SyncGroupRequestData.SyncGroupRequestAssignment&gt; groupAssignmentList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, ByteBuffer&gt; assignment : groupAssignment.entrySet()) &#123;</span><br><span class="line">            groupAssignmentList.add(<span class="keyword">new</span> SyncGroupRequestData.SyncGroupRequestAssignment()</span><br><span class="line">                    .setMemberId(assignment.getKey())</span><br><span class="line">                    .setAssignment(Utils.toArray(assignment.getValue()))</span><br><span class="line">            );</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        SyncGroupRequest.Builder requestBuilder =</span><br><span class="line">                <span class="keyword">new</span> SyncGroupRequest.Builder(</span><br><span class="line">                        <span class="keyword">new</span> SyncGroupRequestData()</span><br><span class="line">                                .setGroupId(groupId)</span><br><span class="line">                                .setMemberId(generation.memberId)</span><br><span class="line">                                .setGroupInstanceId(<span class="keyword">this</span>.groupInstanceId.orElse(<span class="keyword">null</span>))</span><br><span class="line">                                .setGenerationId(generation.generationId)</span><br><span class="line">                                .setAssignments(groupAssignmentList)</span><br><span class="line">                );</span><br><span class="line">        log.debug(<span class="string">&quot;Sending leader SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;</span>, <span class="keyword">this</span>.coordinator, requestBuilder);</span><br><span class="line">        <span class="comment">// 发送sync-group请求</span></span><br><span class="line">        <span class="keyword">return</span> sendSyncGroupRequest(requestBuilder);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (RuntimeException e) &#123;</span><br><span class="line">        <span class="keyword">return</span> RequestFuture.failure(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送SyncGroup请求，获取对partition分配的安排</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">sendSyncGroupRequest</span><span class="params">(SyncGroupRequest.Builder requestBuilder)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (coordinatorUnknown())</span><br><span class="line">        <span class="keyword">return</span> RequestFuture.coordinatorNotAvailable();</span><br><span class="line">    <span class="keyword">return</span> client.send(coordinator, requestBuilder)</span><br><span class="line">            .compose(<span class="keyword">new</span> SyncGroupResponseHandler());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">SyncGroupResponseHandler</span> <span class="keyword">extends</span> <span class="title">CoordinatorResponseHandler</span>&lt;<span class="title">SyncGroupResponse</span>, <span class="title">ByteBuffer</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(SyncGroupResponse syncResponse,</span></span></span><br><span class="line"><span class="function"><span class="params">                       RequestFuture&lt;ByteBuffer&gt; future)</span> </span>&#123;</span><br><span class="line">        Errors error = syncResponse.error();</span><br><span class="line">        <span class="keyword">if</span> (error == Errors.NONE) &#123;</span><br><span class="line">            <span class="comment">// sync-group成功</span></span><br><span class="line">            sensors.syncLatency.record(response.requestLatencyMs());</span><br><span class="line">            future.complete(ByteBuffer.wrap(syncResponse.data.assignment()));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// join的标志位设置为true</span></span><br><span class="line">            requestRejoin();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</span><br><span class="line">                future.raise(<span class="keyword">new</span> GroupAuthorizationException(groupId));</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.REBALANCE_IN_PROGRESS) &#123;</span><br><span class="line">                <span class="comment">// group正在进行rebalance，任务失败</span></span><br><span class="line">                log.debug(<span class="string">&quot;SyncGroup failed because the group began another rebalance&quot;</span>);</span><br><span class="line">                future.raise(error);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.FENCED_INSTANCE_ID) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;Received fatal exception: group.instance.id gets fenced&quot;</span>);</span><br><span class="line">                future.raise(error);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.UNKNOWN_MEMBER_ID</span><br><span class="line">                    || error == Errors.ILLEGAL_GENERATION) &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;SyncGroup failed: &#123;&#125;&quot;</span>, error.message());</span><br><span class="line">                resetGeneration();</span><br><span class="line">                future.raise(error);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_NOT_AVAILABLE</span><br><span class="line">                    || error == Errors.NOT_COORDINATOR) &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;SyncGroup failed: &#123;&#125;&quot;</span>, error.message());</span><br><span class="line">                markCoordinatorUnknown();</span><br><span class="line">                future.raise(error);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                future.raise(<span class="keyword">new</span> KafkaException(<span class="string">&quot;Unexpected error from SyncGroup: &quot;</span> + error.message()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个阶段主要是将分区分配方案同步给各个消费者，这个同步仍然是通过 GroupCoordinator 来转发的。</p>
<blockquote>
<p>分区策略并非由 leader 消费者来决定，而是各个消费者投票决定的，谁的票多就采用什么分区策略。这里的分区策略是通过 <code>partition.assignment.strategy</code> 参数设置的，可以设置多个。如果选举出了消费者不支持的策略，那么就会抛出异常 <code>IllegalArgumentException: Member does not support protocol</code>。</p>
</blockquote>
<p>经过上面的步骤，一个 consumer 实例就已经加入 group 成功了，加入 group 成功后，将会触发 ConsumerCoordinator 的 <code>onJoinComplete ()</code> 方法，其作用就是：更新订阅的 tp 列表、更新其对应的 metadata 及触发注册的 listener。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加入group成功</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onJoinComplete</span><span class="params">(<span class="keyword">int</span> generation,</span></span></span><br><span class="line"><span class="function"><span class="params">                              String memberId,</span></span></span><br><span class="line"><span class="function"><span class="params">                              String assignmentStrategy,</span></span></span><br><span class="line"><span class="function"><span class="params">                              ByteBuffer assignmentBuffer)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// only the leader is responsible for monitoring for metadata changes (i.e. partition changes)</span></span><br><span class="line">    <span class="keyword">if</span> (!isLeader)</span><br><span class="line">        assignmentSnapshot = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    PartitionAssignor assignor = lookupAssignor(assignmentStrategy);</span><br><span class="line">    <span class="keyword">if</span> (assignor == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Coordinator selected invalid assignment protocol: &quot;</span> + assignmentStrategy);</span><br><span class="line"></span><br><span class="line">    Assignment assignment = ConsumerProtocol.deserializeAssignment(assignmentBuffer);</span><br><span class="line">    <span class="keyword">if</span> (!subscriptions.assignFromSubscribed(assignment.partitions())) &#123;</span><br><span class="line">        handleAssignmentMismatch(assignment);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Set&lt;TopicPartition&gt; assignedPartitions = subscriptions.assignedPartitions();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The leader may have assigned partitions which match our subscription pattern, but which</span></span><br><span class="line">    <span class="comment">// were not explicitly requested, so we update the joined subscription here.</span></span><br><span class="line">    maybeUpdateJoinedSubscription(assignedPartitions);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// give the assignor a chance to update internal state based on the received assignment</span></span><br><span class="line">    assignor.onAssignment(assignment, generation);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reschedule the auto commit starting from now</span></span><br><span class="line">    <span class="keyword">if</span> (autoCommitEnabled)</span><br><span class="line">        <span class="keyword">this</span>.nextAutoCommitTimer.updateAndReset(autoCommitIntervalMs);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// execute the user&#x27;s callback after rebalance</span></span><br><span class="line">    ConsumerRebalanceListener listener = subscriptions.rebalanceListener();</span><br><span class="line">    log.info(<span class="string">&quot;Setting newly assigned partitions: &#123;&#125;&quot;</span>, Utils.join(assignedPartitions, <span class="string">&quot;, &quot;</span>));</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        listener.onPartitionsAssigned(assignedPartitions);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (WakeupException | InterruptException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        log.error(<span class="string">&quot;User provided listener &#123;&#125; failed on partition assignment&quot;</span>, listener.getClass().getName(), e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此，一个 consumer 实例算是真正上意义上加入 group 成功。</p>
<p>然后 consumer 就进入正常工作状态，同时 consumer 也通过向 GroupCoordinator 发送心跳来维持它们与消费者组的从属关系，以及它们对分区的所有权关系。只要以正常的间隔发送心跳，就被认为是活跃的，但是如果 GroupCoordinator 没有响应，那么就会发送 LeaveGroup 请求退出消费者组。</p>
<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="http://generalthink.github.io/2019/05/15/how-to-join-kafka-consumer-group/">http://generalthink.github.io/2019/05/15/how-to-join-kafka-consumer-group/</a></p>
<p><a target="_blank" rel="noopener" href="https://matt33.com/2017/10/22/consumer-join-group/">https://matt33.com/2017/10/22/consumer-join-group/</a></p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:wdshfut@163.com">wdshfut@163.com</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/30/kafka-properties/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/30/kafka-properties/" class="post-title-link" itemprop="url">Kafka 的核心配置参数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-30 15:48:10" itemprop="dateCreated datePublished" datetime="2020-10-30T15:48:10+08:00">2020-10-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-05 15:33:28" itemprop="dateModified" datetime="2021-01-05T15:33:28+08:00">2021-01-05</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Kafka-Producer-核心配置参数"><a href="#Kafka-Producer-核心配置参数" class="headerlink" title="Kafka Producer 核心配置参数"></a>Kafka Producer 核心配置参数</h2><p><strong>bootstrap.servers</strong></p>
<p>broke 服务器地址，多个服务器，用逗号隔开。</p>
<p><strong>acks</strong></p>
<p>发送应答，默认：1。</p>
<p>acks 参数指定了生产者希望 leader 返回的用于确认请求完成的确认数量，即必须要有多少个分区副本收到该消息，生产者才会认为消息写入是成功的。</p>
<p>允许以下设置：</p>
<p>acks=0：生产者将完全不等待来自服务器的任何确认。记录将立即添加到 socket 缓冲区，并被认为已发送。在这种情况下，不能保证服务器已经收到记录，重试配置将不会生效 (因为客户机通常不会知道任何失败)。响应里来自服务端的 offset 总是-1。同时，由于不需要等待响应，所以可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量。</p>
<p>acks=1：只需要集群的 leader 收到消息，生产者就会收到一个来自服务器的成功响应。leader 会将记录写到本地日志中，但不会等待所有 follower 的完全确认。在这种情况下，如果 follower 复制数据之前，leader 挂掉，数据就会丢失。</p>
<p>acks=all / -1：当所有参与复制的节点全部收到消息的时候，生产者才会收到一个来自服务器的成功响应，最安全不过延迟比较高。如果需要保证消息不丢失, 需要使用该设置，同时需要设置 broke端 <code>unclean.leader.election.enable</code> 为 true，保证当 ISR 列表为空时，选择其他存活的副本作为新的 leader。</p>
<p><strong>batch.size</strong></p>
<p>批量发送大小，默认：16384，即 16 K。</p>
<p>当有多个消息需要被发送到同一个 partition 的时候，生产者会把他们放到同一个批次里面 (Deque)，该参数指定了一个批次可以使用的内存大小，按照字节数计算，当批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定会等到批次被填满才发送，半满甚至只包含一个消息的批次也有可能被发送。</p>
<p>生产者产生的消息缓存到本地，每次批量发送 <code>batch.size</code> 大小到服务器。太小的 batch 会降低吞吐，太大则会浪费内存。</p>
<p><strong>linger.ms</strong></p>
<p>发送延迟时间，默认：0。</p>
<p>指定了生产者在发送批次之前等待更多消息加入批次的时间。生产者会在批次填满或 <code>linger.ms</code> 达到上限时把批次发送出去。把 <code>linger.ms</code> 设置成比0大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次，虽然这样会增加延迟，但也会提升吞吐量。</p>
<p>说明：<code>batch.size</code> 和 <code>linger.ms</code> 满足任何一个条件都会发送。</p>
<p><strong>buffer.memory</strong></p>
<p>生产者最大可用缓存，默认：33554432，即 32 M。</p>
<p>生产者可以用来缓冲等待发送到服务器的记录的总内存字节。如果应用程序发送消息的速度超过生产者发送消息到服务器的速度，即超出 <code>max.block.ms</code>，将会抛出异常。</p>
<p>该设置应该大致与生产者将使用的总内存相对应，但不是硬绑定，因为生产者使用的内存并非全部都用于缓冲。一些额外的内存将用于压缩 (如果启用了压缩) 以及维护飞行中的请求。</p>
<p><strong>max.block.ms</strong></p>
<p>阻塞时间，默认：60000，即 1 分钟。</p>
<p>指定了在调用 <code>send ()</code> 方法或者 <code>partitionsFor ()</code> 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 <code>max.block.ms</code> 时，就会抛出 <code>new TimeoutException(“Failed to allocate memory within the configured max blocking time “ + maxTimeToBlockMs + “ ms.”);</code>。</p>
<p>用户提供的序列化器或分区程序中的阻塞将不计入此超时。</p>
<p><strong>client.id</strong></p>
<p>生产者 ID，默认：空。</p>
<p>请求时传递给服务器的 id 字符串，用来标识消息来源，后台线程会根据它命名。这样做的目的是通过允许在服务器端请求日志中包含逻辑应用程序名称，从而能够跟踪 ip/端口之外的请求源。</p>
<p><strong>compression.type</strong></p>
<p>生产者数据被发送到服务器之前被压缩的压缩类型，默认：none，即不压缩。</p>
<p>指定给定主题的最终压缩类型。此配置接受标准压缩编解码器 (“gzip”、“snappy”、“lz4”、“zstd”)。</p>
<p>“gzip”：压缩效率高，适合高内存、CPU。</p>
<p>“snappy”：适合带宽敏感性，压缩力度大。</p>
<p><strong>retries</strong></p>
<p>失败重试次数，默认：2147483647。</p>
<p>异常是 RetriableException 类型，或者 TransactionManager 允许重试 (<code>transactionManager.canRetry ()</code> )。</p>
<p>RetriableException 类型异常如下：</p>
<p><img src="/2020/10/30/kafka-properties/843808-20181213103108421-940591020.png" alt="img"></p>
<p><strong>retry.backoff.ms</strong></p>
<p>失败请求重试的间隔时间，默认：100。</p>
<p>这避免了在某些失败场景下以紧密循环的方式重复发送请求。</p>
<p><strong>max.in.flight.requests.per.connection</strong></p>
<p>单个连接上发送的未确认请求的最大数量，默认：5。</p>
<p>阻塞前客户端在单个连接上发送的未确认请求的最大数量。即指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。</p>
<p>如果设置为 1，可以保证消息是按照发送的顺序写入服务器的，即便发生了重试。</p>
<p>如果设置大于 1，在 <code>retries</code> 不为0的情况下可能会出现消息发送顺序的错误。例如将两个批发送到同一个分区，第一个批处理失败并重试，但是第二个批处理成功，那么第二个批处理中的记录可能会先出现。</p>
<p><strong>delivery.timeout.ms</strong></p>
<p>传输时间，默认：120000，即 2 分钟。</p>
<p>生产者发送完请求接受服务器 ACK 的时间，该时间允许重试 ，该配置应该大于 <code>request.timeout.ms</code> + <code>linger.ms</code>。</p>
<p><strong>request.timeout.ms</strong></p>
<p>请求超时时间，默认：30000，即30秒。</p>
<p>配置控制客户端等待请求响应的最长时间。 如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽，则该请求将失败。 这应该大于<code>replica.lag.time.max.ms</code> (broker 端配置)，以减少由于不必要的生产者重试引起的消息重复的可能性。</p>
<p><strong>connections.max.idle.ms</strong></p>
<p>连接空闲超时时间，默认：540000，即 9 分钟。</p>
<p>在此配置指定的毫秒数之后关闭空闲连接。</p>
<p><strong>enable.idempotence</strong></p>
<p>开启幂等，默认：false。</p>
<p>如果设置为 <code>true</code> ，将开启 <code>exactly-once</code> 模式，生产者将确保在流中准确地写入每个消息的副本。如果设置为 <code>false</code>，则由于代理失败而导致生产者重试，等等，可能会在流中写入重试消息的副本。</p>
<p>注意，启用幂等需要以下条件 ：<code>max.in.flight.requests.per.connection</code> 小于或等于 5，<code>retries</code> 大于 0， <code>acks</code> 必须为 all 或者 -1。如果用户没有显式地设置这些值，将选择合适的值。如果设置了不兼容的值，就会抛出 ConfigException。</p>
<p><strong>key.serializer</strong></p>
<p>key 序列化器，默认：无。</p>
<p>需要实现接口：<code>org.apache.kafka.common. serialize .Serializer</code> 。Kafka 提供以下几个默认的 key 序列化器：</p>
<p>String：<code>org.apache.kafka.common.serialization.StringSerializer</code>。</p>
<p><strong>value.serializer</strong></p>
<p>value 序列化器，默认：无。</p>
<p>需要实现接口：<code>org.apache.kafka.common. serialize .Serializer</code>。Kafka 提供以下几个默认的 value 序列化器：</p>
<p>byte[]：<code>org.apache.kafka.common.serialization.ByteArraySerializer</code>。</p>
<p>String：<code>org.apache.kafka.common.serialization.StringSerializer</code>。</p>
<p><strong>max.request.size</strong></p>
<p>请求的最大字节大小，默认：1048576，即 1 M。</p>
<p>该参数用于控制生产者发送的请求大小，单次发送的消息大小超过 <code>max.request.size</code> 时，会抛出异常 ，如：<code>org.apache.kafka.common.errors.RecordTooLargeException: The message is 70459102 bytes when serialized which is larger than the maximum request size you have configured with the max.request.size configuration.</code>。</p>
<p>注意：broker 对可接收的消息最大值也有自己的限制 (通过 <code>message.max.bytes</code> 参数设置)，所以两边的配置最好可以匹配，避免生产者发送的消息被 broker 拒绝。</p>
<p><strong>metric.reporters</strong></p>
<p>自定义指标报告器，默认：无。</p>
<p>用作指标报告器的类的列表，需要实现接口：<code>org.apache.kafka.common.metrics.MetricsReporter</code>，该接口允许插入将在创建新度量时得到通知的类。<code>JmxReporter</code> 始终包含在注册 <code>JMX</code> 统计信息中。</p>
<p><strong>interceptor.classes</strong></p>
<p>拦截器，默认：无。</p>
<p>用作拦截器的类的列表，需要实现接口：<code>org.apache.kafka.clients.producer.ProducerInterceptor</code> 。允许将生产者接收到的记录发布到 Kafka 集群之前拦截它们 (可能还会发生突变)。</p>
<p><strong>partitioner.class</strong></p>
<p>分区策略，默认：<code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code>。</p>
<p>如果自定义分区策略，需要实现接口： <code>org.apache.kafka.clients.producer.Partitioner</code>。</p>
<p><strong>receive.buffer.bytes</strong></p>
<p>默认：32768，即 32 K。</p>
<p>指定了 TCP socket 接收数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。</p>
<p><strong>send.buffer.bytes</strong></p>
<p>默认：131072，即 128 K。</p>
<p>指定了 TCP socket 发送数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。</p>
<p><strong>transaction.timeout.ms</strong></p>
<p>事务协调器等待生产者更新事务状态的最大毫秒数，默认：60000，即 1 分钟。</p>
<p>如果超过该时间，事务协调器会终止进行中的事务。</p>
<p>如果设置的时间大于 broker 端的 <code>max.transaction.timeout.ms</code>，会抛出 <code>InvalidTransactionTimeout</code> 异常。</p>
<p><strong>transactional.id</strong></p>
<p>用于事务传递的 TransactionalId，默认：空，即不使用事务。</p>
<p>这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的 TransactionalId 的事务已经完成。如果没有提供 TransactionalId，则生产者被限制为幂等传递。 </p>
<p>注意：如果配置了 TransactionalId，则必须启用 <code>enable.idempotence</code>。</p>
<h2 id="Kafka-Consumer-核心配置参数"><a href="#Kafka-Consumer-核心配置参数" class="headerlink" title="Kafka Consumer 核心配置参数"></a>Kafka Consumer 核心配置参数</h2><p><strong>bootstrap.servers</strong></p>
<p>broke 服务器地址，多个服务器，用逗号隔开。</p>
<p><strong>enable.auto.commit</strong></p>
<p>是否开启自动提交 offset，默认：true。</p>
<p>如果为 true，consumer 的偏移量将在后台定期提交，自动提交频率通过 <code>auto.commit.interval.ms</code> 设置。</p>
<p><strong>auto.commit.interval.ms</strong></p>
<p>自动提交频率，默认：5000。</p>
<p><strong>auto.offset.reset</strong></p>
<p>初始偏移量，默认：latest。</p>
<p>如果 Kafka 中没有初始偏移量，或者服务器上不再存在当前偏移量 (例如，该数据已被删除)，该怎么处理：</p>
<p>earliest：自动重置偏移到最早的偏移。</p>
<p>latest：自动将偏移量重置为最新偏移量。</p>
<p>none：如果没有为使用者的组找到以前的偏移量，则向使用者抛出 exception。</p>
<p>anything else：向使用者抛出异常。</p>
<p><strong>client.id</strong></p>
<p>客户端 id，默认：空。</p>
<p>便于跟踪日志。</p>
<p><strong>check.crcs</strong></p>
<p>是否开启数据校验，默认：true。</p>
<p>自动检查消耗的记录的 CRC32。这确保不会发生对消息的在线或磁盘损坏。此检查增加了一些开销，因此在寻求极端性能的情况下可能禁用此检查。</p>
<p><strong>group.id</strong></p>
<p>消费者所属的群组，默认：空。</p>
<p>唯一标识用户群组，每个 partition 只会分配给同一个 group 里面的一个 consumer 来消费。</p>
<p><strong>max.poll.records</strong></p>
<p>拉取的最大记录，默认：500。</p>
<p>单次轮询调用 <code>poll ()</code> 方法能返回的记录的最大数量。</p>
<p><strong>max.poll.interval.ms</strong></p>
<p>拉取记录间隔，默认：300000，即 5 分钟。</p>
<p>使用消费者组管理时轮询调用之间的最大延迟。这为使用者在获取更多记录之前空闲的时间设置了上限。如果在此超时过期之前没有调用 <code>poll ()</code>，则认为使用者失败，组将重新平衡，以便将分区重新分配给另一个成员。</p>
<p><strong>request.timeout.ms</strong></p>
<p>请求超时时间，默认：30000 。</p>
<p>配置控制客户机等待请求响应的最长时间。如果在超时之前没有收到响应，客户端将在需要时重新发送请求，或者在重试耗尽时失败请求。</p>
<p><strong>session.timeout.ms</strong></p>
<p>consumer session 超时时间，默认：10000。</p>
<p>用于检测 worker 程序失败的超时。worker 定期发送心跳，以向代理表明其活性。如果在此会话超时过期之前代理没有接收到心跳，则代理将从组中删除。</p>
<p>注意：该值必须在 broker 端配置的 <code>group.min.session.timeout</code> 和 <code>group.max.session.timeout.ms</code> 范围之间。</p>
<p><strong>heartbeat.interval.ms</strong></p>
<p>心跳时间，默认：3000。</p>
<p>心跳是在 consumer 与 coordinator 之间进行的。心跳是确定 consumer 存活，加入或者退出 group 的有效手段。</p>
<p>这个值必须设置的小于 <code>session.timeout.ms</code> 的1/3，因为：</p>
<p>当 consumer 由于某种原因不能发 Heartbeat 到 coordinator 时，并且时间超过 <code>session.timeout.ms</code> 时，就会认为该 consumer 已退出，它所订阅的 partition 会分配到同一 group 内的其它的 consumer 上。</p>
<p><strong>connections.max.idle.ms</strong></p>
<p>连接空闲超时时间，默认：540000，即 9 分钟。</p>
<p>在此配置指定的毫秒数之后关闭空闲连接。</p>
<p><strong>key.deserializer</strong></p>
<p>key 反序列化器，默认：无。</p>
<p>需要实现接口：<code>org.apache.kafka.common.serialize.Deserializer</code>。Kafka 提供以下几个默认的 key 反序列化器：</p>
<p>String：<code>org.apache.kafka.common.serialization.StringDeserializer</code>。</p>
<p><strong>value.deserializer</strong></p>
<p>value 反序列化器，默认：无。</p>
<p>需要实现接口：<code>org.apache.kafka.common. serialize .Deserializer</code>。Kafka 提供以下几个默认的 value 反序列化器：</p>
<p>String：<code>org.apache.kafka.common.serialization.StringDeserializer</code>。</p>
<p><strong>partition.assignment.strategy</strong></p>
<p>consumer订阅分区策略，默认：<code>org.apache.kafka.clients.consumer.RangeAssignor</code>。</p>
<p>当使用组管理时，客户端将使用分区分配策略的类名在使用者实例之间分配分区所有权。</p>
<p><strong>max.partition.fetch.bytes</strong></p>
<p>一次 fetch 请求，从一个 partition 中取得的 records 的最大值，默认：1048576，即 1 M。</p>
<p>如果在从 topic 中第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。</p>
<p>broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 <code>message.max.bytes</code> 配置和 topic 端的 <code>max.message.bytes</code> 配置。</p>
<p><strong>fetch.max.bytes</strong></p>
<p>一次 fetch 请求，从一个 broker 中取得的 records 的最大值，默认：52428800，即 50 M。</p>
<p>如果在从 topic中 第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。</p>
<p>broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 <code>message.max.bytes</code> 配置 和 topic 端的 <code>max.message.bytes</code> 配置。</p>
<p><strong>fetch.min.bytes</strong></p>
<p>一次 fetch 请求，从一个 broker 中取得的 records 的最小值，默认：1。</p>
<p>如果 broker 中数据量不够的话会 wait，直到积累的数据大小满足这个条件。默认值设置为1的目的是：使得 consumer 的请求能够尽快的返回。将此设置为大于 1 的值将导致服务器等待更大数量的数据累积，这可以稍微提高服务器吞吐量，但代价是增加一些延迟。</p>
<p><strong>fetch.max.wait.ms</strong></p>
<p>拉取阻塞时间，默认：500。</p>
<p>如果没有足够的数据立即满足 <code>fetch.min.bytes</code> 提供的要求，服务器在响应 fetch 请求之前将阻塞的最长时间。</p>
<p><strong>exclude.internal.topics</strong></p>
<p>公开内部 topic，默认：true。</p>
<p>是否应该将来自内部主题 (如偏移量) 的记录公开给使用者，consumer 共享 offset。如果设置为 true，从内部主题接收记录的唯一方法是订阅它。</p>
<p><strong>isolation.level</strong></p>
<p>隔离级别，默认：read_uncommitted。</p>
<p>控制如何以事务方式读取写入的消息。如果设置为 read_committed，<code>poll ()</code> 方法将只返回已提交的事务消息。如果设置为 read_uncommitted，<code>poll ()</code> 方法将返回所有消息，甚至是已经中止的事务消息。在任何一种模式下，非事务性消息都将无条件返回。</p>
<h2 id="Kafka-Broker-核心配置参数"><a href="#Kafka-Broker-核心配置参数" class="headerlink" title="Kafka Broker 核心配置参数"></a>Kafka Broker 核心配置参数</h2><p><strong>zookeeper.connect</strong></p>
<p>zookeeper 地址，多个地址用逗号隔开。</p>
<p><strong>broker.id</strong></p>
<p>服务器的 broke id，默认：-1。</p>
<p>每一个 broker 在集群中的唯一表示，要求是正数。</p>
<p>如果未设置，将生成唯一的代理 id。为了避免 zookeeper 生成的 broke id 和用户配置的 broke id 之间的冲突，生成的代理 id 从 <code>reserve.broker.max.id</code> 开始 id + 1。</p>
<p><strong>advertised.host.name</strong></p>
<p>默认：null。</p>
<p>不赞成使用：</p>
<p>在 <code>server.properties</code> 里还有另一个参数是解决这个问题的， <code>advertised.host.name</code> 参数用来配置返回的 <code>host.name</code>值，把这个参数配置为外网 IP 地址即可。</p>
<p>这个参数默认没有启用，默认是返回的 <code>java.net.InetAddress.getCanonicalHostName()</code> 的值，在我的 mac 上这个值并不等于 hostname 的值而是返回 IP，但在 linux 上这个值就是 hostname 的值。</p>
<p><strong>advertised.listeners</strong></p>
<p>hostname 和端口注册到 zookeeper 给生产者和消费者使用的，如果没有设置，将会使用 listeners 的配置，如果 listeners 也没有配置，将使用 <code>java.net.InetAddress.getCanonicalHostName()</code> 来获取这个 hostname 和 port，对于 ipv4，基本就是 localhost 了。</p>
<p><strong>auto.create.topics.enable</strong></p>
<p>是否允许自动创建 topic，默认：true。</p>
<p>如果为 true，第一次发动消息时，允许自动创建 topic。否则，只能通过命令创建 topic。</p>
<p><strong>auto.leader.rebalance.enable</strong></p>
<p>自动 rebalance，默认：true。</p>
<p>支持自动 leader balance。如果需要，后台线程定期检查并触发 leader balance。</p>
<p><strong>background.threads</strong></p>
<p>默认：10。</p>
<p>一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改。</p>
<p><strong>compression.type</strong></p>
<p>压缩类型，默认：producer。</p>
<p>对发送的消息采取的压缩编码方式 (‘gzip’，’snappy’，’lz4’)。 ‘uncompressed’：不压缩， ‘producer’：保持 producer 本身设置的压缩编码。</p>
<p><strong>delete.topic.enable</strong></p>
<p>是否允许删除 topic，默认：true。</p>
<p>如果关闭此配置，则通过管理工具删除主题将无效。</p>
<p><strong>leader.imbalance.check.interval.seconds</strong></p>
<p>rebalance 检测频率，默认：300。</p>
<p>控制器触发分区 rebalance 检查的频率。</p>
<p><strong>leader.imbalance.per.broker.percentage</strong></p>
<p>触发 rebalance 得比率，默认：10，即 10%。</p>
<p>每个 broke 允许的 leader 不平衡比率。如果控制器超过每个 broke 的这个值，控制器将触发一个 leader balance。该值以百分比指定。</p>
<p><strong>log.dir</strong></p>
<p>保存日志数据的目录，默认：/tmp/kafka-logs。</p>
<p><strong>log.dirs</strong></p>
<p>保存日志数据的目录，默认：null。</p>
<p>可以指定多个存储路径，以逗号分隔。如果未设置，使用 <code>log.dir</code> 中设置的值。</p>
<p><strong>log.flush.interval.messages</strong></p>
<p>默认：9223372036854775807。</p>
<p>在将消息刷新到磁盘之前，日志分区上累积的消息数量。</p>
<p>log 文件 ”sync” 到磁盘之前累积的消息条数。因为磁盘 IO 操作是一个慢操作，但又是一个”数据可靠性”的必要手段。所以此参数的设置，需要在”数据可靠性”与”性能”之间做必要的权衡。</p>
<p>如果此值过大，将会导致每次 ”fsync” 的时间较长 (IO 阻塞)；如果此值过小，将会导致 ”fsync” 的次数较多，这也意味着整体的 client 请求有一定的延迟。</p>
<p>物理 server 故障，将会导致没有 fsync 的消息丢失。</p>
<p><strong>log.flush.interval.ms</strong></p>
<p>默认：null。</p>
<p>任何 topic 中的消息在刷新到磁盘之前保存在内存中的最长时间。如果没有设置，则使用 <code>log.flush.scheduler.interval.ms</code> 中的值。</p>
<p><strong>log.flush.scheduler.interval.ms</strong></p>
<p>日志刷新器检查是否需要将任何日志刷新到磁盘的频率，默认：9223372036854775807。</p>
<p><strong>log.flush.offset.checkpoint.interval.ms</strong></p>
<p>作为日志恢复点的上次刷新的持久记录的更新频率，默认：60000。</p>
<p><strong>log.retention.bytes</strong></p>
<p>删除前日志的最大大小，默认：-1。</p>
<p>topic 每个分区的最大文件大小，一个 topic 的大小限制 = 分区数 * <code>log.retention.bytes</code>。</p>
<p><strong>log.retention.hours</strong></p>
<p>日志文件最大保存时间 (小时)，默认：168，即 7 天。</p>
<p>删除日志文件之前保存它的小时数。</p>
<p><strong>log.retention.minutes</strong></p>
<p>日志文件最大保存时间 (分钟)，默认：null。</p>
<p>在删除日志文件之前保存它的分钟数，如果没有设置，则使用 <code>log.retention.hours</code> 中的值。</p>
<p><strong>log.retention.ms</strong></p>
<p>日志文件最大保存时间 (毫秒)，默认：null。</p>
<p>在删除日志文件之前保存它的毫秒数，如果没有设置，则使用 <code>log.retention.minutes</code> 中的值。如果设置为 -1，则没有时间限制。</p>
<p><strong>log.roll.hours</strong></p>
<p>新 segment 产生时间，默认：168，即 7 天。</p>
<p>即使文件没有到达 <code>log.segment.bytes</code> 设置的大小，只要文件创建时间到达此属性，也会强制创建新 segment。</p>
<p><strong>log.roll.ms</strong></p>
<p>新 segment 产生时间，默认：null。</p>
<p>如果未设置，则使用 <code>log.roll.hours</code> 中的值。</p>
<p><strong>log.segment.bytes</strong></p>
<p>单个 segment 文件的最大值，默认：1073741824，即 1 G。</p>
<p><strong>log.segment.delete.delay.ms</strong></p>
<p>segment 删除前等待时间， 默认：60000，即 1 分钟。</p>
<p><strong>message.max.bytes</strong></p>
<p>最大 batch size，默认：1048588，即 1.000011 M。</p>
<p>Kafka 允许的最大 record batch size (如果启用了压缩，则是压缩后的大小)。如果增加了这个值，并且是 0.10.2 版本之前的 consumer，那么也必须增加 consumer 的 fetch 大小，以便他们能够获取这么大的 record batch。在最新的消息格式版本中，记录总是按批进行分组，以提高效率。在以前的消息格式版本中，未压缩记录没有分组成批，这种限制只适用于单个 record。针对每个 topic，可以使用 <code>max.message.bytes</code> 设置。</p>
<p><strong>min.insync.replicas</strong></p>
<p>insync中最小副本值，默认：1。</p>
<p>当生产者将 <code>acks</code> 设置为 “all” (或 “-1”)时，<code>min.insync.replicas</code> 指定了必须确认写操作成功的最小副本数量。如果不能满足这个最小值，则生产者将抛出一个异常 (要么是 <code>NotEnoughReplicas</code>，要么是 <code>NotEnoughReplicasAfterAppend</code>)。</p>
<p>当一起使用时，<code>min.insync.replicas</code> 和 <code>ack</code> 允许你执行更大的持久性保证。一个典型的场景是创建一个复制因子为 3 的主题，设置 <code>min.insync.replicas</code> 为 2，生产者设置 <code>acks</code> 为 “all”，这将确保如果大多数副本没有收到写操作，则生产者会抛出异常。</p>
<p><strong>num.io.threads</strong></p>
<p>服务器用于处理请求的线程数，其中可能包括磁盘 I/O，默认：8。</p>
<p><strong>num.network.threads</strong></p>
<p>服务器用于接收来自网络的请求和向网络发送响应的线程数，默认：3。</p>
<p><strong>num.recovery.threads.per.data.dir</strong></p>
<p>每个数据目录在启动时用于日志恢复和在关闭时用于刷新的线程数，默认：1。</p>
<p><strong>num.replica.alter.log.dirs.threads</strong></p>
<p>可以在日志目录 (可能包括磁盘 I/O) 之间移动副本的线程数，默认：null。</p>
<p><strong>num.replica.fetchers</strong></p>
<p>从 leader 复制数据到 follower 的线程数，默认：1。</p>
<p><strong>offset.metadata.max.bytes</strong></p>
<p>与 offset 提交关联的 metadata 的最大大小，默认：4096。</p>
<p><strong>offsets.commit.timeout.ms</strong></p>
<p>offset 提交将被延迟，直到偏移量主题的所有副本收到提交或达到此超时。这类似于生产者请求超时。默认：5000。</p>
<p><strong>offsets.topic.num.partitions</strong></p>
<p>偏移量提交主题的分区数量 (部署后不应再更改)，默认：50。</p>
<p><strong>offsets.topic.replication.factor</strong></p>
<p>副本大小，默认：3。</p>
<p><strong>offsets.topic.segment.bytes</strong></p>
<p>默认104857600，即 100 M。</p>
<p>segment 映射文件 (index) 文件大小，应该保持相对较小以便加快日志压缩和缓存负载。</p>
<p><strong>queued.max.requests</strong></p>
<p>阻塞网络线程之前，允许排队的请求数，默认：500。</p>
<p><strong>replica.fetch.min.bytes</strong></p>
<p>每个 fetch 响应所需的最小字节，默认：1。</p>
<p>如果字节不够，则等待 replicaMaxWaitTimeMs。</p>
<p><strong>replica.lag.time.max.ms</strong></p>
<p>默认：30000。</p>
<p>如果 follower 没有发送任何获取请求，或者至少在这段时间没有消耗到 leader 日志的结束偏移量，那么 leader 将从 isr 中删除 follower。</p>
<p><strong>transaction.max.timeout.ms</strong></p>
<p>默认：900000，即15分钟。</p>
<p>事务执行最长时间，超时则抛出异常。</p>
<p><strong>unclean.leader.election.enable</strong></p>
<p>默认：false。</p>
<p>指示是否在最后不得已的情况下启用 ISR 集中以外的副本作为 leader，即使这样做可能导致数据丢失。</p>
<p><strong>zookeeper.connection.timeout.ms</strong></p>
<p>默认：null。</p>
<p>客户端等待与 zookeeper 建立连接的最长时间。如果未设置，则使用 <code>zookeeper.session.timeout.ms</code> 中的值。</p>
<p><strong>zookeeper.max.in.flight.requests</strong></p>
<p>默认：10。</p>
<p>阻塞之前 consumer 将发送给 zookeeper 的未确认请求的最大数量。</p>
<p><strong>group.max.session.timeout.ms</strong></p>
<p>默认：1800000，即 30 分钟。</p>
<p>注册使用者允许的最大会话超时。超时时间越长，消费者在心跳之间处理消息的时间就越多，而检测故障的时间就越长。</p>
<p><strong>group.min.session.timeout.ms</strong></p>
<p>默认：6000。</p>
<p>注册使用者允许的最小会话超时。更短的超时导致更快的故障检测，但代价是更频繁的用户心跳，这可能会耗尽 broker 资源。</p>
<p><strong>num.partitions</strong></p>
<p>每个主题的默认日志分区数量，默认：1。</p>
<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wangzhuxing/p/10111831.html#_label0_11">https://www.cnblogs.com/wangzhuxing/p/10111831.html#_label0_11</a></p>
<p><a target="_blank" rel="noopener" href="https://atbug.com/kafka-producer-config/">https://atbug.com/kafka-producer-config/</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jiecxy/article/details/53389892">https://blog.csdn.net/jiecxy/article/details/53389892</a></p>
<p>本文只整理了部分有关 Kafka 的配置，仅作参考，更多的关于 broker，topic，producer 和 consumer 的配置，请参考 <a target="_blank" rel="noopener" href="https://kafka.apache.org/documentation/">Kafka 官网</a>。</p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:wdshfut@163.com">wdshfut@163.com</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/29/kafka-consumer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/29/kafka-consumer/" class="post-title-link" itemprop="url">KafkaConsumer 消费消息的基本流程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-29 15:10:10" itemprop="dateCreated datePublished" datetime="2020-10-29T15:10:10+08:00">2020-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-05 15:33:06" itemprop="dateModified" datetime="2021-01-05T15:33:06+08:00">2021-01-05</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="如何消费数据"><a href="#如何消费数据" class="headerlink" title="如何消费数据"></a>如何消费数据</h2><p>在<a target="_blank" rel="noopener" href="https://acatsmiling.github.io/2020/10/26/kafka-producer/">上一篇文章</a>中，介绍了 KafkaProducer 如何发送数据到 Kafka，既然有数据发送，那么肯定就有数据消费，KafkaConsumer 也是 Kafka 整个体系中不可缺少的一环。</p>
<p>下面是一段创建 KafkaConsumer 的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 必须设置的属性</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.239.131:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;group1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 可选设置的属性</span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;earliest &quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;client.id&quot;</span>, <span class="string">&quot;test_client_id&quot;</span>);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 订阅主题</span></span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">&quot;test&quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 拉取数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">            records.forEach(record -&gt; System.out.printf(<span class="string">&quot;topic = %s, partition = %d, offset = %d, key = %s, value = %s%n&quot;</span>,</span><br><span class="line">                    record.topic(), record.partition(), record.offset(), record.key(), record.value()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="必须设置的属性"><a href="#必须设置的属性" class="headerlink" title="必须设置的属性"></a>必须设置的属性</h3><p>创建 KafkaConsumer 时，必须设置的属性有 4 个：</p>
<ul>
<li><p><code>bootstrap.servers</code>：连接 Kafka 集群的地址，多个地址以逗号分隔。</p>
</li>
<li><p><code>key.deserializer</code>：消息中 key 反序列化类，需要和 KafkaProducer 中 key 序列化类相对应。</p>
</li>
<li><p><code>value.deserializer</code>：消息中 value 的反序列化类，需要和 KafkaProducer 中 value 序列化类相对应。</p>
</li>
<li><p><code>group.id</code>：消费者所属消费者组的唯一标识。</p>
</li>
</ul>
<p>这里着重说一下 <code>group.id</code> 这个属性，KafkaConsumer 和 KafkaProducer 不一样，KafkaConsumer 中有一个 consumer group (消费者组)，由它来决定同一个 consumer group 中的消费者具体拉取哪个 partition 的数据，所以这里必须指定 <code>group.id</code> 属性。</p>
<h3 id="订阅和取消主题"><a href="#订阅和取消主题" class="headerlink" title="订阅和取消主题"></a>订阅和取消主题</h3><ul>
<li>使用 <code>subscribe ()</code> 方式订阅主题</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 订阅指定列表的topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span> </span>&#123;</span><br><span class="line">    subscribe(topics, <span class="keyword">new</span> NoOpConsumerRebalanceListener());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 订阅指定列表的topic，同时指定一个监听器</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern)</span> </span>&#123;</span><br><span class="line">    subscribe(pattern, <span class="keyword">new</span> NoOpConsumerRebalanceListener());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行，同时指定一个监听器</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>使用 <code>assign ()</code> 方式订阅主题和分区</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 手动将分区列表分配给consumer</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用示例 (仅作参考，<code>assign()</code> 方式的用法，应在使用时再做查询)：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitionInfoList = kafkaConsumer.partitionsFor(<span class="string">&quot;test&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (partitionInfoList != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (PartitionInfo partitionInfo : partitionInfoList) &#123;</span><br><span class="line">        kafkaConsumer.assign(Collections.singletonList(<span class="keyword">new</span> TopicPartition(partitionInfo.topic(), partitionInfo.partition())));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>取消主题的三种方式</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafkaConsumer.unsubscribe();</span><br><span class="line">kafkaConsumer.subscribe(<span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line">kafkaConsumer.assign(<span class="keyword">new</span> ArrayList&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure>

<p>上面的三行代码作用相同，都是取消订阅，其中 <code>unsubscribe ()</code> 方法即可以取消通过 <code>subscribe ()</code> 方式实现的订阅，也可以取消通过 <code>assign ()</code> 方式实现的订阅。</p>
<h3 id="拉取数据"><a href="#拉取数据" class="headerlink" title="拉取数据"></a>拉取数据</h3><p><strong>KafkaConsumer 采用的是主动拉取 broker 数据进行消费的。</strong></p>
<p>一般消息中间件存在<strong>推送</strong> (push，server 推送数据给 consumer) 和<strong>拉取</strong> (poll，consumer 主动去 server 拉取数据) 两种方式，这两种方式各有优劣。</p>
<p>如果是选择推送的方式，最大的阻碍就是 server 不清楚 consumer 的消费速度，如果 consumer 中执行的操作是比较耗时的，那么 consumer 可能会不堪重负，甚至会导致系统挂掉。</p>
<p>而采用拉取的方式则可以解决这种情况，consumer 根据自己的状态来拉取数据，可以对服务器的数据进行延迟处理。但是这种方式也有一个劣势就是 server 没有数据的时候可能会一直轮询，不过还好 KafkaConsumer 的 <code>poll ()</code> 方法有参数允许 consumer 请求在”长轮询”中阻塞，以等待数据到达 (并且可选地等待直到给定数量的字节可用以确保传输大小)。</p>
<h2 id="如何更好的消费数据"><a href="#如何更好的消费数据" class="headerlink" title="如何更好的消费数据"></a>如何更好的消费数据</h2><p>文章开头处的代码展示了我们是如何消费数据的，但是代码未免过于简单，我们测试的时候这样写没有问题，但是实际开发过程中我们并不会这样写，我们会选择更加高效的方式，这里提供两种方式供大家参考。</p>
<ul>
<li>一个 consumer group，多个 consumer，数量小于等于 partition 的数量</li>
</ul>
<p><img src="/2020/10/29/kafka-consumer/Kafka_multi_consumer.png" alt="Kafka_multi_consumer"></p>
<ul>
<li>一个 consumer，多线程处理事件</li>
</ul>
<p><img src="/2020/10/29/kafka-consumer/Kafka_multi_event_handler.png" alt="Kafka_multi_event_handler"></p>
<p>第一种方式<strong>每个 consumer 都要维护一个独立的 TCP 连接</strong>，如果 partition 数和创建 consumer 线程的数量过多，会造成不小的系统开销。但是如果处理消息足够快速，消费性能也会提升，如果慢的话就会导致消费性能降低。</p>
<p>第二种方式是采用一个 consumer，多个消息处理线程来处理消息，其实在生产中，瓶颈一般是集中在消息处理上 (因为可能会插入数据到数据库，或者请求第三方 API)，所以我们采用多个线程来处理这些消息。</p>
<p>当然可以结合第一和第二两种方式，采用多 consumer + 多个消息处理线程来消费 Kafka 中的数据，核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; consumerNum; i++) &#123;</span><br><span class="line">  <span class="comment">// 根据属性创建Consumer，并添加到consumer列表中</span></span><br><span class="line">  <span class="keyword">final</span> Consumer&lt;String, <span class="keyword">byte</span>[]&gt; consumer = consumerFactory.getConsumer(getServers(), groupId);</span><br><span class="line">  consumerList.add(consumer);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 订阅主题</span></span><br><span class="line">  consumer.subscribe(Arrays.asList(<span class="keyword">this</span>.getTopic()));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// consumer.poll()拉取数据</span></span><br><span class="line">  BufferedConsumerRecords bufferedConsumerRecords = <span class="keyword">new</span> BufferedConsumerRecords(consumer);</span><br><span class="line"></span><br><span class="line">  getExecutor().scheduleWithFixedDelay(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">long</span> startTime = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 进行消息处理</span></span><br><span class="line">      consumeEvents(bufferedConsumerRecords);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">long</span> sleepTime = intervalMillis - (System.currentTimeMillis() - startTime);</span><br><span class="line">      <span class="keyword">if</span> (sleepTime &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        Thread.sleep(sleepTime);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;, <span class="number">0</span>, <span class="number">1000</span>, TimeUnit.MILLISECONDS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>不过这种方式不能顺序处理数据，如果你的业务是顺序处理，那么第一种方式可能更适合你。所以实际生产中请根据业务选择最适合自己的方式。</p>
<h2 id="消费数据时应该考虑的问题"><a href="#消费数据时应该考虑的问题" class="headerlink" title="消费数据时应该考虑的问题"></a>消费数据时应该考虑的问题</h2><h3 id="什么是-offset？"><a href="#什么是-offset？" class="headerlink" title="什么是 offset？"></a>什么是 offset？</h3><p>在 Kafka 中无论是 KafkarPoducer 往 topic 中写数据，还是 KafkaConsumer 从 topic 中读数据，都避免不了和 offset 打交道，关于 offset 主要有以下几个概念：</p>
<p><img src="/2020/10/29/kafka-consumer/kafka-partition-offset.png" alt="Kafka Offset"></p>
<ul>
<li><strong>Last Committed Offset</strong>：consumer group 最新一次 commit 的 offset，表示这个 consumer group 已经把 Last Committed Offset 之前的数据都消费成功了。</li>
<li><strong>Current Position</strong>：consumer group 当前消费数据的 offset，也就是说，Last Committed Offset 到 Current Position 之间的数据已经拉取成功，可能正在处理，但是还未 commit。</li>
<li><strong>High Watermark</strong>：HW，已经成功备份到其他 replica 中的最新一条数据的 offset，也就是说，<strong>High Watermark 与 Log End Offset 之间的数据已经写入到该 partition 的 leader 中，但是还未完全备份到其他的 replica 中，consumer 也无法消费这部分消息。</strong></li>
<li><strong>Log End Offset</strong>：LEO，记录底层日志 (log) 中的下一条消息的 offset。对 KafkaProducer 来说，就是即将插入下一条消息的 offset。</li>
</ul>
<p>每个 Kafka 副本对象都有两个重要的属性：HW 和 LEO。注意是所有的副本，而不只是 leader 副本。关于这两者更详细解释，参考：[<a target="_blank" rel="noopener" href="https://www.cnblogs.com/huxi2b/p/7453543.html">Kafka 的 High Watermark 与 leader epoch 的讨论</a></p>
<p>对于消费者而言，我们更多时候关注的是消费完成之后如何和服务器进行消费确认，告诉服务器这部分数据我已经消费过了。</p>
<p>这里就涉及到了 2 个 offset，一个是 Current Position，一个是处理完毕向服务器确认的 Last Committed Offset。显然，异步模式下 Last Committed Offset 是落后于 Current Position 的。如果 consumer 挂掉了，那么下一次消费数据又只会从 Last Committed Offset 的位置拉取数据，就会导致数据被重复消费。</p>
<h3 id="如何选择-offset-的提交策略？"><a href="#如何选择-offset-的提交策略？" class="headerlink" title="如何选择 offset 的提交策略？"></a>如何选择 offset 的提交策略？</h3><p>Kafka 提供了三种提交 offset 的方式。</p>
<p><strong>1. 自动提交</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自动提交，默认true</span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"><span class="comment">// 设置自动每1s提交一次</span></span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br></pre></td></tr></table></figure>

<p><strong>2.手动同步提交</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafkaConsumer.commitSync();</span><br></pre></td></tr></table></figure>

<p><strong>3.手动异步提交</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafkaConsumer.commitAsync();</span><br></pre></td></tr></table></figure>

<p>上面说了，既然异步提交 offset 可能会重复消费，那么我使用同步提交是否就可以解决数据重复消费的问题呢？我只能说 too young, too sample。且看如下代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">  records.forEach(record -&gt; &#123;</span><br><span class="line">      insertIntoDB(record);</span><br><span class="line">      kafkaConsumer.commitSync();</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>很明显不行，因为 <code>insertIntoDB ()</code> 和 <code>kafkaConsumer.commitSync ()</code> 两个方法做不到原子操作，如果 <code>insertIntoDB ()</code> 成功了，但是提交 offset 的时候 KafkaConsumer 挂掉了，然后服务器重启，仍然会导致重复消费问题。</p>
<h3 id="是否需要做到不重复消费？"><a href="#是否需要做到不重复消费？" class="headerlink" title="是否需要做到不重复消费？"></a>是否需要做到不重复消费？</h3><p>只要保证处理消息和提交 offset 的操作是原子操作，就可以做到不重复消费。我们可以自己管理 committed offset，而不让 Kafka 来进行管理。</p>
<p>比如如下使用方式：</p>
<p>1.如果消费的数据刚好需要存储在数据库，那么可以把 offset 也存在数据库，就可以在一个事物中提交这两个结果，保证原子操作。</p>
<p>2.借助搜索引擎，把 offset 和数据一起放到索引里面，比如 Elasticsearch。</p>
<p>每条记录都有自己的 offset，所以如果要管理自己的 offset 还得要做下面事情：</p>
<p>1.设置 <code>enable.auto.commit</code> 为 false；</p>
<p>2.使用每个 ConsumerRecord 提供的 offset 来保存消费的位置；</p>
<p>3.在重新启动时使用 <code>seek (TopicPartition partition, long offset)</code> 恢复上次消费的位置。</p>
<p>通过上面的方式就可以在消费端实现 ”Exactly Once” 的语义，即保证只消费一次。但是是否真的需要保证不重复消费呢？这个得看具体业务，如果重复消费数据对整体有什么影响，然后再来决定是否需要做到不重复消费。</p>
<h3 id="再均衡-reblance-时怎么办？"><a href="#再均衡-reblance-时怎么办？" class="headerlink" title="再均衡 (reblance) 时怎么办？"></a>再均衡 (reblance) 时怎么办？</h3><p><strong>再均衡是指分区的所属权从一个消费者转移到另一个消费者的行为，再均衡期间，消费者组内的消费者无法读取消息。</strong>为了更精确的控制消息的消费，我们可以在订阅主题的时候，通过指定监听器的方式来设定发生再均衡动作前后的一些准备或者收尾的动作。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kafkaConsumer.subscribe(Collections.singletonList(<span class="string">&quot;test&quot;</span>), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 再均衡之前和消费者停止读取消息之后被调用</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 重新分配分区之后和消费者开始消费之前被调用</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>具体如何操作，得根据具体的业务逻辑来实现，如果消息比较重要，你可以在再均衡的时候处理 offset，如果不够重要，你可以什么都不做。</p>
<h3 id="无法消费的数据怎么办？"><a href="#无法消费的数据怎么办？" class="headerlink" title="无法消费的数据怎么办？"></a>无法消费的数据怎么办？</h3><p>可能由于你的业务逻辑有些数据没法消费，这个时候怎么办？同样的还是的看你认为这个数据有多重要或者多不重要，如果重要可以记录日志，把它存入文件或者数据库，以便于稍候进行重试或者定向分析。如果不重要就当做什么事情都没有发生好了。</p>
<h2 id="实际开发中我的处理方式"><a href="#实际开发中我的处理方式" class="headerlink" title="实际开发中我的处理方式"></a>实际开发中我的处理方式</h2><p>我开发的项目中，用到 Kafka 的其中一个地方是消息通知 (谁给你发了消息，点赞，评论等)，大概的流程就是用户在 client 端做了某些操作，就会发送数据到 Kafka，然后把这些数据进行一定的处理之后插入到 HBase 中。</p>
<p>其中采用了 N consumer thread + N Event Handler 的方式来消费数据，并采用自动提交 offset。对于无法消费的数据往往只是简单处理下，打印下日志以及消息体 (无法消费的情况非常非常少)。</p>
<p>得益于 HBase 的多 version 控制，即使是重复消费了数据也无关紧要。这样做没有去避免重复消费的问题主要是基于以下几点考虑：</p>
<p>1.重复消费的概率较低，服务器整体性能稳定。</p>
<p>2.即便是重复消费了数据，入库了 HBase，获取数据也是只有一条，不影响结果的正确性。</p>
<p>3.有更高的吞吐量。</p>
<p>4.编程简单，不用单独去处理以及保存 offset。</p>
<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="http://generalthink.github.io/2019/05/06/kafka-consumer-use/">http://generalthink.github.io/2019/05/06/kafka-consumer-use/</a></p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:wdshfut@163.com">wdshfut@163.com</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/26/kafka-producer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/26/kafka-producer/" class="post-title-link" itemprop="url">KafkaProducer 部分源码解析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-26 14:15:52" itemprop="dateCreated datePublished" datetime="2020-10-26T14:15:52+08:00">2020-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-05 15:33:19" itemprop="dateModified" datetime="2021-01-05T15:33:19+08:00">2021-01-05</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>17 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>先来看一段创建 KafkaProducer 的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// bootstrap.servers 必须设置</span></span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;192.168.239.131:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key.serializer    必须设置</span></span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// value.serializer  必须设置</span></span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// client.id</span></span><br><span class="line">        props.put(ProducerConfig.CLIENT_ID_CONFIG, <span class="string">&quot;client-0&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// retries</span></span><br><span class="line">        props.put(ProducerConfig.RETRIES_CONFIG, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// acks</span></span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">&quot;all&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// max.in.flight.requests.per.connection</span></span><br><span class="line">        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// linger.ms</span></span><br><span class="line">        props.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// batch.size</span></span><br><span class="line">        props.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">10240</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// buffer.memory</span></span><br><span class="line">        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">10240</span>);</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定topic，key，value</span></span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;test1&quot;</span>, <span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 异步发送</span></span><br><span class="line">        kafkaProducer.send(record, (recordMetadata, exception) -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 发送失败的处理逻辑</span></span><br><span class="line">                exception.printStackTrace();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 发送成功的处理逻辑</span></span><br><span class="line">                System.out.println(recordMetadata.topic());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 同步发送</span></span><br><span class="line">        <span class="comment">// kafkaProducer.send(record).get();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭Producer</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="主要流程图"><a href="#主要流程图" class="headerlink" title="主要流程图"></a>主要流程图</h2><p><img src="/2020/10/26/kafka-producer/image-20201026150837966.png"></p>
<p>简要说明：</p>
<p>1.<code>new KafkaProducer ()</code> 后，创建一个后台线程 KafkaThread (实际运行线程是 Sender，KafkaThread 是对 Sender 的封装) 扫描 RecordAccumulator 中是否有消息；</p>
<p>2.调用 <code>kafkaProducer.send ()</code> 发送消息，实际是将消息保存到 RecordAccumulator 中，实际上就是保存到一个 Map 中 (<code>ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;</code>)，这条消息会被记录到同一个记录批次 (相同主题相同分区算同一个批次) 里面，这个批次的所有消息会被发送到相同的主题和分区上；</p>
<p>3.后台的独立线程扫描到 RecordAccumulator 中有消息后，会将消息发送到 Kafka 集群中 (不是一有消息就发送，而是要看消息是否 ready)；</p>
<p>4.如果发送成功 (消息成功写入 Kafka)，就返回一个 RecordMetaData 对象，它包换了主题和分区信息，以及记录在分区里的偏移量等信息；</p>
<p>5.如果写入失败，就会返回一个错误，生产者在收到错误之后会尝试重新发送消息 (如果允许的话，此时会将消息在保存到 RecordAccumulator 中)，达到重试次数之后如果还是失败就返回错误消息。</p>
<h2 id="缓存器的创建"><a href="#缓存器的创建" class="headerlink" title="缓存器的创建"></a>缓存器的创建</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.accumulator = <span class="keyword">new</span> RecordAccumulator(logContext,</span><br><span class="line">                    config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),</span><br><span class="line">                    <span class="keyword">this</span>.compressionType,</span><br><span class="line">                    lingerMs(config),</span><br><span class="line">                    retryBackoffMs,</span><br><span class="line">                    deliveryTimeoutMs,</span><br><span class="line">                    metrics,</span><br><span class="line">                    PRODUCER_METRIC_GROUP_NAME,</span><br><span class="line">                    time,</span><br><span class="line">                    apiVersions,</span><br><span class="line">                    transactionManager,</span><br><span class="line">                    <span class="keyword">new</span> BufferPool(<span class="keyword">this</span>.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));</span><br></pre></td></tr></table></figure>

<h2 id="后台线程的创建"><a href="#后台线程的创建" class="headerlink" title="后台线程的创建"></a>后台线程的创建</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.sender = newSender(logContext, kafkaClient, <span class="keyword">this</span>.metadata);</span><br><span class="line">String ioThreadName = NETWORK_THREAD_PREFIX + <span class="string">&quot; | &quot;</span> + clientId;</span><br><span class="line"><span class="keyword">this</span>.ioThread = <span class="keyword">new</span> KafkaThread(ioThreadName, <span class="keyword">this</span>.sender, <span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">this</span>.ioThread.start();</span><br><span class="line"></span><br><span class="line">KafkaClient client = kafkaClient != <span class="keyword">null</span> ? kafkaClient : <span class="keyword">new</span> NetworkClient(</span><br><span class="line">                <span class="keyword">new</span> Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),</span><br><span class="line">                        <span class="keyword">this</span>.metrics, time, <span class="string">&quot;producer&quot;</span>, channelBuilder, logContext),</span><br><span class="line">                metadata,</span><br><span class="line">                clientId,</span><br><span class="line">                maxInflightRequests,</span><br><span class="line">                producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),</span><br><span class="line">                producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),</span><br><span class="line">                producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),</span><br><span class="line">                producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),</span><br><span class="line">                requestTimeoutMs,</span><br><span class="line">                ClientDnsLookup.forConfig(producerConfig.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)),</span><br><span class="line">                time,</span><br><span class="line">                <span class="keyword">true</span>,</span><br><span class="line">                apiVersions,</span><br><span class="line">                throttleTimeSensor,</span><br><span class="line">                logContext);</span><br></pre></td></tr></table></figure>

<p>上述代码中，构造了一个 KafkaClient 负责和 broker 通信，同时构造一个 Sender 并启动一个异步线程，这个线程会被命名为：<code>kafka-producer-network-thread | $&#123;clientId&#125;</code>，如果你在创建 producer 的时候指定 <code>client.id</code> 的值为 myclient，那么线程名称就是 kafka-producer-network-thread | myclient。</p>
<h2 id="发送消息-缓存消息"><a href="#发送消息-缓存消息" class="headerlink" title="发送消息 (缓存消息)"></a>发送消息 (缓存消息)</h2><p>发送消息有同步发送以及异步发送两种方式，我们一般不使用同步发送，毕竟太过于耗时，使用异步发送的时候可以指定回调函数，当消息发送完成的时候 (成功或者失败) 会通过回调通知生产者。</p>
<p>同步 send：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> send(record, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>异步 send：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// intercept the record, which can be potentially modified; this method does not throw exceptions</span></span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptedRecord = <span class="keyword">this</span>.interceptors.onSend(record);</span><br><span class="line">    <span class="keyword">return</span> doSend(interceptedRecord, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，同步和异步实际上调用的是同一个方法，同步发送时，设置回调函数为 null。</p>
<p>消息发送之前，会先对 key 和 value 进行序列化：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] serializedKey;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">&quot;Can&#x27;t convert key of class &quot;</span> + record.key().getClass().getName() +</span><br><span class="line">                                     <span class="string">&quot; to class &quot;</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                                     <span class="string">&quot; specified in key.serializer&quot;</span>, cce);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">byte</span>[] serializedValue;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">&quot;Can&#x27;t convert value of class &quot;</span> + record.value().getClass().getName() +</span><br><span class="line">                                     <span class="string">&quot; to class &quot;</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                                     <span class="string">&quot; specified in value.serializer&quot;</span>, cce);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>计算分区：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br></pre></td></tr></table></figure>

<p>发送消息，实际上是将消息缓存起来，核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                    serializedValue, headers, interceptCallback, remainingWaitMs);</span><br></pre></td></tr></table></figure>

<p>RecordAccumulator 的核心数据结构是 <code>ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;</code>，会将相同 topic 相同 partition 的数据放到一个 Deque (双向队列) 中，这也是我们之前提到的同一个记录批次里面的消息会发送到同一个主题和分区的意思。<code>append ()</code> 方法的核心源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从batchs(ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)中，根据主题分区获取对应的队列，如果没有则new ArrayDeque&lt;&gt;返回</span></span><br><span class="line">Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算同一个记录批次占用空间大小，batchSize根据batch.size参数决定</span></span><br><span class="line"><span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(</span><br><span class="line">    maxUsableMagic, compression, key, value, headers));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为同一个topic，partition分配buffer，如果同一个记录批次的内存不足，那么会阻塞maxTimeToBlock(max.block.ms参数)这么长时间</span></span><br><span class="line">ByteBuffer buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line"></span><br><span class="line"><span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">  <span class="comment">// 创建MemoryRecordBuilder，通过buffer初始化appendStream(DataOutputStream)属性</span></span><br><span class="line">  MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">  ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将key，value写入到MemoryRecordsBuilder中的appendStream(DataOutputStream)中</span></span><br><span class="line">  FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将需要发送的消息放入到队列中</span></span><br><span class="line">  dq.addLast(batch);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="发送消息到-Kafka"><a href="#发送消息到-Kafka" class="headerlink" title="发送消息到 Kafka"></a>发送消息到 Kafka</h2><p>上面已经将消息存储 RecordAccumulator 中去了，现在看看怎么发送消息。前面提到创建 KafkaProducer 的时候，会启动一个异步线程去从 RecordAccumulator 中取得消息然后发送到 Kafka，发送消息的核心代码在 Sender 中，它实现了 Runnable 接口并在后台一直运行处理发送请求并将消息发送到合适的节点，直到 KafkaProducer 被关闭。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata</span></span><br><span class="line"><span class="comment"> * requests to renew its view of the cluster and then sends produce requests to the appropriate nodes.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sender</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The main run loop for the sender thread</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    	<span class="comment">// main loop, runs until close is called</span></span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                runOnce();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;Uncaught error in kafka producer I/O thread: &quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// okay we stopped accepting requests but there may still be</span></span><br><span class="line">        <span class="comment">// requests in the transaction manager, accumulator or waiting for acknowledgment,</span></span><br><span class="line">        <span class="comment">// wait until these are completed.</span></span><br><span class="line">        <span class="keyword">while</span> (!forceClose &amp;&amp; ((<span class="keyword">this</span>.accumulator.hasUndrained() || <span class="keyword">this</span>.client.inFlightRequestCount() &gt; <span class="number">0</span>) || hasPendingTransactionalRequests())) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                runOnce();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;Uncaught error in kafka producer I/O thread: &quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Abort the transaction if any commit or abort didn&#x27;t go through the transaction manager&#x27;s queue</span></span><br><span class="line">        <span class="keyword">while</span> (!forceClose &amp;&amp; transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.hasOngoingTransaction()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!transactionManager.isCompleting()) &#123;</span><br><span class="line">                log.info(<span class="string">&quot;Aborting incomplete transaction due to shutdown&quot;</span>);</span><br><span class="line">                transactionManager.beginAbort();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                runOnce();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;Uncaught error in kafka producer I/O thread: &quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (forceClose) &#123;</span><br><span class="line">            <span class="comment">// We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on</span></span><br><span class="line">            <span class="comment">// the futures.</span></span><br><span class="line">            <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;Aborting incomplete transactional requests due to forced shutdown&quot;</span>);</span><br><span class="line">                transactionManager.close();</span><br><span class="line">            &#125;</span><br><span class="line">            log.debug(<span class="string">&quot;Aborting incomplete batches due to forced shutdown&quot;</span>);</span><br><span class="line">            <span class="keyword">this</span>.accumulator.abortIncompleteBatches();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>KafkaProducer 的关闭方法有2个：<code>close ()</code> 以及 <code>close (Duration timeout)</code>，<code>close (long timeout, TimeUnit timUnit)</code> 已被弃用，其中 timeout 参数的意思是等待生产者完成任何待处理请求的最长时间，第一种方式的 timeout 为 <code>Long.MAX_VALUE</code> 毫秒，如果采用第二种方式关闭，当 timeout = 0 的时候则表示强制关闭，直接关闭 Sender (设置 running = false)。</p>
<p>Send 中，<code>runOnce ()</code> 方法，跳过对 transactionManager 的处理，查看发送消息的主要流程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> currentTimeMs = time.milliseconds();</span><br><span class="line"><span class="comment">// 将记录批次转移到每个节点的生产请求列表中</span></span><br><span class="line"><span class="keyword">long</span> pollTimeout = sendProducerData(currentTimeMs);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 轮询进行消息发送</span></span><br><span class="line">client.poll(pollTimeout, currentTimeMs);</span><br></pre></td></tr></table></figure>

<p>首先，查看 <code>sendProducerData (currentTimeMs)</code> 方法，它的核心逻辑在 <code>sendProduceRequest (batches, now)</code> 方法中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (ProducerBatch batch : batches) &#123;</span><br><span class="line">    TopicPartition tp = batch.topicPartition;</span><br><span class="line">    <span class="comment">// 将ProducerBatch中MemoryRecordsBuilder转换为MemoryRecords(发送的数据就在这里面)</span></span><br><span class="line">    MemoryRecords records = batch.records();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// down convert if necessary to the minimum magic used. In general, there can be a delay between the time</span></span><br><span class="line">    <span class="comment">// that the producer starts building the batch and the time that we send the request, and we may have</span></span><br><span class="line">    <span class="comment">// chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use</span></span><br><span class="line">    <span class="comment">// the new message format, but found that the broker didn&#x27;t support it, so we need to down-convert on the</span></span><br><span class="line">    <span class="comment">// client before sending. This is intended to handle edge cases around cluster upgrades where brokers may</span></span><br><span class="line">    <span class="comment">// not all support the same message format version. For example, if a partition migrates from a broker</span></span><br><span class="line">    <span class="comment">// which is supporting the new magic version to one which doesn&#x27;t, then we will need to convert.</span></span><br><span class="line">    <span class="keyword">if</span> (!records.hasMatchingMagic(minUsedMagic))</span><br><span class="line">        records = batch.records().downConvert(minUsedMagic, <span class="number">0</span>, time).records();</span><br><span class="line">    produceRecordsByPartition.put(tp, records);</span><br><span class="line">    recordsByPartition.put(tp, batch);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout,</span><br><span class="line">                produceRecordsByPartition, transactionalId);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 消息发送完成时的回调(消息发送失败后，在handleProduceResponse中处理)</span></span><br><span class="line">RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>&#123;</span><br><span class="line">        handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据参数构造ClientRequest，此时需要发送的消息在requestBuilder中</span></span><br><span class="line">ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>,</span><br><span class="line">                requestTimeoutMs, callback);</span><br><span class="line"><span class="comment">// 将clientRequest转换成Send对象(Send.java，包含了需要发送数据的buffer)，给KafkaChannel设置该对象，记住这里还没有发送数据</span></span><br><span class="line">client.send(clientRequest, now);</span><br></pre></td></tr></table></figure>

<p>在没有指定 KafkaClient 时，<code>client.send (clientRequest, now)</code> 方法，实际就是 <code>NetworkClient.send (ClientRequest request, long now)</code> 方法，所有的请求 (无论是 producer 发送消息的请求，还是获取 metadata 的请求) 都是通过该方法设置对应的 Send 对象：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Send send = request.toSend(destination, header);</span><br></pre></td></tr></table></figure>

<p>需要知道的是，上面只是设置了发送消息所需要准备的内容。</p>
<p>接下来，查看 <code>client.poll (pollTimeout, currentTimeMs)</code> 方法，进入到发送消息的主流程，发送消息的核心代码最终可以定位到 Selector 的 <code>pollSelectionKeys (Set&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos)</code> 方法中，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* if channel is ready write to any sockets that have space in their buffer and for which we have data */</span></span><br><span class="line"><span class="keyword">if</span> (channel.ready() &amp;&amp; key.isWritable() &amp;&amp; !channel.maybeBeginClientReauthentication(</span><br><span class="line">    () -&gt; channelStartTimeNanos != <span class="number">0</span> ? channelStartTimeNanos : currentTimeNanos)) &#123;</span><br><span class="line">    Send send;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 底层实际调用的是java8 GatheringByteChannel的write方法</span></span><br><span class="line">        send = channel.write();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        sendFailed = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (send != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>.completedSends.add(send);</span><br><span class="line">        <span class="keyword">this</span>.sensors.recordBytesSent(channel.id(), send.size());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>就这样，我们的消息就发送到了 broker 中了，发送流程分析完毕，注意，这个是完美发送的情况。但是总会有发送失败的时候 (消息过大或者没有可用的 leader 等)，那么发送失败后重发又是在哪里完成的呢？还记得上面的回调函数吗，没错，就是在回调函数这里设置的，先来看下回调函数源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Handle a produce response</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleProduceResponse</span><span class="params">(ClientResponse response, Map&lt;TopicPartition, ProducerBatch&gt; batches, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    RequestHeader requestHeader = response.requestHeader();</span><br><span class="line">    <span class="keyword">long</span> receivedTimeMs = response.receivedTimeMs();</span><br><span class="line">    <span class="keyword">int</span> correlationId = requestHeader.correlationId();</span><br><span class="line">    <span class="keyword">if</span> (response.wasDisconnected()) &#123;</span><br><span class="line">        <span class="comment">// 如果是网络断开则构造Errors.NETWORK_EXCEPTION的响应</span></span><br><span class="line">        <span class="keyword">for</span> (ProducerBatch batch : batches.values())</span><br><span class="line">            completeBatch(batch, <span class="keyword">new</span> ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION), correlationId, now, <span class="number">0L</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (response.versionMismatch() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果是版本不匹配，则构造Errors.UNSUPPORTED_VERSION的响应</span></span><br><span class="line">        <span class="keyword">for</span> (ProducerBatch batch : batches.values())</span><br><span class="line">            completeBatch(batch, <span class="keyword">new</span> ProduceResponse.PartitionResponse(Errors.UNSUPPORTED_VERSION), correlationId, now, <span class="number">0L</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// if we have a response, parse it(如果存在response就返回正常的response)</span></span><br><span class="line">        <span class="keyword">if</span> (response.hasResponse()) &#123;</span><br><span class="line">            ProduceResponse produceResponse = (ProduceResponse) response.responseBody();</span><br><span class="line">            <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, ProduceResponse.PartitionResponse&gt; entry : produceResponse.responses().entrySet()) &#123;</span><br><span class="line">                TopicPartition tp = entry.getKey();</span><br><span class="line">                ProduceResponse.PartitionResponse partResp = entry.getValue();</span><br><span class="line">                ProducerBatch batch = batches.get(tp);</span><br><span class="line">                completeBatch(batch, partResp, correlationId, now, receivedTimeMs + produceResponse.throttleTimeMs());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.sensors.recordLatency(response.destination(), response.requestLatencyMs());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// this is the acks = 0 case, just complete all requests(如果acks=0，那么则构造Errors.NONE的响应，因为这种情况只需要发送不需要响应结果)</span></span><br><span class="line">            <span class="keyword">for</span> (ProducerBatch batch : batches.values()) &#123;</span><br><span class="line">                completeBatch(batch, <span class="keyword">new</span> ProduceResponse.PartitionResponse(Errors.NONE), correlationId, now, <span class="number">0L</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 <code>completeBatch ()</code> 方法中我们主要关注失败的逻辑处理，核心源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Complete or retry the given batch of records.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> batch The record batch</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> response The produce response</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> correlationId The correlation id for the request</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> now The current POSIX timestamp in milliseconds</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">completeBatch</span><span class="params">(ProducerBatch batch, ProduceResponse.PartitionResponse response, <span class="keyword">long</span> correlationId,</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="keyword">long</span> now, <span class="keyword">long</span> throttleUntilTimeMs)</span> </span>&#123;</span><br><span class="line">    Errors error = response.error;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (error == Errors.MESSAGE_TOO_LARGE &amp;&amp; batch.recordCount &gt; <span class="number">1</span> &amp;&amp; !batch.isDone() &amp;&amp;</span><br><span class="line">        (batch.magic() &gt;= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) &#123;</span><br><span class="line">        <span class="comment">// If the batch is too large, we split the batch and send the split batches again. We do not decrement</span></span><br><span class="line">        <span class="comment">// the retry attempts in this case.(如果发送的消息太大，需要重新进行分割发送)</span></span><br><span class="line">        <span class="keyword">this</span>.accumulator.splitAndReenqueue(batch);</span><br><span class="line">        maybeRemoveAndDeallocateBatch(batch);</span><br><span class="line">        <span class="keyword">this</span>.sensors.recordBatchSplit();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error != Errors.NONE) &#123;</span><br><span class="line">        <span class="comment">// 发生了错误，如果此时可以retry(retry次数未达到限制以及产生的异常是RetriableException)</span></span><br><span class="line">        <span class="keyword">if</span> (canRetry(batch, response, now)) &#123;</span><br><span class="line">            <span class="keyword">if</span> (transactionManager == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 把需要重试的消息放入队列中，等待重试，实际就是调用deque.addFirst(batch)</span></span><br><span class="line">                reenqueueBatch(batch, now);</span><br><span class="line">            &#125; </span><br><span class="line">            ...</span><br><span class="line">        &#125; </span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>以上，就是 KafkaProducer 发送消息的流程。</p>
<h2 id="补充：分区算法"><a href="#补充：分区算法" class="headerlink" title="补充：分区算法"></a>补充：分区算法</h2><p>在发送消息前，调用的计算分区方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * computes partition for given record.</span></span><br><span class="line"><span class="comment"> * if the record has partition returns the value otherwise</span></span><br><span class="line"><span class="comment"> * calls configured partitioner class to compute the partition.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">        partition :</span><br><span class="line">    partitioner.partition(</span><br><span class="line">        record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果在创建 ProducerRecord 的时候，指定了 partition，则使用指定的，否则调用配置的 partitioner 类来计算分区。</p>
<p>如果没有配置自定义的分区器，Kafka 默认使用 <code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code>，源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The default partitioning strategy:</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If a partition is specified in the record, use it</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic The topic name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 如果key为null，则使用Round Robin算法</span></span><br><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// hash the keyBytes to choose a partition(根据key进行散列，使用murmur2算法)</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123;</span><br><span class="line">            counter = <span class="keyword">new</span> AtomicInteger(ThreadLocalRandom.current().nextInt());</span><br><span class="line">            AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">            <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">                counter = currentCounter;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>DefaultPartitioner 中对于分区的算法有两种情况：</p>
<p>1.如果键值为 null，那么记录键随机地发送到主题内各个可用的分区上。分区器使用轮询 (Round Robin) 算法键消息均衡地分布到各个分区上。</p>
<p>2.如果键不为 null，那么 Kafka 会对键进行散列 (使用 Kafka 自己的散列算法，即使升级 java 版本，散列值也不会发生变化) ，然后根据散列值把消息映射到特定的分区上。应该注意的是，同一个键总是被映射到同一个分区上 (如果分区数量发生了变化则不能保证)，映射的时候会使用主题所有的分区，而不仅仅是可用分区，所以如果写入数据分区是不可用的，那么就会发生错误，当然这种情况很少发生。</p>
<p>当然，如果你想要实现自定义分区，那么只需要实现 Partitioner 接口即可：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将key的hash值，对分区总数取余，以确定消息发送到哪个分区</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        Integer numPartitions = cluster.partitionCountForTopic(topic);</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> InvalidRecordException(<span class="string">&quot;key can not be null&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后，使用 <code>partitioner.class</code> 参数，指定你自定义的分区器的路径：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">&quot;partitioner.class&quot;</span>, <span class="string">&quot;cn.xisun.partitioner.KeyPartitioner&quot;</span>);</span><br></pre></td></tr></table></figure>

<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/">https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/</a></p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:wdshfut@163.com">wdshfut@163.com</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/23/kafka-introduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/23/kafka-introduce/" class="post-title-link" itemprop="url">什么是 Kafka</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-23 15:58:39" itemprop="dateCreated datePublished" datetime="2020-10-23T15:58:39+08:00">2020-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-05 15:33:11" itemprop="dateModified" datetime="2021-01-05T15:33:11+08:00">2021-01-05</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h2><p>分布式系统由多个运行的计算机系统组成，所有这些计算机在一个集群中一起工作，对终端用户来讲只是一个单一节点。</p>
<p><img src="/2020/10/23/kafka-introduce/distributed_system.png" alt="分布式系统"></p>
<p>Kafka 也是分布式的，因为它在不同的节点 (又被称为 broker) 上存储，接受以及发送消息，这样做的好处是具有很高的可扩展性和容错性。</p>
<h2 id="水平可扩展性"><a href="#水平可扩展性" class="headerlink" title="水平可扩展性"></a>水平可扩展性</h2><p>在这之前，先看看什么是垂直可扩展，比如你有一个传统的数据库服务器，它开始过度负载，解决这个问题的办法就是给服务器加配置 (cpu，内存，SSD)，这就叫做垂直扩展。但是这种方式存在两个巨大的劣势：</p>
<p>1.硬件存在限制，不可能无限的添加机器配置。</p>
<p>2.它需要停机时间，通常这是很多公司无法容忍的。</p>
<p>水平可扩展就是通过添加更多的机器来解决同样的问题，添加新机器不需要停机，而且集群中也不会对机器的数量有任何的限制。但问题在于并非所有系统都支持水平可伸缩性，因为它们不是设计用于集群中 (在集群中工作会更加复杂)。</p>
<h2 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h2><p>非分布式系统中容易最致命的问题就是单点失败，如果你唯一的服务器挂掉了，那么我相信你会很崩溃。</p>
<p>而分布式系统的设计方式就是可以以配置的方式来容许失败。比如在 5 个节点的 Kafka 集群中，即使其中两个节点挂掉了，你仍然可以继续工作。</p>
<p>需要注意的是，容错与性能直接相关，你的系统容错程度越高，性能就越差。</p>
<h2 id="提交日志-commit-log"><a href="#提交日志-commit-log" class="headerlink" title="提交日志 (commit log)"></a>提交日志 (commit log)</h2><p>提交日志 (也被称为预写日志或者事物日志) 是仅支持附加的持久有序数据结构，你无法修改或者删除记录，它从左往右读并且保证日志的顺序。</p>
<p><img src="/2020/10/23/kafka-introduce/commint_log.png" alt="commit log"></p>
<p>是不是觉得 Kafka 的数据结构如此简单?</p>
<p>是的，从很多方面来讲，这个数据结构就是 Kafka 的核心。这个数据结构的记录是有序的，而有序的数据可以确保我们的处理流程。这两个在分布式系统中都是极其重要的问题。</p>
<p>Kafka 实际上将所有消息存储到磁盘并在数据结构中对它们进行排序，以便利用顺序磁盘读取。</p>
<p>1.读取和写入都是常量时间 O(1) (当确定了 record id)，与磁盘上其他结构的 O(log N)操作相比是一个巨大的优势，因为每个磁盘搜索都很耗时。</p>
<p>2.读取和写入不会相互影响，写不会锁住读，反之亦然。</p>
<p>这两点有着巨大的优势， 因为数据大小与性能完全分离。无论你的服务器上有 100 KB 还是 100 TB 的数据，Kafka 都具有相同的性能。</p>
<h2 id="如何工作"><a href="#如何工作" class="headerlink" title="如何工作"></a>如何工作</h2><p>生产者消费者模式：生产者 (producer) 发送消息 (record) 到 Kafka 服务器 (broker)，这些消息存储在主题 (topic) 中，然后消费者 (consumer) 订阅该主题，接受新消息后并进行处理。</p>
<p><img src="/2020/10/23/kafka-introduce/work_model.png" alt="工作模式"></p>
<p>随着消息的越来越多，topic 也会越来越大，为了获得更好的性能和可伸缩性，可以在 topic 下建立多个更小的分区 (partition)，在发送消息时，可以根据实际情况，对消息进行分类，同一类的消息发送到同一个 partition (比如存储不同用户发送的消息，可以根据用户名的首字母进行分区匹配)。Kafka 保证 partition 内的所有消息都按照它们的顺序排序，区分特定消息的方式是通过其偏移量 (offset)，你可以将其视为普通数组索引，即为分区中的每个新消息递增的序列号。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_partition.png" alt="分区"></p>
<p>Kafka 遵守着愚蠢的 broker 和聪明的 consumer 的准则。这意味着 Kafka 不会跟踪消费者读取了哪些记录并删除它们，而是会将它们存储一定的时间 (比如 1 天，以 log.retention 开头的来决定日志保留时间)，直到达到某个阈值。消费者自己轮询 Kafka 的新消息并且告诉它自己想要读取哪些记录，这允许它们按照自己的意愿递增/递减它们所处的偏移量，从而能够重放和重新处理事件。</p>
<p>需要注意的是消费者是属于消费者组的 (在创建 consumer 时，必须指定其所属的消费者组的 <code>group.id</code>)，消费者组有一个或多个消费者。为了避免两个进程读取同样的消息两次，每个 partition 只能被一个消费者组中的一个消费者访问。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_consum_data.png" alt="kafka消费数据"></p>
<h2 id="持久化到硬盘"><a href="#持久化到硬盘" class="headerlink" title="持久化到硬盘"></a>持久化到硬盘</h2><p>正如之前提到的，Kafka 实际上是将所有记录存储到硬盘而不在 RAM 中保存任何内容，这背后有很多优化使得这个方案可行。</p>
<p>1.Kafka 有一个将消息分组的协议，这允许网络请求将消息组合在一起并减少网络开销，服务器反过来一次性保留大量消息，消费者一次获取大量线性块。</p>
<p>2.磁盘上线性读写非常快，现代磁盘非常慢的原因是由于大量磁盘寻址，但是在大量的线性操作中不是问题。</p>
<p>3.操作系统对线性操作进行了大量优化，通过预读 (预取大块多次) 和后写 (将小型逻辑写入组成大型物理写入) 技术。</p>
<p>4.操作系统将磁盘文件缓存在空闲 RAM 中。这称为 page cache，而 Kafka 的读写都大量使用了 page cache：</p>
<p>​    ① 写消息的时候消息先从 java 到 page cache，然后异步线程刷盘，消息从 page cache 刷入磁盘；</p>
<p>​    ② 读消息的时候先从 page cache 找，有就直接转入 socket，没有就先从磁盘 load 到 page cache，然后直接从 socket 发出去。</p>
<p>5.由于 Kafka 在整个流程 (producer → broker → consumer) 中以未经修改的标准化二进制格式存储消息，因此它可以使用零拷贝优化。那时操作系统将数据从 page cache 直接复制到 socket，有效地完全绕过了 Kafka broker。</p>
<p>所有这些优化都使 Kafka 能够以接近网络的速度传递消息。</p>
<h2 id="数据分发和复制"><a href="#数据分发和复制" class="headerlink" title="数据分发和复制"></a>数据分发和复制</h2><p>下面来谈谈 Kafka 如何实现容错以及它如何在节点之间分配数据。</p>
<p>为了使得一个 broker 挂掉的时候，数据还能得以保留，分区 (partition) 数据在多个 broker 中复制。</p>
<p>在任何时候，一个 broker 拥有一个 partition，应用程序读取/写入都要通过这个节点，这个节点叫做 partition leader。它将收到的数据复制到 N 个其他 broker，这些接收数据的 broker 叫做 follower，follower 也存储数据，一旦 leader 节点死掉的时候，它们就准备竞争上岗成为 leader。</p>
<p>这可以保证你成功发布的消息不会丢失，通过选择更改副本因子，你可以根据数据的重要性来交换性能以获得更强的持久性保证。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_data_backup.png" alt="4个Kafka broker,副本因子是3"></p>
<p>这样如果 leader 挂掉了，那么其中一个 follower 就会接替它称为 leader。包括 leader 在内的总副本数就是副本因子 (创建 topic 时，使用 <code>--replication-factor</code> 参数指定)，上图有 1 个 leader，2 个 follower，所以副本因子就是 3。</p>
<p>但是你可能会问：producer 或者 consumer 怎么知道 partition leader 是谁？</p>
<p>对生产者/消费者对分区的写/读请求，它们需要知道分区的 leader 是哪一个，对吧？这个信息肯定是可以获取到的，Kafka 使用 ZooKeeper 来存储这些元数据。</p>
<h2 id="什么是-ZooKeeper"><a href="#什么是-ZooKeeper" class="headerlink" title="什么是 ZooKeeper"></a>什么是 ZooKeeper</h2><p>ZooKeeper 是一个分布式键值存储。它针对读取进行了高度优化，但写入速度较慢。它最常用于存储元数据和处理集群的机制 (心跳，分发更新/配置等)。</p>
<p>它允许服务的客户 (Kafka broker) 订阅并在发生变更后发送给他们，这就是 Kafka 如何知道何时切换分区领导者。ZooKeeper 本身维护了一个集群，所以它就有很高的容错性，当然它也应该具有，毕竟 Kafka 很大程度上是依赖于它的。</p>
<p>ZooKeeper 用于存储所有的元数据信息，包括但不限于如下几项：</p>
<ul>
<li>消费者组每个分区的偏移量 (现在客户端在单独的 Kafka topic 上存储偏移量)</li>
<li>ACL —— 权限控制</li>
<li>生产者/消费者的流量控制——每秒生产/消费的数据大小。参考：<a target="_blank" rel="noopener" href="https://shiyueqi.github.io/2017/04/27/Kafka-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6Quota%E5%8A%9F%E8%83%BD/"><em>Kafka - 流量控制 Quota 功能</em></a></li>
<li>partition leader 以及它们的健康信息</li>
</ul>
<p>那么 producer/consumer 是如何知道谁是 partition leader 的呢？</p>
<p>生产者和消费者以前常常直接连接 ZooKeeper 来获取这些信息，但是 Kafka 从 0.8 和 0.9 版本开始移除了这种强耦合关系。客户端直接从 Kafka broker 获取这些元数据，而让 Kafka broker 从 ZooKeeper 那里获取这些元数据。</p>
<p><img src="/2020/10/23/kafka-introduce/get_leader_info.png" alt="获取leader"></p>
<p>更多 ZooKeeper 的讲解参考：<a target="_blank" rel="noopener" href="https://juejin.im/post/6844903608685707271"><em>漫画：什么是 ZooKeeper？</em></a></p>
<h2 id="流式处理-Streaming"><a href="#流式处理-Streaming" class="headerlink" title="流式处理 (Streaming)"></a>流式处理 (Streaming)</h2><p>在 Kafka 中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理并生成数据流以输出到其他主题 (或者外部服务，数据库，容器等等)。</p>
<p>什么是数据流呢？首先，数据流是无边界数据集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的，是因为随着时间推移，新的记录会不断加入进来。比如信用卡交易，股票交易等事件都可以用来表示数据流。</p>
<p>我们可以使用 producer/consumer 的 API 直接进行简单处理，但是对于更加复杂的转换，比如将流连接到一起，Kafka 提供了集成 <a target="_blank" rel="noopener" href="https://kafka.apache.org/documentation/streams/"><em>Stream API</em></a> 库。</p>
<p>这个 API 是在你自己的代码中使用的，它并不是运行在 broker 上，它的工作原理和 consumer API 类似，可帮助你在多个应用程序 (类似于消费者组) 上扩展流处理工作。</p>
<h3 id="无状态处理"><a href="#无状态处理" class="headerlink" title="无状态处理"></a>无状态处理</h3><p>流的无状态处理是确定性处理，其不依赖于任何外部条件，对于任何给定的数据，将始终生成与其他任何内容无关的相同输出。举个例子，我们要做一个简单的数据转换—“zhangsan” → “Hello, zhangsan”</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_stream_process.png" alt="Kafka流处理"></p>
<h3 id="流-表二义性"><a href="#流-表二义性" class="headerlink" title="流-表二义性"></a>流-表二义性</h3><p>重要的是要认识到流和表实质上是一样的，流可以被解释称为表，表也可以被解释称为流。</p>
<h4 id="流作为表"><a href="#流作为表" class="headerlink" title="流作为表"></a>流作为表</h4><p>流可以解释为数据的一系列更新，聚合后的结果就是表的最终结果，这项技术被称为事件溯源 (<a target="_blank" rel="noopener" href="https://martinfowler.com/eaaDev/EventSourcing.html"><em>Event Sourcing</em></a>)。</p>
<p>如果你了解数据库备份同步，你就会知道它们的技术实现被称为流式复制—将对表的每个更改都发送报副本服务器。比如 redis 中的 AOF 以及 Mysql 中的 binlog。</p>
<p>Kafka 流可以用相同的方式解释 - 当累积形成最终状态时的事件。此类流聚合保存在本地 RocksDB 中 (默认情况下)，被称为 KTable。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_stream_table.png" alt="Kafka流转换为表"></p>
<h4 id="表作为流"><a href="#表作为流" class="headerlink" title="表作为流"></a>表作为流</h4><p>可以将表视为流中每个键的最新值的快照。与流记录可以生成表一样，表更新可以生成更改日志流。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_table_stream.png" alt="Kafka表转换为流"></p>
<h3 id="有状态处理"><a href="#有状态处理" class="headerlink" title="有状态处理"></a>有状态处理</h3><p>我们在 java 中常用的一些操作比如 <code>map()</code> 或者 <code>filter()</code> 是没有状态的，它不会要求你保留任何原始数据。但是现实中，大多数的操作都是有状态的 (比如 <code>count()</code>)，因为这需要你存储当前累计的状态。</p>
<p>在流处理器上维护状态的问题是流处理器可能会失败！你需要在哪里保持这种状态才能容错？</p>
<p>一种简单的方法是简单地将所有状态存储在远程数据库中，并通过网络连接到该存储，这样做的问题是大量的网络带宽会使得你的应用程序变慢。一个更微妙但重要的问题是你的流处理作业的正常运行时间将与远程数据库紧密耦合，并且作业将不是自包含的 (其他 team 更改数据库可能会破坏你的处理)。</p>
<p>那么什么是更好的办法呢？</p>
<p>回想一下表和流的二元性。这允许我们将流转换为与我们的处理位于同一位置的表。它还为我们提供了一种处理容错的机制—通过将流存储在 Kafka broker 中。</p>
<p>流处理器可以将其状态保持在本地表 (例如 RocksDB) 中，该表将从输入流 (可能在某些任意转换之后) 更新。当进程失败时，它可以通过重放流来恢复其数据。</p>
<p>你甚至可以将远程数据库作为流的生产者，有效地广播用于在本地重建表的更改日志。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_deal_statful_data.png" alt="Kafka处理有状态数据"></p>
<h3 id="KSQL"><a href="#KSQL" class="headerlink" title="KSQL"></a>KSQL</h3><p>通常，我们不得不使用 JVM 语言编写流处理，因为这是唯一的官方 Kafka Streams API 客户端。<br>2018 年 4 月，KSQL 作为一项新特性被发布，它允许你使用熟悉的类似 SQL 的语言编写简单的 stream jobs。你安装了 KSQL 服务器并通过 CLI 以交互方式查询以及管理。它使用相同的抽象 (KStream 和 KTable)，保证了 Streams API 的相同优点 (可伸缩性，容错性)，并大大简化了流的工作。</p>
<p>这听起来可能不是很多，但在实践中对于测试内容更有用，甚至允许开发之外的人 (例如产品所有者) 使用流处理，可以看看 Confluent 提供的这篇关于<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=A45uRzJiv7I&t=2m13s"> <em>ksql 的使用</em></a>。</p>
<h2 id="什么时候使用-kafka"><a href="#什么时候使用-kafka" class="headerlink" title="什么时候使用 kafka"></a>什么时候使用 kafka</h2><p>正如我们已经介绍的那样，Kafka 允许你通过集中式介质获取大量消息并存储它们，而不必担心性能或数据丢失等问题。</p>
<p>这意味着它非常适合用作系统架构的核心，充当连接不同应用程序的集中式媒体。Kafka 可以成为事件驱动架构的中心部分，使你可以真正地将应用程序彼此分离。</p>
<p><img src="/2020/10/23/kafka-introduce/kafka_when_use.png" alt="什么时候使用Kafka"></p>
<p>Kafka 允许你轻松地分离不同 (微) 服务之间的通信。使用 Streams API，现在可以比以往更轻松地编写业务逻辑，从而丰富 Kafka 主题数据以供服务使用。可能性很大，我恳请你探讨公司如何使用 Kafka。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Apache Kafka 是一个分布式流媒体平台，每天可处理数万亿个事件。Kafka 提供低延迟，高吞吐量，容错的发布和订阅管道，并能够处理事件流。我们回顾了它的基本语义 (producer，broker，consumer，topic)，了解了它的一些优化 (page cache)，通过复制数据了解了它的容错能力，并介绍了它不断增长的强大流媒体功能。Kafka 已经在全球数千家公司中大量采用，其中包括财富 500 强企业中的三分之一。随着 Kafka 的积极开发和最近发布的第一个主要版本 1.0 (2017 年 11 月 1 日)，有预测这个流媒体平台将会与关系数据库一样，是数据平台的重要核心。我希望这篇介绍能帮助你熟悉 Apache Kafka。</p>
<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><p><a target="_blank" rel="noopener" href="http://generalthink.github.io/2019/02/27/introduction-of-kafka/">http://generalthink.github.io/2019/02/27/introduction-of-kafka/</a></p>
<p>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:wdshfut@163.com">wdshfut@163.com</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/23/hexo-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/23/hexo-blog/" class="post-title-link" itemprop="url">使用 hexo 搭建 github 博客</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-23 11:33:51" itemprop="dateCreated datePublished" datetime="2020-10-23T11:33:51+08:00">2020-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-19 14:33:53" itemprop="dateModified" datetime="2021-08-19T14:33:53+08:00">2021-08-19</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="使用工具版本"><a href="#使用工具版本" class="headerlink" title="使用工具版本"></a>使用工具版本</h2><p>默认已经安装 node.js 和 git。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git version: git version 2.27.0.windows.1</span><br><span class="line">npm version: 6.14.7</span><br><span class="line">hexo version: 4.2.0</span><br></pre></td></tr></table></figure>

<h2 id="git-客户端与-github-建立-SSH-连接"><a href="#git-客户端与-github-建立-SSH-连接" class="headerlink" title="git 客户端与 github 建立 SSH 连接"></a>git 客户端与 github 建立 SSH 连接</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Please make sure you have the correct access rights and the repository exists.</span><br></pre></td></tr></table></figure>

<p>当 git 客户端出现以上提示时，说明 SSH 连接过期，需要重新建立连接。参考如下方式：</p>
<ol>
<li>先查看下 name 和 email</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看user的name和email</span></span><br><span class="line">$ git config user.name</span><br><span class="line">$ git config user.email</span><br><span class="line"><span class="comment"># 如果没设置，按如下命令设置</span></span><br><span class="line">$ git config --global user.name &#123;<span class="variable">$yourname</span>&#125;</span><br><span class="line">$ git config --global user.email &#123;<span class="variable">$youremail</span>&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>删除 .ssh 文件夹下的 known_hosts，路径为：<code>C:\Users\&#123;$userrname&#125;\.ssh</code></p>
</li>
<li><p>git bash 输入命令</p>
</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C &#123;<span class="variable">$youremail</span>&#125;</span><br></pre></td></tr></table></figure>

<p>一直按回车，等结束后，.ssh 文件夹下会生成两个文件：id_rsa 和 id_rsa.pub，将 id_rsa.pub 的内容全部复制。</p>
<ol start="4">
<li><p>登录个人 github 账户，进入 Settings → SSH and GPG keys，点击 New SSH key，将复制的内容粘贴到 Key 里，点击 Add SSH key。</p>
</li>
<li><p>git bash 输入命令</p>
</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure>

<p>在弹出的确定对话框输入：yes。</p>
<h2 id="hexo-安装"><a href="#hexo-安装" class="headerlink" title="hexo 安装"></a>hexo 安装</h2><p>在 git bash 中依次输入以下命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-cli -g</span><br><span class="line">$ <span class="built_in">cd</span> f: <span class="comment"># 可以是任何路径</span></span><br><span class="line">$ hexo init blog</span><br><span class="line">$ <span class="built_in">cd</span> blog <span class="comment"># 进入blog目录</span></span><br><span class="line">$ npm install</span><br><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<p>命令执行完成后，会在 <code>F:\</code> 目录下，多一个 blog 文件夹。</p>
<h2 id="修改-config-yml-文件"><a href="#修改-config-yml-文件" class="headerlink" title="修改 _config.yml 文件"></a>修改 _config.yml 文件</h2><p>修改 blog 根目录下的 _config.yml 文件，将 deploy 节点修改为如下内容：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:&#123;$yourname&#125;/&#123;$yourname&#125;.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure>

<p>说明：_config.yml 文件的配置均为 [key: value] 形式，value 前面必须要有一个空格。</p>
<p>然后在 git bash 中输入以下命令，发布博客：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<h2 id="访问自己的博客"><a href="#访问自己的博客" class="headerlink" title="访问自己的博客"></a>访问自己的博客</h2><p>博客地址：<code>https://&#123;$yourname&#125;.github.io/</code></p>
<h2 id="写一个自己的博客"><a href="#写一个自己的博客" class="headerlink" title="写一个自己的博客"></a>写一个自己的博客</h2><p>hexo 的项目结构是在网站根目录的 <code>source\_posts</code> 目录下存放你的博客文档，以 .md 文档格式存储，默认已存在一个 hello-world.md 文章。</p>
<ol>
<li>新建文章</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new &lt;title&gt;</span><br></pre></td></tr></table></figure>

<p>会在 blog 的 <code>source\_posts</code> 目录下，新建一个名叫 &lt;title&gt;.md 文章，如：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO  Validating config</span><br><span class="line">INFO  Created: F:\blog\<span class="built_in">source</span>\_posts\tesss.md</span><br></pre></td></tr></table></figure>

<p>之后，在文章中添加自己的内容即可，建议使用 Typora 编辑，其语法参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/suwanbin-thought/p/11711906.html"><em>如何使用 markdown？</em></a></p>
<ol start="2">
<li>发布文章</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean <span class="comment"># 清楚缓存</span></span><br><span class="line">$ hexo generate <span class="comment"># 生成静态页面</span></span><br><span class="line">$ hexo server <span class="comment"># 本地发布，浏览器输入localhost:4000即可访问博客</span></span><br><span class="line">$ hexo deploy <span class="comment"># 将public中的静态页面复制到.deploy_git文件夹中，并提交到github</span></span><br></pre></td></tr></table></figure>

<p>至此，你的第一个自己的博客发布完成。</p>
<p>说明：以上 hexo 的命令，都要在 <code>F:\blog</code> 目录下执行。</p>
<h2 id="修改博客的-themes"><a href="#修改博客的-themes" class="headerlink" title="修改博客的 themes"></a>修改博客的 themes</h2><p>如果想修改自己博客的 themes，可以下载好想要的，然后拷贝到 blog 的 themes 目录下，然后修改 _config.yml 文件，将 theme 节点的值，修改为你下载好的 themes 的名称，如：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure>

<p>之后，再按照你下载的 themes 的使用说明，做相应修改即可。</p>
<p>参考：<a target="_blank" rel="noopener" href="http://theme-next.iissnan.com/getting-started.html"><em>NexT 的使用</em></a></p>
<h3 id="NexT-中-tags-的使用"><a href="#NexT-中-tags-的使用" class="headerlink" title="NexT 中 tags 的使用"></a>NexT 中 tags 的使用</h3><ol>
<li>修改 NexT 目录下的 _config.yml 文件，取消 menu 菜单下 tags 字段的注释</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>在 blog 根目录的 source 目录下，新建 tags 目录</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new page <span class="string">&quot;tags&quot;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>修改 tags 目录下的 index.md 文件</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">title: tags</span><br><span class="line">date: 2020-10-27 16:35:56</span><br><span class="line"><span class="built_in">type</span>: tags</span><br><span class="line">layout: <span class="string">&quot;tags&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="NexT-中添加字数统计、阅读时长"><a href="#NexT-中添加字数统计、阅读时长" class="headerlink" title="NexT 中添加字数统计、阅读时长"></a>NexT 中添加字数统计、阅读时长</h3><ol>
<li>安装 hexo-symbols-count-time 插件</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-symbols-count-time</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yarn add hexo-symbols-count-time</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>hexo 配置，根目录下的 _config.yaml 文件，添加 symbols_count_time 节点</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Post wordcount display settings</span><br><span class="line">symbols_count_time:</span><br><span class="line">  symbols: true # 文章字数</span><br><span class="line">  time: true # 阅读时长</span><br><span class="line">  total_symbols: true # 所有文章总字数</span><br><span class="line">  total_time: true # 所有文章阅读中时长</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>NexT 配置，themes 目录下的 _config.yml 文件，symbols_count_time 节点</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Post wordcount display settings</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/hexo-symbols-count-time</span></span><br><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: <span class="literal">true</span> <span class="comment"># 是否换行显示 字数统计 及 阅读时长</span></span><br><span class="line">  item_text_post: <span class="literal">true</span> <span class="comment"># 文章 字数统计 阅读时长 使用图标 还是 文本表示</span></span><br><span class="line">  item_text_total: <span class="literal">false</span> <span class="comment"># 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示</span></span><br></pre></td></tr></table></figure>

<h3 id="Next-中添加访客统计、访问次数统计、文章阅读次数统计"><a href="#Next-中添加访客统计、访问次数统计、文章阅读次数统计" class="headerlink" title="Next 中添加访客统计、访问次数统计、文章阅读次数统计"></a>Next 中添加访客统计、访问次数统计、文章阅读次数统计</h3><ol>
<li>打开 next 主题配置文件 \themes\next\_config.yml，搜索 <strong>busuanzi_count</strong>，把 enable 设置为 true。</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show Views / Visitors of the website / page with busuanzi.</span></span><br><span class="line"><span class="comment"># Get more information on http://ibruce.info/2015/04/04/busuanzi</span></span><br><span class="line">busuanzi_count:</span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">  total_visitors: <span class="literal">true</span></span><br><span class="line">  total_visitors_icon: fa fa-user</span><br><span class="line">  total_views: <span class="literal">true</span></span><br><span class="line">  total_views_icon: fa fa-eye</span><br><span class="line">  post_views: <span class="literal">true</span></span><br><span class="line">  post_views_icon: fa fa-eye</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>同样是在 next 主题配置文件 \themes\next\_config.yml 下，搜索 <strong>footer</strong>，在它底下添加 counter，设值为 true。</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">footer:</span><br><span class="line">  <span class="comment"># Specify the date when the site was setup. If not defined, current year will be used.</span></span><br><span class="line">  <span class="comment">#since: 2015</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Icon between year and copyright info.</span></span><br><span class="line">  icon:</span><br><span class="line">    <span class="comment"># Icon name in Font Awesome. See: https://fontawesome.com/icons</span></span><br><span class="line">    name: fa fa-heart</span><br><span class="line">    <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line">    animated: <span class="literal">false</span></span><br><span class="line">    <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line">    color: <span class="string">&quot;#ff0000&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># If not defined, `author` from Hexo `_config.yml` will be used.</span></span><br><span class="line">  copyright:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Powered by Hexo &amp; NexT</span></span><br><span class="line">  powered: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Beian ICP and gongan information for Chinese users. See: https://beian.miit.gov.cn, http://www.beian.gov.cn</span></span><br><span class="line">  beian:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">false</span></span><br><span class="line">    icp:</span><br><span class="line">    <span class="comment"># The digit in the num of gongan beian.</span></span><br><span class="line">    gongan_id:</span><br><span class="line">    <span class="comment"># The full num of gongan beian.</span></span><br><span class="line">    gongan_num:</span><br><span class="line">    <span class="comment"># The icon for gongan beian. See: http://www.beian.gov.cn/portal/download</span></span><br><span class="line">    gongan_icon_url:</span><br><span class="line"></span><br><span class="line">  counter: <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>来到 themes\next\layout\_partials，找到 <strong>footer.swig</strong> 文件，打开编辑，在底下添加代码。</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> theme.footer.counter %&#125;</span><br><span class="line">    &lt;script async src=<span class="string">&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;</span>&gt;&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>站点访客数、访问次数显示在网址底部，文章阅读次数在文章开头。</li>
</ol>
<h2 id="在博客中添加图片"><a href="#在博客中添加图片" class="headerlink" title="在博客中添加图片"></a>在博客中添加图片</h2><p>md 文件中插入图片的语法为：<code>![]()</code>。</p>
<p>其中，方括号是图片描述，圆括号是图片路径。一般来说有三种图片路径，分别是相对路径，绝对路径和网络路径。</p>
<p>相对而言，使用相对路径会更加方便，设置如下：</p>
<ol>
<li>安装 hexo-renderer-marked 插件</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-renderer-marked</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>修改根目录下的 _config.yaml 配置</li>
</ol>
<p>将：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>修改为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: <span class="literal">true</span></span><br><span class="line">marked:</span><br><span class="line">  prependRoot: <span class="literal">true</span></span><br><span class="line">  postAsset: <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>设置 Typora</li>
</ol>
<p>点击文件 → 偏好设置，设置如下：<br><img src="/2020/10/23/hexo-blog/image-20201026100024754.png"></p>
<p>这样，在粘贴图片到文件中时，会自动将图片复制到 <code>source\_posts</code> 目录下，与 .md 文件同名的目录中。</p>
<p>之后，在将博客发布时，会自动上传到文章所生成的静态网页同级目录之下。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="XiSun"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">XiSun</p>
  <div class="site-description" itemprop="description">心如止水者，虽世间繁华之红尘纷扰，已然空无一物</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XiSun</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.4m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">20:53</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
