<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="CentOS  安装 Ceph节点规划 各节点规划：    主机名 ip 磁盘 角色    hadoop102 192.168.10.102 系统盘：sdaosd盘：sdb cephadm，monitor，mgr，rgw，mds，osd，nfs   hadoop103 192.168.10.103 系统盘：sdaosd盘：sdb monitor，mgr，rgw，mds，osd，nfs   hado">
<meta property="og:type" content="article">
<meta property="og:title" content="Ceph 入门">
<meta property="og:url" content="http://example.com/2021/12/21/ceph/index.html">
<meta property="og:site_name" content="XiSun的博客">
<meta property="og:description" content="CentOS  安装 Ceph节点规划 各节点规划：    主机名 ip 磁盘 角色    hadoop102 192.168.10.102 系统盘：sdaosd盘：sdb cephadm，monitor，mgr，rgw，mds，osd，nfs   hadoop103 192.168.10.103 系统盘：sdaosd盘：sdb monitor，mgr，rgw，mds，osd，nfs   hado">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211222150302860.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211222151324901.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211222154745752.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211222155420355.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211222155356339.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211223154742836.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211222162602332.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211223115230538.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211223134042762.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211227113311642.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211227141903939.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211227172731909.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211229155618530.png">
<meta property="og:image" content="http://example.com/2021/12/21/ceph/image-20211230114924366.png">
<meta property="article:published_time" content="2021-12-21T07:59:22.000Z">
<meta property="article:modified_time" content="2022-01-11T01:08:04.615Z">
<meta property="article:author" content="XiSun">
<meta property="article:tag" content="ceph">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/12/21/ceph/image-20211222150302860.png">

<link rel="canonical" href="http://example.com/2021/12/21/ceph/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Ceph 入门 | XiSun的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">XiSun的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Learning is endless</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/21/ceph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="XiSun">
      <meta itemprop="description" content="心如止水者，虽世间繁华之红尘纷扰，已然空无一物">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XiSun的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Ceph 入门
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-21 15:59:22" itemprop="dateCreated datePublished" datetime="2021-12-21T15:59:22+08:00">2021-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-11 09:08:04" itemprop="dateModified" datetime="2022-01-11T09:08:04+08:00">2022-01-11</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>97k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1:28</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="CentOS-安装-Ceph"><a href="#CentOS-安装-Ceph" class="headerlink" title="CentOS  安装 Ceph"></a>CentOS  安装 Ceph</h2><h3 id="节点规划"><a href="#节点规划" class="headerlink" title="节点规划"></a>节点规划</h3><ul>
<li><p>各节点规划：</p>
<table>
<thead>
<tr>
<th>主机名</th>
<th>ip</th>
<th>磁盘</th>
<th>角色</th>
</tr>
</thead>
<tbody><tr>
<td>hadoop102</td>
<td>192.168.10.102</td>
<td>系统盘：sda<br>osd盘：sdb</td>
<td>cephadm，monitor，mgr，rgw，mds，osd，nfs</td>
</tr>
<tr>
<td>hadoop103</td>
<td>192.168.10.103</td>
<td>系统盘：sda<br>osd盘：sdb</td>
<td>monitor，mgr，rgw，mds，osd，nfs</td>
</tr>
<tr>
<td>hadoop104</td>
<td>192.168.10.104</td>
<td>系统盘：sda<br>osd盘：sdb</td>
<td>monitor，mgr，rgw，mds，osd，nfs</td>
</tr>
</tbody></table>
</li>
<li><p>各节点内核版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# uname -r</span><br><span class="line">3.10.0-1160.49.1.el7.x86_64</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点操作系统版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.9.2009 (Core)</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点配置 CentOS 7 yum 阿里云镜像源：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空缓存</span></span><br><span class="line">[root@hadoop102 opt]# yum clean cache</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成缓存</span></span><br><span class="line">[root@hadoop102 opt]# yum makecache</span><br></pre></td></tr></table></figure>

<ul>
<li><p>执行命令 <code>curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</code>，会把 Centos-7.repo 下载到 <code>/etc/yum.repos.d/</code> 目录下，如果该目录下有 CentOS-Base.repo，则会自动覆盖。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# ll /etc/yum.repos.d/</span><br><span class="line">总用量 56</span><br><span class="line">-rw-r--r--. 1 root root 2523 12月 21 14:16 CentOS-Base.repo						# yum源</span><br><span class="line">-rw-r--r--. 1 root root 1309 11月 23 2020 CentOS-CR.repo</span><br><span class="line">-rw-r--r--. 1 root root  649 11月 23 2020 CentOS-Debuginfo.repo</span><br><span class="line">-rw-r--r--. 1 root root  314 11月 23 2020 CentOS-fasttrack.repo</span><br><span class="line">-rw-r--r--. 1 root root  630 11月 23 2020 CentOS-Media.repo</span><br><span class="line">-rw-r--r--. 1 root root 1331 11月 23 2020 CentOS-Sources.repo</span><br><span class="line">-rw-r--r--. 1 root root 8515 11月 23 2020 CentOS-Vault.repo</span><br><span class="line">-rw-r--r--. 1 root root  616 11月 23 2020 CentOS-x86_64-kernel.repo</span><br><span class="line">-rw-r--r--. 1 root root  477 12月 21 15:38 ceph.repo								# ceph源</span><br><span class="line">-rw-r--r--. 1 root root 2081 12月 20 12:37 docker-ce.repo						# docker源</span><br><span class="line">-rw-r--r--. 1 root root 1358 9月   5 01:37 epel.repo</span><br><span class="line">-rw-r--r--. 1 root root 1457 9月   5 01:37 epel-testing.repo</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>各节点更新 yum 包（生产环境中此步操作需慎重，看自己情况，学习的话随便搞，这个命令不是必须执行的）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# yum -y update</span><br><span class="line">[root@hadoop102 opt]# yum -y upgrade</span><br></pre></td></tr></table></figure>

<ul>
<li><code>yum -y update</code>：升级所有包，也升级软件和系统内核。</li>
<li><code>yum -y upgrade</code>：升级所有包，但不升级软件和系统内核。</li>
</ul>
</li>
<li><p>各节点配置主机名：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# vim /etc/hostname</span><br><span class="line">[root@hadoop102 opt]# cat /etc/hostname</span><br><span class="line">hadoop102</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点配置 host 解析：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# vim /etc/hosts</span><br><span class="line">[root@hadoop102 opt]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.10.99 centos7</span><br><span class="line">192.168.10.100 hadoop100</span><br><span class="line">192.168.10.101 hadoop101</span><br><span class="line">192.168.10.102 hadoop102</span><br><span class="line">192.168.10.103 hadoop103</span><br><span class="line">192.168.10.104 hadoop104</span><br><span class="line">192.168.10.105 hadoop105</span><br><span class="line">192.168.10.106 hadoop106</span><br><span class="line">192.168.10.107 hadoop107</span><br><span class="line">192.168.10.108 hadoop108</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><ul>
<li><p>关闭防火墙：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 xisun]# systemctl stop firewalld.service</span><br><span class="line">[root@hadoop102 xisun]# firewall-cmd --state</span><br><span class="line">not running</span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭防火墙开机自启：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 xisun]# systemctl disable firewalld.service </span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看防火墙是否开机自启</span></span><br><span class="line">[root@hadoop102 opt]#  systemctl list-unit-files | grep firewalld.service </span><br><span class="line">firewalld.service                             disabled</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="SSH-免密登录"><a href="#SSH-免密登录" class="headerlink" title="SSH 免密登录"></a>SSH 免密登录</h3><h3 id="服务器时间同步"><a href="#服务器时间同步" class="headerlink" title="服务器时间同步"></a>服务器时间同步</h3><ul>
<li>在 Linux 系统中，可以通过 ntpdate 和 ntpd 两种方式实现 NTP 时间同步，ntpdate 为断点更新，ntpd 为步进式地逐渐调整时间。对于新服务器，可以使用 ntpdate 同步时间，对于已经承载有运行中业务的服务器，建议使用 ntpd 同步时间。</li>
<li>直接同步：使用 ntpdate 命令进行同步，直接进行时间变更。如果服务器上存在一个 12 点运行的任务，当前服务器时间是 13 点，但标准时间时 11 点，使用此命令可能会造成任务重复执行。因此使用 ntpdate 同步可能会引发风险，该命令也多用于配置时钟同步服务时第一次同步时间时使用。</li>
<li>平滑同步：使用 ntpd 进行时钟同步，可以保证一个时间不经历两次，它每次同步时间的偏移量不会太陡，是慢慢来的，这正因为这样，ntpd 平滑同步可能耗费的时间比较长。</li>
</ul>
<h4 id="NTP-服务器配置"><a href="#NTP-服务器配置" class="headerlink" title="NTP 服务器配置"></a>NTP 服务器配置</h4><ul>
<li><p>寻找 NTP Server，<a target="_blank" rel="noopener" href="https://www.ntppool.org/">https://www.ntppool.org/</a> 是 NTP 的官方网站，在这上面我们可以找到离我们城市最近的 NTP Server。NTP 建议我们为了保障时间的准确性，最少找两个 NTP Server。我们找到对应的中国 NTP Server：</p>
<p><img src="/2021/12/21/ceph/image-20211222150302860.png" alt="image-20211222150302860"></p>
</li>
<li><p>也可以在 <a target="_blank" rel="noopener" href="http://www.ntp.org.cn/">http://www.ntp.org.cn/</a> 网站查找 NTP Server，推荐使用域名，而非 IP 地址。后面的 NTP 服务器配置，以下图为准：</p>
<p><img src="/2021/12/21/ceph/image-20211222151324901.png" alt="image-20211222151324901"></p>
</li>
<li><p>设置时区：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置东八区</span></span><br><span class="line">[root@hadoop102 opt]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看时区</span></span><br><span class="line">[root@hadoop102 opt]# timedatectl status | grep &#x27;Time zone&#x27;</span><br><span class="line">       Time zone: Asia/Shanghai (CST, +0800)</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查系统是否安装了 NTP 服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# rpm -qa | grep ntp</span><br><span class="line">python-ntplib-0.3.2-1.el7.noarch</span><br><span class="line">ntpdate-4.2.6p5-29.el7.centos.2.x86_64</span><br><span class="line">ntp-4.2.6p5-29.el7.centos.2.x86_64</span><br><span class="line">fontpackages-filesystem-1.44-8.el7.noarch</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果没有安装，使用下面的命令安装 ntp 和 ntpdate：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# yum -y install ntp ntpdate</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据 NTP 的设置，如果你的系统时间比正确时间快，那么 NTP 是不会帮你调整的，所以要么你把时间设置回去，要么先做一个手动同步，使用下面的命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# ntpdate cn.ntp.org.cn</span><br><span class="line">22 Dec 15:09:33 ntpdate[5303]: adjust time server 114.67.237.130 offset -0.010511 sec</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 NTP 服务器（设定 hadoop102 为 NTP 服务器），NTP 服务器主配置文件 <code>/etc/ntp.conf</code>，配置前做好备份：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# mkdir /home/backup</span><br><span class="line">[root@hadoop102 opt]# cp /etc/ntp.conf /home/backup/</span><br><span class="line">[root@hadoop102 opt]# mv /home/backup/ntp.conf /home/backup/ntp.conf.bak</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 NTP 服务器端配置文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# vim /etc/ntp.conf</span><br></pre></td></tr></table></figure>

<p><img src="/2021/12/21/ceph/image-20211222154745752.png" alt="image-20211222154745752"></p>
<ul>
<li><p>restrict 语法说明：</p>
<p><img src="/2021/12/21/ceph/image-20211222155420355.png" alt="image-20211222155420355"></p>
</li>
<li><p>如果服务器是内网，不能连接外网，则无法使用网络 NTP Server，此时可以将本机作为 NTP Server，按如下方法配置：</p>
<p><img src="/2021/12/21/ceph/image-20211222155356339.png" alt="image-20211222155356339"></p>
</li>
</ul>
</li>
<li><p>ntp 服务，默认只会同步系统时间。如果想要让 ntp 同时同步硬件时间，可以在 <code>/etc/sysconfig/ntpd</code> 文件中，添加 <code>SYNC_HWCLOCK=yes</code>，这样，就可以让硬件时间与系统时间一起同步：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# vim /etc/sysconfig/ntpd</span><br></pre></td></tr></table></figure>

<p><img src="/2021/12/21/ceph/image-20211223154742836.png" alt="image-20211223154742836"></p>
</li>
<li><p>启动 NTP 服务并设置开机自动启动。(Linux 防火墙需要关闭，否则会阻止 NTP 服务端口，NTP 服务默认端口 123)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动ntp服务</span></span><br><span class="line">[root@hadoop102 opt]# systemctl start ntpd</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看ntp服务是否启动</span></span><br><span class="line">[root@hadoop102 opt]# systemctl status ntpd</span><br><span class="line">● ntpd.service - Network Time Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 三 2021-12-22 16:00:12 CST; 1min 27s ago</span><br><span class="line">  Process: 5883 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 5884 (ntpd)</span><br><span class="line">    Tasks: 1</span><br><span class="line">   CGroup: /system.slice/ntpd.service</span><br><span class="line">           └─5884 /usr/sbin/ntpd -u ntp:ntp -g</span><br><span class="line"></span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: Listen and drop on 1 v6wildcard :: UDP 123</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 2 lo 127.0.0.1 UDP 123</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 3 ens33 192.168.10.102 UDP 123</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 4 virbr0 192.168.122.1 UDP 123</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 5 lo ::1 UDP 123</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 6 ens33 fe80::ac1e:7fe1:a566:2670 UDP 123</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: Listening on routing socket on fd #23 for interface updates</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: 0.0.0.0 c016 06 restart</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: 0.0.0.0 c012 02 freq_set kernel 0.000 PPM</span><br><span class="line">12月 22 16:00:13 hadoop102 ntpd[5884]: 0.0.0.0 c011 01 freq_not_set</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看ntpd端口</span></span><br><span class="line">[root@hadoop102 opt]# netstat -ln | grep 123</span><br><span class="line">udp        0      0 192.168.122.1:123       0.0.0.0:*                          </span><br><span class="line">udp        0      0 192.168.10.102:123      0.0.0.0:*                          </span><br><span class="line">udp        0      0 127.0.0.1:123           0.0.0.0:*                          </span><br><span class="line">udp        0      0 0.0.0.0:123             0.0.0.0:*                          </span><br><span class="line">udp6       0      0 fe80::ac1e:7fe1:a56:123 :::*                               </span><br><span class="line">udp6       0      0 ::1:123                 :::*                               </span><br><span class="line">udp6       0      0 :::123                  :::*</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看ntp服务是否开机启动</span></span><br><span class="line">[root@hadoop102 opt]# systemctl list-unit-files | grep ntpd</span><br><span class="line">ntpd.service                                  disabled</span><br><span class="line">ntpdate.service                               disabled</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置开机启动ntp服务</span></span><br><span class="line">[root@hadoop102 opt]# systemctl enable ntpd</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认ntp服务开机启动设置成功</span></span><br><span class="line">[root@hadoop102 opt]# systemctl list-unit-files | grep ntpd</span><br><span class="line">ntpd.service                                  enabled </span><br><span class="line">ntpdate.service                               disabled</span><br></pre></td></tr></table></figure>

<ul>
<li><p>如果是重启 ntpd 服务，使用下面的命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# systemctl restart ntpd</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>查看 NTP 服务器与外部 NTP 服务器同步情况：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 静态查看</span></span><br><span class="line">[root@hadoop102 opt]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line"> 2001:da8:9000:: .STEP.          16 -    -  512    0    0.000    0.000   0.000</span><br><span class="line">*202.118.1.130   .PTP.            1 u   14   64  277   39.772  -39.060   5.282</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 动态查看，一般ntp启动后，需要等5~10分钟左右，NTP服务器才会与外部NTP服务器同步</span></span><br><span class="line">[root@hadoop102 opt]# watch ntpq  -p</span><br></pre></td></tr></table></figure>

<p><img src="/2021/12/21/ceph/image-20211222162602332.png" alt="image-20211222162602332"></p>
<ul>
<li>remote：即网络 NTP 服务器的 IP 或主机名称。注意最左边的符号，如果有 <code>+</code>，则代表目前正在作用中的上层 NTP 服务器；如果是 <code>*</code>，则表示也有连上线，不过是作为次要联机的 NTP 服务器。</li>
<li>refid：参考的上一层 NTP 服务器的地址。</li>
<li>st：即 stratum 阶层，值越小表示 NTP Server 的精准度越高。</li>
<li>when：几秒前曾做过时间同步更新的操作。</li>
<li>poll：每隔多少毫秒与 NTP Server 同步一次。</li>
<li>reach：已经向上层 NTP 服务器要求更新的次数。</li>
<li>delay：网络传输过程中延迟的时间。</li>
<li>offset：时间补偿的结果。</li>
<li>jitter：Linux 系统时间与 BIOS 硬件时间的差异时间。</li>
</ul>
</li>
<li><p>查看系统时间和硬件时间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 系统时间</span></span><br><span class="line">[root@hadoop102 opt]# date</span><br><span class="line">2021年 12月 23日 星期四 16:28:10 CST</span><br><span class="line"><span class="meta">#</span><span class="bash"> 硬件时间</span></span><br><span class="line">[root@hadoop102 opt]# hwclock --show</span><br><span class="line">2021年12月23日 星期四 16时28分15秒  -0.631618 秒</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h4><ul>
<li><p>设置时区：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置东八区</span></span><br><span class="line">[root@hadoop103 opt]# timedatectl set-timezone Asia/Shanghai</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看时区</span></span><br><span class="line">[root@hadoop103 opt]# timedatectl status | grep &#x27;Time zone&#x27;</span><br><span class="line">       Time zone: Asia/Shanghai (CST, +0800)</span><br></pre></td></tr></table></figure>
</li>
<li><p>客户端向 NTP 服务器（192.168.10.102）更新时间时，客户端不需要开启 NTP 服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 opt]# systemctl stop ntpd</span><br><span class="line">[root@hadoop103 opt]# systemctl status ntpd</span><br><span class="line">● ntpd.service - Network Time Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line"></span><br><span class="line">12月 23 11:11:08 hadoop103 ntpd[18278]: Listening on routing socket on fd #24 for interface updates</span><br><span class="line">12月 23 11:11:10 hadoop103 ntpd[18278]: 0.0.0.0 c016 06 restart</span><br><span class="line">12月 23 11:11:10 hadoop103 ntpd[18278]: 0.0.0.0 c012 02 freq_set kernel 0.000 PPM</span><br><span class="line">12月 23 11:11:10 hadoop103 ntpd[18278]: 0.0.0.0 c011 01 freq_not_set</span><br><span class="line">12月 23 11:11:16 hadoop103 ntpd[18278]: 0.0.0.0 c61c 0c clock_step -3.147226 s</span><br><span class="line">12月 23 11:11:13 hadoop103 ntpd[18278]: 0.0.0.0 c614 04 freq_mode</span><br><span class="line">12月 23 11:11:14 hadoop103 ntpd[18278]: 0.0.0.0 c618 08 no_sys_peer</span><br><span class="line">12月 23 11:16:42 hadoop103 ntpd[18278]: ntpd exiting on signal 15</span><br><span class="line">12月 23 11:16:42 hadoop103 systemd[1]: Stopping Network Time Service...</span><br><span class="line">12月 23 11:16:42 hadoop103 systemd[1]: Stopped Network Time Service.</span><br></pre></td></tr></table></figure>
</li>
<li><p>客户端向 NTP 服务器（192.168.10.102）进行时间同步：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 opt]# ntpdate -u 192.168.10.102</span><br><span class="line">23 Dec 11:16:57 ntpdate[18349]: adjust time server 192.168.10.102 offset -0.003982 sec</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置 ntpdate 每次同步系统时间之后，也一并同步硬件时间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 opt]# vim /etc/sysconfig/ntpdate</span><br></pre></td></tr></table></figure>

<ul>
<li><p>修改 ntpdate 文件最后一行 <code>SYNC_HWCLOCK=no</code> 为 <code>SYNC_HWCLOCK=yes</code>：</p>
<p><img src="/2021/12/21/ceph/image-20211223115230538.png" alt="image-20211223115230538"></p>
</li>
</ul>
</li>
<li><p>设置客户端定时向 NTP 服务器（192.168.10.102）进行时间同步：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 opt]# vim /etc/crontab</span><br></pre></td></tr></table></figure>

<ul>
<li><p>向 crontab 文件中添加配置，每天早晨 6 点同步一次时间：</p>
<p><img src="/2021/12/21/ceph/image-20211223134042762.png" alt="image-20211223134042762"></p>
</li>
</ul>
</li>
<li><p>重启 crond 服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 opt]# systemctl restart crond.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看 crond 服务执行情况：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 opt]# systemctl status crond.service </span><br><span class="line">● crond.service - Command Scheduler</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since 四 2021-12-23 13:41:42 CST; 4min 33s ago</span><br><span class="line"> Main PID: 20442 (crond)</span><br><span class="line">    Tasks: 1</span><br><span class="line">   Memory: 644.0K</span><br><span class="line">   CGroup: /system.slice/crond.service</span><br><span class="line">           └─20442 /usr/sbin/crond -n</span><br><span class="line"></span><br><span class="line">12月 23 13:41:42 hadoop103 systemd[1]: Started Command Scheduler.</span><br><span class="line">12月 23 13:41:42 hadoop103 crond[20442]: (CRON) INFO (RANDOM_DELAY will be scaled with factor 33% if used.)</span><br><span class="line">12月 23 13:41:43 hadoop103 crond[20442]: (CRON) INFO (running with inotify support)</span><br><span class="line">12月 23 13:41:43 hadoop103 crond[20442]: (CRON) INFO (@reboot jobs will be run at computer&#x27;s startup.)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看系统时间和硬件时间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 系统时间</span></span><br><span class="line">[root@hadoop103 opt]# date </span><br><span class="line">2021年 12月 23日 星期四 16:30:22 CST</span><br><span class="line"><span class="meta">#</span><span class="bash"> 硬件时间</span></span><br><span class="line">[root@hadoop103 opt]# hwclock </span><br><span class="line">2021年12月23日 星期四 16时30分29秒  -1.020610 秒</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/5615">https://developer.aliyun.com/article/5615</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/aa2bb27debd9">https://www.jianshu.com/p/aa2bb27debd9</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zoulongbin/p/6198186.html">https://www.cnblogs.com/zoulongbin/p/6198186.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/16e68204b3dc">https://www.jianshu.com/p/16e68204b3dc</a></li>
</ul>
<h3 id="安装-Python3"><a href="#安装-Python3" class="headerlink" title="安装 Python3"></a>安装 Python3</h3><ul>
<li><p>Ceph 的 Octopus 版本需要 Python3 支持。</p>
</li>
<li><p>各节点检查是否有 GCC：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看gcc版本</span></span><br><span class="line">[root@hadoop101 software]# gcc --version</span><br><span class="line">bash: gcc: 未找到命令...</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装gcc</span></span><br><span class="line">[root@hadoop101 software]# yum -y install gcc</span><br><span class="line">已加载插件：fastestmirror, langpacks</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Could not get metalink https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=x86_64&amp;infra=stock&amp;content=centos error was</span><br><span class="line">14: curl#7 - &quot;Failed to connect to 2406:da18:39f:a01:35a2:d9e9:8164:a209: 网络不可达&quot;</span><br><span class="line"> * base: mirrors.aliyun.com</span><br><span class="line"> * epel: mirror.sjtu.edu.cn</span><br><span class="line"> * extras: mirrors.aliyun.com</span><br><span class="line"> * updates: mirrors.aliyun.com</span><br><span class="line">正在解决依赖关系</span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在检查事务</span></span><br><span class="line"><span class="meta">---&gt;</span><span class="bash"> 软件包 gcc.x86_64.0.4.8.5-44.el7 将被 安装</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在处理依赖关系 cpp = 4.8.5-44.el7，它被软件包 gcc-4.8.5-44.el7.x86_64 需要</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在处理依赖关系 glibc-devel &gt;= 2.2.90-12，它被软件包 gcc-4.8.5-44.el7.x86_64 需要</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在检查事务</span></span><br><span class="line"><span class="meta">---&gt;</span><span class="bash"> 软件包 cpp.x86_64.0.4.8.5-44.el7 将被 安装</span></span><br><span class="line"><span class="meta">---&gt;</span><span class="bash"> 软件包 glibc-devel.x86_64.0.2.17-325.el7_9 将被 安装</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在处理依赖关系 glibc-headers = 2.17-325.el7_9，它被软件包 glibc-devel-2.17-325.el7_9.x86_64 需要</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在处理依赖关系 glibc-headers，它被软件包 glibc-devel-2.17-325.el7_9.x86_64 需要</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在检查事务</span></span><br><span class="line"><span class="meta">---&gt;</span><span class="bash"> 软件包 glibc-headers.x86_64.0.2.17-325.el7_9 将被 安装</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在处理依赖关系 kernel-headers &gt;= 2.2.1，它被软件包 glibc-headers-2.17-325.el7_9.x86_64 需要</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在处理依赖关系 kernel-headers，它被软件包 glibc-headers-2.17-325.el7_9.x86_64 需要</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 正在检查事务</span></span><br><span class="line"><span class="meta">---&gt;</span><span class="bash"> 软件包 kernel-headers.x86_64.0.3.10.0-1160.49.1.el7 将被 安装</span></span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"> 解决依赖关系完成</span></span><br><span class="line"></span><br><span class="line">依赖关系解决</span><br><span class="line"></span><br><span class="line">========================================================================================================================================================================================================</span><br><span class="line"> Package                                           架构                                      版本                                                      源                                          大小</span><br><span class="line">========================================================================================================================================================================================================</span><br><span class="line">正在安装:</span><br><span class="line"> gcc                                               x86_64                                    4.8.5-44.el7                                              base                                        16 M</span><br><span class="line">为依赖而安装:</span><br><span class="line"> cpp                                               x86_64                                    4.8.5-44.el7                                              base                                       5.9 M</span><br><span class="line"> glibc-devel                                       x86_64                                    2.17-325.el7_9                                            updates                                    1.1 M</span><br><span class="line"> glibc-headers                                     x86_64                                    2.17-325.el7_9                                            updates                                    691 k</span><br><span class="line"> kernel-headers                                    x86_64                                    3.10.0-1160.49.1.el7                                      updates                                    9.0 M</span><br><span class="line"></span><br><span class="line">事务概要</span><br><span class="line">========================================================================================================================================================================================================</span><br><span class="line">安装  1 软件包 (+4 依赖软件包)</span><br><span class="line"></span><br><span class="line">总计：33 M</span><br><span class="line">总下载量：1.1 M</span><br><span class="line">安装大小：59 M</span><br><span class="line">Downloading packages:</span><br><span class="line">No Presto metadata available for updates</span><br><span class="line">glibc-devel-2.17-325.el7_9.x86_64.rpm                                                                                                                                            | 1.1 MB  00:00:00     </span><br><span class="line">Running transaction check</span><br><span class="line">Running transaction test</span><br><span class="line">Transaction test succeeded</span><br><span class="line">Running transaction</span><br><span class="line">  正在安装    : cpp-4.8.5-44.el7.x86_64                                                                                                                                                             1/5 </span><br><span class="line">  正在安装    : kernel-headers-3.10.0-1160.49.1.el7.x86_64                                                                                                                                          2/5 </span><br><span class="line">  正在安装    : glibc-headers-2.17-325.el7_9.x86_64                                                                                                                                                 3/5 </span><br><span class="line">  正在安装    : glibc-devel-2.17-325.el7_9.x86_64                                                                                                                                                   4/5 </span><br><span class="line">  正在安装    : gcc-4.8.5-44.el7.x86_64                                                                                                                                                             5/5 </span><br><span class="line">  验证中      : gcc-4.8.5-44.el7.x86_64                                                                                                                                                             1/5 </span><br><span class="line">  验证中      : glibc-headers-2.17-325.el7_9.x86_64                                                                                                                                                 2/5 </span><br><span class="line">  验证中      : kernel-headers-3.10.0-1160.49.1.el7.x86_64                                                                                                                                          3/5 </span><br><span class="line">  验证中      : glibc-devel-2.17-325.el7_9.x86_64                                                                                                                                                   4/5 </span><br><span class="line">  验证中      : cpp-4.8.5-44.el7.x86_64                                                                                                                                                             5/5 </span><br><span class="line"></span><br><span class="line">已安装:</span><br><span class="line">  gcc.x86_64 0:4.8.5-44.el7                                                                                                                                                                             </span><br><span class="line"></span><br><span class="line">作为依赖被安装:</span><br><span class="line">  cpp.x86_64 0:4.8.5-44.el7              glibc-devel.x86_64 0:2.17-325.el7_9              glibc-headers.x86_64 0:2.17-325.el7_9              kernel-headers.x86_64 0:3.10.0-1160.49.1.el7             </span><br><span class="line"></span><br><span class="line">完毕！</span><br><span class="line">[root@hadoop102 software]# gcc --version</span><br><span class="line">gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)</span><br><span class="line">Copyright © 2015 Free Software Foundation, Inc.</span><br><span class="line">本程序是自由软件；请参看源代码的版权声明。本软件没有任何担保；</span><br><span class="line">包括没有适销性和某一专用目的下的适用性担保。</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点下载相应版本的 Python 包：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# wget https://www.python.org/ftp/python/3.9.9/Python-3.9.9.tar.xz</span><br><span class="line">--2021-12-24 10:16:02--  https://www.python.org/ftp/python/3.9.9/Python-3.9.9.tar.xz</span><br><span class="line">正在解析主机 www.python.org (www.python.org)... 151.101.72.223</span><br><span class="line">正在连接 www.python.org (www.python.org)|151.101.72.223|:443... 已连接。</span><br><span class="line">已发出 HTTP 请求，正在等待回应... 200 OK</span><br><span class="line">长度：19144372 (18M) [application/octet-stream]</span><br><span class="line">正在保存至: “Python-3.9.9.tar.xz”</span><br><span class="line"></span><br><span class="line"><span class="meta">100%</span><span class="bash">[==============================================================================================================================================================&gt;] 19,144,372  7.20MB/s 用时 2.5s   </span></span><br><span class="line"></span><br><span class="line">2021-12-24 10:16:05 (7.20 MB/s) - 已保存 “Python-3.9.9.tar.xz” [19144372/19144372])</span><br><span class="line"></span><br><span class="line">[root@hadoop101 software]# ls -l</span><br><span class="line">总用量 18696</span><br><span class="line">-rw-r--r--. 1 root root 19144372 11月 16 02:49 Python-3.9.9.tar.xz</span><br></pre></td></tr></table></figure>
</li>
<li><p>解压到指定目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# tar -xvJf Python-3.9.9.tar.xz -C /opt/module/</span><br><span class="line">[root@hadoop101 software]# ls -l /opt/module/</span><br><span class="line">总用量 2</span><br><span class="line">drwxrwxr-x. 16 xisun xisun 4096 11月 16 02:05 Python-3.9.9</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点安装依赖，否则会报错：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 software]# yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel</span><br><span class="line">...</span><br><span class="line">已安装:</span><br><span class="line">  bzip2-devel.x86_64 0:1.0.6-13.el7      gdbm-devel.x86_64 0:1.10-8.el7     libdb4-devel.x86_64 0:4.8.30-13.el7    libpcap-devel.x86_64 14:1.5.3-12.el7 ncurses-devel.x86_64 0:5.9-14.20130511.el7_4</span><br><span class="line">  openssl-devel.x86_64 1:1.0.2k-22.el7_9 readline-devel.x86_64 0:6.2-11.el7 sqlite-devel.x86_64 0:3.7.17-8.el7_7.1 tk-devel.x86_64 1:8.5.13-6.el7       xz-devel.x86_64 0:5.2.2-1.el7               </span><br><span class="line">  zlib-devel.x86_64 0:1.2.7-19.el7_9    </span><br><span class="line"></span><br><span class="line">作为依赖被安装:</span><br><span class="line">  expat-devel.x86_64 0:2.1.0-12.el7    fontconfig-devel.x86_64 0:2.13.0-4.3.el7 freetype-devel.x86_64 0:2.8-14.el7_9.1   keyutils-libs-devel.x86_64 0:1.5.8-3.el7 krb5-devel.x86_64 0:1.15.1-51.el7_9</span><br><span class="line">  libXft-devel.x86_64 0:2.3.2-2.el7    libXrender-devel.x86_64 0:0.9.10-1.el7   libcom_err-devel.x86_64 0:1.42.9-19.el7  libdb4.x86_64 0:4.8.30-13.el7            libpng-devel.x86_64 2:1.5.13-8.el7 </span><br><span class="line">  libselinux-devel.x86_64 0:2.5-15.el7 libsepol-devel.x86_64 0:2.5-10.el7       libuuid-devel.x86_64 0:2.23.2-65.el7_9.1 libverto-devel.x86_64 0:0.2.5-4.el7      pcre-devel.x86_64 0:8.32-17.el7</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点安装 Python3：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 指定安装的路径，不指定的话，安装过程中可能软件所需要的文件复制到其他不同目录，删除软件很不方便，复制软件也不方便</span></span><br><span class="line">[root@hadoop101 software]# mkdir /usr/local/python3</span><br><span class="line">[root@hadoop101 software]# cd /opt/module/Python-3.9.9/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置，指定安装目录</span></span><br><span class="line">[root@hadoop101 Python-3.9.9]# ./configure --prefix=/usr/local/python3</span><br><span class="line"><span class="meta">#</span><span class="bash"> 编译安装</span></span><br><span class="line">[root@hadoop101 Python-3.9.9]# make &amp;&amp; make install</span><br></pre></td></tr></table></figure>

<ul>
<li><p>在安装过程中，如果出现错误，在重新安装之前先执行下面的命令，清空缓存：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop101 Python-3.9.9]# make clean</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>各节点添加软链接：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 Python-3.9.9]# ln -s /usr/local/python3/bin/python3 /usr/bin/python3</span><br><span class="line">[root@hadoop102 Python-3.9.9]# ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3</span><br></pre></td></tr></table></figure>

<ul>
<li>软链接位置定为 <code>/usr/bin/python3</code> 和 <code>/usr/bin/pip3</code>。</li>
</ul>
</li>
<li><p>各节点查看版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 Python-3.9.9]# python3 --version</span><br><span class="line">Python 3.9.9</span><br><span class="line">[root@hadoop102 Python-3.9.9]# pip3 --version</span><br><span class="line">pip 21.2.4 from /usr/local/python3/lib/python3.9/site-packages/pip (python 3.9)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h3><ul>
<li><p>Cephadm 基于容器运行所有 Ceph 组件，各节点需要安装 Docker 或 Podman，此处安装 Docker。</p>
</li>
<li><p>各节点卸载旧版本 Docker（如果之前有安装过）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点安装需要的软件包，yum-util 提供 yum-config-manager 功能：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# yum install -y yum-utils</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点设置 yum 的 Docker 源（下面两个都可以用）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 中央仓库</span></span><br><span class="line">[root@hadoop102 opt]# yum-config-manager --add-repo http://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 阿里仓库</span></span><br><span class="line">[root@hadoop102 opt]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点安装最新版本 Docker：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# yum install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure>

<ul>
<li><p>安装特定版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看不同版本</span></span><br><span class="line">[root@hadoop102 opt]# yum list docker-ce --showduplicates | sort -r</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装特定版本</span></span><br><span class="line">[root@hadoop102 opt]# yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>各节点启动 Docker，并设置开机启动：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动</span></span><br><span class="line">[root@hadoop102 opt]# systemctl start docker</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看docker版本</span></span><br><span class="line">[root@hadoop102 opt]# docker --version</span><br><span class="line">Docker version 20.10.12, build e91ed57</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机启动</span></span><br><span class="line">[root@hadoop102 opt]# systemctl enable docker</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看docker是否开机启动</span></span><br><span class="line">[root@hadoop102 opt]# systemctl list-unit-files | grep docker</span><br><span class="line">docker.service                                enabled </span><br><span class="line">docker.socket                                 disabled</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# docker run hello-world</span><br><span class="line"></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">To generate this message, Docker took the following steps:</span><br><span class="line"> 1. The Docker client contacted the Docker daemon.</span><br><span class="line"> 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.</span><br><span class="line">    (amd64)</span><br><span class="line"> 3. The Docker daemon created a new container from that image which runs the</span><br><span class="line">    executable that produces the output you are currently reading.</span><br><span class="line"> 4. The Docker daemon streamed that output to the Docker client, which sent it</span><br><span class="line">    to your terminal.</span><br><span class="line"></span><br><span class="line">To try something more ambitious, you can run an Ubuntu container with:</span><br><span class="line"><span class="meta"> $</span><span class="bash"> docker run -it ubuntu bash</span></span><br><span class="line"></span><br><span class="line">Share images, automate workflows, and more with a free Docker ID:</span><br><span class="line"> https://hub.docker.com/</span><br><span class="line"></span><br><span class="line">For more examples and ideas, visit:</span><br><span class="line"> https://docs.docker.com/get-started/</span><br></pre></td></tr></table></figure>
</li>
<li><p>参考：<a target="_blank" rel="noopener" href="https://docs.docker.com/engine/install/centos/">https://docs.docker.com/engine/install/centos/</a></p>
</li>
</ul>
<h3 id="安装-Cephadm"><a href="#安装-Cephadm" class="headerlink" title="安装 Cephadm"></a>安装 Cephadm</h3><ul>
<li><p>各节点使用 curl 获取 Cephadm 独立脚本的最新版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载</span></span><br><span class="line">[root@hadoop102 opt]# curl --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm</span><br><span class="line"><span class="meta">  %</span><span class="bash"> Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span></span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100   137  100   137    0     0     15      0  0:00:09  0:00:09 --:--:--    18</span><br><span class="line">100  218k  100  218k    0     0  17210      0  0:00:12  0:00:12 --:--:-- 93775</span><br><span class="line">[root@hadoop102 opt]# ll</span><br><span class="line">总用量 220</span><br><span class="line">-rw-r--r--. 1 root  root  223468 12月 26 15:42 cephadm</span><br><span class="line">drwx--x--x. 4 root  root      28 12月 21 14:56 containerd</span><br><span class="line">drwxr-xr-x. 4 xisun xisun     46 12月 24 10:32 module</span><br><span class="line">drwxr-xr-x. 2 xisun xisun     33 12月 24 10:30 software</span><br><span class="line"><span class="meta">#</span><span class="bash"> 赋权可执行</span></span><br><span class="line">[root@hadoop102 opt]# chmod +x cephadm</span><br><span class="line">[root@hadoop102 opt]# ll</span><br><span class="line">总用量 220</span><br><span class="line">-rwxr-xr-x. 1 root  root  223468 12月 26 15:42 cephadm</span><br><span class="line">drwx--x--x. 4 root  root      28 12月 21 14:56 containerd</span><br><span class="line">drwxr-xr-x. 4 xisun xisun     46 12月 24 10:32 module</span><br><span class="line">drwxr-xr-x. 2 xisun xisun     33 12月 24 10:30 software</span><br></pre></td></tr></table></figure>

<ul>
<li>下载可能需要执行多次。</li>
</ul>
</li>
<li><p>各节点添加源信息，指定为 Octopus 版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 添加Ceph源，这个是官方源</span></span><br><span class="line">[root@hadoop102 opt]# ./cephadm add-repo --release octopus</span><br><span class="line">Writing repo to /etc/yum.repos.d/ceph.repo...</span><br><span class="line">Enabling EPEL...</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看Ceph源信息</span></span><br><span class="line">[root@hadoop102 opt]# ll /etc/yum.repos.d/</span><br><span class="line">总用量 56</span><br><span class="line">-rw-r--r--. 1 root root 2523 12月 21 14:16 CentOS-Base.repo</span><br><span class="line">-rw-r--r--. 1 root root 1309 11月 23 2020 CentOS-CR.repo</span><br><span class="line">-rw-r--r--. 1 root root  649 11月 23 2020 CentOS-Debuginfo.repo</span><br><span class="line">-rw-r--r--. 1 root root  314 11月 23 2020 CentOS-fasttrack.repo</span><br><span class="line">-rw-r--r--. 1 root root  630 11月 23 2020 CentOS-Media.repo</span><br><span class="line">-rw-r--r--. 1 root root 1331 11月 23 2020 CentOS-Sources.repo</span><br><span class="line">-rw-r--r--. 1 root root 8515 11月 23 2020 CentOS-Vault.repo</span><br><span class="line">-rw-r--r--. 1 root root  616 11月 23 2020 CentOS-x86_64-kernel.repo</span><br><span class="line">-rw-r--r--. 1 root root  477 12月 26 15:49 ceph.repo						# Ceph源</span><br><span class="line">-rw-r--r--. 1 root root 2081 12月 21 12:38 docker-ce.repo</span><br><span class="line">-rw-r--r--. 1 root root 1358 9月   5 01:37 epel.repo</span><br><span class="line">-rw-r--r--. 1 root root 1457 9月   5 01:37 epel-testing.repo</span><br><span class="line">[root@hadoop102 opt]# cat /etc/yum.repos.d/ceph.repo </span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph $basearch</span><br><span class="line">baseurl=https://download.ceph.com/rpm-octopus/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch</span><br><span class="line">baseurl=https://download.ceph.com/rpm-octopus/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">[Ceph-source]</span><br><span class="line">name=Ceph SRPMS</span><br><span class="line">baseurl=https://download.ceph.com/rpm-octopus/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br></pre></td></tr></table></figure>

<ul>
<li><p>添加 Ceph 源可能需要执行多次，或重启。</p>
</li>
<li><p>如果官方源下载较慢，可以使用阿里云 Ceph 源：</p>
<p><img src="/2021/12/21/ceph/image-20211227113311642.png" alt="image-20211227113311642"></p>
</li>
</ul>
</li>
<li><p>各节点安装 Cephadm：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# ./cephadm install</span><br><span class="line">Installing packages [&#x27;cephadm&#x27;]...</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点验证 Cephadm 安装完成：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# which cephadm</span><br><span class="line">/usr/sbin/cephadm</span><br><span class="line">[root@hadoop102 opt]# cephadm version</span><br><span class="line">Using recent ceph image quay.io/ceph/ceph@sha256:a2c23b6942f7fbc1e15d8cfacd6655a681fe0e44f288e4a158db22030b8d58e3</span><br><span class="line">ceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable)</span><br><span class="line">[root@hadoop102 opt]# cephadm --help</span><br><span class="line">usage: cephadm [-h] [--image IMAGE] [--docker] [--data-dir DATA_DIR] [--log-dir LOG_DIR] [--logrotate-dir LOGROTATE_DIR] [--unit-dir UNIT_DIR] [--verbose] [--timeout TIMEOUT] [--retry RETRY]</span><br><span class="line">               [--env ENV] [--no-container-init]</span><br><span class="line">               &#123;version,pull,inspect-image,ls,list-networks,adopt,rm-daemon,rm-cluster,run,shell,enter,ceph-volume,unit,logs,bootstrap,deploy,check-host,prepare-host,add-repo,rm-repo,install,registry-login,gather-facts&#125;</span><br><span class="line">               ...</span><br><span class="line"></span><br><span class="line">Bootstrap Ceph daemons with systemd and containers.</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  &#123;version,pull,inspect-image,ls,list-networks,adopt,rm-daemon,rm-cluster,run,shell,enter,ceph-volume,unit,logs,bootstrap,deploy,check-host,prepare-host,add-repo,rm-repo,install,registry-login,gather-facts&#125;</span><br><span class="line">                        sub-command</span><br><span class="line">    version             get ceph version from container</span><br><span class="line">    pull                pull latest image version</span><br><span class="line">    inspect-image       inspect local container image</span><br><span class="line">    ls                  list daemon instances on this host</span><br><span class="line">    list-networks       list IP networks</span><br><span class="line">    adopt               adopt daemon deployed with a different tool</span><br><span class="line">    rm-daemon           remove daemon instance</span><br><span class="line">    rm-cluster          remove all daemons for a cluster</span><br><span class="line">    run                 run a ceph daemon, in a container, in the foreground</span><br><span class="line">    shell               run an interactive shell inside a daemon container</span><br><span class="line">    enter               run an interactive shell inside a running daemon container</span><br><span class="line">    ceph-volume         run ceph-volume inside a container</span><br><span class="line">    unit                operate on the daemon&#x27;s systemd unit</span><br><span class="line">    logs                print journald logs for a daemon container</span><br><span class="line">    bootstrap           bootstrap a cluster (mon + mgr daemons)</span><br><span class="line">    deploy              deploy a daemon</span><br><span class="line">    check-host          check host configuration</span><br><span class="line">    prepare-host        prepare a host for cephadm use</span><br><span class="line">    add-repo            configure package repository</span><br><span class="line">    rm-repo             remove package repository configuration</span><br><span class="line">    install             install ceph package(s)</span><br><span class="line">    registry-login      log host into authenticated registry</span><br><span class="line">    gather-facts        gather and return host related information (JSON format)</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message and exit</span><br><span class="line">  --image IMAGE         container image. Can also be set via the &quot;CEPHADM_IMAGE&quot; env var (default: None)</span><br><span class="line">  --docker              use docker instead of podman (default: False)</span><br><span class="line">  --data-dir DATA_DIR   base directory for daemon data (default: /var/lib/ceph)</span><br><span class="line">  --log-dir LOG_DIR     base directory for daemon logs (default: /var/log/ceph)</span><br><span class="line">  --logrotate-dir LOGROTATE_DIR</span><br><span class="line">                        location of logrotate configuration files (default: /etc/logrotate.d)</span><br><span class="line">  --unit-dir UNIT_DIR   base directory for systemd units (default: /etc/systemd/system)</span><br><span class="line">  --verbose, -v         Show debug-level log messages (default: False)</span><br><span class="line">  --timeout TIMEOUT     timeout in seconds (default: None)</span><br><span class="line">  --retry RETRY         max number of retries (default: 10)</span><br><span class="line">  --env ENV, -e ENV     set environment variable (default: [])</span><br><span class="line">  --no-container-init   Do not run podman/docker with `--init` (default: True)</span><br></pre></td></tr></table></figure>
</li>
<li><p>为方便后续使用，各节点安装 ceph-common 包，里面包含了所有的 ceph 命令，其中包括 ceph，rbd，mount.ceph（用于安装 CephFS 文件系统）等：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装</span></span><br><span class="line">[root@hadoop102 opt]# cephadm install ceph-common</span><br><span class="line">Installing packages [&#x27;ceph-common&#x27;]...</span><br><span class="line"><span class="meta">#</span><span class="bash"> 确认可以使用</span></span><br><span class="line">[root@hadoop102 opt]# ceph -v</span><br><span class="line">ceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="创建-Ceph-新集群"><a href="#创建-Ceph-新集群" class="headerlink" title="创建 Ceph 新集群"></a>创建 Ceph 新集群</h3><ul>
<li><p>在 hadoop102 上创建一个可以被任何访问 Ceph 集群的主机访问的网络，指定 mon-ip，并将生成的配置文件写进 <code>/etc/ceph</code> 目录里：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# cephadm bootstrap --mon-ip 192.168.10.102</span><br><span class="line">Verifying podman|docker is present...</span><br><span class="line">Verifying lvm2 is present...</span><br><span class="line">Verifying time synchronization is in place...</span><br><span class="line">Unit ntpd.service is enabled and running</span><br><span class="line">Repeating the final host check...</span><br><span class="line">podman|docker (/usr/bin/docker) is present</span><br><span class="line">systemctl is present</span><br><span class="line">lvcreate is present</span><br><span class="line">Unit ntpd.service is enabled and running</span><br><span class="line">Host looks OK</span><br><span class="line">Cluster fsid: 81b469b6-662d-11ec-b2eb-000c29c51d96</span><br><span class="line">Verifying IP 192.168.10.102 port 3300 ...</span><br><span class="line">Verifying IP 192.168.10.102 port 6789 ...</span><br><span class="line">Mon IP 192.168.10.102 is in CIDR network 192.168.10.0/24</span><br><span class="line">Pulling container image quay.io/ceph/ceph:v15...</span><br><span class="line">Extracting ceph user uid/gid from container image...</span><br><span class="line">Creating initial keys...</span><br><span class="line">Creating initial monmap...</span><br><span class="line">Creating mon...</span><br><span class="line">Waiting for mon to start...</span><br><span class="line">Waiting for mon...</span><br><span class="line">mon is available</span><br><span class="line">Assimilating anything we can from ceph.conf...</span><br><span class="line">Generating new minimal ceph.conf...</span><br><span class="line">Restarting the monitor...</span><br><span class="line">Setting mon public_network...</span><br><span class="line">Creating mgr...</span><br><span class="line">Verifying port 9283 ...</span><br><span class="line">Wrote keyring to /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">Wrote config to /etc/ceph/ceph.conf</span><br><span class="line">Waiting for mgr to start...</span><br><span class="line">Waiting for mgr...</span><br><span class="line">mgr not available, waiting (1/10)...</span><br><span class="line">mgr not available, waiting (2/10)...</span><br><span class="line">mgr not available, waiting (3/10)...</span><br><span class="line">mgr not available, waiting (4/10)...</span><br><span class="line">mgr not available, waiting (5/10)...</span><br><span class="line">mgr is available</span><br><span class="line">Enabling cephadm module...</span><br><span class="line">Waiting for the mgr to restart...</span><br><span class="line">Waiting for Mgr epoch 5...</span><br><span class="line">Mgr epoch 5 is available</span><br><span class="line">Setting orchestrator backend to cephadm...</span><br><span class="line">Generating ssh key...</span><br><span class="line">Wrote public SSH key to to /etc/ceph/ceph.pub</span><br><span class="line">Adding key to root@localhost&#x27;s authorized_keys...</span><br><span class="line">Adding host hadoop102...</span><br><span class="line">Deploying mon service with default placement...</span><br><span class="line">Deploying mgr service with default placement...</span><br><span class="line">Deploying crash service with default placement...</span><br><span class="line">Enabling mgr prometheus module...</span><br><span class="line">Deploying prometheus service with default placement...</span><br><span class="line">Deploying grafana service with default placement...</span><br><span class="line">Deploying node-exporter service with default placement...</span><br><span class="line">Deploying alertmanager service with default placement...</span><br><span class="line">Enabling the dashboard module...</span><br><span class="line">Waiting for the mgr to restart...</span><br><span class="line">Waiting for Mgr epoch 13...</span><br><span class="line">Mgr epoch 13 is available</span><br><span class="line">Generating a dashboard self-signed certificate...</span><br><span class="line">Creating initial admin user...</span><br><span class="line">Fetching dashboard port number...</span><br><span class="line">Ceph Dashboard is now available at:</span><br><span class="line"></span><br><span class="line">	     URL: https://hadoop102:8443/</span><br><span class="line">	    User: admin</span><br><span class="line">	Password: v5b0064nc4</span><br><span class="line"></span><br><span class="line">You can access the Ceph CLI with:</span><br><span class="line"></span><br><span class="line">	sudo /usr/sbin/cephadm shell --fsid 81b469b6-662d-11ec-b2eb-000c29c51d96 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line">Please consider enabling telemetry to help improve Ceph:</span><br><span class="line"></span><br><span class="line">	ceph telemetry on</span><br><span class="line"></span><br><span class="line">For more information see:</span><br><span class="line"></span><br><span class="line">	https://docs.ceph.com/docs/master/mgr/telemetry/</span><br><span class="line"></span><br><span class="line">Bootstrap complete.</span><br></pre></td></tr></table></figure>

<ul>
<li><p>该命令执行如下操作：</p>
<ul>
<li><p>在本地主机上为新集群创建 monitor  和 manager daemon 守护程序。</p>
</li>
<li><p>为 Ceph 集群生成一个新的 SSH 密钥，并将其添加到 root 用户的 <code>/root/.ssh/authorized_keys</code> 文件中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkEXI7QySiz2VlLNrYW7OypfS9ZtMXS2h725W+pTpv71e4z6YjjxIpt6DvggtuD/UhktL1HxIvAREYKQ5vNYhGuV9jHI4VU+y4evpPEMR8GJ8OTlhfwhwovsik4TfYW0SmHUw01bxBpH2BjX4u/dSpyN9KG9710WXTIGJQ6zbHjxPg0JQX4C1KLuaD1WMVo1z6xscRGs7RAZ7sYAIQdf8LOI4IYVzO1n66NJNY4vpAJx1wCLajoGN0689Do42jX6t0/Qn6RGddXermTbbPW+hcD20pRIHO4f9VCQwqSNdcEVDUS74VMJRuOxd+N0uAxvHz8lTdCSfUnfPeKEvEH+cr root@hadoop102</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9lRXTDUhUKdfu5PznYhoFsJFw1n6qXKJXCgFmuFCBB3T75na5xSgGbUEcQ5HYIWk8ywjynGjzCQMlyG+DTaQVvxOcpNNyKv57sm2quegFzu+c86Q726npspidr8Knl7dJALXF3q8k3eQrscWrRyJEA6vh1PikrNtDwzdB92WoYkAH5dJj3MxuHPJXoX+UbOZ8jWV7dARBT2LXFiS09bJy+7P2QZSa5BVirWFzoRjAD4JlEeaW2OPcujHTe+IjHXale4fjNh0tUDtteMRcwLnfMpTl6M7qgpUXm/IrlJHfAqtPmYPQUiDCVFQ9wntTeF5ph6WM9qk/2BJni5BX44VH root@hadoop103</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDsgI3CnXcTkQkSHxvhGvBGY1Fh7gaIXqhI5n7rtEM3fmJ2k8/VoWjLwxE1xqSm7tsGufabIhpLdUJBfj4cpqa+PazgSVF59cPlYArp115YFOCGWv2z9tZ5Cq70i9EEfNCyhWNhNUEd9cqiei/G1by4yQzOwvpiuDhiGlDtQk3pRGvvdjGQV3YVDVjDv6JG3QQkxPY+oYQFjDBy+usGEvcuIFDJXmS4hlB8xDMZCw8R0gcvBEnA9RxIIb0N0jLf0uZt0varFZqlguwJJdT6nBCWJ+NYxu5aggX7PGgHB+F97gCoIRusqjT/scoS+jSK8yx4WzJ6ZfCp1wQEXJ+XgrbX root@hadoop104</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDCfFd837abQYj+trazR8y5OSOc7ET+vzn0o6pR+JF3N1knD5JGKXyB0umMsYehwcU55XkMARHRS+jothz0PAqWUpdV73pVZyy+Nxhu/5rySf+NiaoYd8IYAtcgbw174E5deudPF5Ac7IIUxJAgPxEb/NnrpYEPFLHg31ZCKUvzHkEbP9I1iMgM9LtKN3M/aAbaHY2Pd046joVhOo7DsIlYyd147X1MgFhwTeVThpu+zdjPwNoenQGIhrH1J46thBL8TyTC0Wjui5YVwim9Nxoet7E46BiuqrAS6TBvLOTAZlD/g2UB+6NbGYdQKpOs8k2XQqrEBtPB4Wo/hZ0efg8gvCs4AaOqICaMOUt52lfJHInnsycB99nxh95sLDhkHfQIlqTEvGPQDiJVU8tPS5mo7+oeqpCTsvnIINQTMlCtA6eR6kmNumCdSpwbCMkuwVxHjGgNxMy3ykBRItU0vH9mAv/5OxyfjKQutELHQn+5fb5v0lrDq/WZ60jMJYGpTh8= ceph-81b469b6-662d-11ec-b2eb-000c29c51d96</span><br></pre></td></tr></table></figure>
</li>
<li><p>将与新群集进行通信所需的最小配置文件保存到 <code>/etc/ceph/ceph.conf</code>。</p>
</li>
<li><p>向 <code>/etc/ceph/ceph.client.admin.keyring</code> 写入 <code>client.admin</code> 管理 secret key 的副本（特权！）。</p>
</li>
<li><p>将 public key 的副本写入 <code>/etc/ceph/ceph.pub</code>。</p>
</li>
</ul>
</li>
<li><p>查看当前配置文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# ll /etc/ceph/</span><br><span class="line">总用量 12</span><br><span class="line">-rw-------. 1 root root  63 12月 26 17:23 ceph.client.admin.keyring</span><br><span class="line">-rw-r--r--. 1 root root 179 12月 26 17:23 ceph.conf</span><br><span class="line">-rw-r--r--. 1 root root 595 12月 26 17:24 ceph.pub</span><br><span class="line">[root@hadoop102 opt]# cat /etc/ceph/ceph.conf </span><br><span class="line"><span class="meta">#</span><span class="bash"> minimal ceph.conf <span class="keyword">for</span> 81b469b6-662d-11ec-b2eb-000c29c51d96</span></span><br><span class="line">[global]</span><br><span class="line">	fsid = 81b469b6-662d-11ec-b2eb-000c29c51d96</span><br><span class="line">	mon_host = [v2:192.168.10.102:3300/0,v1:192.168.10.102:6789/0]</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看拉取的镜像和启动的容器：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装之前</span></span><br><span class="line">[root@hadoop104 opt]# docker images</span><br><span class="line">REPOSITORY          TAG       IMAGE ID       CREATED        SIZE</span><br><span class="line">hello-world         latest    feb5d9fea6a5   3 months ago   13.3kB</span><br><span class="line">[root@hadoop104 opt]# docker ps -a</span><br><span class="line">CONTAINER ID   IMAGE         COMMAND    CREATED        STATUS                  PORTS     NAMES</span><br><span class="line">72c655a613fc   hello-world   &quot;/hello&quot;   41 hours ago   Created                           gallant_cohen</span><br><span class="line">de6240f04135   hello-world   &quot;/hello&quot;   5 days ago     Exited (0) 5 days ago             priceless_nash</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装之后</span></span><br><span class="line">[root@hadoop102 opt]# docker images</span><br><span class="line">REPOSITORY                         TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">quay.io/ceph/ceph                  v15       3437f7bed968   2 months ago    1.08GB		# Ceph组件</span><br><span class="line">hello-world                        latest    feb5d9fea6a5   3 months ago    13.3kB</span><br><span class="line">quay.io/ceph/ceph-grafana          6.7.4     557c83e11646   4 months ago    486MB		# Ceph组件</span><br><span class="line">quay.io/prometheus/prometheus      v2.18.1   de242295e225   19 months ago   140MB		# Ceph组件</span><br><span class="line">quay.io/prometheus/alertmanager    v0.20.0   0881eb8f169f   2 years ago     52.1MB		# Ceph组件</span><br><span class="line">quay.io/prometheus/node-exporter   v0.18.1   e5a616e4b9cf   2 years ago     22.9MB		# Ceph组件</span><br><span class="line">[root@hadoop102 opt]# docker ps -a</span><br><span class="line">CONTAINER ID   IMAGE                                      COMMAND                  CREATED          STATUS                    PORTS     NAMES</span><br><span class="line">5e62fe89c60f   quay.io/ceph/ceph-grafana:6.7.4            &quot;/bin/sh -c &#x27;grafana…&quot;   45 minutes ago   Up 45 minutes                       ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-grafana.hadoop102				# Ceph组件</span><br><span class="line">fcf1abbde49f   quay.io/prometheus/alertmanager:v0.20.0    &quot;/bin/alertmanager -…&quot;   45 minutes ago   Up 45 minutes                       ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-alertmanager.hadoop102		# Ceph组件</span><br><span class="line">ee65841914dd   quay.io/prometheus/prometheus:v2.18.1      &quot;/bin/prometheus --c…&quot;   45 minutes ago   Up 45 minutes                       ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-prometheus.hadoop102			# Ceph组件</span><br><span class="line">b248a474c78e   quay.io/prometheus/node-exporter:v0.18.1   &quot;/bin/node_exporter …&quot;   46 minutes ago   Up 46 minutes                       ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-node-exporter.hadoop102		# Ceph组件</span><br><span class="line">31dd5f2a7479   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-crash…&quot;   50 minutes ago   Up 50 minutes                       ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-crash.hadoop102				# Ceph组件</span><br><span class="line">443c33b517e0   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-mgr -…&quot;   52 minutes ago   Up 52 minutes                       ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-mgr.hadoop102.kwrjaw			# Ceph组件</span><br><span class="line">895542296c01   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-mon -…&quot;   52 minutes ago   Up 52 minutes                       ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-mon.hadoop102					# Ceph组件</span><br><span class="line">7213f1a0510a   hello-world                                &quot;/hello&quot;                 27 hours ago     Exited (0) 27 hours ago             hungry_driscoll</span><br><span class="line">ad4e221a918b   hello-world                                &quot;/hello&quot;                 5 days ago       Exited (0) 5 days ago               kind_elbakyan</span><br></pre></td></tr></table></figure>

<ul>
<li>此时已经运行了以下组件：<ul>
<li>ceph-mgr：Ceph 管理程序。</li>
<li>ceph-monitor：Ceph 监视器。</li>
<li>ceph-crash：崩溃数据收集模块。</li>
<li>prometheus：prometheus 监控组件。</li>
<li>grafana：监控数据展示 dashboard。</li>
<li>alertmanager：prometheus 告警组件。</li>
<li>node_exporter：prometheus 节点数据收集组件。</li>
</ul>
</li>
</ul>
</li>
<li><p>参阅下面的一些对某些用户可能有用的选项，或者运行 <code>cephadm bootstrap -h</code> 命令查看所有可用选项：</p>
<ul>
<li>为了方便起见，Bootstrap 会将访问新集群所需的文件写入 <code>/etc/ceph</code>，以便主机上安装的任何 Ceph 软件包（例如，访问命令行界面）都可以轻松找到它们。</li>
<li>但是使用 cephadm 部署的 daemon 容器根本不需要 <code>/etc/ceph</code>。避免与同一主机上的现有 Ceph 配置（cephadm 或其他方式）存在潜在冲突，可以使用 –output-dir 选项将它们放置在不同的目录中。</li>
<li>可以使用 –config 选项将任何初始 Ceph 配置选项传递到新集群，方法是将它们放在标准 ini 样式的配置文件中。</li>
</ul>
</li>
<li><p>查看容器状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# cephadm ls</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;mon.hadoop102&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@mon.hadoop102&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;db378568f82337811eff2c7c44ad1a713d955a5b04fcde47c18674671e735704&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;15.2.15&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2021-12-27T09:11:27.486408Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2021-12-26T09:23:53.780886Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2021-12-26T09:23:52.866923Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2021-12-26T09:30:45.494492Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;mgr.hadoop102.kwrjaw&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@mgr.hadoop102.kwrjaw&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;32e7e6a303b2af413b90739fde7e1f3b60c26214a1a5fe8d34d36133f733ea61&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;15.2.15&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2021-12-27T09:11:24.388823Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2021-12-26T09:23:57.956720Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2021-12-26T09:23:57.302746Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2021-12-26T09:30:46.551456Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;alertmanager.hadoop102&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@alertmanager.hadoop102&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;843d8e781d923fa302322c78e5251ab1fa4b7b3d5a5998906ee30e089cc2e506&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/prometheus/alertmanager:v0.20.0&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;0881eb8f169f5556a292b4e2c01d683172b12830a62a9225a98a8e206bb734f0&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;0.20.0&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2021-12-27T09:11:30.498509Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2021-12-26T09:25:51.990176Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2021-12-26T09:25:51.485197Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2021-12-26T09:30:48.260398Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;crash.hadoop102&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@crash.hadoop102&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;2d08d2614605c2a4350c56cf1733f85cd5916afc03a899ce65c600a437c36184&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;15.2.15&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2021-12-27T09:11:28.253480Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2021-12-26T09:25:54.341083Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2021-12-26T09:25:53.839103Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2021-12-26T09:25:54.341083Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;grafana.hadoop102&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@grafana.hadoop102&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;cbb4a8a923e2f3205e02e9dd026d3fdcf1a07c55a3eda6b9de13408dadc0465a&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph-grafana:6.7.4&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;557c83e11646f123a27b5e4b62ac6c45e7bb8b2e90d6044034d0db5b7019415c&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;6.7.4&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2021-12-27T09:11:27.676553Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2021-12-26T09:29:44.175925Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2021-12-26T09:29:42.555990Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2021-12-26T09:30:51.194299Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;node-exporter.hadoop102&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@node-exporter.hadoop102&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;ee29b873a834252d1d55c20c67ecfbff9a3b6acf8621063efd9070a79cb1cba5&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/prometheus/node-exporter:v0.18.1&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;e5a616e4b9cf68dfcad7782b78e118be4310022e874d52da85c55923fb615f87&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;0.18.1&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2021-12-27T09:11:21.758252Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2021-12-26T09:29:47.899777Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2021-12-26T09:29:47.307801Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2021-12-26T09:29:47.899777Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;prometheus.hadoop102&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@prometheus.hadoop102&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;f236295847bf07d2db2b3c79d3a209f5db369cf2a61ae7bb7d2ef7597982eb23&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/prometheus/prometheus:v2.18.1&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;de242295e2257c37c8cadfd962369228f8f10b2d48a44259b65fef44ad4f6490&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;2.18.1&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2021-12-27T09:11:25.545626Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2021-12-26T09:30:36.125856Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2021-12-26T09:30:35.546879Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2021-12-26T09:30:36.125856Z&quot;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据初始化完成的提示使用浏览器访问 dashboard：</p>
<p><img src="/2021/12/21/ceph/image-20211227141903939.png" alt="image-20211227141903939"></p>
<ul>
<li><p>使用虚拟机自带的火狐浏览器登陆 dashboard，初次登陆需要修改密码（admin，xisun_ceph001）：</p>
<p><img src="/2021/12/21/ceph/image-20211227172731909.png" alt="image-20211227172731909"></p>
</li>
</ul>
</li>
<li><p>如果上述引导集群的命令执行过程发生了异常，需要删除已经添加的配置文件，并关闭已经启动的 Ceph 组件进程，然后重新执行命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看已经添加的配置文件</span></span><br><span class="line">[root@hadoop102 opt]# ll /etc/ceph/</span><br><span class="line">总用量 12</span><br><span class="line">-rw-------. 1 root root  63 12月 26 17:23 ceph.client.admin.keyring</span><br><span class="line">-rw-r--r--. 1 root root 179 12月 26 17:23 ceph.conf</span><br><span class="line">-rw-r--r--. 1 root root 595 12月 26 17:24 ceph.pub</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除已经添加的配置文件，或者重新执行命令时，添加--allow-overwrite参数</span></span><br><span class="line">[root@hadoop102 opt]# rm /etc/ceph/*</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看已经启动的Ceph组件所占用的端口</span></span><br><span class="line">[root@hadoop102 opt]# netstat -ntlp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span><br><span class="line">tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      713/rpcbind         </span><br><span class="line">tcp        0      0 0.0.0.0:6800            0.0.0.0:*               LISTEN      2864/ceph-mgr       # 删除</span><br><span class="line">tcp        0      0 0.0.0.0:6801            0.0.0.0:*               LISTEN      2864/ceph-mgr       # 删除</span><br><span class="line">tcp        0      0 192.168.122.1:53        0.0.0.0:*               LISTEN      1467/dnsmasq        </span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1041/sshd           </span><br><span class="line">tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1044/cupsd          </span><br><span class="line">tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1234/master         </span><br><span class="line">tcp        0      0 127.0.0.1:6010          0.0.0.0:*               LISTEN      2131/sshd: xisun@pt </span><br><span class="line">tcp        0      0 192.168.10.102:3300     0.0.0.0:*               LISTEN      3026/ceph-mon       # 删除</span><br><span class="line">tcp        0      0 192.168.10.102:6789     0.0.0.0:*               LISTEN      3026/ceph-mon       # 删除</span><br><span class="line">tcp6       0      0 :::111                  :::*                    LISTEN      713/rpcbind         </span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      1041/sshd           </span><br><span class="line">tcp6       0      0 ::1:631                 :::*                    LISTEN      1044/cupsd          </span><br><span class="line">tcp6       0      0 ::1:25                  :::*                    LISTEN      1234/master         </span><br><span class="line">tcp6       0      0 ::1:6010                :::*                    LISTEN      2131/sshd: xisun@pt </span><br><span class="line"><span class="meta">#</span><span class="bash"> 同时删除ceph-mgr和ceph-mon这两个进程</span></span><br><span class="line">[root@hadoop102 opt]# kill -9 2864 3026</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h3 id="启用-Ceph-命令"><a href="#启用-Ceph-命令" class="headerlink" title="启用 Ceph 命令"></a>启用 Ceph 命令</h3><ul>
<li><p>Cephadm 不需要在主机上安装任何 Ceph 包。但是，建议启用对 Ceph 命令的简单访问。</p>
</li>
<li><p><code>cephadm shell</code> 命令在安装了所有 Ceph 包的容器中启动一个 bash shell。默认情况下，如果在主机上的 <code>/etc/ceph</code> 路径中找到配置和 keyring 文件，则会将它们传递到容器环境中，这样 shell 就可以完全正常工作。注意，在 MON 主机上执行时，<code>cephadm shell</code> 将从 MON 容器推断配置，而不是使用默认配置。如果给定 –mount，则主机（文件或目录）将显示在容器中的 <code>/mnt</code> 下。在 hadoop102 上执行下面的命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动一个bash shell</span></span><br><span class="line">[root@hadoop102 opt]# cephadm shell</span><br><span class="line">Inferring fsid 81b469b6-662d-11ec-b2eb-000c29c51d96</span><br><span class="line">Inferring config /var/lib/ceph/81b469b6-662d-11ec-b2eb-000c29c51d96/mon.hadoop102/config</span><br><span class="line">Using recent ceph image quay.io/ceph/ceph@sha256:a2c23b6942f7fbc1e15d8cfacd6655a681fe0e44f288e4a158db22030b8d58e3</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建别名</span></span><br><span class="line">[ceph: root@hadoop102 /]# alias ceph=&#x27;cephadm shell -- ceph&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出</span></span><br><span class="line">[ceph: root@hadoop102 /]# exit</span><br><span class="line">exit</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看集群状态，使用ceph -s 或者 ceph status命令</span></span><br><span class="line">[root@hadoop102 opt]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     81b469b6-662d-11ec-b2eb-000c29c51d96</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            Reduced data availability: 1 pg inactive</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum hadoop102 (age 17m)</span><br><span class="line">    mgr: hadoop102.kwrjaw(active, since 15m)</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 1 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:     100.000% pgs unknown</span><br><span class="line">             1 unknown</span><br><span class="line"> </span><br><span class="line">[root@hadoop102 opt]# ceph status</span><br><span class="line">  cluster:</span><br><span class="line">    id:     81b469b6-662d-11ec-b2eb-000c29c51d96</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            Reduced data availability: 1 pg inactive</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum hadoop102 (age 17m)</span><br><span class="line">    mgr: hadoop102.kwrjaw(active, since 15m)</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 1 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:     100.000% pgs unknown</span><br><span class="line">             1 unknown</span><br><span class="line"> </span><br><span class="line">[root@hadoop102 opt]# ceph health</span><br><span class="line">HEALTH_WARN Reduced data availability: 1 pg inactive; OSD count 0 &lt; osd_pool_default_size 3</span><br></pre></td></tr></table></figure>

<ul>
<li><p>在执行过程中发生了异常，解决如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ceph]# ceph shell</span><br><span class="line">2021-12-27T14:45:22.661+0800 7faad8e1c700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2,1]</span><br><span class="line">[errno 13] RADOS permission denied (error connecting to the cluster)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有的集群数据文件夹</span></span><br><span class="line">[root@hadoop102 ceph]# ll /var/lib/ceph</span><br><span class="line">总用量 0</span><br><span class="line">drwx------.  5 ceph           ceph   68 12月 26 16:42 6f08de88-6627-11ec-ad8e-000c29c51d96</span><br><span class="line">drwx------. 10 libstoragemgmt cgred 205 12月 26 17:30 81b469b6-662d-11ec-b2eb-000c29c51d96</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看配置中的集群数据文件夹</span></span><br><span class="line">[root@hadoop102 ceph]# cat /etc/ceph/ceph.conf </span><br><span class="line"><span class="meta">#</span><span class="bash"> minimal ceph.conf <span class="keyword">for</span> 81b469b6-662d-11ec-b2eb-000c29c51d96</span></span><br><span class="line">[global]</span><br><span class="line">	fsid = 81b469b6-662d-11ec-b2eb-000c29c51d96</span><br><span class="line">	mon_host = [v2:192.168.10.102:3300/0,v1:192.168.10.102:6789/0]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除旧的集群数据文件夹，然后重新执行命令</span></span><br><span class="line">[root@hadoop102 ceph]# rm -rf /var/lib/ceph/6f08de88-6627-11ec-ad8e-000c29c51d96/</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h3 id="添加新主机到集群中"><a href="#添加新主机到集群中" class="headerlink" title="添加新主机到集群中"></a>添加新主机到集群中</h3><ul>
<li><p>第一步，在 hadoop102 上执行命令，将集群的公共 SSH 密钥添加到新主机的根用户 authorized_keys 文件中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 opt]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@hadoop103</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;</span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;root@hadoop103&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"></span><br><span class="line">[root@hadoop102 opt]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@hadoop104</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;</span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;root@hadoop104&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line">[root@hadoop102 opt]# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkEXI7QySiz2VlLNrYW7OypfS9ZtMXS2h725W+pTpv71e4z6YjjxIpt6DvggtuD/UhktL1HxIvAREYKQ5vNYhGuV9jHI4VU+y4evpPEMR8GJ8OTlhfwhwovsik4TfYW0SmHUw01bxBpH2BjX4u/dSpyN9KG9710WXTIGJQ6zbHjxPg0JQX4C1KLuaD1WMVo1z6xscRGs7RAZ7sYAIQdf8LOI4IYVzO1n66NJNY4vpAJx1wCLajoGN0689Do42jX6t0/Qn6RGddXermTbbPW+hcD20pRIHO4f9VCQwqSNdcEVDUS74VMJRuOxd+N0uAxvHz8lTdCSfUnfPeKEvEH+cr root@hadoop102</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9lRXTDUhUKdfu5PznYhoFsJFw1n6qXKJXCgFmuFCBB3T75na5xSgGbUEcQ5HYIWk8ywjynGjzCQMlyG+DTaQVvxOcpNNyKv57sm2quegFzu+c86Q726npspidr8Knl7dJALXF3q8k3eQrscWrRyJEA6vh1PikrNtDwzdB92WoYkAH5dJj3MxuHPJXoX+UbOZ8jWV7dARBT2LXFiS09bJy+7P2QZSa5BVirWFzoRjAD4JlEeaW2OPcujHTe+IjHXale4fjNh0tUDtteMRcwLnfMpTl6M7qgpUXm/IrlJHfAqtPmYPQUiDCVFQ9wntTeF5ph6WM9qk/2BJni5BX44VH root@hadoop103</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDsgI3CnXcTkQkSHxvhGvBGY1Fh7gaIXqhI5n7rtEM3fmJ2k8/VoWjLwxE1xqSm7tsGufabIhpLdUJBfj4cpqa+PazgSVF59cPlYArp115YFOCGWv2z9tZ5Cq70i9EEfNCyhWNhNUEd9cqiei/G1by4yQzOwvpiuDhiGlDtQk3pRGvvdjGQV3YVDVjDv6JG3QQkxPY+oYQFjDBy+usGEvcuIFDJXmS4hlB8xDMZCw8R0gcvBEnA9RxIIb0N0jLf0uZt0varFZqlguwJJdT6nBCWJ+NYxu5aggX7PGgHB+F97gCoIRusqjT/scoS+jSK8yx4WzJ6ZfCp1wQEXJ+XgrbX root@hadoop104</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDCfFd837abQYj+trazR8y5OSOc7ET+vzn0o6pR+JF3N1knD5JGKXyB0umMsYehwcU55XkMARHRS+jothz0PAqWUpdV73pVZyy+Nxhu/5rySf+NiaoYd8IYAtcgbw174E5deudPF5Ac7IIUxJAgPxEb/NnrpYEPFLHg31ZCKUvzHkEbP9I1iMgM9LtKN3M/aAbaHY2Pd046joVhOo7DsIlYyd147X1MgFhwTeVThpu+zdjPwNoenQGIhrH1J46thBL8TyTC0Wjui5YVwim9Nxoet7E46BiuqrAS6TBvLOTAZlD/g2UB+6NbGYdQKpOs8k2XQqrEBtPB4Wo/hZ0efg8gvCs4AaOqICaMOUt52lfJHInnsycB99nxh95sLDhkHfQIlqTEvGPQDiJVU8tPS5mo7+oeqpCTsvnIINQTMlCtA6eR6kmNumCdSpwbCMkuwVxHjGgNxMy3ykBRItU0vH9mAv/5OxyfjKQutELHQn+5fb5v0lrDq/WZ60jMJYGpTh8= ceph-81b469b6-662d-11ec-b2eb-000c29c51d96</span><br></pre></td></tr></table></figure>
</li>
<li><p>在新结点上查看密钥是否添加成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 opt]# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkEXI7QySiz2VlLNrYW7OypfS9ZtMXS2h725W+pTpv71e4z6YjjxIpt6DvggtuD/UhktL1HxIvAREYKQ5vNYhGuV9jHI4VU+y4evpPEMR8GJ8OTlhfwhwovsik4TfYW0SmHUw01bxBpH2BjX4u/dSpyN9KG9710WXTIGJQ6zbHjxPg0JQX4C1KLuaD1WMVo1z6xscRGs7RAZ7sYAIQdf8LOI4IYVzO1n66NJNY4vpAJx1wCLajoGN0689Do42jX6t0/Qn6RGddXermTbbPW+hcD20pRIHO4f9VCQwqSNdcEVDUS74VMJRuOxd+N0uAxvHz8lTdCSfUnfPeKEvEH+cr root@hadoop102</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9lRXTDUhUKdfu5PznYhoFsJFw1n6qXKJXCgFmuFCBB3T75na5xSgGbUEcQ5HYIWk8ywjynGjzCQMlyG+DTaQVvxOcpNNyKv57sm2quegFzu+c86Q726npspidr8Knl7dJALXF3q8k3eQrscWrRyJEA6vh1PikrNtDwzdB92WoYkAH5dJj3MxuHPJXoX+UbOZ8jWV7dARBT2LXFiS09bJy+7P2QZSa5BVirWFzoRjAD4JlEeaW2OPcujHTe+IjHXale4fjNh0tUDtteMRcwLnfMpTl6M7qgpUXm/IrlJHfAqtPmYPQUiDCVFQ9wntTeF5ph6WM9qk/2BJni5BX44VH root@hadoop103</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDsgI3CnXcTkQkSHxvhGvBGY1Fh7gaIXqhI5n7rtEM3fmJ2k8/VoWjLwxE1xqSm7tsGufabIhpLdUJBfj4cpqa+PazgSVF59cPlYArp115YFOCGWv2z9tZ5Cq70i9EEfNCyhWNhNUEd9cqiei/G1by4yQzOwvpiuDhiGlDtQk3pRGvvdjGQV3YVDVjDv6JG3QQkxPY+oYQFjDBy+usGEvcuIFDJXmS4hlB8xDMZCw8R0gcvBEnA9RxIIb0N0jLf0uZt0varFZqlguwJJdT6nBCWJ+NYxu5aggX7PGgHB+F97gCoIRusqjT/scoS+jSK8yx4WzJ6ZfCp1wQEXJ+XgrbX root@hadoop104</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDCfFd837abQYj+trazR8y5OSOc7ET+vzn0o6pR+JF3N1knD5JGKXyB0umMsYehwcU55XkMARHRS+jothz0PAqWUpdV73pVZyy+Nxhu/5rySf+NiaoYd8IYAtcgbw174E5deudPF5Ac7IIUxJAgPxEb/NnrpYEPFLHg31ZCKUvzHkEbP9I1iMgM9LtKN3M/aAbaHY2Pd046joVhOo7DsIlYyd147X1MgFhwTeVThpu+zdjPwNoenQGIhrH1J46thBL8TyTC0Wjui5YVwim9Nxoet7E46BiuqrAS6TBvLOTAZlD/g2UB+6NbGYdQKpOs8k2XQqrEBtPB4Wo/hZ0efg8gvCs4AaOqICaMOUt52lfJHInnsycB99nxh95sLDhkHfQIlqTEvGPQDiJVU8tPS5mo7+oeqpCTsvnIINQTMlCtA6eR6kmNumCdSpwbCMkuwVxHjGgNxMy3ykBRItU0vH9mAv/5OxyfjKQutELHQn+5fb5v0lrDq/WZ60jMJYGpTh8= ceph-81b469b6-662d-11ec-b2eb-000c29c51d96			# 公共SSH密钥</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop104 opt]# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkEXI7QySiz2VlLNrYW7OypfS9ZtMXS2h725W+pTpv71e4z6YjjxIpt6DvggtuD/UhktL1HxIvAREYKQ5vNYhGuV9jHI4VU+y4evpPEMR8GJ8OTlhfwhwovsik4TfYW0SmHUw01bxBpH2BjX4u/dSpyN9KG9710WXTIGJQ6zbHjxPg0JQX4C1KLuaD1WMVo1z6xscRGs7RAZ7sYAIQdf8LOI4IYVzO1n66NJNY4vpAJx1wCLajoGN0689Do42jX6t0/Qn6RGddXermTbbPW+hcD20pRIHO4f9VCQwqSNdcEVDUS74VMJRuOxd+N0uAxvHz8lTdCSfUnfPeKEvEH+cr root@hadoop102</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9lRXTDUhUKdfu5PznYhoFsJFw1n6qXKJXCgFmuFCBB3T75na5xSgGbUEcQ5HYIWk8ywjynGjzCQMlyG+DTaQVvxOcpNNyKv57sm2quegFzu+c86Q726npspidr8Knl7dJALXF3q8k3eQrscWrRyJEA6vh1PikrNtDwzdB92WoYkAH5dJj3MxuHPJXoX+UbOZ8jWV7dARBT2LXFiS09bJy+7P2QZSa5BVirWFzoRjAD4JlEeaW2OPcujHTe+IjHXale4fjNh0tUDtteMRcwLnfMpTl6M7qgpUXm/IrlJHfAqtPmYPQUiDCVFQ9wntTeF5ph6WM9qk/2BJni5BX44VH root@hadoop103</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDsgI3CnXcTkQkSHxvhGvBGY1Fh7gaIXqhI5n7rtEM3fmJ2k8/VoWjLwxE1xqSm7tsGufabIhpLdUJBfj4cpqa+PazgSVF59cPlYArp115YFOCGWv2z9tZ5Cq70i9EEfNCyhWNhNUEd9cqiei/G1by4yQzOwvpiuDhiGlDtQk3pRGvvdjGQV3YVDVjDv6JG3QQkxPY+oYQFjDBy+usGEvcuIFDJXmS4hlB8xDMZCw8R0gcvBEnA9RxIIb0N0jLf0uZt0varFZqlguwJJdT6nBCWJ+NYxu5aggX7PGgHB+F97gCoIRusqjT/scoS+jSK8yx4WzJ6ZfCp1wQEXJ+XgrbX root@hadoop104</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDCfFd837abQYj+trazR8y5OSOc7ET+vzn0o6pR+JF3N1knD5JGKXyB0umMsYehwcU55XkMARHRS+jothz0PAqWUpdV73pVZyy+Nxhu/5rySf+NiaoYd8IYAtcgbw174E5deudPF5Ac7IIUxJAgPxEb/NnrpYEPFLHg31ZCKUvzHkEbP9I1iMgM9LtKN3M/aAbaHY2Pd046joVhOo7DsIlYyd147X1MgFhwTeVThpu+zdjPwNoenQGIhrH1J46thBL8TyTC0Wjui5YVwim9Nxoet7E46BiuqrAS6TBvLOTAZlD/g2UB+6NbGYdQKpOs8k2XQqrEBtPB4Wo/hZ0efg8gvCs4AaOqICaMOUt52lfJHInnsycB99nxh95sLDhkHfQIlqTEvGPQDiJVU8tPS5mo7+oeqpCTsvnIINQTMlCtA6eR6kmNumCdSpwbCMkuwVxHjGgNxMy3ykBRItU0vH9mAv/5OxyfjKQutELHQn+5fb5v0lrDq/WZ60jMJYGpTh8= ceph-81b469b6-662d-11ec-b2eb-000c29c51d96			# 公共SSH密钥</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步，告诉 Ceph，新节点是集群的一部分：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">  ```</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">- s</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Ubuntu 安装 Ceph</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 节点规划</span></span></span><br><span class="line"></span><br><span class="line">- 各节点规划：</span><br><span class="line"></span><br><span class="line">  | 主机名 | ip           | 磁盘                        | 角色                                      |</span><br><span class="line">  | ------ | ------------ | --------------------------- | ----------------------------------------- |</span><br><span class="line">  | ceph1  | 192.168.1.91 | 系统盘：sda&lt;br/&gt;osd盘：sdb  | cephadm，monitor，mgr，rgw，mds，osd，nfs |</span><br><span class="line">  | ceph2  | 192.168.1.92 | 系统盘：sda&lt;br/&gt;osd盘：sdb  | monitor，mgr，rgw，mds，osd，nfs          |</span><br><span class="line">  | ceph3  | 192.168.1.93 | 系统盘：sda&lt;br/&gt;osd盘：sdb  | monitor，mgr，rgw，mds，osd，nfs          |</span><br><span class="line">  | ceph4  | 192.168.1.94 | 系统盘：sda&lt;br /&gt;osd盘：sdb | monitor，mgr，rgw，mds，osd，nfs          |</span><br><span class="line"></span><br><span class="line">- 各节点版本：</span><br><span class="line"></span><br><span class="line">  ```shell</span><br><span class="line"><span class="meta">  #</span><span class="bash"> 内核版本</span></span><br><span class="line">  root@ceph1:/home# uname -r</span><br><span class="line">  5.4.0-91-generic</span><br><span class="line">  </span><br><span class="line"><span class="meta">  #</span><span class="bash"> Ubuntu版本，方法一</span></span><br><span class="line">  root@ceph1:/home# cat /etc/issue</span><br><span class="line">  Ubuntu 20.04.3 LTS \n \l</span><br><span class="line">  </span><br><span class="line"><span class="meta">  #</span><span class="bash"> Ubuntu版本，方法二，查看所有信息</span></span><br><span class="line">  root@ceph1:/home# lsb_release -a</span><br><span class="line">  No LSB modules are available.</span><br><span class="line">  Distributor ID:	Ubuntu</span><br><span class="line">  Description:	Ubuntu 20.04.3 LTS</span><br><span class="line">  Release:	20.04</span><br><span class="line">  Codename:	focal</span><br><span class="line">  </span><br><span class="line"><span class="meta">  #</span><span class="bash"> Ubuntu版本代号</span></span><br><span class="line">  root@ceph4:/home# lsb_release -c</span><br><span class="line">  Codename:	focal</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点配置 Ubuntu 20.04 apt 阿里云镜像源：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/home# ll /etc/apt/</span><br><span class="line">total 40</span><br><span class="line">drwxr-xr-x   7 root root 4096 Dec 29 15:23 ./</span><br><span class="line">drwxr-xr-x 105 root root 4096 Dec 29 10:52 ../</span><br><span class="line">drwxr-xr-x   2 root root 4096 Aug 24 16:47 apt.conf.d/</span><br><span class="line">drwxr-xr-x   2 root root 4096 Apr  9  2020 auth.conf.d/</span><br><span class="line">drwxr-xr-x   2 root root 4096 Dec 17 17:04 preferences.d/</span><br><span class="line">-rw-r--r--   1 root root 2777 Dec  8 10:29 sources.list</span><br><span class="line">-rw-r--r--   1 root root 2743 Aug 24 16:47 sources.list.curtin.old</span><br><span class="line">drwxr-xr-x   2 root root 4096 Dec 17 17:04 sources.list.d/</span><br><span class="line">-rw-r--r--   1 root root 1143 Dec  8 15:43 trusted.gpg</span><br><span class="line">drwxr-xr-x   2 root root 4096 Dec 17 17:04 trusted.gpg.d/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 备份默认的源</span></span><br><span class="line">root@ceph1:/home# cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line"><span class="meta">#</span><span class="bash"> 替换阿里云镜像源，将默认源cn.archive.ubuntu.com替换成mirrors.aliyun.com（先验证下默认apt源是不是cn.archive.ubuntu.com）</span></span><br><span class="line">root@ceph1:/home# sed -i &quot;s/cn.archive.ubuntu.com/mirrors.aliyun.com/g&quot; /etc/apt/sources.list</span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新软件包列表</span></span><br><span class="line">root@ceph1:/home# apt update</span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新已安装的软件包</span></span><br><span class="line">root@ceph1:/home# apt upgrade</span><br></pre></td></tr></table></figure>

<p><img src="/2021/12/21/ceph/image-20211229155618530.png" alt="image-20211229155618530"></p>
<ul>
<li>说明：生产环境不要随意使用 <code>apt update</code> 和 <code>apt upgrade</code> 命令，因为生产环境下可能要求使用特定的软件版本，不要轻易的更新。</li>
</ul>
</li>
<li><p>各节点配置主机名：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/home# vim /etc/hostname </span><br><span class="line">root@ceph1:/home# cat /etc/hostname </span><br><span class="line">ceph1</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点配置 host 解析：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/home# vim /etc/hosts</span><br><span class="line">root@ceph1:/home# cat /etc/hosts</span><br><span class="line">127.0.0.1 localhost</span><br><span class="line">127.0.0.1 ceph1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following lines are desirable <span class="keyword">for</span> IPv6 capable hosts</span></span><br><span class="line">::1     ip6-localhost ip6-loopback</span><br><span class="line">fe00::0 ip6-localnet</span><br><span class="line">ff00::0 ip6-mcastprefix</span><br><span class="line">ff02::1 ip6-allnodes</span><br><span class="line">ff02::2 ip6-allrouters</span><br><span class="line"></span><br><span class="line">192.168.1.91 ceph1</span><br><span class="line">192.168.1.92 ceph2</span><br><span class="line">192.168.1.93 ceph3</span><br><span class="line">192.168.1.94 ceph4</span><br></pre></td></tr></table></figure>


</li>
</ul>
<ul>
<li><p>各节点网络信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置文件</span></span><br><span class="line">root@ceph1:/opt# cat /etc/netplan/00-installer-config.yaml </span><br><span class="line"><span class="meta">#</span><span class="bash"> This is the network config written by <span class="string">&#x27;subiquity&#x27;</span></span></span><br><span class="line">network:</span><br><span class="line">  ethernets:</span><br><span class="line">    ens160:</span><br><span class="line">      dhcp4: false</span><br><span class="line">      addresses:</span><br><span class="line">        - 192.168.1.91/24</span><br><span class="line">      gateway4: 192.168.1.1</span><br><span class="line">      nameservers:</span><br><span class="line">        addresses: [114.114.114.114, 8.8.8.8]</span><br><span class="line">  version: 2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 网络信息</span></span><br><span class="line">root@ceph1:/opt# ifconfig</span><br><span class="line">docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255</span><br><span class="line">        ether 02:42:31:45:9c:e4  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.1.91  netmask 255.255.255.0  broadcast 192.168.1.255</span><br><span class="line">        inet6 fe80::250:56ff:fe9e:7c0f  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 00:50:56:9e:7c:0f  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 1493933  bytes 708616888 (708.6 MB)</span><br><span class="line">        RX errors 0  dropped 62  overruns 0  frame 0</span><br><span class="line">        TX packets 215266  bytes 15560179 (15.5 MB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536</span><br><span class="line">        inet 127.0.0.1  netmask 255.0.0.0</span><br><span class="line">        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 400  bytes 38180 (38.1 KB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 400  bytes 38180 (38.1 KB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置文件</span></span><br><span class="line">root@ceph2:/opt# cat /etc/netplan/00-installer-config.yaml </span><br><span class="line"><span class="meta">#</span><span class="bash"> This is the network config written by <span class="string">&#x27;subiquity&#x27;</span></span></span><br><span class="line">network:</span><br><span class="line">  ethernets:</span><br><span class="line">    ens160:</span><br><span class="line">      dhcp4: false</span><br><span class="line">      addresses:</span><br><span class="line">        - 192.168.1.92/24</span><br><span class="line">      gateway4: 192.168.1.1</span><br><span class="line">      nameservers:</span><br><span class="line">        addresses: [114.114.114.114, 8.8.8.8]</span><br><span class="line">  version: 2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 网络信息</span></span><br><span class="line">root@ceph2:/opt# ifconfig </span><br><span class="line">docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255</span><br><span class="line">        ether 02:42:07:d8:d7:04  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.1.92  netmask 255.255.255.0  broadcast 192.168.1.255</span><br><span class="line">        inet6 fe80::250:56ff:fe9e:e4e8  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 00:50:56:9e:e4:e8  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 1474736  bytes 708635562 (708.6 MB)</span><br><span class="line">        RX errors 0  dropped 1  overruns 0  frame 0</span><br><span class="line">        TX packets 219015  bytes 16146130 (16.1 MB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536</span><br><span class="line">        inet 127.0.0.1  netmask 255.0.0.0</span><br><span class="line">        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 588  bytes 56542 (56.5 KB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 588  bytes 56542 (56.5 KB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置文件</span></span><br><span class="line">root@ceph3:/opt# cat /etc/netplan/00-installer-config.yaml </span><br><span class="line"><span class="meta">#</span><span class="bash"> This is the network config written by <span class="string">&#x27;subiquity&#x27;</span></span></span><br><span class="line">network:</span><br><span class="line">  ethernets:</span><br><span class="line">    ens160:</span><br><span class="line">      dhcp4: false</span><br><span class="line">      addresses:</span><br><span class="line">        - 192.168.1.93/24</span><br><span class="line">      gateway4: 192.168.1.1</span><br><span class="line">      nameservers:</span><br><span class="line">        addresses: [114.114.114.114, 8.8.8.8]</span><br><span class="line">  version: 2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 网络信息</span></span><br><span class="line">root@ceph3:/opt# ifconfig</span><br><span class="line">docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255</span><br><span class="line">        ether 02:42:e2:11:a5:b9  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.1.93  netmask 255.255.255.0  broadcast 192.168.1.255</span><br><span class="line">        inet6 fe80::250:56ff:fe9e:7d03  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 00:50:56:9e:7d:03  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 1490677  bytes 706047066 (706.0 MB)</span><br><span class="line">        RX errors 0  dropped 46  overruns 0  frame 0</span><br><span class="line">        TX packets 208014  bytes 14700697 (14.7 MB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536</span><br><span class="line">        inet 127.0.0.1  netmask 255.0.0.0</span><br><span class="line">        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 620  bytes 60174 (60.1 KB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 620  bytes 60174 (60.1 KB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置文件</span></span><br><span class="line">root@ceph4:/opt# cat /etc/netplan/00-installer-config.yaml </span><br><span class="line"><span class="meta">#</span><span class="bash"> This is the network config written by <span class="string">&#x27;subiquity&#x27;</span></span></span><br><span class="line">network:</span><br><span class="line">  ethernets:</span><br><span class="line">    ens160:</span><br><span class="line">      dhcp4: false</span><br><span class="line">      addresses:</span><br><span class="line">        - 192.168.1.94/24</span><br><span class="line">      gateway4: 192.168.1.1</span><br><span class="line">      nameservers:</span><br><span class="line">        addresses: [114.114.114.114, 8.8.8.8]</span><br><span class="line">  version: 2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 网络信息</span></span><br><span class="line">root@ceph4:/opt# ifconfig</span><br><span class="line">docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255</span><br><span class="line">        inet6 fe80::42:78ff:febe:2005  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 02:42:78:be:20:05  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 2  bytes 196 (196.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.1.94  netmask 255.255.255.0  broadcast 192.168.1.255</span><br><span class="line">        inet6 fe80::250:56ff:fe9e:7855  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 00:50:56:9e:78:55  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 1572297  bytes 814622359 (814.6 MB)</span><br><span class="line">        RX errors 0  dropped 50  overruns 0  frame 0</span><br><span class="line">        TX packets 250469  bytes 17903836 (17.9 MB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536</span><br><span class="line">        inet 127.0.0.1  netmask 255.0.0.0</span><br><span class="line">        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;</span><br><span class="line">        loop  txqueuelen 1000  (Local Loopback)</span><br><span class="line">        RX packets 826  bytes 84430 (84.4 KB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 826  bytes 84430 (84.4 KB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure>

<ul>
<li>Ubuntu 20.04 修改 IP 地址的方法参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38505969/article/details/110501609">https://blog.csdn.net/qq_38505969/article/details/110501609</a></li>
</ul>
</li>
</ul>
<h3 id="关闭防火墙-1"><a href="#关闭防火墙-1" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><ul>
<li><p>各节点关闭防火墙：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装ufw</span></span><br><span class="line">root@ceph1:/opt# apt install -y ufw</span><br><span class="line"><span class="meta">#</span><span class="bash"> 防火墙版本</span></span><br><span class="line">root@ceph1:/home# ufw version</span><br><span class="line">ufw 0.36</span><br><span class="line">Copyright 2008-2015 Canonical Ltd.</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启防火墙</span></span><br><span class="line">root@ceph1:/home# ufw enable</span><br><span class="line">Command may disrupt existing ssh connections. Proceed with operation (y|n)? y</span><br><span class="line">Firewall is active and enabled on system startup</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看防火墙状态</span></span><br><span class="line">root@ceph1:/home# ufw status</span><br><span class="line">Status: active</span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭防火墙</span></span><br><span class="line">root@ceph1:/home# ufw disable</span><br><span class="line">Firewall stopped and disabled on system startup</span><br><span class="line">root@ceph1:/home# ufw status</span><br><span class="line">Status: inactive</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启防火墙</span></span><br><span class="line">root@ceph1:/home# ufw reload</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="SSH-免密登录-1"><a href="#SSH-免密登录-1" class="headerlink" title="SSH 免密登录"></a>SSH 免密登录</h3><ul>
<li><p>各节点执行如下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/home# sed -i &#x27;/PermitRootLogin/d&#x27; /etc/ssh/sshd_config</span><br><span class="line">root@ceph1:/home# echo &quot;PermitRootLogin yes&quot; &gt;&gt; /etc/ssh/sshd_config</span><br><span class="line">root@ceph1:/home# service sshd reload</span><br><span class="line">root@ceph1:/home# ssh-keygen</span><br><span class="line">root@ceph1:/home# ssh-copy-id -o StrictHostKeyChecking=no root@ceph1</span><br><span class="line">root@ceph1:/home# ssh-copy-id -o StrictHostKeyChecking=no root@ceph2</span><br><span class="line">root@ceph1:/home# ssh-copy-id -o StrictHostKeyChecking=no root@ceph3</span><br><span class="line">root@ceph1:/home# ssh-copy-id -o StrictHostKeyChecking=no root@ceph4</span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试是否能免密登录其他节点</span></span><br><span class="line">root@ceph1:/home# ssh ceph1</span><br><span class="line">root@ceph1:/home# ssh ceph2</span><br><span class="line">root@ceph1:/home# ssh ceph3</span><br><span class="line">root@ceph1:/home# ssh ceph4</span><br></pre></td></tr></table></figure>

<ul>
<li>ceph2，ceph3 和 ceph4 参考 ceph1 配置。</li>
</ul>
</li>
</ul>
<h3 id="服务器时间同步-1"><a href="#服务器时间同步-1" class="headerlink" title="服务器时间同步"></a>服务器时间同步</h3><ul>
<li><p>NTP 是通过网络来同步时间的一种 TCP/IP 协议。通常客户端向服务器请求当前的时间，并根据结果来设置其时钟。这个描述是挺简单的，实现这一功能却是极为复杂的：首先要有多层 NTP 服务器，第一层 NTP 服务器连接原子时钟，第二层、第三层服务器则担起负载均衡的责任，以处理因特网传来的所有请求。另外，客户端可能也超乎你想象的复杂：它必须排除通讯延迟，调整时间的同时不干扰其它在服务器中运行的进程。幸运的是，所有的这些复杂性都进行了封装，你是不可见也不需要见到的。</p>
</li>
<li><p>在 Ubuntu 中，可以使用 ntpdate 和 ntpd 来同步时间。而在最新的 Ubuntu 版本中，timedatectl 替代了老旧的 ntpdate。默认情况下，timedatectl 在系统启动的时候会立刻同步时间，并在稍后网络连接激活后通过 Socket 再次检查一次。</p>
</li>
<li><p>各节点查看时间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/home# date</span><br><span class="line">Thu 30 Dec 2021 10:32:19 AM CST</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点设置时间同步服务器：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看时间同步服务器状态</span></span><br><span class="line">root@ceph1:/opt# systemctl status systemd-timesyncd.service</span><br><span class="line">● systemd-timesyncd.service - Network Time Synchronization</span><br><span class="line">     Loaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendor preset: enabled)</span><br><span class="line">     Active: active (running) since Thu 2021-12-30 11:02:44 CST; 36min ago</span><br><span class="line">       Docs: man:systemd-timesyncd.service(8)</span><br><span class="line">   Main PID: 719 (systemd-timesyn)</span><br><span class="line">     Status: &quot;Initial synchronization to time server 91.189.91.157:123 (ntp.ubuntu.com).&quot;	# 默认ntp.ubuntu.com</span><br><span class="line">      Tasks: 2 (limit: 19110)</span><br><span class="line">     Memory: 1.7M</span><br><span class="line">     CGroup: /system.slice/systemd-timesyncd.service</span><br><span class="line">             └─719 /lib/systemd/systemd-timesyncd</span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加阿里云NTP服务器地址</span></span><br><span class="line">root@ceph1:/opt# vim /etc/systemd/timesyncd.conf</span><br><span class="line">root@ceph1:/opt# cat /etc/systemd/timesyncd.conf </span><br><span class="line"><span class="meta">#</span><span class="bash">  This file is part of systemd.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  systemd is free software; you can redistribute it and/or modify it</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  under the terms of the GNU Lesser General Public License as published by</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  the Free Software Foundation; either version 2.1 of the License, or</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  (at your option) any later version.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Entries <span class="keyword">in</span> this file show the compile time defaults.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You can change settings by editing this file.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Defaults can be restored by simply deleting this file.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See timesyncd.conf(5) <span class="keyword">for</span> details.</span></span><br><span class="line"></span><br><span class="line">[Time]</span><br><span class="line"><span class="meta">#</span><span class="bash">NTP=</span></span><br><span class="line"><span class="meta">#</span><span class="bash">FallbackNTP=ntp.ubuntu.com</span></span><br><span class="line"><span class="meta">#</span><span class="bash">RootDistanceMaxSec=5</span></span><br><span class="line"><span class="meta">#</span><span class="bash">PollIntervalMinSec=32</span></span><br><span class="line"><span class="meta">#</span><span class="bash">PollIntervalMaxSec=2048</span></span><br><span class="line">NTP=ntp1.aliyun.com</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启时间同步服务</span></span><br><span class="line">root@ceph1:/opt# systemctl restart systemd-timesyncd.service</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查询时间同步服务状态，确认服务是否正常启动，是否从指定的NTP服务器上进行校时</span></span><br><span class="line">root@ceph1:/opt# systemctl status systemd-timesyncd.service</span><br><span class="line">● systemd-timesyncd.service - Network Time Synchronization</span><br><span class="line">     Loaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendor preset: enabled)</span><br><span class="line">     Active: active (running) since Thu 2021-12-30 11:50:14 CST; 14s ago</span><br><span class="line">       Docs: man:systemd-timesyncd.service(8)</span><br><span class="line">   Main PID: 3966 (systemd-timesyn)</span><br><span class="line">     Status: &quot;Initial synchronization to time server 120.25.115.20:123 (ntp1.aliyun.com).&quot;	# 修改为阿里云服务器</span><br><span class="line">      Tasks: 2 (limit: 19110)</span><br><span class="line">     Memory: 1.4M</span><br><span class="line">     CGroup: /system.slice/systemd-timesyncd.service</span><br><span class="line">             └─3966 /lib/systemd/systemd-timesyncd</span><br><span class="line"></span><br><span class="line">Dec 30 11:50:14 ceph2 systemd[1]: Starting Network Time Synchronization...</span><br><span class="line">Dec 30 11:50:14 ceph2 systemd[1]: Started Network Time Synchronization.</span><br><span class="line">Dec 30 11:50:14 ceph2 systemd-timesyncd[3966]: Initial synchronization to time server 120.25.115.20:123 (ntp1.aliyun.com).</span><br></pre></td></tr></table></figure>

<ul>
<li><p>系统默认同步的 NTP 服务器为 ntp.ubuntu.com。</p>
</li>
<li><p>添加一个阿里云 NTP 服务器地址，如果需要添加多个 NTP 服务器地址，则中间用空格隔开：</p>
<p><img src="/2021/12/21/ceph/image-20211230114924366.png" alt="image-20211230114924366"></p>
</li>
</ul>
</li>
<li><p>各节点设置时区：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置东八区</span></span><br><span class="line">root@ceph1:/home# timedatectl set-timezone Asia/Shanghai</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看时区</span></span><br><span class="line">root@ceph1:/home# cat /etc/timezone </span><br><span class="line">Asia/Shanghai</span><br><span class="line">root@ceph1:/home# timedatectl status | grep &#x27;Time zone&#x27;</span><br><span class="line">                Time zone: Asia/Shanghai (CST, +0800)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点查看时钟是否与互联网同步：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/home# timedatectl </span><br><span class="line">               Local time: Thu 2021-12-30 10:33:17 CST		# 本地时间</span><br><span class="line">           Universal time: Thu 2021-12-30 02:33:17 UTC		# 协调世界时</span><br><span class="line">                 RTC time: Thu 2021-12-30 02:33:17    		# 硬件时间</span><br><span class="line">                Time zone: Asia/Shanghai (CST, +0800) 		# 时区</span><br><span class="line">System clock synchronized: yes                       		# 如果和远程NTP服务器成功同步，则显示为yes </span><br><span class="line">              NTP service: active                     		# NTP时间同步是否开启，systemd-timesyncd服务活跃即开启了NTP时间同步</span><br><span class="line">          RTC in local TZ: no								# no表示硬件时钟设置为协调世界时（UTC），yes表示硬件时钟设置为本地时间</span><br></pre></td></tr></table></figure>

<ul>
<li>timedatectl 命令会显示本地时间、世界时、时区、系统时钟是否与互联网服务器同步，以及 <code>systemd-timesyncd.service</code> 是处于活跃状态还是非活跃状态。</li>
</ul>
</li>
</ul>
<h3 id="安装-Python3-1"><a href="#安装-Python3-1" class="headerlink" title="安装 Python3"></a>安装 Python3</h3><ul>
<li><p>各节点查看 Python3 版本，服务器已安装 Python3，此处省略安装步骤：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# python3 --version</span><br><span class="line">Python 3.8.10</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h3 id="安装-Docker-1"><a href="#安装-Docker-1" class="headerlink" title="安装 Docker"></a>安装 Docker</h3><ul>
<li><p>各节点卸载旧版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 卸载旧版本</span></span><br><span class="line">root@ceph1:/opt# apt remove docker docker-engine docker.io containerd runc</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除旧版本数据</span></span><br><span class="line">root@ceph1:/opt# rm -rf /var/lib/docker/</span><br><span class="line">root@ceph1:/opt# rm -rf /var/lib/containerd/</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点安装需要的软件包：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# apt install ca-certificates curl gnupg lsb-release</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点添加 Docker 的官方 GPG key：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line">OK</span><br><span class="line">W: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点添加 apt 的 Docker 源：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br><span class="line">Hit:1 http://mirrors.aliyun.com/ubuntu focal InRelease</span><br><span class="line">Hit:2 http://mirrors.aliyun.com/ubuntu focal-updates InRelease                                       </span><br><span class="line">Hit:3 http://mirrors.aliyun.com/ubuntu focal-backports InRelease                                                                                         </span><br><span class="line">Hit:4 http://mirrors.aliyun.com/ubuntu focal-security InRelease                                                                                          </span><br><span class="line">Get:5 https://download.docker.com/linux/ubuntu focal InRelease [57.7 kB]                                                  </span><br><span class="line">Get:6 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages [13.5 kB] </span><br><span class="line">Get:7 https://download.ceph.com/debian-octopus focal InRelease [8,571 B]</span><br><span class="line">Get:8 https://download.ceph.com/debian-octopus focal/main amd64 Packages [15.9 kB]</span><br><span class="line">Fetched 95.7 kB in 2s (50.4 kB/s)     </span><br><span class="line">Reading package lists... Done</span><br><span class="line">W: http://mirrors.aliyun.com/ubuntu/dists/focal/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.</span><br><span class="line">W: http://mirrors.aliyun.com/ubuntu/dists/focal-updates/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.</span><br><span class="line">W: http://mirrors.aliyun.com/ubuntu/dists/focal-backports/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.</span><br><span class="line">W: http://mirrors.aliyun.com/ubuntu/dists/focal-security/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.</span><br><span class="line">W: https://download.docker.com/linux/ubuntu/dists/focal/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.</span><br><span class="line">W: https://download.ceph.com/debian-octopus/dists/focal/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.</span><br></pre></td></tr></table></figure>

<ul>
<li><p>如果提示 <code>bash: add-apt-repository: command not found</code>，执行下面命令安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# apt install -y software-properties-common</span><br><span class="line">root@ceph1:/opt# apt update</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>各节点安装最新版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# apt install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure>
</li>
<li><p>Docker 安装完成后会自动启动：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看docker状态</span></span><br><span class="line">root@ceph1:/opt# systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)</span><br><span class="line">     Active: active (running) since Thu 2021-12-30 17:28:18 CST; 34s ago</span><br><span class="line">TriggeredBy: ● docker.socket</span><br><span class="line">       Docs: https://docs.docker.com</span><br><span class="line">   Main PID: 24185 (dockerd)</span><br><span class="line">      Tasks: 11</span><br><span class="line">     Memory: 29.1M</span><br><span class="line">     CGroup: /system.slice/docker.service</span><br><span class="line">             └─24185 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看docker版本</span></span><br><span class="line">root@ceph1:/opt# docker --version</span><br><span class="line">Docker version 20.10.12, build e91ed57</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# docker run hello-world</span><br><span class="line">Unable to find image &#x27;hello-world:latest&#x27; locally</span><br><span class="line">latest: Pulling from library/hello-world</span><br><span class="line">2db29710123e: Pull complete </span><br><span class="line">Digest: sha256:2498fce14358aa50ead0cc6c19990fc6ff866ce72aeb5546e1d59caac3d0d60f</span><br><span class="line">Status: Downloaded newer image for hello-world:latest</span><br><span class="line"></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">To generate this message, Docker took the following steps:</span><br><span class="line"> 1. The Docker client contacted the Docker daemon.</span><br><span class="line"> 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.</span><br><span class="line">    (amd64)</span><br><span class="line"> 3. The Docker daemon created a new container from that image which runs the</span><br><span class="line">    executable that produces the output you are currently reading.</span><br><span class="line"> 4. The Docker daemon streamed that output to the Docker client, which sent it</span><br><span class="line">    to your terminal.</span><br><span class="line"></span><br><span class="line">To try something more ambitious, you can run an Ubuntu container with:</span><br><span class="line"><span class="meta"> $</span><span class="bash"> docker run -it ubuntu bash</span></span><br><span class="line"></span><br><span class="line">Share images, automate workflows, and more with a free Docker ID:</span><br><span class="line"> https://hub.docker.com/</span><br><span class="line"></span><br><span class="line">For more examples and ideas, visit:</span><br><span class="line"> https://docs.docker.com/get-started/</span><br><span class="line"></span><br><span class="line">root@ceph1:/opt# docker images</span><br><span class="line">REPOSITORY    TAG       IMAGE ID       CREATED        SIZE</span><br><span class="line">hello-world   latest    feb5d9fea6a5   3 months ago   13.3kB</span><br><span class="line">root@ceph1:/opt# docker ps -a</span><br><span class="line">CONTAINER ID   IMAGE         COMMAND    CREATED              STATUS                          PORTS     NAMES</span><br><span class="line">acf9377ed00d   hello-world   &quot;/hello&quot;   About a minute ago   Exited (0) About a minute ago             ecstatic_gould</span><br></pre></td></tr></table></figure>
</li>
<li><p>参考：<a target="_blank" rel="noopener" href="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a></p>
</li>
</ul>
<h3 id="安装-Cephadm-1"><a href="#安装-Cephadm-1" class="headerlink" title="安装 Cephadm"></a>安装 Cephadm</h3><ul>
<li><p>各节点使用 apt 安装 Cephadm：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装</span></span><br><span class="line">root@ceph1:/opt# apt install -y cephadm</span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree       </span><br><span class="line">Reading state information... Done</span><br><span class="line">Recommended packages:</span><br><span class="line">  podman | docker.io</span><br><span class="line">The following NEW packages will be installed:</span><br><span class="line">  cephadm</span><br><span class="line">0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.</span><br><span class="line">Need to get 53.2 kB of archives.</span><br><span class="line">After this operation, 245 kB of additional disk space will be used.</span><br><span class="line">Get:1 https://download.ceph.com/debian-octopus focal/main amd64 cephadm amd64 15.2.15-1focal [53.2 kB]</span><br><span class="line">Fetched 53.2 kB in 2s (31.4 kB/s)  </span><br><span class="line">Selecting previously unselected package cephadm.</span><br><span class="line">(Reading database ... 115380 files and directories currently installed.)</span><br><span class="line">Preparing to unpack .../cephadm_15.2.15-1focal_amd64.deb ...</span><br><span class="line">Unpacking cephadm (15.2.15-1focal) ...</span><br><span class="line">Setting up cephadm (15.2.15-1focal) ...</span><br><span class="line">Adding system user cephadm....done</span><br><span class="line">Processing triggers for man-db (2.9.1-1) ...</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点验证 Cephadm 安装成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# which cephadm</span><br><span class="line">/usr/sbin/cephadm</span><br><span class="line">root@ceph1:/opt# cephadm version</span><br><span class="line">ceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable)</span><br></pre></td></tr></table></figure>
</li>
<li><p>各节点安装 ceph-common 工具：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 添加源</span></span><br><span class="line">root@ceph1:/opt# cephadm add-repo --release octopus</span><br><span class="line">Installing repo GPG key from https://download.ceph.com/keys/release.asc...</span><br><span class="line">Installing repo file at /etc/apt/sources.list.d/ceph.list...</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装</span></span><br><span class="line">root@ceph1:/opt# cephadm install ceph-common</span><br><span class="line">Installing packages [&#x27;ceph-common&#x27;]...</span><br><span class="line"><span class="meta">#</span><span class="bash"> 验证</span></span><br><span class="line">root@ceph1:/opt# ceph -v</span><br><span class="line">ceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="创建-Ceph-新集群-1"><a href="#创建-Ceph-新集群-1" class="headerlink" title="创建 Ceph 新集群"></a>创建 Ceph 新集群</h3><ul>
<li><p>在 ceph1 上创建一个可以被任何访问 Ceph 集群的主机访问的网络，指定 mon-ip，并将生成的配置文件写进 <code>/etc/ceph</code> 目录里：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# cephadm bootstrap --mon-ip 192.168.1.91</span><br><span class="line">Verifying podman|docker is present...</span><br><span class="line">Verifying lvm2 is present...</span><br><span class="line">Verifying time synchronization is in place...</span><br><span class="line">Unit systemd-timesyncd.service is enabled and running</span><br><span class="line">Repeating the final host check...</span><br><span class="line">podman|docker (/usr/bin/docker) is present</span><br><span class="line">systemctl is present</span><br><span class="line">lvcreate is present</span><br><span class="line">Unit systemd-timesyncd.service is enabled and running</span><br><span class="line">Host looks OK</span><br><span class="line">Cluster fsid: 79a6cb92-6f92-11ec-b1e0-6f6f27397286</span><br><span class="line">Verifying IP 192.168.1.91 port 3300 ...</span><br><span class="line">Verifying IP 192.168.1.91 port 6789 ...</span><br><span class="line">Mon IP 192.168.1.91 is in CIDR network 192.168.1.0/24</span><br><span class="line">Pulling container image quay.io/ceph/ceph:v15...</span><br><span class="line">Extracting ceph user uid/gid from container image...</span><br><span class="line">Creating initial keys...</span><br><span class="line">Creating initial monmap...</span><br><span class="line">Creating mon...</span><br><span class="line">Waiting for mon to start...</span><br><span class="line">Waiting for mon...</span><br><span class="line">mon is available</span><br><span class="line">Assimilating anything we can from ceph.conf...</span><br><span class="line">Generating new minimal ceph.conf...</span><br><span class="line">Restarting the monitor...</span><br><span class="line">Setting mon public_network...</span><br><span class="line">Creating mgr...</span><br><span class="line">Verifying port 9283 ...</span><br><span class="line">Wrote keyring to /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">Wrote config to /etc/ceph/ceph.conf</span><br><span class="line">Waiting for mgr to start...</span><br><span class="line">Waiting for mgr...</span><br><span class="line">mgr not available, waiting (1/10)...</span><br><span class="line">mgr not available, waiting (2/10)...</span><br><span class="line">mgr not available, waiting (3/10)...</span><br><span class="line">mgr not available, waiting (4/10)...</span><br><span class="line">mgr is available</span><br><span class="line">Enabling cephadm module...</span><br><span class="line">Waiting for the mgr to restart...</span><br><span class="line">Waiting for Mgr epoch 5...</span><br><span class="line">Mgr epoch 5 is available</span><br><span class="line">Setting orchestrator backend to cephadm...</span><br><span class="line">Generating ssh key...</span><br><span class="line">Wrote public SSH key to to /etc/ceph/ceph.pub</span><br><span class="line">Adding key to root@localhost&#x27;s authorized_keys...</span><br><span class="line">Adding host ceph1...</span><br><span class="line">Deploying mon service with default placement...</span><br><span class="line">Deploying mgr service with default placement...</span><br><span class="line">Deploying crash service with default placement...</span><br><span class="line">Enabling mgr prometheus module...</span><br><span class="line">Deploying prometheus service with default placement...</span><br><span class="line">Deploying grafana service with default placement...</span><br><span class="line">Deploying node-exporter service with default placement...</span><br><span class="line">Deploying alertmanager service with default placement...</span><br><span class="line">Enabling the dashboard module...</span><br><span class="line">Waiting for the mgr to restart...</span><br><span class="line">Waiting for Mgr epoch 13...</span><br><span class="line">Mgr epoch 13 is available</span><br><span class="line">Generating a dashboard self-signed certificate...</span><br><span class="line">Creating initial admin user...</span><br><span class="line">Fetching dashboard port number...</span><br><span class="line">Ceph Dashboard is now available at:</span><br><span class="line"></span><br><span class="line">	     URL: https://localhost:8443/</span><br><span class="line">	    User: admin</span><br><span class="line">	Password: vzmh8b8mp6</span><br><span class="line"></span><br><span class="line">You can access the Ceph CLI with:</span><br><span class="line"></span><br><span class="line">	sudo /usr/sbin/cephadm shell --fsid 79a6cb92-6f92-11ec-b1e0-6f6f27397286 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line">Please consider enabling telemetry to help improve Ceph:</span><br><span class="line"></span><br><span class="line">	ceph telemetry on</span><br><span class="line"></span><br><span class="line">For more information see:</span><br><span class="line"></span><br><span class="line">	https://docs.ceph.com/docs/master/mgr/telemetry/</span><br><span class="line"></span><br><span class="line">Bootstrap complete.</span><br></pre></td></tr></table></figure>

<ul>
<li><p>该命令执行如下操作：</p>
<ul>
<li><p>在本地主机上为新集群创建 monitor  和 manager daemon 守护程序。</p>
</li>
<li><p>为 Ceph 集群生成一个新的公共 SSH 密钥，并将其添加到 root 用户的 <code>/root/.ssh/authorized_keys</code> 文件中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1		# ceph1免密登录</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2		# ceph1免密登录</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3		# ceph3免密登录</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4		# ceph4免密登录</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286																						# 公共SSH密钥</span><br></pre></td></tr></table></figure>
</li>
<li><p>将与新群集进行通信所需的最小配置文件保存到 <code>/etc/ceph/ceph.conf</code>。</p>
</li>
<li><p>向 <code>/etc/ceph/ceph.client.admin.keyring</code> 写入 <code>client.admin</code> 管理 secret key 的副本（特权！）。</p>
</li>
<li><p>将 public key 的副本写入 <code>/etc/ceph/ceph.pub</code>。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>查看当前配置文件：</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# ll /etc/ceph/</span><br><span class="line">total 24</span><br><span class="line">drwxr-xr-x   2 root root 4096 Jan  7 16:19 ./</span><br><span class="line">drwxr-xr-x 104 root root 4096 Jan  7 16:02 ../</span><br><span class="line">-rw-------   1 root root   63 Jan  7 16:19 ceph.client.admin.keyring</span><br><span class="line">-rw-r--r--   1 root root  175 Jan  7 16:19 ceph.conf</span><br><span class="line">-rw-r--r--   1 root root  595 Jan  7 16:19 ceph.pub</span><br><span class="line">-rw-r--r--   1 root root   92 Oct 20 22:31 rbdmap</span><br><span class="line">root@ceph1:/opt# cat /etc/ceph/ceph.conf </span><br><span class="line"><span class="meta">#</span><span class="bash"> minimal ceph.conf <span class="keyword">for</span> 79a6cb92-6f92-11ec-b1e0-6f6f27397286</span></span><br><span class="line">[global]</span><br><span class="line">	fsid = 79a6cb92-6f92-11ec-b1e0-6f6f27397286</span><br><span class="line">	mon_host = [v2:192.168.1.91:3300/0,v1:192.168.1.91:6789/0]</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看当前拉取的镜像和启动的容器：</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# docker images</span><br><span class="line">REPOSITORY                         TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">quay.io/ceph/ceph                  v15       3437f7bed968   2 months ago    1.08GB</span><br><span class="line">hello-world                        latest    feb5d9fea6a5   3 months ago    13.3kB</span><br><span class="line">quay.io/ceph/ceph-grafana          6.7.4     557c83e11646   5 months ago    486MB</span><br><span class="line">quay.io/prometheus/prometheus      v2.18.1   de242295e225   20 months ago   140MB</span><br><span class="line">quay.io/prometheus/alertmanager    v0.20.0   0881eb8f169f   2 years ago     52.1MB</span><br><span class="line">quay.io/prometheus/node-exporter   v0.18.1   e5a616e4b9cf   2 years ago     22.9MB</span><br><span class="line">root@ceph1:/opt# docker ps -a</span><br><span class="line">CONTAINER ID   IMAGE                                      COMMAND                  CREATED          STATUS                   PORTS     NAMES</span><br><span class="line">1fdde1918435   quay.io/prometheus/alertmanager:v0.20.0    &quot;/bin/alertmanager -…&quot;   47 minutes ago   Up 47 minutes                      ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-alertmanager.ceph1</span><br><span class="line">5fc44d4adaa8   quay.io/ceph/ceph-grafana:6.7.4            &quot;/bin/sh -c &#x27;grafana…&quot;   47 minutes ago   Up 47 minutes                      ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-grafana.ceph1</span><br><span class="line">85cac2612c10   quay.io/prometheus/prometheus:v2.18.1      &quot;/bin/prometheus --c…&quot;   47 minutes ago   Up 47 minutes                      ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-prometheus.ceph1</span><br><span class="line">d1aa75b76a39   quay.io/prometheus/node-exporter:v0.18.1   &quot;/bin/node_exporter …&quot;   48 minutes ago   Up 48 minutes                      ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-node-exporter.ceph1</span><br><span class="line">c20775682c08   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-crash…&quot;   48 minutes ago   Up 48 minutes                      ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-crash.ceph1</span><br><span class="line">7c3cf4e445d1   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-mgr -…&quot;   54 minutes ago   Up 54 minutes                      ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mgr.ceph1.tlpgcb</span><br><span class="line">6410679d498a   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-mon -…&quot;   55 minutes ago   Up 54 minutes                      ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mon.ceph1</span><br><span class="line">acf9377ed00d   hello-world                                &quot;/hello&quot;                 6 hours ago      Exited (0) 6 hours ago             ecstatic_gould</span><br></pre></td></tr></table></figure>

<ul>
<li>alertmanager 组件：prometheus 告警组件。</li>
<li>grafana 组件：监控数据展示 dashboard。</li>
<li>prometheus 组件：prometheus 监控组件。</li>
<li>node_exporter 组件：prometheus 节点数据收集组件。</li>
<li>ceph-crash 组件：崩溃数据收集模块。</li>
<li>ceph-mgr 组件：Ceph 管理程序。</li>
<li>ceph-monitor 组件：Ceph 监视器。</li>
</ul>
</li>
<li><p>查看所有组件运行状态：</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# ceph orch ps</span><br><span class="line">NAME                 HOST   STATUS        REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID  </span><br><span class="line">alertmanager.ceph1   ceph1  running (5h)  8m ago     5h   0.20.0   quay.io/prometheus/alertmanager:v0.20.0   0881eb8f169f  1fdde1918435  </span><br><span class="line">crash.ceph1          ceph1  running (5h)  8m ago     5h   15.2.15  quay.io/ceph/ceph:v15                     3437f7bed968  c20775682c08  </span><br><span class="line">grafana.ceph1        ceph1  running (5h)  8m ago     5h   6.7.4    quay.io/ceph/ceph-grafana:6.7.4           557c83e11646  5fc44d4adaa8  </span><br><span class="line">mgr.ceph1.tlpgcb     ceph1  running (5h)  8m ago     5h   15.2.15  quay.io/ceph/ceph:v15                     3437f7bed968  7c3cf4e445d1  </span><br><span class="line">mon.ceph1            ceph1  running (5h)  8m ago     5h   15.2.15  quay.io/ceph/ceph:v15                     3437f7bed968  6410679d498a  </span><br><span class="line">node-exporter.ceph1  ceph1  running (5h)  8m ago     5h   0.18.1   quay.io/prometheus/node-exporter:v0.18.1  e5a616e4b9cf  d1aa75b76a39  </span><br><span class="line">prometheus.ceph1     ceph1  running (5h)  8m ago     5h   2.18.1   quay.io/prometheus/prometheus:v2.18.1     de242295e225  85cac2612c10</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看某个组件运行状态：</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# ceph orch ps --daemon-type mgr</span><br><span class="line">NAME              HOST   STATUS        REFRESHED  AGE  VERSION  IMAGE NAME             IMAGE ID      CONTAINER ID  </span><br><span class="line">mgr.ceph1.tlpgcb  ceph1  running (5h)  4m ago     5h   15.2.15  quay.io/ceph/ceph:v15  3437f7bed968  7c3cf4e445d1</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看容器状态：</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# cephadm ls</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;crash.ceph1&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@crash.ceph1&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;c20775682c087aaf8d1dc25802386ac4fbaf40b3ab8a4ae7ef54843e26725d94&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;15.2.15&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2022-01-07T08:25:11.762461Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2022-01-07T08:25:11.702112Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2022-01-07T08:25:11.318096Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2022-01-07T08:25:11.702112Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;grafana.ceph1&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@grafana.ceph1&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;5fc44d4adaa8365aa4a290d769fd08b25a2dee4c5235f4ef377aff0ced55ef2b&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph-grafana:6.7.4&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;557c83e11646f123a27b5e4b62ac6c45e7bb8b2e90d6044034d0db5b7019415c&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;6.7.4&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2022-01-07T08:26:37.418809Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2022-01-07T08:25:51.875753Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2022-01-07T08:25:51.487738Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2022-01-07T08:26:37.229606Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;mgr.ceph1.tlpgcb&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@mgr.ceph1.tlpgcb&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;7c3cf4e445d1113caa67c7502ffd5265961ecba5430e909f34f853b1fc021f1e&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;15.2.15&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2022-01-07T08:19:10.362590Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2022-01-07T08:19:10.279314Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2022-01-07T08:19:09.911299Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2022-01-07T08:26:37.749627Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;alertmanager.ceph1&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@alertmanager.ceph1&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;1fdde19184350e4c731b484bf10b3943e95f0ec27a306daa7ff547414e9e587e&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/prometheus/alertmanager:v0.20.0&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;0881eb8f169f5556a292b4e2c01d683172b12830a62a9225a98a8e206bb734f0&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;0.20.0&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2022-01-07T08:26:38.462159Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2022-01-07T08:25:09.854037Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2022-01-07T08:25:09.486022Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2022-01-07T08:26:38.297650Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;prometheus.ceph1&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@prometheus.ceph1&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;85cac2612c105ec9809a88fa97f99633b50b50ba12ee0a9b2beb95c608be73ff&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/prometheus/prometheus:v2.18.1&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;de242295e2257c37c8cadfd962369228f8f10b2d48a44259b65fef44ad4f6490&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;2.18.1&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2022-01-07T08:26:35.419299Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2022-01-07T08:26:35.361530Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2022-01-07T08:26:34.993515Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2022-01-07T08:26:35.361530Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;mon.ceph1&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@mon.ceph1&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;6410679d498a31428927c7a532548d39f5261c80f94c867849902a6bea12208b&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;15.2.15&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2022-01-07T08:19:09.667576Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2022-01-07T08:19:08.539242Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2022-01-07T08:19:07.699208Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2022-01-07T08:26:38.785670Z&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;style&quot;: &quot;cephadm:v1&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;node-exporter.ceph1&quot;,</span><br><span class="line">        &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;,</span><br><span class="line">        &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@node-exporter.ceph1&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;state&quot;: &quot;running&quot;,</span><br><span class="line">        &quot;container_id&quot;: &quot;d1aa75b76a39554b9b5cc7b5e9c4cf9c4ca4d00b9401af79b3c8eda6009fc215&quot;,</span><br><span class="line">        &quot;container_image_name&quot;: &quot;quay.io/prometheus/node-exporter:v0.18.1&quot;,</span><br><span class="line">        &quot;container_image_id&quot;: &quot;e5a616e4b9cf68dfcad7782b78e118be4310022e874d52da85c55923fb615f87&quot;,</span><br><span class="line">        &quot;version&quot;: &quot;0.18.1&quot;,</span><br><span class="line">        &quot;started&quot;: &quot;2022-01-07T08:26:02.530410Z&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2022-01-07T08:25:52.639785Z&quot;,</span><br><span class="line">        &quot;deployed&quot;: &quot;2022-01-07T08:25:52.179766Z&quot;,</span><br><span class="line">        &quot;configured&quot;: &quot;2022-01-07T08:25:52.639785Z&quot;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据初始化完成的提示使用浏览器访问 dashboard：略。</p>
</li>
</ul>
<h3 id="启用-Ceph-命令-1"><a href="#启用-Ceph-命令-1" class="headerlink" title="启用 Ceph 命令"></a>启用 Ceph 命令</h3><ul>
<li><p>在 ceph1 上执行命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# cephadm shell</span><br><span class="line">Inferring fsid 79a6cb92-6f92-11ec-b1e0-6f6f27397286</span><br><span class="line">Inferring config /var/lib/ceph/79a6cb92-6f92-11ec-b1e0-6f6f27397286/mon.ceph1/config</span><br><span class="line">Using recent ceph image quay.io/ceph/ceph@sha256:a2c23b6942f7fbc1e15d8cfacd6655a681fe0e44f288e4a158db22030b8d58e3</span><br><span class="line">root@ceph1:/# alias ceph=&#x27;cephadm shell -- ceph&#x27;</span><br><span class="line">root@ceph1:/# exit</span><br><span class="line">exit</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看集群状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     79a6cb92-6f92-11ec-b1e0-6f6f27397286</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1 (age 21h)</span><br><span class="line">    mgr: ceph1.tlpgcb(active, since 21h)</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:     </span><br><span class="line"> </span><br><span class="line">root@ceph1:/opt# ceph status</span><br><span class="line">  cluster:</span><br><span class="line">    id:     79a6cb92-6f92-11ec-b1e0-6f6f27397286</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1 (age 21h)</span><br><span class="line">    mgr: ceph1.tlpgcb(active, since 21h)</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:     </span><br></pre></td></tr></table></figure>
<h3 id="添加新主机到集群中-1"><a href="#添加新主机到集群中-1" class="headerlink" title="添加新主机到集群中"></a>添加新主机到集群中</h3></li>
<li><p>第一步，在 ceph1 上执行命令，将集群的公共 SSH 密钥添加到新主机的根用户 authorized_keys 文件中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph2</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;</span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;root@ceph2&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"></span><br><span class="line">root@ceph1:/opt# ll /etc/ceph/</span><br><span class="line">total 24</span><br><span class="line">drwxr-xr-x   2 root root 4096 Jan  7 16:19 ./</span><br><span class="line">drwxr-xr-x 104 root root 4096 Jan  7 16:02 ../</span><br><span class="line">-rw-------   1 root root   63 Jan  7 16:19 ceph.client.admin.keyring</span><br><span class="line">-rw-r--r--   1 root root  175 Jan  7 16:19 ceph.conf</span><br><span class="line">-rw-r--r--   1 root root  595 Jan  7 16:19 ceph.pub</span><br><span class="line">-rw-r--r--   1 root root   92 Oct 20 22:31 rbdmap</span><br><span class="line">root@ceph1:/opt# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph3</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;</span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;root@ceph3&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"></span><br><span class="line">root@ceph1:/opt# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph4</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;</span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;root@ceph4&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line">root@ceph1:/opt# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286		# 公共SSH密钥</span><br></pre></td></tr></table></figure>
</li>
<li><p>在新结点上查看密钥是否添加成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ceph2:/opt# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286			# 公共SSH密钥</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ceph3:/opt# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286			# 公共SSH密钥</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ceph4:/opt# cat /root/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4</span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286			# 公共SSH密钥</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步，在 ceph1 上执行命令，告诉 Ceph，新节点是集群的一部分：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# ceph orch host add ceph2</span><br><span class="line">Added host &#x27;ceph2&#x27;</span><br><span class="line">root@ceph1:/opt# ceph orch host add ceph3</span><br><span class="line">Added host &#x27;ceph3&#x27;</span><br><span class="line">root@ceph1:/opt# ceph orch host add ceph4</span><br><span class="line">Added host &#x27;ceph4&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看 Ceph 纳管的所有节点：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# ceph orch host ls</span><br><span class="line">HOST   ADDR   LABELS  STATUS  </span><br><span class="line">ceph1  ceph1                  </span><br><span class="line">ceph2  ceph2                  </span><br><span class="line">ceph3  ceph3                  </span><br><span class="line">ceph4  ceph4</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加完成后 Ceph 会自动扩展 monitor 和 manager 到另外 3 个节点，在另外 3 个节点查看，自动运行了以下容器：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ceph2:/opt# docker ps</span><br><span class="line">CONTAINER ID   IMAGE                                      COMMAND                  CREATED        STATUS        PORTS     NAMES</span><br><span class="line">9631037be5f4   quay.io/prometheus/node-exporter:v0.18.1   &quot;/bin/node_exporter …&quot;   13 hours ago   Up 13 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-node-exporter.ceph2</span><br><span class="line">08b248e6e233   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-mgr -…&quot;   13 hours ago   Up 13 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mgr.ceph2.nxltpk</span><br><span class="line">9a57743caf61   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-crash…&quot;   13 hours ago   Up 13 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-crash.ceph2</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ceph3:/opt# docker ps</span><br><span class="line">CONTAINER ID   IMAGE                                      COMMAND                  CREATED        STATUS        PORTS     NAMES</span><br><span class="line">f9ed0005d967   quay.io/prometheus/node-exporter:v0.18.1   &quot;/bin/node_exporter …&quot;   14 hours ago   Up 14 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-node-exporter.ceph3</span><br><span class="line">57c0c9ba4221   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-mon -…&quot;   14 hours ago   Up 14 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mon.ceph3</span><br><span class="line">6ad0d2342d96   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-crash…&quot;   14 hours ago   Up 14 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-crash.ceph3</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ceph4:/opt# docker ps</span><br><span class="line">CONTAINER ID   IMAGE                                      COMMAND                  CREATED        STATUS        PORTS     NAMES</span><br><span class="line">fedc448936a7   quay.io/prometheus/node-exporter:v0.18.1   &quot;/bin/node_exporter …&quot;   14 hours ago   Up 14 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-node-exporter.ceph4</span><br><span class="line">028820a77622   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-mon -…&quot;   14 hours ago   Up 14 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mon.ceph4</span><br><span class="line">d2cee165f5de   quay.io/ceph/ceph:v15                      &quot;/usr/bin/ceph-crash…&quot;   14 hours ago   Up 14 hours             ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-crash.ceph4</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看 Ceph 集群状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root@ceph1:/opt# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     79a6cb92-6f92-11ec-b1e0-6f6f27397286</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            Reduced data availability: 1 pg inactive</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph4,ceph3 (age 14h)</span><br><span class="line">    mgr: ceph1.tlpgcb(active, since 2d), standbys: ceph2.nxltpk</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 1 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:     100.000% pgs unknown</span><br><span class="line">             1 unknown</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="本文参考"><a href="#本文参考" class="headerlink" title="本文参考"></a>本文参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/install/#recommended-methods">https://docs.ceph.com/en/latest/install/#recommended-methods</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/cephadm/install/">https://docs.ceph.com/en/latest/cephadm/install/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kancloud.cn/willseecloud/ceph/1788314">https://www.kancloud.cn/willseecloud/ceph/1788314</a></li>
<li>声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 <a href="mailto:wdshfut@163.com">wdshfut@163.com</a>。</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ceph/" rel="tag"># ceph</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/10/18/python-scientific/" rel="prev" title="python 科学计算工具">
      <i class="fa fa-chevron-left"></i> python 科学计算工具
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/01/04/java-md5/" rel="next" title="java-md5">
      java-md5 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CentOS-%E5%AE%89%E8%A3%85-Ceph"><span class="nav-number">1.</span> <span class="nav-text">CentOS  安装 Ceph</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E8%A7%84%E5%88%92"><span class="nav-number">1.1.</span> <span class="nav-text">节点规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99"><span class="nav-number">1.2.</span> <span class="nav-text">关闭防火墙</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SSH-%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="nav-number">1.3.</span> <span class="nav-text">SSH 免密登录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="nav-number">1.4.</span> <span class="nav-text">服务器时间同步</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NTP-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE"><span class="nav-number">1.4.1.</span> <span class="nav-text">NTP 服务器配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE"><span class="nav-number">1.4.2.</span> <span class="nav-text">客户端配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">1.4.3.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Python3"><span class="nav-number">1.5.</span> <span class="nav-text">安装 Python3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Docker"><span class="nav-number">1.6.</span> <span class="nav-text">安装 Docker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Cephadm"><span class="nav-number">1.7.</span> <span class="nav-text">安装 Cephadm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-Ceph-%E6%96%B0%E9%9B%86%E7%BE%A4"><span class="nav-number">1.8.</span> <span class="nav-text">创建 Ceph 新集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E7%94%A8-Ceph-%E5%91%BD%E4%BB%A4"><span class="nav-number">1.9.</span> <span class="nav-text">启用 Ceph 命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E6%96%B0%E4%B8%BB%E6%9C%BA%E5%88%B0%E9%9B%86%E7%BE%A4%E4%B8%AD"><span class="nav-number">1.10.</span> <span class="nav-text">添加新主机到集群中</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99-1"><span class="nav-number">1.11.</span> <span class="nav-text">关闭防火墙</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SSH-%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95-1"><span class="nav-number">1.12.</span> <span class="nav-text">SSH 免密登录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5-1"><span class="nav-number">1.13.</span> <span class="nav-text">服务器时间同步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Python3-1"><span class="nav-number">1.14.</span> <span class="nav-text">安装 Python3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Docker-1"><span class="nav-number">1.15.</span> <span class="nav-text">安装 Docker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Cephadm-1"><span class="nav-number">1.16.</span> <span class="nav-text">安装 Cephadm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-Ceph-%E6%96%B0%E9%9B%86%E7%BE%A4-1"><span class="nav-number">1.17.</span> <span class="nav-text">创建 Ceph 新集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E7%94%A8-Ceph-%E5%91%BD%E4%BB%A4-1"><span class="nav-number">1.18.</span> <span class="nav-text">启用 Ceph 命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E6%96%B0%E4%B8%BB%E6%9C%BA%E5%88%B0%E9%9B%86%E7%BE%A4%E4%B8%AD-1"><span class="nav-number">1.19.</span> <span class="nav-text">添加新主机到集群中</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AC%E6%96%87%E5%8F%82%E8%80%83"><span class="nav-number">2.</span> <span class="nav-text">本文参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="XiSun"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">XiSun</p>
  <div class="site-description" itemprop="description">心如止水者，虽世间繁华之红尘纷扰，已然空无一物</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XiSun</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.8m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">27:20</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
