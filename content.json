{"meta":{"title":"XiSun的博客","subtitle":"Learning is endless","description":"心如止水者，虽世间繁华之红尘纷扰，已然空无一物","author":"XiSun","url":"http://example.com","root":"/"},"pages":[{"title":"tags","date":"2020-10-27T08:35:56.000Z","updated":"2020-10-27T08:40:16.641Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"guestbook","date":"2021-08-17T09:27:26.000Z","updated":"2021-08-19T06:40:35.623Z","comments":true,"path":"guestbook/index.html","permalink":"http://example.com/guestbook/index.html","excerpt":"","text":""}],"posts":[{"title":"git","slug":"git","date":"2022-01-04T03:28:46.000Z","updated":"2022-01-11T13:01:18.824Z","comments":true,"path":"2022/01/04/git/","link":"","permalink":"http://example.com/2022/01/04/git/","excerpt":"","text":"G：https://www.jianshu.com/p/9140b1e9ecc1 https://blog.csdn.net/weixin_41287260/article/details/89743120","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"}]},{"title":"java-md5","slug":"java-md5","date":"2022-01-04T02:51:22.000Z","updated":"2022-01-04T02:52:52.531Z","comments":true,"path":"2022/01/04/java-md5/","link":"","permalink":"http://example.com/2022/01/04/java-md5/","excerpt":"","text":"JAjava 获取 md5 的四种方法：https://www.cnblogs.com/pcheng/p/7724863.html","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Ceph 入门","slug":"ceph","date":"2021-12-21T07:59:22.000Z","updated":"2022-01-11T01:08:04.615Z","comments":true,"path":"2021/12/21/ceph/","link":"","permalink":"http://example.com/2021/12/21/ceph/","excerpt":"","text":"CentOS 安装 Ceph节点规划 各节点规划： 主机名 ip 磁盘 角色 hadoop102 192.168.10.102 系统盘：sdaosd盘：sdb cephadm，monitor，mgr，rgw，mds，osd，nfs hadoop103 192.168.10.103 系统盘：sdaosd盘：sdb monitor，mgr，rgw，mds，osd，nfs hadoop104 192.168.10.104 系统盘：sdaosd盘：sdb monitor，mgr，rgw，mds，osd，nfs 各节点内核版本： 12[root@hadoop102 opt]# uname -r3.10.0-1160.49.1.el7.x86_64 各节点操作系统版本： 12[root@hadoop102 opt]# cat /etc/redhat-releaseCentOS Linux release 7.9.2009 (Core) 各节点配置 CentOS 7 yum 阿里云镜像源： 12345[root@hadoop102 opt]# curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# 清空缓存[root@hadoop102 opt]# yum clean cache# 生成缓存[root@hadoop102 opt]# yum makecache 执行命令 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo，会把 Centos-7.repo 下载到 /etc/yum.repos.d/ 目录下，如果该目录下有 CentOS-Base.repo，则会自动覆盖。 1234567891011121314[root@hadoop102 opt]# ll /etc/yum.repos.d/总用量 56-rw-r--r--. 1 root root 2523 12月 21 14:16 CentOS-Base.repo # yum源-rw-r--r--. 1 root root 1309 11月 23 2020 CentOS-CR.repo-rw-r--r--. 1 root root 649 11月 23 2020 CentOS-Debuginfo.repo-rw-r--r--. 1 root root 314 11月 23 2020 CentOS-fasttrack.repo-rw-r--r--. 1 root root 630 11月 23 2020 CentOS-Media.repo-rw-r--r--. 1 root root 1331 11月 23 2020 CentOS-Sources.repo-rw-r--r--. 1 root root 8515 11月 23 2020 CentOS-Vault.repo-rw-r--r--. 1 root root 616 11月 23 2020 CentOS-x86_64-kernel.repo-rw-r--r--. 1 root root 477 12月 21 15:38 ceph.repo # ceph源-rw-r--r--. 1 root root 2081 12月 20 12:37 docker-ce.repo # docker源-rw-r--r--. 1 root root 1358 9月 5 01:37 epel.repo-rw-r--r--. 1 root root 1457 9月 5 01:37 epel-testing.repo 各节点更新 yum 包（生产环境中此步操作需慎重，看自己情况，学习的话随便搞，这个命令不是必须执行的）： 12[root@hadoop102 opt]# yum -y update[root@hadoop102 opt]# yum -y upgrade yum -y update：升级所有包，也升级软件和系统内核。 yum -y upgrade：升级所有包，但不升级软件和系统内核。 各节点配置主机名： 123[root@hadoop102 opt]# vim /etc/hostname[root@hadoop102 opt]# cat /etc/hostnamehadoop102 各节点配置 host 解析： 123456789101112131415[root@hadoop102 opt]# vim /etc/hosts[root@hadoop102 opt]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.10.99 centos7192.168.10.100 hadoop100192.168.10.101 hadoop101192.168.10.102 hadoop102192.168.10.103 hadoop103192.168.10.104 hadoop104192.168.10.105 hadoop105192.168.10.106 hadoop106192.168.10.107 hadoop107192.168.10.108 hadoop108 关闭防火墙 关闭防火墙： 123[root@hadoop102 xisun]# systemctl stop firewalld.service[root@hadoop102 xisun]# firewall-cmd --statenot running 关闭防火墙开机自启： 123456[root@hadoop102 xisun]# systemctl disable firewalld.service Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.# 查看防火墙是否开机自启[root@hadoop102 opt]# systemctl list-unit-files | grep firewalld.service firewalld.service disabled SSH 免密登录服务器时间同步 在 Linux 系统中，可以通过 ntpdate 和 ntpd 两种方式实现 NTP 时间同步，ntpdate 为断点更新，ntpd 为步进式地逐渐调整时间。对于新服务器，可以使用 ntpdate 同步时间，对于已经承载有运行中业务的服务器，建议使用 ntpd 同步时间。 直接同步：使用 ntpdate 命令进行同步，直接进行时间变更。如果服务器上存在一个 12 点运行的任务，当前服务器时间是 13 点，但标准时间时 11 点，使用此命令可能会造成任务重复执行。因此使用 ntpdate 同步可能会引发风险，该命令也多用于配置时钟同步服务时第一次同步时间时使用。 平滑同步：使用 ntpd 进行时钟同步，可以保证一个时间不经历两次，它每次同步时间的偏移量不会太陡，是慢慢来的，这正因为这样，ntpd 平滑同步可能耗费的时间比较长。 NTP 服务器配置 寻找 NTP Server，https://www.ntppool.org/ 是 NTP 的官方网站，在这上面我们可以找到离我们城市最近的 NTP Server。NTP 建议我们为了保障时间的准确性，最少找两个 NTP Server。我们找到对应的中国 NTP Server： 也可以在 http://www.ntp.org.cn/ 网站查找 NTP Server，推荐使用域名，而非 IP 地址。后面的 NTP 服务器配置，以下图为准： 设置时区： 12345# 设置东八区[root@hadoop102 opt]# timedatectl set-timezone Asia/Shanghai# 查看时区[root@hadoop102 opt]# timedatectl status | grep &#x27;Time zone&#x27; Time zone: Asia/Shanghai (CST, +0800) 检查系统是否安装了 NTP 服务： 12345[root@hadoop102 opt]# rpm -qa | grep ntppython-ntplib-0.3.2-1.el7.noarchntpdate-4.2.6p5-29.el7.centos.2.x86_64ntp-4.2.6p5-29.el7.centos.2.x86_64fontpackages-filesystem-1.44-8.el7.noarch 如果没有安装，使用下面的命令安装 ntp 和 ntpdate： 1[root@hadoop102 opt]# yum -y install ntp ntpdate 根据 NTP 的设置，如果你的系统时间比正确时间快，那么 NTP 是不会帮你调整的，所以要么你把时间设置回去，要么先做一个手动同步，使用下面的命令： 12[root@hadoop102 opt]# ntpdate cn.ntp.org.cn22 Dec 15:09:33 ntpdate[5303]: adjust time server 114.67.237.130 offset -0.010511 sec 配置 NTP 服务器（设定 hadoop102 为 NTP 服务器），NTP 服务器主配置文件 /etc/ntp.conf，配置前做好备份： 123[root@hadoop102 opt]# mkdir /home/backup[root@hadoop102 opt]# cp /etc/ntp.conf /home/backup/[root@hadoop102 opt]# mv /home/backup/ntp.conf /home/backup/ntp.conf.bak 配置 NTP 服务器端配置文件： 1[root@hadoop102 opt]# vim /etc/ntp.conf restrict 语法说明： 如果服务器是内网，不能连接外网，则无法使用网络 NTP Server，此时可以将本机作为 NTP Server，按如下方法配置： ntp 服务，默认只会同步系统时间。如果想要让 ntp 同时同步硬件时间，可以在 /etc/sysconfig/ntpd 文件中，添加 SYNC_HWCLOCK=yes，这样，就可以让硬件时间与系统时间一起同步： 1[root@hadoop102 opt]# vim /etc/sysconfig/ntpd 启动 NTP 服务并设置开机自动启动。(Linux 防火墙需要关闭，否则会阻止 NTP 服务端口，NTP 服务默认端口 123) 1234567891011121314151617181920212223242526272829303132# 启动ntp服务[root@hadoop102 opt]# systemctl start ntpd# 查看ntp服务是否启动[root@hadoop102 opt]# systemctl status ntpd● ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled) Active: active (running) since 三 2021-12-22 16:00:12 CST; 1min 27s ago Process: 5883 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS) Main PID: 5884 (ntpd) Tasks: 1 CGroup: /system.slice/ntpd.service └─5884 /usr/sbin/ntpd -u ntp:ntp -g12月 22 16:00:13 hadoop102 ntpd[5884]: Listen and drop on 1 v6wildcard :: UDP 12312月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 2 lo 127.0.0.1 UDP 12312月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 3 ens33 192.168.10.102 UDP 12312月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 4 virbr0 192.168.122.1 UDP 12312月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 5 lo ::1 UDP 12312月 22 16:00:13 hadoop102 ntpd[5884]: Listen normally on 6 ens33 fe80::ac1e:7fe1:a566:2670 UDP 12312月 22 16:00:13 hadoop102 ntpd[5884]: Listening on routing socket on fd #23 for interface updates12月 22 16:00:13 hadoop102 ntpd[5884]: 0.0.0.0 c016 06 restart12月 22 16:00:13 hadoop102 ntpd[5884]: 0.0.0.0 c012 02 freq_set kernel 0.000 PPM12月 22 16:00:13 hadoop102 ntpd[5884]: 0.0.0.0 c011 01 freq_not_set# 查看ntpd端口[root@hadoop102 opt]# netstat -ln | grep 123udp 0 0 192.168.122.1:123 0.0.0.0:* udp 0 0 192.168.10.102:123 0.0.0.0:* udp 0 0 127.0.0.1:123 0.0.0.0:* udp 0 0 0.0.0.0:123 0.0.0.0:* udp6 0 0 fe80::ac1e:7fe1:a56:123 :::* udp6 0 0 ::1:123 :::* udp6 0 0 :::123 :::* 1234567891011# 查看ntp服务是否开机启动[root@hadoop102 opt]# systemctl list-unit-files | grep ntpdntpd.service disabledntpdate.service disabled# 设置开机启动ntp服务[root@hadoop102 opt]# systemctl enable ntpdCreated symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.# 确认ntp服务开机启动设置成功[root@hadoop102 opt]# systemctl list-unit-files | grep ntpdntpd.service enabled ntpdate.service disabled 如果是重启 ntpd 服务，使用下面的命令： 1[root@hadoop102 opt]# systemctl restart ntpd 查看 NTP 服务器与外部 NTP 服务器同步情况： 123456789# 静态查看[root@hadoop102 opt]# ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== 2001:da8:9000:: .STEP. 16 - - 512 0 0.000 0.000 0.000*202.118.1.130 .PTP. 1 u 14 64 277 39.772 -39.060 5.282# 动态查看，一般ntp启动后，需要等5~10分钟左右，NTP服务器才会与外部NTP服务器同步[root@hadoop102 opt]# watch ntpq -p remote：即网络 NTP 服务器的 IP 或主机名称。注意最左边的符号，如果有 +，则代表目前正在作用中的上层 NTP 服务器；如果是 *，则表示也有连上线，不过是作为次要联机的 NTP 服务器。 refid：参考的上一层 NTP 服务器的地址。 st：即 stratum 阶层，值越小表示 NTP Server 的精准度越高。 when：几秒前曾做过时间同步更新的操作。 poll：每隔多少毫秒与 NTP Server 同步一次。 reach：已经向上层 NTP 服务器要求更新的次数。 delay：网络传输过程中延迟的时间。 offset：时间补偿的结果。 jitter：Linux 系统时间与 BIOS 硬件时间的差异时间。 查看系统时间和硬件时间： 123456# 系统时间[root@hadoop102 opt]# date2021年 12月 23日 星期四 16:28:10 CST# 硬件时间[root@hadoop102 opt]# hwclock --show2021年12月23日 星期四 16时28分15秒 -0.631618 秒 客户端配置 设置时区： 12345# 设置东八区[root@hadoop103 opt]# timedatectl set-timezone Asia/Shanghai# 查看时区[root@hadoop103 opt]# timedatectl status | grep &#x27;Time zone&#x27; Time zone: Asia/Shanghai (CST, +0800) 客户端向 NTP 服务器（192.168.10.102）更新时间时，客户端不需要开启 NTP 服务： 12345678910111213141516[root@hadoop103 opt]# systemctl stop ntpd[root@hadoop103 opt]# systemctl status ntpd● ntpd.service - Network Time Service Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled) Active: inactive (dead)12月 23 11:11:08 hadoop103 ntpd[18278]: Listening on routing socket on fd #24 for interface updates12月 23 11:11:10 hadoop103 ntpd[18278]: 0.0.0.0 c016 06 restart12月 23 11:11:10 hadoop103 ntpd[18278]: 0.0.0.0 c012 02 freq_set kernel 0.000 PPM12月 23 11:11:10 hadoop103 ntpd[18278]: 0.0.0.0 c011 01 freq_not_set12月 23 11:11:16 hadoop103 ntpd[18278]: 0.0.0.0 c61c 0c clock_step -3.147226 s12月 23 11:11:13 hadoop103 ntpd[18278]: 0.0.0.0 c614 04 freq_mode12月 23 11:11:14 hadoop103 ntpd[18278]: 0.0.0.0 c618 08 no_sys_peer12月 23 11:16:42 hadoop103 ntpd[18278]: ntpd exiting on signal 1512月 23 11:16:42 hadoop103 systemd[1]: Stopping Network Time Service...12月 23 11:16:42 hadoop103 systemd[1]: Stopped Network Time Service. 客户端向 NTP 服务器（192.168.10.102）进行时间同步： 12[root@hadoop103 opt]# ntpdate -u 192.168.10.10223 Dec 11:16:57 ntpdate[18349]: adjust time server 192.168.10.102 offset -0.003982 sec 设置 ntpdate 每次同步系统时间之后，也一并同步硬件时间： 1[root@hadoop103 opt]# vim /etc/sysconfig/ntpdate 修改 ntpdate 文件最后一行 SYNC_HWCLOCK=no 为 SYNC_HWCLOCK=yes： 设置客户端定时向 NTP 服务器（192.168.10.102）进行时间同步： 1[root@hadoop103 opt]# vim /etc/crontab 向 crontab 文件中添加配置，每天早晨 6 点同步一次时间： 重启 crond 服务： 1[root@hadoop103 opt]# systemctl restart crond.service 查看 crond 服务执行情况： 1234567891011121314[root@hadoop103 opt]# systemctl status crond.service ● crond.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled) Active: active (running) since 四 2021-12-23 13:41:42 CST; 4min 33s ago Main PID: 20442 (crond) Tasks: 1 Memory: 644.0K CGroup: /system.slice/crond.service └─20442 /usr/sbin/crond -n12月 23 13:41:42 hadoop103 systemd[1]: Started Command Scheduler.12月 23 13:41:42 hadoop103 crond[20442]: (CRON) INFO (RANDOM_DELAY will be scaled with factor 33% if used.)12月 23 13:41:43 hadoop103 crond[20442]: (CRON) INFO (running with inotify support)12月 23 13:41:43 hadoop103 crond[20442]: (CRON) INFO (@reboot jobs will be run at computer&#x27;s startup.) 查看系统时间和硬件时间： 123456# 系统时间[root@hadoop103 opt]# date 2021年 12月 23日 星期四 16:30:22 CST# 硬件时间[root@hadoop103 opt]# hwclock 2021年12月23日 星期四 16时30分29秒 -1.020610 秒 参考 https://developer.aliyun.com/article/5615 https://www.jianshu.com/p/aa2bb27debd9 https://www.cnblogs.com/zoulongbin/p/6198186.html https://www.jianshu.com/p/16e68204b3dc 安装 Python3 Ceph 的 Octopus 版本需要 Python3 支持。 各节点检查是否有 GCC： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# 查看gcc版本[root@hadoop101 software]# gcc --versionbash: gcc: 未找到命令...# 安装gcc[root@hadoop101 software]# yum -y install gcc已加载插件：fastestmirror, langpacksLoading mirror speeds from cached hostfileCould not get metalink https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=x86_64&amp;infra=stock&amp;content=centos error was14: curl#7 - &quot;Failed to connect to 2406:da18:39f:a01:35a2:d9e9:8164:a209: 网络不可达&quot; * base: mirrors.aliyun.com * epel: mirror.sjtu.edu.cn * extras: mirrors.aliyun.com * updates: mirrors.aliyun.com正在解决依赖关系--&gt; 正在检查事务---&gt; 软件包 gcc.x86_64.0.4.8.5-44.el7 将被 安装--&gt; 正在处理依赖关系 cpp = 4.8.5-44.el7，它被软件包 gcc-4.8.5-44.el7.x86_64 需要--&gt; 正在处理依赖关系 glibc-devel &gt;= 2.2.90-12，它被软件包 gcc-4.8.5-44.el7.x86_64 需要--&gt; 正在检查事务---&gt; 软件包 cpp.x86_64.0.4.8.5-44.el7 将被 安装---&gt; 软件包 glibc-devel.x86_64.0.2.17-325.el7_9 将被 安装--&gt; 正在处理依赖关系 glibc-headers = 2.17-325.el7_9，它被软件包 glibc-devel-2.17-325.el7_9.x86_64 需要--&gt; 正在处理依赖关系 glibc-headers，它被软件包 glibc-devel-2.17-325.el7_9.x86_64 需要--&gt; 正在检查事务---&gt; 软件包 glibc-headers.x86_64.0.2.17-325.el7_9 将被 安装--&gt; 正在处理依赖关系 kernel-headers &gt;= 2.2.1，它被软件包 glibc-headers-2.17-325.el7_9.x86_64 需要--&gt; 正在处理依赖关系 kernel-headers，它被软件包 glibc-headers-2.17-325.el7_9.x86_64 需要--&gt; 正在检查事务---&gt; 软件包 kernel-headers.x86_64.0.3.10.0-1160.49.1.el7 将被 安装--&gt; 解决依赖关系完成依赖关系解决======================================================================================================================================================================================================== Package 架构 版本 源 大小========================================================================================================================================================================================================正在安装: gcc x86_64 4.8.5-44.el7 base 16 M为依赖而安装: cpp x86_64 4.8.5-44.el7 base 5.9 M glibc-devel x86_64 2.17-325.el7_9 updates 1.1 M glibc-headers x86_64 2.17-325.el7_9 updates 691 k kernel-headers x86_64 3.10.0-1160.49.1.el7 updates 9.0 M事务概要========================================================================================================================================================================================================安装 1 软件包 (+4 依赖软件包)总计：33 M总下载量：1.1 M安装大小：59 MDownloading packages:No Presto metadata available for updatesglibc-devel-2.17-325.el7_9.x86_64.rpm | 1.1 MB 00:00:00 Running transaction checkRunning transaction testTransaction test succeededRunning transaction 正在安装 : cpp-4.8.5-44.el7.x86_64 1/5 正在安装 : kernel-headers-3.10.0-1160.49.1.el7.x86_64 2/5 正在安装 : glibc-headers-2.17-325.el7_9.x86_64 3/5 正在安装 : glibc-devel-2.17-325.el7_9.x86_64 4/5 正在安装 : gcc-4.8.5-44.el7.x86_64 5/5 验证中 : gcc-4.8.5-44.el7.x86_64 1/5 验证中 : glibc-headers-2.17-325.el7_9.x86_64 2/5 验证中 : kernel-headers-3.10.0-1160.49.1.el7.x86_64 3/5 验证中 : glibc-devel-2.17-325.el7_9.x86_64 4/5 验证中 : cpp-4.8.5-44.el7.x86_64 5/5 已安装: gcc.x86_64 0:4.8.5-44.el7 作为依赖被安装: cpp.x86_64 0:4.8.5-44.el7 glibc-devel.x86_64 0:2.17-325.el7_9 glibc-headers.x86_64 0:2.17-325.el7_9 kernel-headers.x86_64 0:3.10.0-1160.49.1.el7 完毕！[root@hadoop102 software]# gcc --versiongcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)Copyright © 2015 Free Software Foundation, Inc.本程序是自由软件；请参看源代码的版权声明。本软件没有任何担保；包括没有适销性和某一专用目的下的适用性担保。 各节点下载相应版本的 Python 包： 123456789101112131415[root@hadoop101 software]# wget https://www.python.org/ftp/python/3.9.9/Python-3.9.9.tar.xz--2021-12-24 10:16:02-- https://www.python.org/ftp/python/3.9.9/Python-3.9.9.tar.xz正在解析主机 www.python.org (www.python.org)... 151.101.72.223正在连接 www.python.org (www.python.org)|151.101.72.223|:443... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度：19144372 (18M) [application/octet-stream]正在保存至: “Python-3.9.9.tar.xz”100%[==============================================================================================================================================================&gt;] 19,144,372 7.20MB/s 用时 2.5s 2021-12-24 10:16:05 (7.20 MB/s) - 已保存 “Python-3.9.9.tar.xz” [19144372/19144372])[root@hadoop101 software]# ls -l总用量 18696-rw-r--r--. 1 root root 19144372 11月 16 02:49 Python-3.9.9.tar.xz 解压到指定目录： 1234[root@hadoop101 software]# tar -xvJf Python-3.9.9.tar.xz -C /opt/module/[root@hadoop101 software]# ls -l /opt/module/总用量 2drwxrwxr-x. 16 xisun xisun 4096 11月 16 02:05 Python-3.9.9 各节点安装依赖，否则会报错： 1234567891011[root@hadoop101 software]# yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel...已安装: bzip2-devel.x86_64 0:1.0.6-13.el7 gdbm-devel.x86_64 0:1.10-8.el7 libdb4-devel.x86_64 0:4.8.30-13.el7 libpcap-devel.x86_64 14:1.5.3-12.el7 ncurses-devel.x86_64 0:5.9-14.20130511.el7_4 openssl-devel.x86_64 1:1.0.2k-22.el7_9 readline-devel.x86_64 0:6.2-11.el7 sqlite-devel.x86_64 0:3.7.17-8.el7_7.1 tk-devel.x86_64 1:8.5.13-6.el7 xz-devel.x86_64 0:5.2.2-1.el7 zlib-devel.x86_64 0:1.2.7-19.el7_9 作为依赖被安装: expat-devel.x86_64 0:2.1.0-12.el7 fontconfig-devel.x86_64 0:2.13.0-4.3.el7 freetype-devel.x86_64 0:2.8-14.el7_9.1 keyutils-libs-devel.x86_64 0:1.5.8-3.el7 krb5-devel.x86_64 0:1.15.1-51.el7_9 libXft-devel.x86_64 0:2.3.2-2.el7 libXrender-devel.x86_64 0:0.9.10-1.el7 libcom_err-devel.x86_64 0:1.42.9-19.el7 libdb4.x86_64 0:4.8.30-13.el7 libpng-devel.x86_64 2:1.5.13-8.el7 libselinux-devel.x86_64 0:2.5-15.el7 libsepol-devel.x86_64 0:2.5-10.el7 libuuid-devel.x86_64 0:2.23.2-65.el7_9.1 libverto-devel.x86_64 0:0.2.5-4.el7 pcre-devel.x86_64 0:8.32-17.el7 各节点安装 Python3： 1234567# 指定安装的路径，不指定的话，安装过程中可能软件所需要的文件复制到其他不同目录，删除软件很不方便，复制软件也不方便[root@hadoop101 software]# mkdir /usr/local/python3[root@hadoop101 software]# cd /opt/module/Python-3.9.9/# 配置，指定安装目录[root@hadoop101 Python-3.9.9]# ./configure --prefix=/usr/local/python3# 编译安装[root@hadoop101 Python-3.9.9]# make &amp;&amp; make install 在安装过程中，如果出现错误，在重新安装之前先执行下面的命令，清空缓存： 1[root@hadoop101 Python-3.9.9]# make clean 各节点添加软链接： 12[root@hadoop102 Python-3.9.9]# ln -s /usr/local/python3/bin/python3 /usr/bin/python3[root@hadoop102 Python-3.9.9]# ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 软链接位置定为 /usr/bin/python3 和 /usr/bin/pip3。 各节点查看版本： 1234[root@hadoop102 Python-3.9.9]# python3 --versionPython 3.9.9[root@hadoop102 Python-3.9.9]# pip3 --versionpip 21.2.4 from /usr/local/python3/lib/python3.9/site-packages/pip (python 3.9) 安装 Docker Cephadm 基于容器运行所有 Ceph 组件，各节点需要安装 Docker 或 Podman，此处安装 Docker。 各节点卸载旧版本 Docker（如果之前有安装过）： 1[root@hadoop102 opt]# yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine 各节点安装需要的软件包，yum-util 提供 yum-config-manager 功能： 1[root@hadoop102 opt]# yum install -y yum-utils 各节点设置 yum 的 Docker 源（下面两个都可以用）： 12345# 中央仓库[root@hadoop102 opt]# yum-config-manager --add-repo http://download.docker.com/linux/centos/docker-ce.repo# 阿里仓库[root@hadoop102 opt]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 各节点安装最新版本 Docker： 1[root@hadoop102 opt]# yum install docker-ce docker-ce-cli containerd.io 安装特定版本： 12345# 查看不同版本[root@hadoop102 opt]# yum list docker-ce --showduplicates | sort -r# 安装特定版本[root@hadoop102 opt]# yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io 各节点启动 Docker，并设置开机启动： 123456789101112# 启动[root@hadoop102 opt]# systemctl start docker# 查看docker版本[root@hadoop102 opt]# docker --versionDocker version 20.10.12, build e91ed57# 开机启动[root@hadoop102 opt]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.# 查看docker是否开机启动[root@hadoop102 opt]# systemctl list-unit-files | grep dockerdocker.service enabled docker.socket disabled 各节点测试： 12345678910111213141516171819202122[root@hadoop102 opt]# docker run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 参考：https://docs.docker.com/engine/install/centos/ 安装 Cephadm 各节点使用 curl 获取 Cephadm 独立脚本的最新版本： 1234567891011121314151617181920# 下载[root@hadoop102 opt]# curl --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 137 100 137 0 0 15 0 0:00:09 0:00:09 --:--:-- 18100 218k 100 218k 0 0 17210 0 0:00:12 0:00:12 --:--:-- 93775[root@hadoop102 opt]# ll总用量 220-rw-r--r--. 1 root root 223468 12月 26 15:42 cephadmdrwx--x--x. 4 root root 28 12月 21 14:56 containerddrwxr-xr-x. 4 xisun xisun 46 12月 24 10:32 moduledrwxr-xr-x. 2 xisun xisun 33 12月 24 10:30 software# 赋权可执行[root@hadoop102 opt]# chmod +x cephadm[root@hadoop102 opt]# ll总用量 220-rwxr-xr-x. 1 root root 223468 12月 26 15:42 cephadmdrwx--x--x. 4 root root 28 12月 21 14:56 containerddrwxr-xr-x. 4 xisun xisun 46 12月 24 10:32 moduledrwxr-xr-x. 2 xisun xisun 33 12月 24 10:30 software 下载可能需要执行多次。 各节点添加源信息，指定为 Octopus 版本： 12345678910111213141516171819202122232425262728293031323334353637383940# 添加Ceph源，这个是官方源[root@hadoop102 opt]# ./cephadm add-repo --release octopusWriting repo to /etc/yum.repos.d/ceph.repo...Enabling EPEL...# 查看Ceph源信息[root@hadoop102 opt]# ll /etc/yum.repos.d/总用量 56-rw-r--r--. 1 root root 2523 12月 21 14:16 CentOS-Base.repo-rw-r--r--. 1 root root 1309 11月 23 2020 CentOS-CR.repo-rw-r--r--. 1 root root 649 11月 23 2020 CentOS-Debuginfo.repo-rw-r--r--. 1 root root 314 11月 23 2020 CentOS-fasttrack.repo-rw-r--r--. 1 root root 630 11月 23 2020 CentOS-Media.repo-rw-r--r--. 1 root root 1331 11月 23 2020 CentOS-Sources.repo-rw-r--r--. 1 root root 8515 11月 23 2020 CentOS-Vault.repo-rw-r--r--. 1 root root 616 11月 23 2020 CentOS-x86_64-kernel.repo-rw-r--r--. 1 root root 477 12月 26 15:49 ceph.repo # Ceph源-rw-r--r--. 1 root root 2081 12月 21 12:38 docker-ce.repo-rw-r--r--. 1 root root 1358 9月 5 01:37 epel.repo-rw-r--r--. 1 root root 1457 9月 5 01:37 epel-testing.repo[root@hadoop102 opt]# cat /etc/yum.repos.d/ceph.repo [Ceph]name=Ceph $basearchbaseurl=https://download.ceph.com/rpm-octopus/el7/$basearchenabled=1gpgcheck=1gpgkey=https://download.ceph.com/keys/release.asc[Ceph-noarch]name=Ceph noarchbaseurl=https://download.ceph.com/rpm-octopus/el7/noarchenabled=1gpgcheck=1gpgkey=https://download.ceph.com/keys/release.asc[Ceph-source]name=Ceph SRPMSbaseurl=https://download.ceph.com/rpm-octopus/el7/SRPMSenabled=1gpgcheck=1gpgkey=https://download.ceph.com/keys/release.asc 添加 Ceph 源可能需要执行多次，或重启。 如果官方源下载较慢，可以使用阿里云 Ceph 源： 各节点安装 Cephadm： 12[root@hadoop102 opt]# ./cephadm installInstalling packages [&#x27;cephadm&#x27;]... 各节点验证 Cephadm 安装完成： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@hadoop102 opt]# which cephadm/usr/sbin/cephadm[root@hadoop102 opt]# cephadm versionUsing recent ceph image quay.io/ceph/ceph@sha256:a2c23b6942f7fbc1e15d8cfacd6655a681fe0e44f288e4a158db22030b8d58e3ceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable)[root@hadoop102 opt]# cephadm --helpusage: cephadm [-h] [--image IMAGE] [--docker] [--data-dir DATA_DIR] [--log-dir LOG_DIR] [--logrotate-dir LOGROTATE_DIR] [--unit-dir UNIT_DIR] [--verbose] [--timeout TIMEOUT] [--retry RETRY] [--env ENV] [--no-container-init] &#123;version,pull,inspect-image,ls,list-networks,adopt,rm-daemon,rm-cluster,run,shell,enter,ceph-volume,unit,logs,bootstrap,deploy,check-host,prepare-host,add-repo,rm-repo,install,registry-login,gather-facts&#125; ...Bootstrap Ceph daemons with systemd and containers.positional arguments: &#123;version,pull,inspect-image,ls,list-networks,adopt,rm-daemon,rm-cluster,run,shell,enter,ceph-volume,unit,logs,bootstrap,deploy,check-host,prepare-host,add-repo,rm-repo,install,registry-login,gather-facts&#125; sub-command version get ceph version from container pull pull latest image version inspect-image inspect local container image ls list daemon instances on this host list-networks list IP networks adopt adopt daemon deployed with a different tool rm-daemon remove daemon instance rm-cluster remove all daemons for a cluster run run a ceph daemon, in a container, in the foreground shell run an interactive shell inside a daemon container enter run an interactive shell inside a running daemon container ceph-volume run ceph-volume inside a container unit operate on the daemon&#x27;s systemd unit logs print journald logs for a daemon container bootstrap bootstrap a cluster (mon + mgr daemons) deploy deploy a daemon check-host check host configuration prepare-host prepare a host for cephadm use add-repo configure package repository rm-repo remove package repository configuration install install ceph package(s) registry-login log host into authenticated registry gather-facts gather and return host related information (JSON format)optional arguments: -h, --help show this help message and exit --image IMAGE container image. Can also be set via the &quot;CEPHADM_IMAGE&quot; env var (default: None) --docker use docker instead of podman (default: False) --data-dir DATA_DIR base directory for daemon data (default: /var/lib/ceph) --log-dir LOG_DIR base directory for daemon logs (default: /var/log/ceph) --logrotate-dir LOGROTATE_DIR location of logrotate configuration files (default: /etc/logrotate.d) --unit-dir UNIT_DIR base directory for systemd units (default: /etc/systemd/system) --verbose, -v Show debug-level log messages (default: False) --timeout TIMEOUT timeout in seconds (default: None) --retry RETRY max number of retries (default: 10) --env ENV, -e ENV set environment variable (default: []) --no-container-init Do not run podman/docker with `--init` (default: True) 为方便后续使用，各节点安装 ceph-common 包，里面包含了所有的 ceph 命令，其中包括 ceph，rbd，mount.ceph（用于安装 CephFS 文件系统）等： 123456# 安装[root@hadoop102 opt]# cephadm install ceph-commonInstalling packages [&#x27;ceph-common&#x27;]...# 确认可以使用[root@hadoop102 opt]# ceph -vceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable) 创建 Ceph 新集群 在 hadoop102 上创建一个可以被任何访问 Ceph 集群的主机访问的网络，指定 mon-ip，并将生成的配置文件写进 /etc/ceph 目录里： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[root@hadoop102 opt]# cephadm bootstrap --mon-ip 192.168.10.102Verifying podman|docker is present...Verifying lvm2 is present...Verifying time synchronization is in place...Unit ntpd.service is enabled and runningRepeating the final host check...podman|docker (/usr/bin/docker) is presentsystemctl is presentlvcreate is presentUnit ntpd.service is enabled and runningHost looks OKCluster fsid: 81b469b6-662d-11ec-b2eb-000c29c51d96Verifying IP 192.168.10.102 port 3300 ...Verifying IP 192.168.10.102 port 6789 ...Mon IP 192.168.10.102 is in CIDR network 192.168.10.0/24Pulling container image quay.io/ceph/ceph:v15...Extracting ceph user uid/gid from container image...Creating initial keys...Creating initial monmap...Creating mon...Waiting for mon to start...Waiting for mon...mon is availableAssimilating anything we can from ceph.conf...Generating new minimal ceph.conf...Restarting the monitor...Setting mon public_network...Creating mgr...Verifying port 9283 ...Wrote keyring to /etc/ceph/ceph.client.admin.keyringWrote config to /etc/ceph/ceph.confWaiting for mgr to start...Waiting for mgr...mgr not available, waiting (1/10)...mgr not available, waiting (2/10)...mgr not available, waiting (3/10)...mgr not available, waiting (4/10)...mgr not available, waiting (5/10)...mgr is availableEnabling cephadm module...Waiting for the mgr to restart...Waiting for Mgr epoch 5...Mgr epoch 5 is availableSetting orchestrator backend to cephadm...Generating ssh key...Wrote public SSH key to to /etc/ceph/ceph.pubAdding key to root@localhost&#x27;s authorized_keys...Adding host hadoop102...Deploying mon service with default placement...Deploying mgr service with default placement...Deploying crash service with default placement...Enabling mgr prometheus module...Deploying prometheus service with default placement...Deploying grafana service with default placement...Deploying node-exporter service with default placement...Deploying alertmanager service with default placement...Enabling the dashboard module...Waiting for the mgr to restart...Waiting for Mgr epoch 13...Mgr epoch 13 is availableGenerating a dashboard self-signed certificate...Creating initial admin user...Fetching dashboard port number...Ceph Dashboard is now available at: URL: https://hadoop102:8443/ User: admin Password: v5b0064nc4You can access the Ceph CLI with: sudo /usr/sbin/cephadm shell --fsid 81b469b6-662d-11ec-b2eb-000c29c51d96 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyringPlease consider enabling telemetry to help improve Ceph: ceph telemetry onFor more information see: https://docs.ceph.com/docs/master/mgr/telemetry/Bootstrap complete. 该命令执行如下操作： 在本地主机上为新集群创建 monitor 和 manager daemon 守护程序。 为 Ceph 集群生成一个新的 SSH 密钥，并将其添加到 root 用户的 /root/.ssh/authorized_keys 文件中。 12345[root@hadoop102 opt]# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkEXI7QySiz2VlLNrYW7OypfS9ZtMXS2h725W+pTpv71e4z6YjjxIpt6DvggtuD/UhktL1HxIvAREYKQ5vNYhGuV9jHI4VU+y4evpPEMR8GJ8OTlhfwhwovsik4TfYW0SmHUw01bxBpH2BjX4u/dSpyN9KG9710WXTIGJQ6zbHjxPg0JQX4C1KLuaD1WMVo1z6xscRGs7RAZ7sYAIQdf8LOI4IYVzO1n66NJNY4vpAJx1wCLajoGN0689Do42jX6t0/Qn6RGddXermTbbPW+hcD20pRIHO4f9VCQwqSNdcEVDUS74VMJRuOxd+N0uAxvHz8lTdCSfUnfPeKEvEH+cr root@hadoop102ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9lRXTDUhUKdfu5PznYhoFsJFw1n6qXKJXCgFmuFCBB3T75na5xSgGbUEcQ5HYIWk8ywjynGjzCQMlyG+DTaQVvxOcpNNyKv57sm2quegFzu+c86Q726npspidr8Knl7dJALXF3q8k3eQrscWrRyJEA6vh1PikrNtDwzdB92WoYkAH5dJj3MxuHPJXoX+UbOZ8jWV7dARBT2LXFiS09bJy+7P2QZSa5BVirWFzoRjAD4JlEeaW2OPcujHTe+IjHXale4fjNh0tUDtteMRcwLnfMpTl6M7qgpUXm/IrlJHfAqtPmYPQUiDCVFQ9wntTeF5ph6WM9qk/2BJni5BX44VH root@hadoop103ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDsgI3CnXcTkQkSHxvhGvBGY1Fh7gaIXqhI5n7rtEM3fmJ2k8/VoWjLwxE1xqSm7tsGufabIhpLdUJBfj4cpqa+PazgSVF59cPlYArp115YFOCGWv2z9tZ5Cq70i9EEfNCyhWNhNUEd9cqiei/G1by4yQzOwvpiuDhiGlDtQk3pRGvvdjGQV3YVDVjDv6JG3QQkxPY+oYQFjDBy+usGEvcuIFDJXmS4hlB8xDMZCw8R0gcvBEnA9RxIIb0N0jLf0uZt0varFZqlguwJJdT6nBCWJ+NYxu5aggX7PGgHB+F97gCoIRusqjT/scoS+jSK8yx4WzJ6ZfCp1wQEXJ+XgrbX root@hadoop104ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDCfFd837abQYj+trazR8y5OSOc7ET+vzn0o6pR+JF3N1knD5JGKXyB0umMsYehwcU55XkMARHRS+jothz0PAqWUpdV73pVZyy+Nxhu/5rySf+NiaoYd8IYAtcgbw174E5deudPF5Ac7IIUxJAgPxEb/NnrpYEPFLHg31ZCKUvzHkEbP9I1iMgM9LtKN3M/aAbaHY2Pd046joVhOo7DsIlYyd147X1MgFhwTeVThpu+zdjPwNoenQGIhrH1J46thBL8TyTC0Wjui5YVwim9Nxoet7E46BiuqrAS6TBvLOTAZlD/g2UB+6NbGYdQKpOs8k2XQqrEBtPB4Wo/hZ0efg8gvCs4AaOqICaMOUt52lfJHInnsycB99nxh95sLDhkHfQIlqTEvGPQDiJVU8tPS5mo7+oeqpCTsvnIINQTMlCtA6eR6kmNumCdSpwbCMkuwVxHjGgNxMy3ykBRItU0vH9mAv/5OxyfjKQutELHQn+5fb5v0lrDq/WZ60jMJYGpTh8= ceph-81b469b6-662d-11ec-b2eb-000c29c51d96 将与新群集进行通信所需的最小配置文件保存到 /etc/ceph/ceph.conf。 向 /etc/ceph/ceph.client.admin.keyring 写入 client.admin 管理 secret key 的副本（特权！）。 将 public key 的副本写入 /etc/ceph/ceph.pub。 查看当前配置文件： 12345678910[root@hadoop102 opt]# ll /etc/ceph/总用量 12-rw-------. 1 root root 63 12月 26 17:23 ceph.client.admin.keyring-rw-r--r--. 1 root root 179 12月 26 17:23 ceph.conf-rw-r--r--. 1 root root 595 12月 26 17:24 ceph.pub[root@hadoop102 opt]# cat /etc/ceph/ceph.conf # minimal ceph.conf for 81b469b6-662d-11ec-b2eb-000c29c51d96[global] fsid = 81b469b6-662d-11ec-b2eb-000c29c51d96 mon_host = [v2:192.168.10.102:3300/0,v1:192.168.10.102:6789/0] 查看拉取的镜像和启动的容器： 1234567891011121314151617181920212223242526272829# 安装之前[root@hadoop104 opt]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest feb5d9fea6a5 3 months ago 13.3kB[root@hadoop104 opt]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES72c655a613fc hello-world &quot;/hello&quot; 41 hours ago Created gallant_cohende6240f04135 hello-world &quot;/hello&quot; 5 days ago Exited (0) 5 days ago priceless_nash# 安装之后[root@hadoop102 opt]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEquay.io/ceph/ceph v15 3437f7bed968 2 months ago 1.08GB # Ceph组件hello-world latest feb5d9fea6a5 3 months ago 13.3kBquay.io/ceph/ceph-grafana 6.7.4 557c83e11646 4 months ago 486MB # Ceph组件quay.io/prometheus/prometheus v2.18.1 de242295e225 19 months ago 140MB # Ceph组件quay.io/prometheus/alertmanager v0.20.0 0881eb8f169f 2 years ago 52.1MB # Ceph组件quay.io/prometheus/node-exporter v0.18.1 e5a616e4b9cf 2 years ago 22.9MB # Ceph组件[root@hadoop102 opt]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5e62fe89c60f quay.io/ceph/ceph-grafana:6.7.4 &quot;/bin/sh -c &#x27;grafana…&quot; 45 minutes ago Up 45 minutes ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-grafana.hadoop102 # Ceph组件fcf1abbde49f quay.io/prometheus/alertmanager:v0.20.0 &quot;/bin/alertmanager -…&quot; 45 minutes ago Up 45 minutes ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-alertmanager.hadoop102 # Ceph组件ee65841914dd quay.io/prometheus/prometheus:v2.18.1 &quot;/bin/prometheus --c…&quot; 45 minutes ago Up 45 minutes ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-prometheus.hadoop102 # Ceph组件b248a474c78e quay.io/prometheus/node-exporter:v0.18.1 &quot;/bin/node_exporter …&quot; 46 minutes ago Up 46 minutes ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-node-exporter.hadoop102 # Ceph组件31dd5f2a7479 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-crash…&quot; 50 minutes ago Up 50 minutes ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-crash.hadoop102 # Ceph组件443c33b517e0 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-mgr -…&quot; 52 minutes ago Up 52 minutes ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-mgr.hadoop102.kwrjaw # Ceph组件895542296c01 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-mon -…&quot; 52 minutes ago Up 52 minutes ceph-81b469b6-662d-11ec-b2eb-000c29c51d96-mon.hadoop102 # Ceph组件7213f1a0510a hello-world &quot;/hello&quot; 27 hours ago Exited (0) 27 hours ago hungry_driscollad4e221a918b hello-world &quot;/hello&quot; 5 days ago Exited (0) 5 days ago kind_elbakyan 此时已经运行了以下组件： ceph-mgr：Ceph 管理程序。 ceph-monitor：Ceph 监视器。 ceph-crash：崩溃数据收集模块。 prometheus：prometheus 监控组件。 grafana：监控数据展示 dashboard。 alertmanager：prometheus 告警组件。 node_exporter：prometheus 节点数据收集组件。 参阅下面的一些对某些用户可能有用的选项，或者运行 cephadm bootstrap -h 命令查看所有可用选项： 为了方便起见，Bootstrap 会将访问新集群所需的文件写入 /etc/ceph，以便主机上安装的任何 Ceph 软件包（例如，访问命令行界面）都可以轻松找到它们。 但是使用 cephadm 部署的 daemon 容器根本不需要 /etc/ceph。避免与同一主机上的现有 Ceph 配置（cephadm 或其他方式）存在潜在冲突，可以使用 –output-dir 选项将它们放置在不同的目录中。 可以使用 –config 选项将任何初始 Ceph 配置选项传递到新集群，方法是将它们放在标准 ini 样式的配置文件中。 查看容器状态： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115[root@hadoop102 opt]# cephadm ls[ &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;mon.hadoop102&quot;, &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;, &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@mon.hadoop102&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;db378568f82337811eff2c7c44ad1a713d955a5b04fcde47c18674671e735704&quot;, &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;, &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;, &quot;version&quot;: &quot;15.2.15&quot;, &quot;started&quot;: &quot;2021-12-27T09:11:27.486408Z&quot;, &quot;created&quot;: &quot;2021-12-26T09:23:53.780886Z&quot;, &quot;deployed&quot;: &quot;2021-12-26T09:23:52.866923Z&quot;, &quot;configured&quot;: &quot;2021-12-26T09:30:45.494492Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;mgr.hadoop102.kwrjaw&quot;, &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;, &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@mgr.hadoop102.kwrjaw&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;32e7e6a303b2af413b90739fde7e1f3b60c26214a1a5fe8d34d36133f733ea61&quot;, &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;, &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;, &quot;version&quot;: &quot;15.2.15&quot;, &quot;started&quot;: &quot;2021-12-27T09:11:24.388823Z&quot;, &quot;created&quot;: &quot;2021-12-26T09:23:57.956720Z&quot;, &quot;deployed&quot;: &quot;2021-12-26T09:23:57.302746Z&quot;, &quot;configured&quot;: &quot;2021-12-26T09:30:46.551456Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;alertmanager.hadoop102&quot;, &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;, &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@alertmanager.hadoop102&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;843d8e781d923fa302322c78e5251ab1fa4b7b3d5a5998906ee30e089cc2e506&quot;, &quot;container_image_name&quot;: &quot;quay.io/prometheus/alertmanager:v0.20.0&quot;, &quot;container_image_id&quot;: &quot;0881eb8f169f5556a292b4e2c01d683172b12830a62a9225a98a8e206bb734f0&quot;, &quot;version&quot;: &quot;0.20.0&quot;, &quot;started&quot;: &quot;2021-12-27T09:11:30.498509Z&quot;, &quot;created&quot;: &quot;2021-12-26T09:25:51.990176Z&quot;, &quot;deployed&quot;: &quot;2021-12-26T09:25:51.485197Z&quot;, &quot;configured&quot;: &quot;2021-12-26T09:30:48.260398Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;crash.hadoop102&quot;, &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;, &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@crash.hadoop102&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;2d08d2614605c2a4350c56cf1733f85cd5916afc03a899ce65c600a437c36184&quot;, &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;, &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;, &quot;version&quot;: &quot;15.2.15&quot;, &quot;started&quot;: &quot;2021-12-27T09:11:28.253480Z&quot;, &quot;created&quot;: &quot;2021-12-26T09:25:54.341083Z&quot;, &quot;deployed&quot;: &quot;2021-12-26T09:25:53.839103Z&quot;, &quot;configured&quot;: &quot;2021-12-26T09:25:54.341083Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;grafana.hadoop102&quot;, &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;, &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@grafana.hadoop102&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;cbb4a8a923e2f3205e02e9dd026d3fdcf1a07c55a3eda6b9de13408dadc0465a&quot;, &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph-grafana:6.7.4&quot;, &quot;container_image_id&quot;: &quot;557c83e11646f123a27b5e4b62ac6c45e7bb8b2e90d6044034d0db5b7019415c&quot;, &quot;version&quot;: &quot;6.7.4&quot;, &quot;started&quot;: &quot;2021-12-27T09:11:27.676553Z&quot;, &quot;created&quot;: &quot;2021-12-26T09:29:44.175925Z&quot;, &quot;deployed&quot;: &quot;2021-12-26T09:29:42.555990Z&quot;, &quot;configured&quot;: &quot;2021-12-26T09:30:51.194299Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;node-exporter.hadoop102&quot;, &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;, &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@node-exporter.hadoop102&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;ee29b873a834252d1d55c20c67ecfbff9a3b6acf8621063efd9070a79cb1cba5&quot;, &quot;container_image_name&quot;: &quot;quay.io/prometheus/node-exporter:v0.18.1&quot;, &quot;container_image_id&quot;: &quot;e5a616e4b9cf68dfcad7782b78e118be4310022e874d52da85c55923fb615f87&quot;, &quot;version&quot;: &quot;0.18.1&quot;, &quot;started&quot;: &quot;2021-12-27T09:11:21.758252Z&quot;, &quot;created&quot;: &quot;2021-12-26T09:29:47.899777Z&quot;, &quot;deployed&quot;: &quot;2021-12-26T09:29:47.307801Z&quot;, &quot;configured&quot;: &quot;2021-12-26T09:29:47.899777Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;prometheus.hadoop102&quot;, &quot;fsid&quot;: &quot;81b469b6-662d-11ec-b2eb-000c29c51d96&quot;, &quot;systemd_unit&quot;: &quot;ceph-81b469b6-662d-11ec-b2eb-000c29c51d96@prometheus.hadoop102&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;f236295847bf07d2db2b3c79d3a209f5db369cf2a61ae7bb7d2ef7597982eb23&quot;, &quot;container_image_name&quot;: &quot;quay.io/prometheus/prometheus:v2.18.1&quot;, &quot;container_image_id&quot;: &quot;de242295e2257c37c8cadfd962369228f8f10b2d48a44259b65fef44ad4f6490&quot;, &quot;version&quot;: &quot;2.18.1&quot;, &quot;started&quot;: &quot;2021-12-27T09:11:25.545626Z&quot;, &quot;created&quot;: &quot;2021-12-26T09:30:36.125856Z&quot;, &quot;deployed&quot;: &quot;2021-12-26T09:30:35.546879Z&quot;, &quot;configured&quot;: &quot;2021-12-26T09:30:36.125856Z&quot; &#125;] 根据初始化完成的提示使用浏览器访问 dashboard： 使用虚拟机自带的火狐浏览器登陆 dashboard，初次登陆需要修改密码（admin，xisun_ceph001）： 如果上述引导集群的命令执行过程发生了异常，需要删除已经添加的配置文件，并关闭已经启动的 Ceph 组件进程，然后重新执行命令： 123456789101112131415161718192021222324252627282930# 查看已经添加的配置文件[root@hadoop102 opt]# ll /etc/ceph/总用量 12-rw-------. 1 root root 63 12月 26 17:23 ceph.client.admin.keyring-rw-r--r--. 1 root root 179 12月 26 17:23 ceph.conf-rw-r--r--. 1 root root 595 12月 26 17:24 ceph.pub# 删除已经添加的配置文件，或者重新执行命令时，添加--allow-overwrite参数[root@hadoop102 opt]# rm /etc/ceph/*# 查看已经启动的Ceph组件所占用的端口[root@hadoop102 opt]# netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 713/rpcbind tcp 0 0 0.0.0.0:6800 0.0.0.0:* LISTEN 2864/ceph-mgr # 删除tcp 0 0 0.0.0.0:6801 0.0.0.0:* LISTEN 2864/ceph-mgr # 删除tcp 0 0 192.168.122.1:53 0.0.0.0:* LISTEN 1467/dnsmasq tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1041/sshd tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN 1044/cupsd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1234/master tcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 2131/sshd: xisun@pt tcp 0 0 192.168.10.102:3300 0.0.0.0:* LISTEN 3026/ceph-mon # 删除tcp 0 0 192.168.10.102:6789 0.0.0.0:* LISTEN 3026/ceph-mon # 删除tcp6 0 0 :::111 :::* LISTEN 713/rpcbind tcp6 0 0 :::22 :::* LISTEN 1041/sshd tcp6 0 0 ::1:631 :::* LISTEN 1044/cupsd tcp6 0 0 ::1:25 :::* LISTEN 1234/master tcp6 0 0 ::1:6010 :::* LISTEN 2131/sshd: xisun@pt # 同时删除ceph-mgr和ceph-mon这两个进程[root@hadoop102 opt]# kill -9 2864 3026 启用 Ceph 命令 Cephadm 不需要在主机上安装任何 Ceph 包。但是，建议启用对 Ceph 命令的简单访问。 cephadm shell 命令在安装了所有 Ceph 包的容器中启动一个 bash shell。默认情况下，如果在主机上的 /etc/ceph 路径中找到配置和 keyring 文件，则会将它们传递到容器环境中，这样 shell 就可以完全正常工作。注意，在 MON 主机上执行时，cephadm shell 将从 MON 容器推断配置，而不是使用默认配置。如果给定 –mount，则主机（文件或目录）将显示在容器中的 /mnt 下。在 hadoop102 上执行下面的命令： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 启动一个bash shell[root@hadoop102 opt]# cephadm shellInferring fsid 81b469b6-662d-11ec-b2eb-000c29c51d96Inferring config /var/lib/ceph/81b469b6-662d-11ec-b2eb-000c29c51d96/mon.hadoop102/configUsing recent ceph image quay.io/ceph/ceph@sha256:a2c23b6942f7fbc1e15d8cfacd6655a681fe0e44f288e4a158db22030b8d58e3# 创建别名[ceph: root@hadoop102 /]# alias ceph=&#x27;cephadm shell -- ceph&#x27;# 退出[ceph: root@hadoop102 /]# exitexit# 查看集群状态，使用ceph -s 或者 ceph status命令[root@hadoop102 opt]# ceph -s cluster: id: 81b469b6-662d-11ec-b2eb-000c29c51d96 health: HEALTH_WARN Reduced data availability: 1 pg inactive OSD count 0 &lt; osd_pool_default_size 3 services: mon: 1 daemons, quorum hadoop102 (age 17m) mgr: hadoop102.kwrjaw(active, since 15m) osd: 0 osds: 0 up, 0 in data: pools: 1 pools, 1 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 100.000% pgs unknown 1 unknown [root@hadoop102 opt]# ceph status cluster: id: 81b469b6-662d-11ec-b2eb-000c29c51d96 health: HEALTH_WARN Reduced data availability: 1 pg inactive OSD count 0 &lt; osd_pool_default_size 3 services: mon: 1 daemons, quorum hadoop102 (age 17m) mgr: hadoop102.kwrjaw(active, since 15m) osd: 0 osds: 0 up, 0 in data: pools: 1 pools, 1 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 100.000% pgs unknown 1 unknown [root@hadoop102 opt]# ceph healthHEALTH_WARN Reduced data availability: 1 pg inactive; OSD count 0 &lt; osd_pool_default_size 3 在执行过程中发生了异常，解决如下： 12345678910111213141516[root@hadoop102 ceph]# ceph shell2021-12-27T14:45:22.661+0800 7faad8e1c700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2,1][errno 13] RADOS permission denied (error connecting to the cluster)# 查看所有的集群数据文件夹[root@hadoop102 ceph]# ll /var/lib/ceph总用量 0drwx------. 5 ceph ceph 68 12月 26 16:42 6f08de88-6627-11ec-ad8e-000c29c51d96drwx------. 10 libstoragemgmt cgred 205 12月 26 17:30 81b469b6-662d-11ec-b2eb-000c29c51d96# 查看配置中的集群数据文件夹[root@hadoop102 ceph]# cat /etc/ceph/ceph.conf # minimal ceph.conf for 81b469b6-662d-11ec-b2eb-000c29c51d96[global] fsid = 81b469b6-662d-11ec-b2eb-000c29c51d96 mon_host = [v2:192.168.10.102:3300/0,v1:192.168.10.102:6789/0]# 删除旧的集群数据文件夹，然后重新执行命令[root@hadoop102 ceph]# rm -rf /var/lib/ceph/6f08de88-6627-11ec-ad8e-000c29c51d96/ 添加新主机到集群中 第一步，在 hadoop102 上执行命令，将集群的公共 SSH 密钥添加到新主机的根用户 authorized_keys 文件中： 1234567891011121314151617181920[root@hadoop102 opt]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@hadoop103/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;root@hadoop103&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@hadoop102 opt]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@hadoop104/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;root@hadoop104&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@hadoop102 opt]# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkEXI7QySiz2VlLNrYW7OypfS9ZtMXS2h725W+pTpv71e4z6YjjxIpt6DvggtuD/UhktL1HxIvAREYKQ5vNYhGuV9jHI4VU+y4evpPEMR8GJ8OTlhfwhwovsik4TfYW0SmHUw01bxBpH2BjX4u/dSpyN9KG9710WXTIGJQ6zbHjxPg0JQX4C1KLuaD1WMVo1z6xscRGs7RAZ7sYAIQdf8LOI4IYVzO1n66NJNY4vpAJx1wCLajoGN0689Do42jX6t0/Qn6RGddXermTbbPW+hcD20pRIHO4f9VCQwqSNdcEVDUS74VMJRuOxd+N0uAxvHz8lTdCSfUnfPeKEvEH+cr root@hadoop102ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9lRXTDUhUKdfu5PznYhoFsJFw1n6qXKJXCgFmuFCBB3T75na5xSgGbUEcQ5HYIWk8ywjynGjzCQMlyG+DTaQVvxOcpNNyKv57sm2quegFzu+c86Q726npspidr8Knl7dJALXF3q8k3eQrscWrRyJEA6vh1PikrNtDwzdB92WoYkAH5dJj3MxuHPJXoX+UbOZ8jWV7dARBT2LXFiS09bJy+7P2QZSa5BVirWFzoRjAD4JlEeaW2OPcujHTe+IjHXale4fjNh0tUDtteMRcwLnfMpTl6M7qgpUXm/IrlJHfAqtPmYPQUiDCVFQ9wntTeF5ph6WM9qk/2BJni5BX44VH root@hadoop103ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDsgI3CnXcTkQkSHxvhGvBGY1Fh7gaIXqhI5n7rtEM3fmJ2k8/VoWjLwxE1xqSm7tsGufabIhpLdUJBfj4cpqa+PazgSVF59cPlYArp115YFOCGWv2z9tZ5Cq70i9EEfNCyhWNhNUEd9cqiei/G1by4yQzOwvpiuDhiGlDtQk3pRGvvdjGQV3YVDVjDv6JG3QQkxPY+oYQFjDBy+usGEvcuIFDJXmS4hlB8xDMZCw8R0gcvBEnA9RxIIb0N0jLf0uZt0varFZqlguwJJdT6nBCWJ+NYxu5aggX7PGgHB+F97gCoIRusqjT/scoS+jSK8yx4WzJ6ZfCp1wQEXJ+XgrbX root@hadoop104ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDCfFd837abQYj+trazR8y5OSOc7ET+vzn0o6pR+JF3N1knD5JGKXyB0umMsYehwcU55XkMARHRS+jothz0PAqWUpdV73pVZyy+Nxhu/5rySf+NiaoYd8IYAtcgbw174E5deudPF5Ac7IIUxJAgPxEb/NnrpYEPFLHg31ZCKUvzHkEbP9I1iMgM9LtKN3M/aAbaHY2Pd046joVhOo7DsIlYyd147X1MgFhwTeVThpu+zdjPwNoenQGIhrH1J46thBL8TyTC0Wjui5YVwim9Nxoet7E46BiuqrAS6TBvLOTAZlD/g2UB+6NbGYdQKpOs8k2XQqrEBtPB4Wo/hZ0efg8gvCs4AaOqICaMOUt52lfJHInnsycB99nxh95sLDhkHfQIlqTEvGPQDiJVU8tPS5mo7+oeqpCTsvnIINQTMlCtA6eR6kmNumCdSpwbCMkuwVxHjGgNxMy3ykBRItU0vH9mAv/5OxyfjKQutELHQn+5fb5v0lrDq/WZ60jMJYGpTh8= ceph-81b469b6-662d-11ec-b2eb-000c29c51d96 在新结点上查看密钥是否添加成功： 12345[root@hadoop103 opt]# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkEXI7QySiz2VlLNrYW7OypfS9ZtMXS2h725W+pTpv71e4z6YjjxIpt6DvggtuD/UhktL1HxIvAREYKQ5vNYhGuV9jHI4VU+y4evpPEMR8GJ8OTlhfwhwovsik4TfYW0SmHUw01bxBpH2BjX4u/dSpyN9KG9710WXTIGJQ6zbHjxPg0JQX4C1KLuaD1WMVo1z6xscRGs7RAZ7sYAIQdf8LOI4IYVzO1n66NJNY4vpAJx1wCLajoGN0689Do42jX6t0/Qn6RGddXermTbbPW+hcD20pRIHO4f9VCQwqSNdcEVDUS74VMJRuOxd+N0uAxvHz8lTdCSfUnfPeKEvEH+cr root@hadoop102ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9lRXTDUhUKdfu5PznYhoFsJFw1n6qXKJXCgFmuFCBB3T75na5xSgGbUEcQ5HYIWk8ywjynGjzCQMlyG+DTaQVvxOcpNNyKv57sm2quegFzu+c86Q726npspidr8Knl7dJALXF3q8k3eQrscWrRyJEA6vh1PikrNtDwzdB92WoYkAH5dJj3MxuHPJXoX+UbOZ8jWV7dARBT2LXFiS09bJy+7P2QZSa5BVirWFzoRjAD4JlEeaW2OPcujHTe+IjHXale4fjNh0tUDtteMRcwLnfMpTl6M7qgpUXm/IrlJHfAqtPmYPQUiDCVFQ9wntTeF5ph6WM9qk/2BJni5BX44VH root@hadoop103ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDsgI3CnXcTkQkSHxvhGvBGY1Fh7gaIXqhI5n7rtEM3fmJ2k8/VoWjLwxE1xqSm7tsGufabIhpLdUJBfj4cpqa+PazgSVF59cPlYArp115YFOCGWv2z9tZ5Cq70i9EEfNCyhWNhNUEd9cqiei/G1by4yQzOwvpiuDhiGlDtQk3pRGvvdjGQV3YVDVjDv6JG3QQkxPY+oYQFjDBy+usGEvcuIFDJXmS4hlB8xDMZCw8R0gcvBEnA9RxIIb0N0jLf0uZt0varFZqlguwJJdT6nBCWJ+NYxu5aggX7PGgHB+F97gCoIRusqjT/scoS+jSK8yx4WzJ6ZfCp1wQEXJ+XgrbX root@hadoop104ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDCfFd837abQYj+trazR8y5OSOc7ET+vzn0o6pR+JF3N1knD5JGKXyB0umMsYehwcU55XkMARHRS+jothz0PAqWUpdV73pVZyy+Nxhu/5rySf+NiaoYd8IYAtcgbw174E5deudPF5Ac7IIUxJAgPxEb/NnrpYEPFLHg31ZCKUvzHkEbP9I1iMgM9LtKN3M/aAbaHY2Pd046joVhOo7DsIlYyd147X1MgFhwTeVThpu+zdjPwNoenQGIhrH1J46thBL8TyTC0Wjui5YVwim9Nxoet7E46BiuqrAS6TBvLOTAZlD/g2UB+6NbGYdQKpOs8k2XQqrEBtPB4Wo/hZ0efg8gvCs4AaOqICaMOUt52lfJHInnsycB99nxh95sLDhkHfQIlqTEvGPQDiJVU8tPS5mo7+oeqpCTsvnIINQTMlCtA6eR6kmNumCdSpwbCMkuwVxHjGgNxMy3ykBRItU0vH9mAv/5OxyfjKQutELHQn+5fb5v0lrDq/WZ60jMJYGpTh8= ceph-81b469b6-662d-11ec-b2eb-000c29c51d96 # 公共SSH密钥 12345[root@hadoop104 opt]# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkEXI7QySiz2VlLNrYW7OypfS9ZtMXS2h725W+pTpv71e4z6YjjxIpt6DvggtuD/UhktL1HxIvAREYKQ5vNYhGuV9jHI4VU+y4evpPEMR8GJ8OTlhfwhwovsik4TfYW0SmHUw01bxBpH2BjX4u/dSpyN9KG9710WXTIGJQ6zbHjxPg0JQX4C1KLuaD1WMVo1z6xscRGs7RAZ7sYAIQdf8LOI4IYVzO1n66NJNY4vpAJx1wCLajoGN0689Do42jX6t0/Qn6RGddXermTbbPW+hcD20pRIHO4f9VCQwqSNdcEVDUS74VMJRuOxd+N0uAxvHz8lTdCSfUnfPeKEvEH+cr root@hadoop102ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9lRXTDUhUKdfu5PznYhoFsJFw1n6qXKJXCgFmuFCBB3T75na5xSgGbUEcQ5HYIWk8ywjynGjzCQMlyG+DTaQVvxOcpNNyKv57sm2quegFzu+c86Q726npspidr8Knl7dJALXF3q8k3eQrscWrRyJEA6vh1PikrNtDwzdB92WoYkAH5dJj3MxuHPJXoX+UbOZ8jWV7dARBT2LXFiS09bJy+7P2QZSa5BVirWFzoRjAD4JlEeaW2OPcujHTe+IjHXale4fjNh0tUDtteMRcwLnfMpTl6M7qgpUXm/IrlJHfAqtPmYPQUiDCVFQ9wntTeF5ph6WM9qk/2BJni5BX44VH root@hadoop103ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDsgI3CnXcTkQkSHxvhGvBGY1Fh7gaIXqhI5n7rtEM3fmJ2k8/VoWjLwxE1xqSm7tsGufabIhpLdUJBfj4cpqa+PazgSVF59cPlYArp115YFOCGWv2z9tZ5Cq70i9EEfNCyhWNhNUEd9cqiei/G1by4yQzOwvpiuDhiGlDtQk3pRGvvdjGQV3YVDVjDv6JG3QQkxPY+oYQFjDBy+usGEvcuIFDJXmS4hlB8xDMZCw8R0gcvBEnA9RxIIb0N0jLf0uZt0varFZqlguwJJdT6nBCWJ+NYxu5aggX7PGgHB+F97gCoIRusqjT/scoS+jSK8yx4WzJ6ZfCp1wQEXJ+XgrbX root@hadoop104ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDCfFd837abQYj+trazR8y5OSOc7ET+vzn0o6pR+JF3N1knD5JGKXyB0umMsYehwcU55XkMARHRS+jothz0PAqWUpdV73pVZyy+Nxhu/5rySf+NiaoYd8IYAtcgbw174E5deudPF5Ac7IIUxJAgPxEb/NnrpYEPFLHg31ZCKUvzHkEbP9I1iMgM9LtKN3M/aAbaHY2Pd046joVhOo7DsIlYyd147X1MgFhwTeVThpu+zdjPwNoenQGIhrH1J46thBL8TyTC0Wjui5YVwim9Nxoet7E46BiuqrAS6TBvLOTAZlD/g2UB+6NbGYdQKpOs8k2XQqrEBtPB4Wo/hZ0efg8gvCs4AaOqICaMOUt52lfJHInnsycB99nxh95sLDhkHfQIlqTEvGPQDiJVU8tPS5mo7+oeqpCTsvnIINQTMlCtA6eR6kmNumCdSpwbCMkuwVxHjGgNxMy3ykBRItU0vH9mAv/5OxyfjKQutELHQn+5fb5v0lrDq/WZ60jMJYGpTh8= ceph-81b469b6-662d-11ec-b2eb-000c29c51d96 # 公共SSH密钥 第二步，告诉 Ceph，新节点是集群的一部分： 1234567891011121314151617181920212223242526272829303132333435363738394041 ``` - s## Ubuntu 安装 Ceph### 节点规划- 各节点规划： | 主机名 | ip | 磁盘 | 角色 | | ------ | ------------ | --------------------------- | ----------------------------------------- | | ceph1 | 192.168.1.91 | 系统盘：sda&lt;br/&gt;osd盘：sdb | cephadm，monitor，mgr，rgw，mds，osd，nfs | | ceph2 | 192.168.1.92 | 系统盘：sda&lt;br/&gt;osd盘：sdb | monitor，mgr，rgw，mds，osd，nfs | | ceph3 | 192.168.1.93 | 系统盘：sda&lt;br/&gt;osd盘：sdb | monitor，mgr，rgw，mds，osd，nfs | | ceph4 | 192.168.1.94 | 系统盘：sda&lt;br /&gt;osd盘：sdb | monitor，mgr，rgw，mds，osd，nfs |- 各节点版本： ```shell # 内核版本 root@ceph1:/home# uname -r 5.4.0-91-generic # Ubuntu版本，方法一 root@ceph1:/home# cat /etc/issue Ubuntu 20.04.3 LTS \\n \\l # Ubuntu版本，方法二，查看所有信息 root@ceph1:/home# lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 20.04.3 LTS Release: 20.04 Codename: focal # Ubuntu版本代号 root@ceph4:/home# lsb_release -c Codename: focal 各节点配置 Ubuntu 20.04 apt 阿里云镜像源： 1234567891011121314151617181920root@ceph1:/home# ll /etc/apt/total 40drwxr-xr-x 7 root root 4096 Dec 29 15:23 ./drwxr-xr-x 105 root root 4096 Dec 29 10:52 ../drwxr-xr-x 2 root root 4096 Aug 24 16:47 apt.conf.d/drwxr-xr-x 2 root root 4096 Apr 9 2020 auth.conf.d/drwxr-xr-x 2 root root 4096 Dec 17 17:04 preferences.d/-rw-r--r-- 1 root root 2777 Dec 8 10:29 sources.list-rw-r--r-- 1 root root 2743 Aug 24 16:47 sources.list.curtin.olddrwxr-xr-x 2 root root 4096 Dec 17 17:04 sources.list.d/-rw-r--r-- 1 root root 1143 Dec 8 15:43 trusted.gpgdrwxr-xr-x 2 root root 4096 Dec 17 17:04 trusted.gpg.d/# 备份默认的源root@ceph1:/home# cp /etc/apt/sources.list /etc/apt/sources.list.bak# 替换阿里云镜像源，将默认源cn.archive.ubuntu.com替换成mirrors.aliyun.com（先验证下默认apt源是不是cn.archive.ubuntu.com）root@ceph1:/home# sed -i &quot;s/cn.archive.ubuntu.com/mirrors.aliyun.com/g&quot; /etc/apt/sources.list# 更新软件包列表root@ceph1:/home# apt update# 更新已安装的软件包root@ceph1:/home# apt upgrade 说明：生产环境不要随意使用 apt update 和 apt upgrade 命令，因为生产环境下可能要求使用特定的软件版本，不要轻易的更新。 各节点配置主机名： 123root@ceph1:/home# vim /etc/hostname root@ceph1:/home# cat /etc/hostname ceph1 各节点配置 host 解析： 12345678910111213141516root@ceph1:/home# vim /etc/hostsroot@ceph1:/home# cat /etc/hosts127.0.0.1 localhost127.0.0.1 ceph1# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters192.168.1.91 ceph1192.168.1.92 ceph2192.168.1.93 ceph3192.168.1.94 ceph4 各节点网络信息： 12345678910111213141516171819202122232425262728293031323334353637383940# 配置文件root@ceph1:/opt# cat /etc/netplan/00-installer-config.yaml # This is the network config written by &#x27;subiquity&#x27;network: ethernets: ens160: dhcp4: false addresses: - 192.168.1.91/24 gateway4: 192.168.1.1 nameservers: addresses: [114.114.114.114, 8.8.8.8] version: 2# 网络信息root@ceph1:/opt# ifconfigdocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:31:45:9c:e4 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.1.91 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fe80::250:56ff:fe9e:7c0f prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:9e:7c:0f txqueuelen 1000 (Ethernet) RX packets 1493933 bytes 708616888 (708.6 MB) RX errors 0 dropped 62 overruns 0 frame 0 TX packets 215266 bytes 15560179 (15.5 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 400 bytes 38180 (38.1 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 400 bytes 38180 (38.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 12345678910111213141516171819202122232425262728293031323334353637383940# 配置文件root@ceph2:/opt# cat /etc/netplan/00-installer-config.yaml # This is the network config written by &#x27;subiquity&#x27;network: ethernets: ens160: dhcp4: false addresses: - 192.168.1.92/24 gateway4: 192.168.1.1 nameservers: addresses: [114.114.114.114, 8.8.8.8] version: 2# 网络信息root@ceph2:/opt# ifconfig docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:07:d8:d7:04 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.1.92 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fe80::250:56ff:fe9e:e4e8 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:9e:e4:e8 txqueuelen 1000 (Ethernet) RX packets 1474736 bytes 708635562 (708.6 MB) RX errors 0 dropped 1 overruns 0 frame 0 TX packets 219015 bytes 16146130 (16.1 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 588 bytes 56542 (56.5 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 588 bytes 56542 (56.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 12345678910111213141516171819202122232425262728293031323334353637383940# 配置文件root@ceph3:/opt# cat /etc/netplan/00-installer-config.yaml # This is the network config written by &#x27;subiquity&#x27;network: ethernets: ens160: dhcp4: false addresses: - 192.168.1.93/24 gateway4: 192.168.1.1 nameservers: addresses: [114.114.114.114, 8.8.8.8] version: 2# 网络信息root@ceph3:/opt# ifconfigdocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:e2:11:a5:b9 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.1.93 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fe80::250:56ff:fe9e:7d03 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:9e:7d:03 txqueuelen 1000 (Ethernet) RX packets 1490677 bytes 706047066 (706.0 MB) RX errors 0 dropped 46 overruns 0 frame 0 TX packets 208014 bytes 14700697 (14.7 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 620 bytes 60174 (60.1 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 620 bytes 60174 (60.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 1234567891011121314151617181920212223242526272829303132333435363738394041# 配置文件root@ceph4:/opt# cat /etc/netplan/00-installer-config.yaml # This is the network config written by &#x27;subiquity&#x27;network: ethernets: ens160: dhcp4: false addresses: - 192.168.1.94/24 gateway4: 192.168.1.1 nameservers: addresses: [114.114.114.114, 8.8.8.8] version: 2# 网络信息root@ceph4:/opt# ifconfigdocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fe80::42:78ff:febe:2005 prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:42:78:be:20:05 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 196 (196.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.1.94 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fe80::250:56ff:fe9e:7855 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:50:56:9e:78:55 txqueuelen 1000 (Ethernet) RX packets 1572297 bytes 814622359 (814.6 MB) RX errors 0 dropped 50 overruns 0 frame 0 TX packets 250469 bytes 17903836 (17.9 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 826 bytes 84430 (84.4 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 826 bytes 84430 (84.4 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Ubuntu 20.04 修改 IP 地址的方法参考：https://blog.csdn.net/qq_38505969/article/details/110501609 关闭防火墙 各节点关闭防火墙： 1234567891011121314151617181920# 安装ufwroot@ceph1:/opt# apt install -y ufw# 防火墙版本root@ceph1:/home# ufw versionufw 0.36Copyright 2008-2015 Canonical Ltd.# 开启防火墙root@ceph1:/home# ufw enableCommand may disrupt existing ssh connections. Proceed with operation (y|n)? yFirewall is active and enabled on system startup# 查看防火墙状态root@ceph1:/home# ufw statusStatus: active# 关闭防火墙root@ceph1:/home# ufw disableFirewall stopped and disabled on system startuproot@ceph1:/home# ufw statusStatus: inactive# 重启防火墙root@ceph1:/home# ufw reload SSH 免密登录 各节点执行如下命令： 12345678910111213root@ceph1:/home# sed -i &#x27;/PermitRootLogin/d&#x27; /etc/ssh/sshd_configroot@ceph1:/home# echo &quot;PermitRootLogin yes&quot; &gt;&gt; /etc/ssh/sshd_configroot@ceph1:/home# service sshd reloadroot@ceph1:/home# ssh-keygenroot@ceph1:/home# ssh-copy-id -o StrictHostKeyChecking=no root@ceph1root@ceph1:/home# ssh-copy-id -o StrictHostKeyChecking=no root@ceph2root@ceph1:/home# ssh-copy-id -o StrictHostKeyChecking=no root@ceph3root@ceph1:/home# ssh-copy-id -o StrictHostKeyChecking=no root@ceph4# 测试是否能免密登录其他节点root@ceph1:/home# ssh ceph1root@ceph1:/home# ssh ceph2root@ceph1:/home# ssh ceph3root@ceph1:/home# ssh ceph4 ceph2，ceph3 和 ceph4 参考 ceph1 配置。 服务器时间同步 NTP 是通过网络来同步时间的一种 TCP/IP 协议。通常客户端向服务器请求当前的时间，并根据结果来设置其时钟。这个描述是挺简单的，实现这一功能却是极为复杂的：首先要有多层 NTP 服务器，第一层 NTP 服务器连接原子时钟，第二层、第三层服务器则担起负载均衡的责任，以处理因特网传来的所有请求。另外，客户端可能也超乎你想象的复杂：它必须排除通讯延迟，调整时间的同时不干扰其它在服务器中运行的进程。幸运的是，所有的这些复杂性都进行了封装，你是不可见也不需要见到的。 在 Ubuntu 中，可以使用 ntpdate 和 ntpd 来同步时间。而在最新的 Ubuntu 版本中，timedatectl 替代了老旧的 ntpdate。默认情况下，timedatectl 在系统启动的时候会立刻同步时间，并在稍后网络连接激活后通过 Socket 再次检查一次。 各节点查看时间： 12root@ceph1:/home# dateThu 30 Dec 2021 10:32:19 AM CST 各节点设置时间同步服务器： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 查看时间同步服务器状态root@ceph1:/opt# systemctl status systemd-timesyncd.service● systemd-timesyncd.service - Network Time Synchronization Loaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-12-30 11:02:44 CST; 36min ago Docs: man:systemd-timesyncd.service(8) Main PID: 719 (systemd-timesyn) Status: &quot;Initial synchronization to time server 91.189.91.157:123 (ntp.ubuntu.com).&quot; # 默认ntp.ubuntu.com Tasks: 2 (limit: 19110) Memory: 1.7M CGroup: /system.slice/systemd-timesyncd.service └─719 /lib/systemd/systemd-timesyncd# 添加阿里云NTP服务器地址root@ceph1:/opt# vim /etc/systemd/timesyncd.confroot@ceph1:/opt# cat /etc/systemd/timesyncd.conf # This file is part of systemd.## systemd is free software; you can redistribute it and/or modify it# under the terms of the GNU Lesser General Public License as published by# the Free Software Foundation; either version 2.1 of the License, or# (at your option) any later version.## Entries in this file show the compile time defaults.# You can change settings by editing this file.# Defaults can be restored by simply deleting this file.## See timesyncd.conf(5) for details.[Time]#NTP=#FallbackNTP=ntp.ubuntu.com#RootDistanceMaxSec=5#PollIntervalMinSec=32#PollIntervalMaxSec=2048NTP=ntp1.aliyun.com# 重启时间同步服务root@ceph1:/opt# systemctl restart systemd-timesyncd.service# 查询时间同步服务状态，确认服务是否正常启动，是否从指定的NTP服务器上进行校时root@ceph1:/opt# systemctl status systemd-timesyncd.service● systemd-timesyncd.service - Network Time Synchronization Loaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-12-30 11:50:14 CST; 14s ago Docs: man:systemd-timesyncd.service(8) Main PID: 3966 (systemd-timesyn) Status: &quot;Initial synchronization to time server 120.25.115.20:123 (ntp1.aliyun.com).&quot; # 修改为阿里云服务器 Tasks: 2 (limit: 19110) Memory: 1.4M CGroup: /system.slice/systemd-timesyncd.service └─3966 /lib/systemd/systemd-timesyncdDec 30 11:50:14 ceph2 systemd[1]: Starting Network Time Synchronization...Dec 30 11:50:14 ceph2 systemd[1]: Started Network Time Synchronization.Dec 30 11:50:14 ceph2 systemd-timesyncd[3966]: Initial synchronization to time server 120.25.115.20:123 (ntp1.aliyun.com). 系统默认同步的 NTP 服务器为 ntp.ubuntu.com。 添加一个阿里云 NTP 服务器地址，如果需要添加多个 NTP 服务器地址，则中间用空格隔开： 各节点设置时区： 12345678# 设置东八区root@ceph1:/home# timedatectl set-timezone Asia/Shanghai# 查看时区root@ceph1:/home# cat /etc/timezone Asia/Shanghairoot@ceph1:/home# timedatectl status | grep &#x27;Time zone&#x27; Time zone: Asia/Shanghai (CST, +0800) 各节点查看时钟是否与互联网同步： 12345678root@ceph1:/home# timedatectl Local time: Thu 2021-12-30 10:33:17 CST # 本地时间 Universal time: Thu 2021-12-30 02:33:17 UTC # 协调世界时 RTC time: Thu 2021-12-30 02:33:17 # 硬件时间 Time zone: Asia/Shanghai (CST, +0800) # 时区System clock synchronized: yes # 如果和远程NTP服务器成功同步，则显示为yes NTP service: active # NTP时间同步是否开启，systemd-timesyncd服务活跃即开启了NTP时间同步 RTC in local TZ: no # no表示硬件时钟设置为协调世界时（UTC），yes表示硬件时钟设置为本地时间 timedatectl 命令会显示本地时间、世界时、时区、系统时钟是否与互联网服务器同步，以及 systemd-timesyncd.service 是处于活跃状态还是非活跃状态。 安装 Python3 各节点查看 Python3 版本，服务器已安装 Python3，此处省略安装步骤： 12root@ceph1:/opt# python3 --versionPython 3.8.10 安装 Docker 各节点卸载旧版本： 12345# 卸载旧版本root@ceph1:/opt# apt remove docker docker-engine docker.io containerd runc# 删除旧版本数据root@ceph1:/opt# rm -rf /var/lib/docker/root@ceph1:/opt# rm -rf /var/lib/containerd/ 各节点安装需要的软件包： 1root@ceph1:/opt# apt install ca-certificates curl gnupg lsb-release 各节点添加 Docker 的官方 GPG key： 123root@ceph1:/opt# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -OKW: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype. 各节点添加 apt 的 Docker 源： 1234567891011121314151617root@ceph1:/opt# add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;Hit:1 http://mirrors.aliyun.com/ubuntu focal InReleaseHit:2 http://mirrors.aliyun.com/ubuntu focal-updates InRelease Hit:3 http://mirrors.aliyun.com/ubuntu focal-backports InRelease Hit:4 http://mirrors.aliyun.com/ubuntu focal-security InRelease Get:5 https://download.docker.com/linux/ubuntu focal InRelease [57.7 kB] Get:6 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages [13.5 kB] Get:7 https://download.ceph.com/debian-octopus focal InRelease [8,571 B]Get:8 https://download.ceph.com/debian-octopus focal/main amd64 Packages [15.9 kB]Fetched 95.7 kB in 2s (50.4 kB/s) Reading package lists... DoneW: http://mirrors.aliyun.com/ubuntu/dists/focal/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.W: http://mirrors.aliyun.com/ubuntu/dists/focal-updates/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.W: http://mirrors.aliyun.com/ubuntu/dists/focal-backports/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.W: http://mirrors.aliyun.com/ubuntu/dists/focal-security/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.W: https://download.docker.com/linux/ubuntu/dists/focal/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype.W: https://download.ceph.com/debian-octopus/dists/focal/InRelease: The key(s) in the keyring /etc/apt/trusted.gpg.d/ceph.release.gpg are ignored as the file has an unsupported filetype. 如果提示 bash: add-apt-repository: command not found，执行下面命令安装： 12root@ceph1:/opt# apt install -y software-properties-commonroot@ceph1:/opt# apt update 各节点安装最新版本： 1root@ceph1:/opt# apt install docker-ce docker-ce-cli containerd.io Docker 安装完成后会自动启动： 123456789101112131415# 查看docker状态root@ceph1:/opt# systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2021-12-30 17:28:18 CST; 34s agoTriggeredBy: ● docker.socket Docs: https://docs.docker.com Main PID: 24185 (dockerd) Tasks: 11 Memory: 29.1M CGroup: /system.slice/docker.service └─24185 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock# 查看docker版本root@ceph1:/opt# docker --versionDocker version 20.10.12, build e91ed57 各节点测试： 12345678910111213141516171819202122232425262728293031323334root@ceph1:/opt# docker run hello-worldUnable to find image &#x27;hello-world:latest&#x27; locallylatest: Pulling from library/hello-world2db29710123e: Pull complete Digest: sha256:2498fce14358aa50ead0cc6c19990fc6ff866ce72aeb5546e1d59caac3d0d60fStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/root@ceph1:/opt# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest feb5d9fea6a5 3 months ago 13.3kBroot@ceph1:/opt# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESacf9377ed00d hello-world &quot;/hello&quot; About a minute ago Exited (0) About a minute ago ecstatic_gould 参考：https://docs.docker.com/engine/install/ubuntu/ 安装 Cephadm 各节点使用 apt 安装 Cephadm： 123456789101112131415161718192021# 安装root@ceph1:/opt# apt install -y cephadmReading package lists... DoneBuilding dependency tree Reading state information... DoneRecommended packages: podman | docker.ioThe following NEW packages will be installed: cephadm0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.Need to get 53.2 kB of archives.After this operation, 245 kB of additional disk space will be used.Get:1 https://download.ceph.com/debian-octopus focal/main amd64 cephadm amd64 15.2.15-1focal [53.2 kB]Fetched 53.2 kB in 2s (31.4 kB/s) Selecting previously unselected package cephadm.(Reading database ... 115380 files and directories currently installed.)Preparing to unpack .../cephadm_15.2.15-1focal_amd64.deb ...Unpacking cephadm (15.2.15-1focal) ...Setting up cephadm (15.2.15-1focal) ...Adding system user cephadm....doneProcessing triggers for man-db (2.9.1-1) ... 各节点验证 Cephadm 安装成功： 1234root@ceph1:/opt# which cephadm/usr/sbin/cephadmroot@ceph1:/opt# cephadm versionceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable) 各节点安装 ceph-common 工具： 12345678910# 添加源root@ceph1:/opt# cephadm add-repo --release octopusInstalling repo GPG key from https://download.ceph.com/keys/release.asc...Installing repo file at /etc/apt/sources.list.d/ceph.list...# 安装root@ceph1:/opt# cephadm install ceph-commonInstalling packages [&#x27;ceph-common&#x27;]...# 验证root@ceph1:/opt# ceph -vceph version 15.2.15 (2dfb18841cfecc2f7eb7eb2afd65986ca4d95985) octopus (stable) 创建 Ceph 新集群 在 ceph1 上创建一个可以被任何访问 Ceph 集群的主机访问的网络，指定 mon-ip，并将生成的配置文件写进 /etc/ceph 目录里： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081root@ceph1:/opt# cephadm bootstrap --mon-ip 192.168.1.91Verifying podman|docker is present...Verifying lvm2 is present...Verifying time synchronization is in place...Unit systemd-timesyncd.service is enabled and runningRepeating the final host check...podman|docker (/usr/bin/docker) is presentsystemctl is presentlvcreate is presentUnit systemd-timesyncd.service is enabled and runningHost looks OKCluster fsid: 79a6cb92-6f92-11ec-b1e0-6f6f27397286Verifying IP 192.168.1.91 port 3300 ...Verifying IP 192.168.1.91 port 6789 ...Mon IP 192.168.1.91 is in CIDR network 192.168.1.0/24Pulling container image quay.io/ceph/ceph:v15...Extracting ceph user uid/gid from container image...Creating initial keys...Creating initial monmap...Creating mon...Waiting for mon to start...Waiting for mon...mon is availableAssimilating anything we can from ceph.conf...Generating new minimal ceph.conf...Restarting the monitor...Setting mon public_network...Creating mgr...Verifying port 9283 ...Wrote keyring to /etc/ceph/ceph.client.admin.keyringWrote config to /etc/ceph/ceph.confWaiting for mgr to start...Waiting for mgr...mgr not available, waiting (1/10)...mgr not available, waiting (2/10)...mgr not available, waiting (3/10)...mgr not available, waiting (4/10)...mgr is availableEnabling cephadm module...Waiting for the mgr to restart...Waiting for Mgr epoch 5...Mgr epoch 5 is availableSetting orchestrator backend to cephadm...Generating ssh key...Wrote public SSH key to to /etc/ceph/ceph.pubAdding key to root@localhost&#x27;s authorized_keys...Adding host ceph1...Deploying mon service with default placement...Deploying mgr service with default placement...Deploying crash service with default placement...Enabling mgr prometheus module...Deploying prometheus service with default placement...Deploying grafana service with default placement...Deploying node-exporter service with default placement...Deploying alertmanager service with default placement...Enabling the dashboard module...Waiting for the mgr to restart...Waiting for Mgr epoch 13...Mgr epoch 13 is availableGenerating a dashboard self-signed certificate...Creating initial admin user...Fetching dashboard port number...Ceph Dashboard is now available at: URL: https://localhost:8443/ User: admin Password: vzmh8b8mp6You can access the Ceph CLI with: sudo /usr/sbin/cephadm shell --fsid 79a6cb92-6f92-11ec-b1e0-6f6f27397286 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyringPlease consider enabling telemetry to help improve Ceph: ceph telemetry onFor more information see: https://docs.ceph.com/docs/master/mgr/telemetry/Bootstrap complete. 该命令执行如下操作： 在本地主机上为新集群创建 monitor 和 manager daemon 守护程序。 为 Ceph 集群生成一个新的公共 SSH 密钥，并将其添加到 root 用户的 /root/.ssh/authorized_keys 文件中。 123456root@ceph1:/opt# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1 # ceph1免密登录ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2 # ceph1免密登录ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3 # ceph3免密登录ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4 # ceph4免密登录ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286 # 公共SSH密钥 将与新群集进行通信所需的最小配置文件保存到 /etc/ceph/ceph.conf。 向 /etc/ceph/ceph.client.admin.keyring 写入 client.admin 管理 secret key 的副本（特权！）。 将 public key 的副本写入 /etc/ceph/ceph.pub。 查看当前配置文件： 12345678910111213root@ceph1:/opt# ll /etc/ceph/total 24drwxr-xr-x 2 root root 4096 Jan 7 16:19 ./drwxr-xr-x 104 root root 4096 Jan 7 16:02 ../-rw------- 1 root root 63 Jan 7 16:19 ceph.client.admin.keyring-rw-r--r-- 1 root root 175 Jan 7 16:19 ceph.conf-rw-r--r-- 1 root root 595 Jan 7 16:19 ceph.pub-rw-r--r-- 1 root root 92 Oct 20 22:31 rbdmaproot@ceph1:/opt# cat /etc/ceph/ceph.conf # minimal ceph.conf for 79a6cb92-6f92-11ec-b1e0-6f6f27397286[global] fsid = 79a6cb92-6f92-11ec-b1e0-6f6f27397286 mon_host = [v2:192.168.1.91:3300/0,v1:192.168.1.91:6789/0] 查看当前拉取的镜像和启动的容器： 123456789101112131415161718root@ceph1:/opt# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEquay.io/ceph/ceph v15 3437f7bed968 2 months ago 1.08GBhello-world latest feb5d9fea6a5 3 months ago 13.3kBquay.io/ceph/ceph-grafana 6.7.4 557c83e11646 5 months ago 486MBquay.io/prometheus/prometheus v2.18.1 de242295e225 20 months ago 140MBquay.io/prometheus/alertmanager v0.20.0 0881eb8f169f 2 years ago 52.1MBquay.io/prometheus/node-exporter v0.18.1 e5a616e4b9cf 2 years ago 22.9MBroot@ceph1:/opt# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1fdde1918435 quay.io/prometheus/alertmanager:v0.20.0 &quot;/bin/alertmanager -…&quot; 47 minutes ago Up 47 minutes ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-alertmanager.ceph15fc44d4adaa8 quay.io/ceph/ceph-grafana:6.7.4 &quot;/bin/sh -c &#x27;grafana…&quot; 47 minutes ago Up 47 minutes ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-grafana.ceph185cac2612c10 quay.io/prometheus/prometheus:v2.18.1 &quot;/bin/prometheus --c…&quot; 47 minutes ago Up 47 minutes ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-prometheus.ceph1d1aa75b76a39 quay.io/prometheus/node-exporter:v0.18.1 &quot;/bin/node_exporter …&quot; 48 minutes ago Up 48 minutes ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-node-exporter.ceph1c20775682c08 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-crash…&quot; 48 minutes ago Up 48 minutes ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-crash.ceph17c3cf4e445d1 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-mgr -…&quot; 54 minutes ago Up 54 minutes ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mgr.ceph1.tlpgcb6410679d498a quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-mon -…&quot; 55 minutes ago Up 54 minutes ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mon.ceph1acf9377ed00d hello-world &quot;/hello&quot; 6 hours ago Exited (0) 6 hours ago ecstatic_gould alertmanager 组件：prometheus 告警组件。 grafana 组件：监控数据展示 dashboard。 prometheus 组件：prometheus 监控组件。 node_exporter 组件：prometheus 节点数据收集组件。 ceph-crash 组件：崩溃数据收集模块。 ceph-mgr 组件：Ceph 管理程序。 ceph-monitor 组件：Ceph 监视器。 查看所有组件运行状态： 123456789root@ceph1:/opt# ceph orch psNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID alertmanager.ceph1 ceph1 running (5h) 8m ago 5h 0.20.0 quay.io/prometheus/alertmanager:v0.20.0 0881eb8f169f 1fdde1918435 crash.ceph1 ceph1 running (5h) 8m ago 5h 15.2.15 quay.io/ceph/ceph:v15 3437f7bed968 c20775682c08 grafana.ceph1 ceph1 running (5h) 8m ago 5h 6.7.4 quay.io/ceph/ceph-grafana:6.7.4 557c83e11646 5fc44d4adaa8 mgr.ceph1.tlpgcb ceph1 running (5h) 8m ago 5h 15.2.15 quay.io/ceph/ceph:v15 3437f7bed968 7c3cf4e445d1 mon.ceph1 ceph1 running (5h) 8m ago 5h 15.2.15 quay.io/ceph/ceph:v15 3437f7bed968 6410679d498a node-exporter.ceph1 ceph1 running (5h) 8m ago 5h 0.18.1 quay.io/prometheus/node-exporter:v0.18.1 e5a616e4b9cf d1aa75b76a39 prometheus.ceph1 ceph1 running (5h) 8m ago 5h 2.18.1 quay.io/prometheus/prometheus:v2.18.1 de242295e225 85cac2612c10 查看某个组件运行状态： 123root@ceph1:/opt# ceph orch ps --daemon-type mgrNAME HOST STATUS REFRESHED AGE VERSION IMAGE NAME IMAGE ID CONTAINER ID mgr.ceph1.tlpgcb ceph1 running (5h) 4m ago 5h 15.2.15 quay.io/ceph/ceph:v15 3437f7bed968 7c3cf4e445d1 查看容器状态： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115root@ceph1:/opt# cephadm ls[ &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;crash.ceph1&quot;, &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;, &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@crash.ceph1&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;c20775682c087aaf8d1dc25802386ac4fbaf40b3ab8a4ae7ef54843e26725d94&quot;, &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;, &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;, &quot;version&quot;: &quot;15.2.15&quot;, &quot;started&quot;: &quot;2022-01-07T08:25:11.762461Z&quot;, &quot;created&quot;: &quot;2022-01-07T08:25:11.702112Z&quot;, &quot;deployed&quot;: &quot;2022-01-07T08:25:11.318096Z&quot;, &quot;configured&quot;: &quot;2022-01-07T08:25:11.702112Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;grafana.ceph1&quot;, &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;, &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@grafana.ceph1&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;5fc44d4adaa8365aa4a290d769fd08b25a2dee4c5235f4ef377aff0ced55ef2b&quot;, &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph-grafana:6.7.4&quot;, &quot;container_image_id&quot;: &quot;557c83e11646f123a27b5e4b62ac6c45e7bb8b2e90d6044034d0db5b7019415c&quot;, &quot;version&quot;: &quot;6.7.4&quot;, &quot;started&quot;: &quot;2022-01-07T08:26:37.418809Z&quot;, &quot;created&quot;: &quot;2022-01-07T08:25:51.875753Z&quot;, &quot;deployed&quot;: &quot;2022-01-07T08:25:51.487738Z&quot;, &quot;configured&quot;: &quot;2022-01-07T08:26:37.229606Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;mgr.ceph1.tlpgcb&quot;, &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;, &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@mgr.ceph1.tlpgcb&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;7c3cf4e445d1113caa67c7502ffd5265961ecba5430e909f34f853b1fc021f1e&quot;, &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;, &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;, &quot;version&quot;: &quot;15.2.15&quot;, &quot;started&quot;: &quot;2022-01-07T08:19:10.362590Z&quot;, &quot;created&quot;: &quot;2022-01-07T08:19:10.279314Z&quot;, &quot;deployed&quot;: &quot;2022-01-07T08:19:09.911299Z&quot;, &quot;configured&quot;: &quot;2022-01-07T08:26:37.749627Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;alertmanager.ceph1&quot;, &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;, &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@alertmanager.ceph1&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;1fdde19184350e4c731b484bf10b3943e95f0ec27a306daa7ff547414e9e587e&quot;, &quot;container_image_name&quot;: &quot;quay.io/prometheus/alertmanager:v0.20.0&quot;, &quot;container_image_id&quot;: &quot;0881eb8f169f5556a292b4e2c01d683172b12830a62a9225a98a8e206bb734f0&quot;, &quot;version&quot;: &quot;0.20.0&quot;, &quot;started&quot;: &quot;2022-01-07T08:26:38.462159Z&quot;, &quot;created&quot;: &quot;2022-01-07T08:25:09.854037Z&quot;, &quot;deployed&quot;: &quot;2022-01-07T08:25:09.486022Z&quot;, &quot;configured&quot;: &quot;2022-01-07T08:26:38.297650Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;prometheus.ceph1&quot;, &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;, &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@prometheus.ceph1&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;85cac2612c105ec9809a88fa97f99633b50b50ba12ee0a9b2beb95c608be73ff&quot;, &quot;container_image_name&quot;: &quot;quay.io/prometheus/prometheus:v2.18.1&quot;, &quot;container_image_id&quot;: &quot;de242295e2257c37c8cadfd962369228f8f10b2d48a44259b65fef44ad4f6490&quot;, &quot;version&quot;: &quot;2.18.1&quot;, &quot;started&quot;: &quot;2022-01-07T08:26:35.419299Z&quot;, &quot;created&quot;: &quot;2022-01-07T08:26:35.361530Z&quot;, &quot;deployed&quot;: &quot;2022-01-07T08:26:34.993515Z&quot;, &quot;configured&quot;: &quot;2022-01-07T08:26:35.361530Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;mon.ceph1&quot;, &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;, &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@mon.ceph1&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;6410679d498a31428927c7a532548d39f5261c80f94c867849902a6bea12208b&quot;, &quot;container_image_name&quot;: &quot;quay.io/ceph/ceph:v15&quot;, &quot;container_image_id&quot;: &quot;3437f7bed9688a799444b439e1947a6f00d1f9b1fc986e843aec7e3ac8acd12b&quot;, &quot;version&quot;: &quot;15.2.15&quot;, &quot;started&quot;: &quot;2022-01-07T08:19:09.667576Z&quot;, &quot;created&quot;: &quot;2022-01-07T08:19:08.539242Z&quot;, &quot;deployed&quot;: &quot;2022-01-07T08:19:07.699208Z&quot;, &quot;configured&quot;: &quot;2022-01-07T08:26:38.785670Z&quot; &#125;, &#123; &quot;style&quot;: &quot;cephadm:v1&quot;, &quot;name&quot;: &quot;node-exporter.ceph1&quot;, &quot;fsid&quot;: &quot;79a6cb92-6f92-11ec-b1e0-6f6f27397286&quot;, &quot;systemd_unit&quot;: &quot;ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286@node-exporter.ceph1&quot;, &quot;enabled&quot;: true, &quot;state&quot;: &quot;running&quot;, &quot;container_id&quot;: &quot;d1aa75b76a39554b9b5cc7b5e9c4cf9c4ca4d00b9401af79b3c8eda6009fc215&quot;, &quot;container_image_name&quot;: &quot;quay.io/prometheus/node-exporter:v0.18.1&quot;, &quot;container_image_id&quot;: &quot;e5a616e4b9cf68dfcad7782b78e118be4310022e874d52da85c55923fb615f87&quot;, &quot;version&quot;: &quot;0.18.1&quot;, &quot;started&quot;: &quot;2022-01-07T08:26:02.530410Z&quot;, &quot;created&quot;: &quot;2022-01-07T08:25:52.639785Z&quot;, &quot;deployed&quot;: &quot;2022-01-07T08:25:52.179766Z&quot;, &quot;configured&quot;: &quot;2022-01-07T08:25:52.639785Z&quot; &#125;] 根据初始化完成的提示使用浏览器访问 dashboard：略。 启用 Ceph 命令 在 ceph1 上执行命令： 1234567root@ceph1:/opt# cephadm shellInferring fsid 79a6cb92-6f92-11ec-b1e0-6f6f27397286Inferring config /var/lib/ceph/79a6cb92-6f92-11ec-b1e0-6f6f27397286/mon.ceph1/configUsing recent ceph image quay.io/ceph/ceph@sha256:a2c23b6942f7fbc1e15d8cfacd6655a681fe0e44f288e4a158db22030b8d58e3root@ceph1:/# alias ceph=&#x27;cephadm shell -- ceph&#x27;root@ceph1:/# exitexit 查看集群状态： 123456789101112131415161718192021222324252627282930313233root@ceph1:/opt# ceph -s cluster: id: 79a6cb92-6f92-11ec-b1e0-6f6f27397286 health: HEALTH_WARN OSD count 0 &lt; osd_pool_default_size 3 services: mon: 1 daemons, quorum ceph1 (age 21h) mgr: ceph1.tlpgcb(active, since 21h) osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: root@ceph1:/opt# ceph status cluster: id: 79a6cb92-6f92-11ec-b1e0-6f6f27397286 health: HEALTH_WARN OSD count 0 &lt; osd_pool_default_size 3 services: mon: 1 daemons, quorum ceph1 (age 21h) mgr: ceph1.tlpgcb(active, since 21h) osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 添加新主机到集群中 第一步，在 ceph1 上执行命令，将集群的公共 SSH 密钥添加到新主机的根用户 authorized_keys 文件中： 12345678910111213141516171819202122232425262728293031323334353637root@ceph1:/opt# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;root@ceph2&#x27;&quot;and check to make sure that only the key(s) you wanted were added.root@ceph1:/opt# ll /etc/ceph/total 24drwxr-xr-x 2 root root 4096 Jan 7 16:19 ./drwxr-xr-x 104 root root 4096 Jan 7 16:02 ../-rw------- 1 root root 63 Jan 7 16:19 ceph.client.admin.keyring-rw-r--r-- 1 root root 175 Jan 7 16:19 ceph.conf-rw-r--r-- 1 root root 595 Jan 7 16:19 ceph.pub-rw-r--r-- 1 root root 92 Oct 20 22:31 rbdmaproot@ceph1:/opt# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph3/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;root@ceph3&#x27;&quot;and check to make sure that only the key(s) you wanted were added.root@ceph1:/opt# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph4/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/etc/ceph/ceph.pub&quot;Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;root@ceph4&#x27;&quot;and check to make sure that only the key(s) you wanted were added.root@ceph1:/opt# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286 # 公共SSH密钥 在新结点上查看密钥是否添加成功： 123456root@ceph2:/opt# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286 # 公共SSH密钥 123456root@ceph3:/opt# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286 # 公共SSH密钥 123456root@ceph4:/opt# cat /root/.ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iDQVKo2J9hNGVjq6XegDCAlLRuweTxbHruBauegj+7i5lTwJx0inpRcnshbPJxqRNimH54lr8mDYzQ2kVuVgFqpftB9Q+Y5VnMnvZWnlzaJgEQ0BOj4RmydRHKztbhay9YNyQebUxmSC/EYPCBRloM86A4gGGEoU4uroV9d6QZZ8memabUQRLuATygXEWK0upU9B5bRtoyEsHg+R9Ypc12/rz9sssOOwABmFd+ushouvF7oIdXmKQ4ehOY8t86YZV4PmOnEmuF/tfTYu+oLRkLro13Q0AmmmtTTjOiI5baekeldxiHf26DlFFxkzpAx7obTl+JrsS9DAiAwc/W51q2J70hRuVfSpA+U+TO7bUip+O8fpMwY3xDv2CZch0i7VN+MF8v/uB89ecW1AGz7Fu4xhaauTqlDXgJ1pG9jw6UnJRXwZI4UdAGaG0/tPUWMqOJ+HSnt8kjFa/U7Jp1t6W6ryHZTZXt+1JeuV4clAL0f3ic1R9d4k1jpnH4QsObM= root@ceph1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCnzZyF16dJR8DpOTzyBwd4YbVE60blx7TOjn3VwPhPkU4f2lcJ2uPmFApEwHOTHvFi6Mz9jnE6BymrAHAUIzXPU5D8fasoJ5tNOKODYEtaETsdimEN1hLiLFKET8JLU/EUoy7pgHllxDlCsfwZvFoJ2XUvX1ab5rCs5AB4c/pU3IG24zN/UBsl7pynIA8XLAWFSSg2KHPEAYwNMF/oBLt4+/z5xBGDYBtdLId1lB/UrYyj9RamJywuSMEZckUbEEo8AqZeGzXbXKFwHcy2dxUesCFT0C5B95c71HU6g/3zOu/hYjXLUU/NeX7ggsPwKDr19p18JNxX+iyjpfrupK1PxgALniuZXKaHpiC7geGoosb8P0SR1Cq0RJhs/PWwPFpPqTRI2svUWLskb5WG2xIYVE6uuRhJAdTirKSX/+l8Bw5iau8BEYZ4zCT8g/qMhb+tpYeM7lGh8usr0RtsK/U/Z48zu1HqYMKVvK5olmwFKgRe6Ss5JqJ7GqX3T37UYL0= root@ceph2ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCiBgLpJH5BcUdTG4tT/6ln++GnCoMIpKlV3rjlYoWAa5efn60d1JKqgTBMRusa79D9kIihGCWjxBio2o0QplrKyHtjLKNLLaNShVOCZESJATrSf2ZwGmj6pS2RT4Bg0xchByZlQXIBwEyeLQ0T4hC45JYoXDeiPEezp+pPYqTIxDoks/wbb4IqUtsdtoXI1m/3GT1KCl1FofWWoc5PaXSiFwg+YZ5QAW2y6i5oRZH2cTyPpcrCtlqPRa1h8V8+C/vN8EcEMMxofW0sS4XCrtqYUcZcoPjcbHofQlC6cTf5/IhSNBME7Re2bMSFvn68n5vdS0z6fITSjCZdkm8I9V4omXpbGoztbbK4J7TDPL2IZWw6bh/LP7L4YuaUFPWmkHAWrckVtNEciYpXBgE9cKWgxpq0/UWXRFmMUgYmejz126/mtSwGpXV+VhjlXlw/wFsIS6k8BYEYsrFZrdYNgQA+6L0DenEwPqcfFb3NEqRH66Urgen3NCibBzYtpI5PlR8= root@ceph3ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzntkcguxOqBlvUxBX0S80BzQd36MGxg3TkDLJWq5D/gi6hcUpQOrFwMtXctQWv+8AK708fXssaj07SwF939rxQFBrU83M59ZdrILAFH3dSZNOh4YYj30weHOmsj/uBkH9rQOYoc2JApVDV76OSjTm8Uztez8cQ0r355rd0wwHmX1eVkL1mgCu7PAemD64a4J7fZUOl0eYdz1HyqjGNprYZItAeqldAFwMfi8wQZubyfzeQX2hM5rWehaP5OxhvlCrHUdGhwM3m82fyNWzJcfCexnIfoaJkyYcEXrcjtvWfUeUcoLmYd6jSa19jhr2AkkDSxyxZ43p/73d5tgzJFzX5etjBK33P/zDfIG1HIiD56Tn9jOXJ/Kud9JF+RLGmEaVsvqWwZJLJIN8bin/S+Viv9kVtytOWFpilea1LQGg2Sap2Y/8oQJbDCk24wCHGKbRFmFYOalqEuX9GwM3h0mRwt+UBVv7M3x/JsliyA7tKlN9V+oPH+3LxVpw+Wfq8rU= root@ceph4ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDGii1dKJdramvUhkqwyix/G39kAEw+CGt1NNdM78QmrumLlWqtToO6TwmfgMqu/2geBafkOs9fhLJB+qV/oQ8mpBaGgzXgBOthtCMyEWBtBB78wJJ+cgn2EkkQLsmJmMfwbp+dnMJMvqWKPKYZoyODyZvhnS4uofFfpBqMI9xLjpnNgVerQbyN1DIH+WkBnrhonirzr78DPlsU3zpEdB5wlVduIqaPNrnW4DUNje5quhFcTd7D40kQeAKZBm5Py94R93BqEUyhRUYaz5dHAF4Wh/dTcD/aA3aq7fiOcdxNjOBD9Z/bCG5B32Eb+4iehzk3O4JoTNnl3FtYMVz50MaPphWQh9yBTbrUY/pxupvvderhDNz7yAxmSzM6MKVPlVaSpSaQf2MKUqWRNzcJDaOw8/aQi6GuJe/Iljpbvse1NO3cTtZF0IRuTjham90DptKyHfDgms9+gmhiDg6uLzM8jLmY74He8W8FOtalXBhALCYFP2lX9IYx1HFSnt42UdE= ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286 # 公共SSH密钥 第二步，在 ceph1 上执行命令，告诉 Ceph，新节点是集群的一部分： 123456root@ceph1:/opt# ceph orch host add ceph2Added host &#x27;ceph2&#x27;root@ceph1:/opt# ceph orch host add ceph3Added host &#x27;ceph3&#x27;root@ceph1:/opt# ceph orch host add ceph4Added host &#x27;ceph4&#x27; 查看 Ceph 纳管的所有节点： 123456root@ceph1:/opt# ceph orch host lsHOST ADDR LABELS STATUS ceph1 ceph1 ceph2 ceph2 ceph3 ceph3 ceph4 ceph4 添加完成后 Ceph 会自动扩展 monitor 和 manager 到另外 3 个节点，在另外 3 个节点查看，自动运行了以下容器： 12345root@ceph2:/opt# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9631037be5f4 quay.io/prometheus/node-exporter:v0.18.1 &quot;/bin/node_exporter …&quot; 13 hours ago Up 13 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-node-exporter.ceph208b248e6e233 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-mgr -…&quot; 13 hours ago Up 13 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mgr.ceph2.nxltpk9a57743caf61 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-crash…&quot; 13 hours ago Up 13 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-crash.ceph2 12345root@ceph3:/opt# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf9ed0005d967 quay.io/prometheus/node-exporter:v0.18.1 &quot;/bin/node_exporter …&quot; 14 hours ago Up 14 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-node-exporter.ceph357c0c9ba4221 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-mon -…&quot; 14 hours ago Up 14 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mon.ceph36ad0d2342d96 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-crash…&quot; 14 hours ago Up 14 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-crash.ceph3 12345root@ceph4:/opt# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfedc448936a7 quay.io/prometheus/node-exporter:v0.18.1 &quot;/bin/node_exporter …&quot; 14 hours ago Up 14 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-node-exporter.ceph4028820a77622 quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-mon -…&quot; 14 hours ago Up 14 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-mon.ceph4d2cee165f5de quay.io/ceph/ceph:v15 &quot;/usr/bin/ceph-crash…&quot; 14 hours ago Up 14 hours ceph-79a6cb92-6f92-11ec-b1e0-6f6f27397286-crash.ceph4 查看 Ceph 集群状态： 123456789101112131415161718root@ceph1:/opt# ceph -s cluster: id: 79a6cb92-6f92-11ec-b1e0-6f6f27397286 health: HEALTH_WARN Reduced data availability: 1 pg inactive OSD count 0 &lt; osd_pool_default_size 3 services: mon: 3 daemons, quorum ceph1,ceph4,ceph3 (age 14h) mgr: ceph1.tlpgcb(active, since 2d), standbys: ceph2.nxltpk osd: 0 osds: 0 up, 0 in data: pools: 1 pools, 1 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 100.000% pgs unknown 1 unknown 本文参考 https://docs.ceph.com/en/latest/install/#recommended-methods https://docs.ceph.com/en/latest/cephadm/install/ https://www.kancloud.cn/willseecloud/ceph/1788314 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"ceph","slug":"ceph","permalink":"http://example.com/tags/ceph/"}]},{"title":"python 科学计算工具","slug":"python-scientific","date":"2021-10-18T11:41:42.000Z","updated":"2022-01-05T07:01:54.224Z","comments":true,"path":"2021/10/18/python-scientific/","link":"","permalink":"http://example.com/2021/10/18/python-scientific/","excerpt":"","text":"Numpy安装123456789(base) PS C:\\Users\\XiSun&gt; conda env list# conda environments:#base * D:\\Program\\Miniconda3py38 D:\\Program\\Miniconda3\\envs\\py38(base) PS C:\\Users\\XiSun&gt; conda activate py38(py38) PS C:\\Users\\XiSun&gt; conda init(py38) PS C:\\Users\\XiSun&gt; conda install numpy -y 使用Pandashttps://blog.csdn.net/qq_43060552/article/details/104862200 https://www.cnblogs.com/zzay/p/13157863.html https://www.cnblogs.com/wenqiangit/p/11252859.html 本文参考正则： https://github.com/memect/kg-beijing/wiki/%E7%AC%AC%E4%B8%80%E6%9C%9Fw1%EF%BC%9A%E7%9F%A5%E8%AF%86%E6%8F%90%E5%8F%96","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"Python 入门","slug":"python","date":"2021-09-06T02:09:24.000Z","updated":"2021-12-17T03:30:57.515Z","comments":true,"path":"2021/09/06/python/","link":"","permalink":"http://example.com/2021/09/06/python/","excerpt":"","text":"计算机基础知识Windows 的命令行 用户界面分成两种：TUI（文本交互界面）和 GUI（图形化交互界面）。 命令行就是文本交互界面，通过命令行可以使用一个一个的指令来操作计算机。 任何计算机的操作系统中都包含有命令行（Windows、Linux、MacOS），命令行有多个不同的名字：命令行、命令行窗口、DOS 窗口、命令提示符、CMD 窗口、Shell、终端、Terminal。 命令行的进入方式：win 键 + R，出现运行窗口，输入cmd，然后回车。 命令行的结构 1234Microsoft Windows [版本 10.0.19042.1165](c) Microsoft Corporation。保留所有权利。C:\\Users\\Xisun&gt; 上面两行为版本及版权声明（一般没有什么用） 最下面一行为命令提示符 - `C`：当前所在的磁盘根目录，通过 `x:` 来切换盘符（x 表示你的盘符）。 - `\\Users\\Xisun`：当前所在磁盘的路径，通过 `cd` 来切换目录。 - `&gt;`：命令提示符，在符号后边可以直接输入指令。 常用的 DOS 命令 语法：命令 [参数] [选项] dir：查看当前目录下的所有文件（夹） cd：进入到指定的目录 .：表示当前目录。 ..：表示上一级目录。 md：创建一个目录。 rd：删除一个目录。 del：删除一个文件。 cls：清除屏幕。 小技巧：方向键上下，查看命令的历史记录；tab 键自动补全命令。 环境变量 环境变量指的就是操作系统当中的一些变量。可以通过修改环境变量，来对计算机进行配置（主要是来配置一些路径的）。 查看环境变量 右键计算机（此电脑）—&gt; 选择属性 —&gt; 系统界面左侧选择高级系统设置 —&gt; 选择环境变量。 环境变量界面分成了两个部分，上边是用户环境变量，下边是系统环境变量。 用户环境变量只对当前用户有效，系统环境变量对所有用户有效。 添加环境变量 通过新建按钮添加环境变量。 一个环境变量可以由多个值，值与值之间使用 ; 隔开。 修改环境变量 通过编辑按钮来修改环境变量。 删除环境变量 通过删除按钮来删除环境变量。 path 环境变量 path 环境变量中保存的是一个一个的路径。当我们在命令行中输入一个命令（或访问一个文件时），系统会首先在当前目录下寻找，如果找到了则直接执行或打开；如果没有找到，则会依次去 path 环境变量的路径中去寻找，直到找到为止；如果 path 环境变量中的路径都没有找到，则报错：&#39;xxx&#39; 不是内部或外部命令，也不是可运行的程序或批处理文件。 将一些经常需要访问到的文件或程序的路径，添加到 path 环境变量中，这样就可以在任意的位置访问到这些文件或程序。 注意事项： 如果环境变量中没有 path，可以手动添加。 path 环境变量不区分大小写：PATH、Path 或 path。 修改完环境变量必须重新启动命令行窗口。 多个路径之间使用 ; 隔开。 进制 十进制 十进制是最常用的进制。 十进制算法：满十进一。 十进制当中一共有 10 个数字：0，1，2，3，4，5，6，7，8，9。 十进制如何计数：0，1，2，3，4，5，6，7，8，9；10，11，12，。。。，19；20，。。。，29；30，… 个位表示有几个 1，十位表示有几个 10，百位表示有几个 100，千位表示有几个 1000，以此类推。如：5421。 二进制 二进制是计算机底层使用的进制。 所有的数据在计算机底层都是以二进制的形式保存的，计算机只认二进制。 可以将内存想象为一个一个的小格子，小格子中可以存储一个 0 或一个 1。 内存中的每一个小格子，我们称为 1 bit（1 位）。 bit 是计算机中的最小的单位。 byte 是我们可操作的最小的单位。 8 bit = 1 byte（字节） 1024 byte = 1 kb（千字节） 1024 kb = 1 mb（兆字节） 1024 mb = 1 gb（吉字节） 1024 gb = 1 tb（太字节） 二进制算法：满二进一。 二进制中一共有 2 个数字：0，1。 二进制如何计数：0，1；10，11；100，101，110，111；1000，… 第一位表示有几个 1，第二位表示有几个 2，第三位表示有几个 4，第四位表示有几个 8，依次类推。如：1011。 八进制 一般不用。 八进制算法：满八进一。 八进制中一共有 8 个数字：0，1，2，3，4，5，6，7。 八进制如何计数：0，1，2，3，4，5，6，7；10，11，…，17；20，21，…，27；… 十六进制 在查看二进制数据时，一般会以十六进制的形式显示。 十六进制算法：满十六进一。 十六进制中一共有 16 个数字：0，1，2，3，4，5，6，7，8，9，a，b，c，d，e，f 。 由于十六进制是满 16 才进位，所以十六进制中引入了 a，b，c，d，e，f 来表示 10，11，12，13，14，15。 十六进制如何计数：0，1，2，3，4，5，6，7，8，9，a，b，c，d，e，f ；10，11，12 ，…，1a，1b，1c，1d，1e，1f，20，21，22，…，2a，2b，2c，2d，2e，2f；30，… 文本文件和字符集 文本分成两种，一种叫做纯文本，还有一种叫做富文本。 纯文本中只能保存单一的文本内容，无法保存内容无关的东西（如字体、颜色、图片等）。常见的纯文本如记事本。 富文本中可以保存文本以外的内容。常见的富文本如 word 文档。 在开发时，编写程序使用的全都是纯文本！ 纯文本在计算机底层也会转换为二进制保存： 将字符转换为二进制码的过程，称为编码。 将二进制码转换为字符的过程，称为解码。 编码和解码时所采用的规则，称为字符集。 常见的字符集： ASCII 美国人编码，使用 7 位来对美国常用的字符进行编码。 包含 128 个字符。 ISO-8859-1 欧洲的编码，使用 8 位来对欧洲常用的字符进行编码。 包含 256 个字符。 GB2312，GBK 国标码，中国的编码。 Unicode 万国码，包含世界上所有的语言和符号，编写程序时一般都会使用 Unicode 编码。 Unicode 编码有多种实现，如 UTF-8，UTF-16，UTF-32，最常用的就是 UTF-8。 乱码：编写程序时，如果发现程序代码出现乱码的情况，就要马上去检查字符集是否正确。 计算机语言 计算机语言就是用来控制计算机的编程语言。 计算机语言发展经历了三个阶段： 机器语言 机器语言通过二进制编码来编写程序。 执行效率好，但编写起来太麻烦。 符号语言/汇编语言 使用符号来代替机器码。 编写程序时，不需要使用二进制，而是直接编写符号。 编写完成后，需要将符号转换为机器码，然后再由计算机执行。 将符号转换为机器码的过程，称为汇编。 将机器码转换为符号的过程，称为反汇编。 汇编语言一般只适用于某些硬件，兼容性比较差。 高级语言 高级语言的语法基本和现在英语语法类似，并且和硬件的关系没有那么紧密了。 也就是说我们通过高级语言开发的程序，可以在不同的硬件系统中执行。 并且高级语言学习起来也更加的容易，现在我们知道的语言基本都是高级语言。比如：C、C++、C#、Java、JavaScript、Python等。 编译型语言和解释型语言 计算机只能识别二进制编码（机器码），所以任何的语言在交由计算机执行时必须要先转换为机器码，也就是像 print(&#39;hello&#39;) 必需要转换为类似 1010101 这样的机器码。 根据转换时机的不同，语言分成了两大类： 编译型语言 会在代码执行前将代码编译为机器码，然后将机器码交由计算机执行。最典型的就是 C 语言。 过程：a（源码）—&gt; 编译 —&gt; b（编译后的机器码）。 特点： 执行速度特别快。 跨平台性比较差。 解释型语言 不会在执行前对代码进行编译，而是在执行的同时一边执行一边编译。比如 Python，JS，Java 等。 过程：a（源码）—&gt; 解释器 —&gt; 解释执行。 特点： 执行速度比较慢。 跨平台性比较好 。 Python 简介 现在，全世界差不多有 600 多种编程语言，但流行的编程语言也就那么 20 来种。 总的来说，每种编程语言各有千秋。C 语言是可以用来编写操作系统的贴近硬件的语言，所以，C 语言适合开发那些追求运行速度、充分发挥硬件性能的程序。而 Python 是用来编写应用程序的高级编程语言。 Python 提供了非常完善的基础代码库，覆盖了网络、文件、GUI、数据库、文本等大量内容，被形象地称作“内置电池（batteries included）”。用 Python 开发，许多功能不必从零编写，直接使用现成的即可。 除了内置的库外，Python 还有大量的第三方库，也就是别人开发的，供你直接使用的东西。当然，如果你开发的代码通过很好的封装，也可以作为第三方库给别人使用。 Python 的定位是“优雅”、“明确”、“简单”。Python 的哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码。 Python 适合开发的应用类型： 首选是网络应用，包括网站、后台服务等等。 其次是许多日常需要的小工具，包括系统管理员需要的脚本任务等等。 另外就是把其他语言开发的程序再包装起来，方便使用。 Python 的缺点： 第一个缺点就是运行速度慢，和 C 程序相比非常慢，因为 Python 是解释型语言，编写的代码在执行时会一行一行地翻译成 CPU 能理解的机器码，这个翻译过程非常耗时，所以很慢。而 C 程序是运行前直接编译成 CPU 能执行的机器码，所以非常快。 第二个缺点就是代码不能加密。如果要发布编写的 Python 程序，实际上就是发布源代码，这一点跟 C 语言不同，C 语言不用发布源代码，只需要把编译后的机器码（也就是在 Windows 上常见的 xxx.exe 文件）发布出去。要从机器码反推出 C 代码是不可能的，所以，凡是编译型的语言，都没有这个问题，而解释型的语言，则必须把源码发布出去。 Python 安装 Python 是跨平台的，它可以运行在 Windows、Mac 和各种 Linux/Unix 系统上。在 Windows 上写 Python 程序，放到 Linux 上也是能够运行的。 要开始学习 Python 编程，首先就得把 Python 安装到你的电脑里。安装后，你会得到 Python 解释器（就是负责运行 Python 程序的），一个命令行交互环境，还有一个简单的集成开发环境。 目前，Python 有两个版本，一个是 2.x 版，一个是 3.x 版，这两个版本是不兼容的。由于 3.x 版越来越普及，此处选择安装 Windows 系统的 3.8 版本。 官方安装包 Python 官网：https://www.python.org/ Python 下载： Python 安装： 验证 Python 是否安装完成： 12345678910Windows PowerShell版权所有 (C) Microsoft Corporation。保留所有权利。尝试新的跨平台 PowerShell https://aka.ms/pscore6PS C:\\Users\\Xisun&gt; pythonPython 3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; exit()PS C:\\Users\\Xisun&gt; 提示符 &gt;&gt;&gt; 表示已经处于 Python 的交互模式中，可以输入任何 Python 代码，输入完的指令将会被 Python 的解释器立即执行。 输入 exit() 并回车，可以退出 Python 交互模式（直接关掉命令行窗口也可以）。 安装 Python 的同时，会自动安装一个 Python 的开发工具 IDLE，通过 IDLE 也可以进入到交互模式。IDLE 中可以通过 tab 键来查看语句的提示，并且可以将代码保存。 交互模式下只能输入一行代码，执行一行，所以不适用于我们日常的开发，仅可以用来做一些日常的简单的测试！ 我们一般会将 Python 代码编写到一个 .py 文件中，然后通过 python 指令来执行文件中的代码。 Minicoda 安装 Miniconda 是一款小巧的 Python 环境管理工具，其安装程序中包含 conda 软件包管理器和 Python。一旦安装了 Miniconda，就可以使用 conda 命令安装任何其他软件工具包并创建环境等。 官网：https://conda.io/en/latest/miniconda.html 下载： 安装： 添加系统环境变量： 创建 Python 3.8 环境： 查看所有的环境： 1PS C:\\Users\\XiSun&gt;conda env list 创建一个新环境： 1PS C:\\Users\\XiSun&gt;conda create -n py38 python=3.8 建议针对不同的 Python 版本，创建不同的环境，-n 参数后是环境的名字，可以自取。 Miniconda 自身的 Python 环境，不建议直接使用，避免污染。 新环境位于 Miniconda 安装目录下，如 D:\\miniconda3\\envs\\py38。 激活新创建的环境： 1PS C:\\Users\\XiSun&gt; conda activate py38 安装模块，可以多个一起安装： 12PS C:\\Users\\XiSun&gt; conda initPS C:\\Users\\XiSun&gt; conda install numpy scipy pandas jupyter -y -y 参数表示安装过程中询问是否继续操作时，默认输入 y。 使用 jupyter，首先创建一个目录，用于存放文件，如 D:\\notebook： 1PS C:\\Users\\XiSun&gt; jupyter notebook --notebook-dir D:\\notebook\\ jupyter 的部分操作： Pycharm 引用创建的 py38 环境： Python 解释器 当编写 Python 代码时，得到的是一个包含 Python 代码的以 .py 为扩展名的文本文件。要运行代码，就需要 Python 解释器去执行 .py 文件。 由于整个 Python 语言从规范到解释器都是开源的，所以理论上，只要水平够高，任何人都可以编写 Python 解释器来执行 Python 代码（当然难度很大）。事实上，确实存在多种 Python 解释器。 Python 的解释器很多，但使用最广泛的还是 CPython。如果要和 Java 或 .Net 平台交互，最好的办法不是用 Jython 或 IronPython，而是通过网络调用来交互，确保各程序之间的独立性。 CPython 从 Python 官网下载并安装好 Python 3.x 后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用 C 语言开发的，所以叫 CPython。在命令行下运行 python 命令就是启动 CPython 解释器。 CPython 是使用最广的 Python 解释器。 IPython IPython 是基于 CPython 之上的一个交互式解释器，也就是说，IPython 只是在交互方式上有所增强，但是执行 Python 代码的功能和 CPython 是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了 IE。 CPython 用 &gt;&gt;&gt; 作为提示符，而 IPython 用 In [序号]: 作为提示符。 PyPy PyPy 是另一个 Python 解释器，它的目标是执行速度。PyPy 采用 JIT 技术，对 Python 代码进行动态编译（注意不是解释），所以可以显著提高 Python 代码的执行速度。 绝大部分 Python 代码都可以在 PyPy 下运行，但是 PyPy 和 CPython 有一些是不同的，这就导致相同的 Python 代码在两种解释器下执行可能会有不同的结果。如果你的代码要放到 PyPy 下执行，就需要了解 PyPy和CPython的不同点。 Jython Jython 是运行在 Java 平台上的 Python 解释器，可以直接把 Python 代码编译成 Java 字节码执行。 IronPython IronPython 和 Jython 类似，只不过 IronPython 是运行在微软 .Net 平台上的 Python 解释器，可以直接把 Python 代码编译成 .Net 字节码。 Python 基础 Python 的语法比较简单，采用缩进方式，写出来的代码就像下面的样子： 123456# print absolute value of an integer:a = 100if a &gt;= 0: print(a)else: print(-a) Python 程序是大小写敏感的，如果写错了大小写，程序会报错。 Python 中的每一行就是一条语句，每条语句以换行结束，每一行语句不要过长（规范中建议每行不要超过 80 个字符）。当语句以冒号 : 结尾时，缩进的语句视为代码块。 Python 一条语句可以分多行编写，多行编写时语句后边以 \\ 结尾。 Python 是缩进严格的语言，所以在 Python 中不要随便写缩进。按照约定俗成的惯例，应该始终坚持使用 4 个空格的缩进。当重构代码时，粘贴过去的代码必须重新检查缩进是否正确。此外，IDE 很难像格式化 Java 代码那样格式化 Python 代码。 Python 中使用 # 来表示注释，# 后的内容都属于注释，注释的内容将会被解释器所忽略。注释要求简单明了，一般习惯上 # 后边会跟着一个空格。 字面量和变量 字面量就是一个一个的值，比如：1，2，3，4，5，6，‘HELLO’。字面量所表示的意思就是它的字面的值，在程序中可以直接使用字面量。 变量（variable）可以用来保存字面量，并且变量中保存的字面量是不定的。变量本身没有任何意思，它会根据不同的字面量表示不同的意思。 在 Python 中，等号是赋值语句，可以把任意数据类型赋值给变量，同一个变量可以反复赋值，而且可以是不同类型的变量。 1234a = 123 # a是整数print(a) # 123a = &#x27;ABC&#x27; # a变为字符串print(a) # ABC 这种变量本身类型不固定的语言称之为动态语言，与之对应的是静态语言。静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错。例如 Java 是静态语言。 等号也可以把一个变量 a 赋值给另一个变量 b，这个操作实际上是把变量 b 指向变量 a 所指向的数据。 1234a = &#x27;ABC&#x27;b = aa = &#x27;XYZ&#x27;print(b) # ABC，不是XYZ 常量就是不能变的变量，比如常用的数学常数 π 就是一个常量。在 Python 中，通常用全部大写的变量名表示常量： 1PI = 3.14159265359 事实上 PI 仍然是一个变量，Python 根本没有任何机制保证 PI 不会被改变，所以，用全部大写的变量名表示常量只是一个习惯上的用法，如果你一定要改变变量 PI 的值，也没人能拦住你。 一般在开发时，很少直接使用字面量，都是将字面量保存到变量中，通过变量来引用字面量。 变量和标识符 Python 中使用变量，不需要声明，直接为变量赋值即可。 Python 中不能使用没有进行过赋值的变量，如果使用没有赋值过的变量，会报错 NameError: name &#39;b&#39; is not defined。 Python 是一个动态类型的语言，可以为变量赋任意类型的值，也可以任意修改变量的值。 Python 中所有可以自主命名的内容都属于标识符，比如变量名、函数名、类名。 标识符必须遵循标识符的规范： 标识符中可以含有字母、数字、_，但是不能使用数字开头。 标识符不能是 Python 中的关键字和保留字，也不建议使用 Python 中的函数名作为标识符，因为这样会导致函数被覆盖。 在 Python 中注意遵循两种命名规范： 下划线命名法：所有字母小写，单词之间使用 _ 分割。如：max_length、min_length、hello_world。 帕斯卡命名法：大驼峰命名法，首字母大写，每个单词开头字母大写，其余字母小写。如：MaxLength、MinLength、HelloWorld。 如果使用不符合标准的标识符，会报错 SyntaxError: invalid syntax。 数据类型 数据类型指的就是变量的值的类型，也就是可以为变量赋哪些值。 数值 Python 中，数值分成了三种：整数、浮点数（小数）、复数。 整数 Python中，所有的整数都是 int 类型。 Python 可以处理任意大小的整数，包括负整数，在程序中的表示方法和数学上的写法一模一样，例如：1，100，-8080，0，等等。 Python 的整数没有大小限制，而某些语言的整数根据其存储长度是有大小限制的，例如 Java 对 32 位整数的范围限制在 -2147483648 ~ 2147483647。 对于很大的数，例如 10000000000，很难数清楚 0 的个数。Python 允许在数字中间以 _ 分隔，因此，写成 10_000_000_000 和 10000000000 是完全一样的。十六进制数也可以写成 0xa1b2_c3d4。 计算机由于使用二进制，所以，有时候用十六进制表示整数比较方便，十六进制用 0x 前缀和 0 - 9，a - f 表示，例如：0xff00，0xa5b4c3d2，等等。 十进制的数，不能以 0 开头。二进制以 0b 开头，八进制以 0o 开头，十六进制以 0x 开头。 其他进制的整数，只要是数字，打印时一定是以十进制的形式显示的。 浮点数 Python 中，所有的浮点数都是 float 类型。 浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，比如，1.23 x 10^9 和 12.3 x 10^8 是完全相等的。浮点数可以用数学写法，如 1.23，3.14，-9.01，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把 10 用 e 替代，1.23 x 10^9 就是 1.23e9，或者 12.3e8，0.000012 可以写成 1.2e-5，等等。 Python 的浮点数也没有大小限制，但是超出一定范围就直接表示为 inf（无限大）。 整数和浮点数在计算机内部存储的方式是不同的，整数运算永远是精确的（除法难道也是精确的？是的！），而浮点数运算则可能会有四舍五入的误差，得到一个不精确的结果。 在Python中，有两种除法，一种除法是 /，/ 除法的计算结果是浮点数，即使是两个整数恰好整除，结果也是浮点数。 1234&gt;&gt;&gt; 10 / 33.3333333333333335&gt;&gt;&gt; 9 / 33.0 还有一种除法是 //，称为地板除，// 除法只取结果的整数部分。 1234&gt;&gt;&gt; 10 // 33&gt;&gt;&gt; 9 // 33 相对于取整运算，Python 还提供一个余数运算，可以得到两个整数相除的余数。 1234&gt;&gt;&gt; 10 % 31&gt;&gt;&gt; 9 % 30 无论整数做 // 除法还是取余数，结果永远是整数，所以，整数运算结果永远是精确的。 字符串 字符串用来表示一段文本信息，字符串是程序中使用的最多的数据类型。 字符串是以单引号 &#39; 或双引号 &quot; 括起来的任意文本（不要混用），比如 &#39;abc&#39;，&quot;xyz&quot; 等等。请注意，&#39;&#39; 或 &quot;&quot; 本身只是一种表示方式，不是字符串的一部分，因此，字符串 &#39;abc&#39; 只有 a，b，c 这 3 个字符。如果 &#39; 本身也是一个字符，那就可以用 &quot;&quot; 括起来，比如 &quot;I&#39;m OK&quot; 包含的字符是 I，&#39;，m，空格，O，K 这 6 个字符。 相同的引号之间不能嵌套。如果字符串内部既包含 &#39; 又包含 &quot;，可以用转义字符 \\ 来标识，比如 &#39;I\\&#39;m \\&quot;OK\\&quot;!&#39;。 转义字符 \\ 可以转义很多字符。比如：\\t 表示制表符，\\n 表示换行符，\\uxxxx 表示 Unicode 编码，可以打印出一些特殊的符号。 12345678&gt;&gt;&gt; print(&#x27;I\\&#x27;m ok.&#x27;)I&#x27;m ok.&gt;&gt;&gt; print(&#x27;I\\&#x27;m learning\\nPython.&#x27;)I&#x27;m learningPython.&gt;&gt;&gt; print(&#x27;\\\\\\n\\\\&#x27;)\\\\ 如果字符串里面有很多字符都需要转义，就需要加很多 \\，为了简化，Python 还允许用 r&#39;&#39; 表示 ‘ ‘ 内部的字符串默认不转义。 1234&gt;&gt;&gt; print(&#x27;\\\\\\t\\\\&#x27;)\\ \\&gt;&gt;&gt; print(r&#x27;\\\\\\t\\\\&#x27;)\\\\\\t\\\\ 如果字符串内部有很多换行，用 \\n 写在一行里不好阅读，为了简化，Python 允许用 &#39;&#39;&#39;...&#39;&#39;&#39; 或 &quot;&quot;&quot;...&quot;&quot;&quot; 的格式表示多行内容。三重引号中可以换行，并且会保留字符串中的格式。 12345678910&gt;&gt;&gt; print(&#x27;&#x27;&#x27;line1... line2... line3&#x27;&#x27;&#x27;)line1line2line3&gt;&gt;&gt; print(r&#x27;&#x27;&#x27;hello,\\n... world&#x27;&#x27;&#x27;)hello,\\nworld 上面是在交互式命令行内输入，在输入多行内容时，提示符由 &gt;&gt;&gt; 变为 ...，提示可以接着上一行输入，注意 ... 是提示符，不是代码的一部分。 格式化字符串： 拼串：字符串之间可以进行加法运算，如果将两个字符串进行相加，则会自动将两个字符串拼接为一个。 123name = &#x27;孙悟空&#x27;print(&#x27;欢迎 &#x27; + name + &#x27; 光临！&#x27;) # 欢迎 孙悟空 光临！ 多参数：字符串只能和字符串拼接，不能和其他的类型进行加法运算。print(&quot;a = &quot; + a) 这种写法在 Python 中不常见，因为 a 可能不是字符串，容易出错，常写作 print(&#39;a = &#39;, a)。 123name = &#x27;孙悟空&#x27;print(&#x27;欢迎&#x27;, name, &#x27;光临！&#x27;) # 欢迎 孙悟空 光临！ 占位符：在创建字符串时，可以在字符串中指定占位符：%s 在字符串中表示任意字符，%f 表示浮点数占位符，%d 表示整数占位符。 12345678b = &#x27;Hello %s&#x27; % &#x27;孙悟空&#x27; # 一个占位符：Hello 孙悟空b = &#x27;hello %s 你好 %s&#x27; % (&#x27;tom&#x27;, &#x27;孙悟空&#x27;) # 两个占位符：hello tom 你好 孙悟空b = &#x27;hello %3s&#x27; % &#x27;ab&#x27; # 占位符处的字符串长度至少为3，不足3的，在前面填充空格：hello abb = &#x27;hello %3.5s&#x27; % &#x27;abcdefg&#x27; # 占位符处的字符串的长度限制在3-5之间：hello abcdeb = &#x27;hello %.2f&#x27; % 123.456 # 保留2位小数，四舍五入：hello 123.46b = &#x27;hello %d&#x27; % 123.95 # 直接舍弃小数位：hello 123print(b) 格式化字符串：可以通过在字符串前添加一个 f 来创建一个格式化字符串，在格式化字符串中可以直接嵌入变量。 123456a = 123b = &#x27;呵呵&#x27;c = f&#x27;hello &#123;a&#125; &#123;b&#125;&#x27;print(f&#x27;a = &#123;a&#125;&#x27;) # a = 123print(c) # hello 123 呵呵 字符串的复制：将字符串和数字相乘。* 在语言中表示乘法，如果将字符串和数字相乘，则解释器会将字符串重复指定的次数并返回。 1234a = &#x27;abc&#x27;a = a * 5print(a) # abcabcabcabcabc 布尔值 布尔值和布尔代数的表示完全一致，一个布尔值只有 True、False 两种值，要么是 True，要么是 False，在 Python 中，可以直接用 True、False 表示布尔值（请注意大小写），也可以通过布尔运算计算出来。 12345678&gt;&gt;&gt; TrueTrue&gt;&gt;&gt; FalseFalse&gt;&gt;&gt; 3 &gt; 2True&gt;&gt;&gt; 2 &gt; 5False 布尔值可以用 and、or 和 not 运算。 and 运算是与运算，只有所有都为 True，and 运算结果才是 True： 12345678910&gt;&gt;&gt; True and TrueTrue&gt;&gt;&gt; True and FalseFalse&gt;&gt;&gt; False and TrueFalse&gt;&gt;&gt; False and FalseFalse&gt;&gt;&gt; 5 &gt; 3 and 3 &gt; 1True or 运算是或运算，只要其中有一个为 True，or 运算结果就是 True： 12345678910&gt;&gt;&gt; True or TrueTrue&gt;&gt;&gt; True or FalseTrue&gt;&gt;&gt; False or TrueTrue&gt;&gt;&gt; False or FalseFalse&gt;&gt;&gt; 5 &gt; 3 or 1 &gt; 3True not 运算是非运算，它是一个单目运算符，把 True 变成 False，False 变成 True： 123456&gt;&gt;&gt; not TrueFalse&gt;&gt;&gt; not FalseTrue&gt;&gt;&gt; not 5 &gt; 3False 布尔值经常用在条件判断中，比如： 1234if age &gt;= 18: print(&#x27;adult&#x27;)else: print(&#x27;teenager&#x27;) 布尔值实际上也属于整型，True 就相当于 1，False 就相当于 0。 12print(1 + False) # 1print(1 + True) # 2 空值 空值是 Python 里一个特殊的值，用 None 表示。None 不能理解为 0，因为 0 是有意义的，而 None 是一个特殊的空值。 类型检查 通过类型检查，可以检查变量对应的值的类型。 type() 用来检查值的类型，该函数会将检查的结果作为返回值返回，可以通过变量来接收函数的返回值。 12345678910111213a = 123 # 数值b = &#x27;123&#x27; # 字符串print(type(a)) # &lt;class &#x27;int&#x27;&gt;print(type(b)) # &lt;class &#x27;str&#x27;&gt;c = type(a)print(c) # &lt;class &#x27;int&#x27;&gt;c = type(&#x27;123&#x27;)print(c) # &lt;class &#x27;str&#x27;&gt;print(type(1)) # &lt;class &#x27;int&#x27;&gt;print(type(1.5)) # &lt;class &#x27;float&#x27;&gt;print(type(True)) # &lt;class &#x27;bool&#x27;&gt;print(type(&#x27;hello&#x27;)) # &lt;class &#x27;str&#x27;&gt;print(type(None)) # &lt;class &#x27;NoneType&#x27;&gt; 对象 Python 是一门面向对象的语言。 一切皆对象！ 程序运行当中，所有的数据都是存储到内存当中然后再运行的！ 对象就是内存中专门用来存储指定数据的一块区域。 对象实际上就是一个容器，专门用来存储数据。 像我们之前学习的数值、字符串、布尔值、None 都是对象。 对象的结构 每个对象中都要保存三种数据： id（标识） id 用来标识对象的唯一性，每一个对象都有唯一的 id。 对象的 id 就相当于人的身份证号一样。 可以通过 id() 函数来查看对象的 id。 12print(id(123)) # 140708231919200print(id(&#x27;a&#x27;)) # 2270491024176 id 是由解析器生成的，在 CPython 中，id 就是对象的内存地址。 对象一旦创建，则它的 id 永远不能再改变。 type（类型） 类型用来标识当前对象所属的类型，比如：int，str，float，bool。 类型决定了对象有哪些功能。 通过 type() 函数来查看对象的 type。 12print(type(123)) # &lt;class &#x27;int&#x27;&gt;print(type(&#x27;a&#x27;)) # &lt;class &#x27;str&#x27;&gt; Python 是一门强类型的语言，对象一旦创建类型便不能修改。 value（值） 值就是对象中存储的具体的数据。 对于有些对象，值是可以改变的。 对象分成两大类：可变对象和不可变对象。 可变对象的值可以改变。 不可变对象的值不能改变，之前学习的数值、字符串等都是不可变对象。 变量和对象 对象并没有直接存储到变量中，在 Python 中变量更像是给对象起了一个别名。 变量中存储的不是对象的值，而是对象的 id（内存地址），当我们使用变量时，实际上就是在通过对象 id 在查找对象。 变量中保存的对象，只有在为变量重新赋值时才会改变。 变量和变量之间是相互独立的，修改一个变量不会影响另一个变量。 123456789print(a) # 10print(b) # 10print(id(a)) # 140708432125984print(id(b)) # 140708432125984a = 456print(a) # 456print(b) # 10print(id(a)) # 2898418807216print(id(b)) # 140708432125984 类型转换 所谓的类型转换，是将一个类型的对象转换为其他对象。 类型转换不是改变对象本身的类型，而是根据当前对象的值来创建一个新对象。 类型转换四个函数：int()，float()，str()，bool()。 int() 可以用来将其他的对象转换为整型。 1234567891011121314151617181920212223a = Truea = int(a)print(a) # 1a = Falsea = int(a)print(a) # 0a = &#x27;123&#x27;a = int(a)print(a) # 123a = 11.6a = int(a)print(a) # 11a = &#x27;11.5&#x27;a = int(a) # ValueErrorprint(a)a = Nonea = int(a) # TypeErrorprint(a) 布尔值：True —&gt; 1，False —&gt; 0。 浮点数：直接取整，省略小数点后的内容。 字符串：如果是一个合法的整数字符串，则直接转换为对应的数字。如果不是一个合法的整数字符串，则报错 ValueError: invalid literal for int() with base 10: &#39;11.5&#39;。 对于其他不可转换为整型的对象，直接抛出异常 TypeError。 int() 函数不会对原来的变量产生影响，他是将对象转换为指定的类型并将其作为返回值返回，如果希望修改原来的变量，则需要对变量进行重新赋值。 float() 和 int() 基本一致，不同的是它会将对象转换为浮点数。 1234567891011a = 1a = float(a)print(a) # 1.0a = Falsea = float(a)print(a) # 0.0a = Truea = float(a)print(a) # 1.0 str() 可以将对象转换为字符串。 123456a = 123a = str(a)print(a) # 123b = 456print(&#x27;hello&#x27; + str(b)) # hello456 True —&gt; ‘True’。 False —&gt; ‘False’。 123 —&gt; ‘123’ 。 。。。 bool() 可以将对象转换为布尔值，任何对象都可以转换为布尔值。 1234567891011a = Nonea = bool(a)print(a) # Falsea = 0a = bool(a)print(a) # Falsea = &#x27;&#x27;a = bool(a)print(a) # False 规则：对于所有表示空性的对象都会转换为 False，其余的转换为 True。表示空性的对象：0、None、&#39;&#39; 等。 运算符（操作符） 运算符可以对一个值或多个值进行运算或各种操作。比如 +、-、= 都属于运算符。 常用运算符的分类： 算术运算符 赋值运算符 关系运算符（比较运算符） 逻辑运算符 条件运算符（三元运算符） 算术运算符12345678910111213141516171819202122232425262728293031323334353637# 加法运算符a = 10 + 5 # 15a = &#x27;hello&#x27; + &#x27; &#x27; + &#x27;world&#x27; # hello world，拼串# 减法运算符a = 10 - 5 # 5a = 5 - True # 4a = a - 2 # 2，用变量a的值减去2，然后再赋值给a# a = &#x27;hello&#x27; - &#x27;h&#x27; # TypeError# 乘法运算符a = 5 * 5 # 25# 除法运算符a = 10 / 5 # 2.0a = 5 / 2 # 2.5# a = 5 / 0 # ZeroDivisionError: division by zeroa = 10 / 3 # 3.33333...# 幂运算a = 2 ** 2 # 4a = 10 ** 5 # 100000a = 16 ** 0.5 # 4.0，求16的平方根# 整除a = 10 // 3 # 3a = 5 // 2 # 2a = 10 // -3 # -4a = -5 // 2 # -3# 取模a = 10 % 5 # 0a = 10 % 4 # 2a = 10 % 3 # 1a = 10 % 2 # 0print(&quot;a =&quot;, a) 加法运算符：+。如果是两个字符串之间进行加法运算，则会进行拼串操作。 减法运算符：-。 乘法运算符：*。如果将字符串和数字相乘，则会对字符串进行复制操作，将字符串重复指定次数。 除法运算符：/。运算时结果总会返回一个浮点类型。 幂运算：**。求一个值的几次幂。 整除：//。向下取整，只会保留计算后的整数位，总会返回一个整型。 取模：%。即取余，求两个数相除的余数。 运算时，注意正负号的问题。 在对浮点数做算术运算时，结果也会返回一个浮点数。 12b = 25.0 / 5print(b) # 5.0 赋值运算符 赋值运算符 = 可以将等号右侧的值赋值给等号左侧的变量。 +=：a += 5 相当于 a = a + 5。 -=：a -= 5 相当于 a = a - 5。 *=：a *= 5 相当于 a = a * 5。 /=：a /= 5 相当于 a = a / 5。 **=：a **= 5 相当于 a = a ** 5。 //=：a //= 5 相当于 a = a // 5。 %=：a %= 5 相当于 a = a % 5。 关系运算符（比较运算符）12345678910111213141516171819202122232425262728result = 10 &gt; 20 # Falseresult = 10 &gt;= 10 # Trueresult = 2 &gt; True # True# result = 2 &gt; &#x27;1&#x27; # TypeError: &#x27;&gt;&#x27; not supported between instances of &#x27;int&#x27; and &#x27;str&#x27;result = 30 &lt; 20 # False# 在Python中可以对两个字符串进行大于（等于）或小于（等于）的运算，# 当对字符串进行比较时，实际上比较的是字符串的Unicode编码# 比较两个字符串的Unicode编码时，是逐位比较的，比较出现结果时，直接返回，后续不再比较# 利用该特性可以对字符串按照字母顺序进行排序，但是对于中文来说意义不是特别大# 注意：如果不希望比较两个字符串的Unicode编码，则需要将其转换为数字然后再比较result = &#x27;2&#x27; &gt; &#x27;1&#x27; # True，2的unicode编码为0032，1的unicode编码为0031result = &#x27;2&#x27; &gt; &#x27;11&#x27; # True，逐位比较，先拿2和1比，结果为True，直接返回，后续不再比较result = &#x27;a&#x27; &gt; &#x27;b&#x27; # False，a的unicode编码为0061，b的unicode编码为0062result = &#x27;c&#x27; &lt; &#x27;d&#x27; # Trueresult = &#x27;ab&#x27; &gt; &#x27;b&#x27; # False，逐位比较，先拿a和b比，结果为False，直接返回，后续不再比较print(int(&#x27;2&#x27;) &gt; int(&#x27;11&#x27;)) # Falseresult = 1 == 1 # Trueresult = &#x27;hello&#x27; == &#x27;hello&#x27; # Trueresult = &#x27;abc&#x27; == &#x27;bcd&#x27; # Falseresult = &#x27;abc&#x27; != &#x27;bcd&#x27; # Trueresult = 1 == True # Trueresult = 1 is True # Falseresult = 1 is not True # Trueprint(id(1), id(True)) # 140708473153312 140708472870736 关系运算符用来比较两个值之间的关系，总会返回一个布尔值。如果关系成立，返回 True，否则返回 False。 &gt;：比较左侧值是否大于右侧值。 &gt;=：比较左侧的值是否大于或等于右侧的值。 &lt;：比较左侧值是否小于右侧值。 &lt;=：比较左侧的值是否小于或等于右侧的值。 ==：比较两个对象的值是否相等，比较的是对象的值。 !=：比较两个对象的值是否不相等，比较的是对象的值。 is：比较两个对象是否是同一个对象，比较的是对象的 id。 is not：比较两个对象是否不是同一个对象，比较的是对象的 id。 逻辑运算符 逻辑运算符主要用来做一些逻辑判断。 and：逻辑与。 and 可以对符号两侧的值进行与运算。 只有在符号两侧的值都为 True 时，才会返回 True，只要有一个 False 就返回 False。 Python 中的与运算是短路的与，如果第一个值为 False，则不再看第二个值，直接返回 False。 12True and print(&#x27;男：你猜我出来吗？&#x27;) # 第一个值是True，会看第二个值，所以print()会执行False and print(&#x27;女：你猜我出来吗？&#x27;) # 第一个值是False，不会看第二个值，所以print()不会执行 or：逻辑或。 or 可以对符号两侧的值进行或运算。 或运算两个值中只要有一个 True，就会返回 True。 Python 中的或运算是短路的或，如果第一个值为 True，则不再看第二个值，直接返回 True。 12False or print(&#x27;男：你猜我出来吗？&#x27;) # 第一个值为False，继续看第二个，所以打印语句执行True or print(&#x27;女：你猜我出来吗？&#x27;) # 第一个值为True，不看第二个，所以打印语句不执行 not：逻辑非。 not 可以对符号右侧的值进行非运算。 对于布尔值，非运算会对其进行取反操作，True 变 False，False 变 True。 对于非布尔值，非运算会先将其转换为布尔值，然后再取反。 非布尔值的与或运算：当我们对非布尔值进行与或运算时，Python 会将其当做布尔值运算，最终会返回原值。 与运算的规则： 1234567891011# True and Trueresult = 1 and 2 # 2# True and Falseresult = 1 and 0 # 0# False and Trueresult = 0 and 1 # 0# False and Falseresult = None and 0 # None 与运算是找 False 的，如果第一个值是 False，则不看第二个值。 如果第一个值是 False，则直接返回第一个值，否则返回第二个值。 或运算的规则： 1234567891011# True or Trueresult = 1 or 2 # 1# True or Falseresult = 1 or 0 # 1# False or Trueresult = 0 or 1 # 1# False or Falseresult = None or 0 # 0 或运算是找 True 的，如果第一个值是 True，则不看第二个值。 如果第一个值是 True，则直接返回第一个值，否则返回第二个值。 条件运算符（三元运算符）1234567a = 30b = 50print(&#x27;a的值比较大！&#x27;) if a &gt; b else print(&#x27;b的值比较大！&#x27;)# 获取a和b之间的较大值maxNum = a if a &gt; b else bprint(maxNum) 语法：语句1 if 条件表达式 else 语句2。 na = input(‘请输入任意内容：’)print(‘用户输入的内容是:’, a) 获取用户输入的用户名username = input(‘请输入你的用户名: ‘) 判断用户名是否是adminif username == ‘admin’: print(&#39;欢迎管理员光临！&#39;) 该函数用来获取用户的输入。 input() 调用后，程序会立即暂停，等待用户输入，用户输入完内容以后，点击回车程序才会继续向下执行。用户输入完成以后，其所输入的的内容会以返回值的形式返回。 注意：input() 的返回值是一个字符串。 input() 函数中可以设置一个字符串作为参数，这个字符串将会作为提示文字显示。 input() 也可以用于暂时阻止程序结束。 Python 流程控制语句 Python代码在执行时是按照自上向下顺序执行的。 通过流程控制语句，可以改变程序的执行顺序，也可以让指定的程序反复执行多次。 流程控制语句分成两大类：条件判断语句，循环语句。 条件判断语句（if 语句）123456789num = 10if num &gt; 10: print(&#x27;num比10大！&#x27;)if num &gt; 10 and num &lt; 20: print(&#x27;num比10大, num比20小！&#x27;)if 10 &lt; num &lt; 20: print(&#x27;num比10大, num比20小！&#x27;) 语法： 12if 条件表达式: 代码块 执行的流程：if 语句在执行时，会先对条件表达式进行求值判断，如果为 True，则执行 if 后的语句；如果为 False，则不执行。 默认情况下，if 语句只会控制紧随其后的那条语句，如果希望 if 可以控制多条语句，则可以在 if 后跟着一个代码块。 代码块： 代码块中保存着一组代码，同一个代码块中的代码，要么都执行要么都不执行。 代码块就是一种为代码分组的机制。 如果要编写代码块，语句就不能紧随在 : 后边，而是要写在下一行。 代码块以缩进开始，直到代码恢复到之前的缩进级别时结束。 缩进有两种方式，一种是使用 tab 键，一种是使用空格（四个）。 Python 代码中使用的缩进方式必须统一。 Python 的官方文档中推荐我们使用空格来缩进。 可以使用逻辑运算符来连接多个条件，如果希望所有条件同时满足，则需要使用 and；如果希望只要有一个条件满足即可，则需要使用 or。 if - else123456789101112age = 7if age &gt; 17: print(&#x27;你已经成年了~~&#x27;)else: print(&#x27;你还未成年~~&#x27;)# 如果一个年份可以被4整除不能被100整除，或者可以被400整除，这个年份就是闰年year = int(input(&#x27;请输入一个任意的年份:&#x27;))if year % 4 == 0 and year % 100 != 0 or year % 400 == 0: print(year, &#x27;是闰年&#x27;)else: print(year, &#x27;是平年&#x27;) 语法： 1234if 条件表达式: 代码块1else: 代码块2 执行流程：if - else 语句在执行时，先对 if 后的条件表达式进行求值判断，如果为 True，则执行 if 后的代码块 1；如果为 False，则执行 else 后的代码块 2。 if - elif - else1234567891011121314151617181920212223age = 210if age &gt; 200: print(&#x27;活的可真久！&#x27;)elif age &gt; 100: print(&#x27;你也是老大不小了！&#x27;)elif age &gt;= 60: print(&#x27;你已经退休了！&#x27;)elif age &gt;= 30: print(&#x27;你已经是中年了！&#x27;)elif age &gt;= 18: print(&#x27;你已经成年了！&#x27;)else: print(&#x27;你还是个小孩！&#x27;)age = 68if 18 &lt;= age &lt; 30: print(&#x27;你已经成年了！&#x27;)elif 30 &lt;= age &lt; 60: print(&#x27;你已经中年了！&#x27;)else: print(&#x27;你已经退休了！&#x27;) 语法： 12345678if 条件表达式: 代码块elif 条件表达式: 代码块elif 条件表达式: 代码块else: 代码块 执行流程：if - elif - else 语句在执行时，会自上向下依次对条件表达式进行求值判断，如果表达式的结果为 True，则执行当前代码块，然后语句结束；如果表达式的结果为 False，则继续向下判断，直到找到 True 为止；如果所有的表达式都是 False，则执行 else 后的代码块。 if - elif - else 中只会有一个代码块会执行。 循环语句 循环语句可以使指定的代码块重复指定的次数。 循环语句分成两种，while 循环和 for 循环。 while 循环123456789101112131415161718192021222324252627282930# 创建一个执行十次的循环i = 0while i &lt; 10: i += 1 print(i, &#x27;hello&#x27;)else: print(&#x27;else中的代码块&#x27;) # 水仙花数是指一个 n 位数（n≥3 ），它的每个位上的数字的 n 次幂之和等于它本身（例如：1**3 + 5**3 + 3**3 = 153）。# 求1000以内所有的水仙花数# 获取1000以内的三位数i = 100while i &lt; 1000: # 假设，i的百位数是a，十位数b，个位数c # 求i的百位数 a = i // 100 # 求i的十位数 # b = i // 10 % 10 b = (i - a * 100) // 10 # 求i的个位数字 c = i % 10 # print(i , a , b , c) # 判断i是否是水仙花数 if a**3 + b**3 + c**3 == i : print(i) i += 1 语法： 1234while 条件表达式: 代码块else: 代码块 执行流程：while 语句在执行时，会先对 while 后的条件表达式进行求值判断，如果判断结果为 True，则执行循环体（代码块），循环体执行完毕，继续对条件表达式进行求值判断，以此类推，直到判断结果为 False，则循环终止，如果循环有对应的 else，则执行 else 后的代码块。 条件表达式恒为 True 的循环语句，称为死循环。 循环的三个要件： 初始化表达式：通过初始化表达式初始化一个变量。 条件表达式：条件表达式用来设置循环执行的条件。 更新表达式：修改初始化变量的值。 循环嵌套 打印图形： 123456789101112131415161718192021222324252627282930313233343536373839404142# 在控制台中打印如下图形# *****# *****# *****# *****# *****## 创建一个循环来控制图形的高度# 循环嵌套时，外层循环没执行一次，内存循环就要执行一圈i = 0while i &lt; 5: # 创建一个内层循环来控制图形的宽度 j = 0 while j &lt; 5: # print()默认在结尾打印\\n换行符，添加end参数，打印时不要换行 print(&quot;* &quot;, end=&#x27;&#x27;) j += 1 # 每一行打印完毕后，再打印一个换行符 print() i += 1## * j&lt;1 i=0# ** j&lt;2 i=1# *** j&lt;3 i=2# **** j&lt;4 i=3# ***** j&lt;5 i=4## *****# ****# ***# **# *i = 0while i &lt; 5: j = 0 while j &lt; i + 1: print(&quot;* &quot;, end=&#x27;&#x27;) j += 1 print() i += 1 99 乘法表： 123456789101112131415161718# 打印99乘法表# 1*1=1# 1*2=2 2*2=4# 1*3=3 2*3=6 3*3=9# ... 9*9=81# 创建一个外层循环来控制图形的高度i = 0while i &lt; 9: i += 1 # 创建一个内层循环来控制图形的宽度 j = 0 while j &lt; i: j += 1 print(f&quot;&#123;j&#125;*&#123;i&#125;=&#123;i*j&#125; &quot;,end=&quot;&quot;) print() break 和 continue123456789101112131415161718192021i = 0while i &lt; 5: if i == 3: break print(i) # 结束循环，结果：0 1 2 i += 1else: print(&#x27;循环结束&#x27;)i = 0while i &lt; 5: i += 1 if i == 2: continue # 跳出当前循环，结果：1 3 4 5 循环结束 print(i)else: print(&#x27;循环结束&#x27;)i = 0if i &lt; 5: pass break 可以用来立即退出循环语句（包括 else）。 continue 可以用来跳过当次循环。 break 和 continue 都是只对离他最近的循环起作用。 pass 是用来在判断或循环语句中占位的，无实际意义。如果循环体内容没想好怎么写，可以先用 pass 占位，这样不会影响程序执行。 模块引入123456789101112131415161718192021222324252627282930313233343536373839404142434445# 模块，通过模块可以对Python进行扩展# 引入一个time模块，来统计程序执行的时间from time import *# time()函数可以用来获取当前的时间，返回的单位是秒# 获取程序开始的时间# 优化前：# 10000个数 12.298秒# 100000个数 没有结果# 第一次优化：加break# 10000个数 1.577秒# 100000个数 170.645秒# 第二次优化：循环到根号为止# 10000个数 0.068秒# 100000个数 1.646秒## 36的因数# 2 18# 3 12# 4 9# 6 6# begin = time()i = 2while i &lt;= 100000: flag = True j = 2 while j &lt;= i ** 0.5: if i % j == 0: flag = False # 一旦进入判断，则证明i一定不是质数，此时内层循环没有继续执行的必要 # 使用break来退出内层的循环 break j += 1 if flag: # print(i) pass i += 1# 获取程序结束的时间end = time()# 计算程序执行的时间print(&quot;程序执行花费了：&quot;, end - begin, &quot;秒&quot;) for 循环 语法： 12for 变量 in 序列 : 代码块 for 循环除了创建方式以外，其余的都和 while 循环一样，包括 else、break、continue 都可以在 for 循环中使用。 Python 数据结构序列（sequence） 计算机中数据存储的方式叫数据结构，序列是 Python 中最基本的一种数据结构。 序列用于保存一组有序的数据，所有的数据在序列当中都有一个唯一的位置（索引），并且序列中的数据会按照添加的顺序来分配索引。 序列存储的数据，称为元素。 序列的分类： 可变序列（序列中的元素可以改变） 列表（list） 不可变序列（序列中的元素不能改变） 元组（tuple） 字符串（str） range() 函数：用来生成一个自然数的序列。 123456789101112131415# range()是一个函数，可以用来生成一个自然数的序列# 该函数需要三个参数# 1.起始位置（包含，可以省略，默认是0）# 2.结束位置（不包含）# 3.步长（可以省略，默认是1）r = range(5)print(list(r)) # [0, 1, 2, 3, 4]r = range(0, 10, 2)print(list(r)) # [0, 2, 4, 6, 8]r = range(10, 0, -1)print(list(r)) # [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]# 通过range()可以创建一个执行指定次数的for循环for i in range(30): print(i) 通用操作 + 可以将两个序列拼接为一个序列。 * 可以将序列重复指定的次数。 in 用来检查指定元素是否存在于序列中，如果存在，返回 True，否则返回 False。 not in 用来检查指定元素是否不在序列中，如果不在，返回 True，否则返回 False。 len() 函数获取序列的长度，即序列中的元素的个数。该长度值，是序列的最大索引加 1。 min() 函数获取序列中的最小值。 max() 函数获取序列中的最大值。 可以通过索引（index）来获取序列中的元素： 语法：序列[索引]。 索引是元素在序列中的位置，序列中的每一个元素都有一个索引。 索引是从 0 开始的整数，序列第一个位置索引为 0，第二个位置索引为 1，第三个位置索引为 2，以此类推。 索引可以是负数，表示从后向前获取元素，-1 表示倒数第一个元素，-2 表示倒数第二个元素，以此类推。 如果使用的索引超过了序列最大的范围，会抛出异常 IndexError: list index out of range。 s.index() 方法获取指定元素在序列中的第一次出现时索引。 方法和函数基本上是一样，只不过方法必须通过 对象.方法() 的形式调用，方法实际上就是和对象关系紧密的函数。 index() 的第二个参数，表示查找的起始位置，第三个参数，表示查找的结束位置。 如果要获取序列中没有的元素，会抛出异常。 s.count() 方法统计指定元素在序列中出现的次数。 切片 切片指从现有序列中，获取一个子序列。 语法一：序列[起始:结束]。 通过切片获取元素时，会包括起始位置的元素，不会包括结束位置的元素。 做切片操作时，总会返回一个新的序列，但不会影响原来的序列。 起始和结束位置的索引都可以省略不写。 如果省略起始位置，则会从第一个元素开始截取。 如果省略结束位置，则会一直截取到最后。 如果起始位置和结束位置全部省略，则相当于创建了一个序列的副本。 语法二：序列[起始:结束:步长]。 步长表示，每次获取元素的间隔，默认值是 1。 步长不能是 0，但可以是负数。 步长如果是负数，则从序列的后边向前边取元素。 分类列表（list） 列表是 Python 中的一个对象。 对象（object）就是内存中专门用来存储数据的一块区域，之前我们学习的对象，像数值，它只能保存一个单一的数据。 列表是用来存储对象的对象，列表中可以保存多个有序的数据。 列表中可以保存任意的对象，但一般不会这样操作，尽可能保证列表中元素属性一致。 列表的创建 1234567891011121314my_list = [] # 创建了一个空列表print(my_list, type(my_list)) # [] &lt;class &#x27;list&#x27;&gt;my_list = [10] # 创建一个只包含一个元素的列表my_list = [10, 20, 30, 40, 50] # 创建了一个包含有5个元素的列表# my_list = [10, &#x27;hello&#x27;, True, None, [1, 2, 3], print] # 列表可以保存任意对象，但一般不会这样操作print(my_list[4]) # 50print((my_list[-2])) # 40print(len(my_list)) # 5 使用 [] 来创建列表。 一个列表中可以存储多个元素，也可以在创建列表时，来指定列表中的元素。 当向列表中添加多个元素时，多个元素之间使用 , 隔开。 列表中的对象都会按照插入的顺序存储到列表中，第一个插入的对象保存到第一个位置，第二个保存到第二个位置，以此类推。 列表的通用操作 1234567891011121314151617my_list = [1, 2, 3] + [4, 5, 6]print(len(my_list)) # 6my_list = [1, 2, 3] * 5print(len(my_list)) # 15stus = [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;, &#x27;沙和尚&#x27;, &#x27;沙和尚&#x27;]print(&#x27;牛魔王&#x27; in stus) # Falseprint(&#x27;牛魔王&#x27; not in stus) # Truearr = [10, 1, 2, 5, 100, 77]print(min(arr), max(arr)) # 1 100print(stus.index(&#x27;沙和尚&#x27;)) # 2print(stus.index(&#x27;沙和尚&#x27;, 3, 7)) # 6# print(stus.index(&#x27;牛魔王&#x27;)) # ValueError: &#x27;牛魔王&#x27; is not in listprint(stus.count(&#x27;牛魔王&#x27;)) # 0 列表的切片操作 123456789stus = [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]print(stus[1:]) # [&#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]print(stus[:3]) # [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;]print(stus[:]) # [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]print(stus) # [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]print(stus[0:5:3]) # [&#x27;孙悟空&#x27;, &#x27;唐僧&#x27;]# print(stus[::0]) # ValueError: slice step cannot be zeroprint(stus[::-1]) # [&#x27;白骨精&#x27;, &#x27;蜘蛛精&#x27;, &#x27;唐僧&#x27;, &#x27;沙和尚&#x27;, &#x27;猪八戒&#x27;, &#x27;孙悟空&#x27;]，列表反转 列表元素的修改 1234567891011121314151617181920212223242526272829303132333435363738stus = [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]# 通过索引来修改元素stus[0] = &#x27;sunwukong&#x27;stus[2] = &#x27;哈哈&#x27;print(stus) # [&#x27;sunwukong&#x27;, &#x27;猪八戒&#x27;, &#x27;哈哈&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]# 通过del来删除元素del stus[2] # 删除索引为2的元素print(stus) # [&#x27;sunwukong&#x27;, &#x27;猪八戒&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]# 通过切片来修改列表# 在给切片进行赋值时，只能使用序列stus[0:2] = [&#x27;牛魔王&#x27;, &#x27;红孩儿&#x27;] # 使用新的元素替换旧元素print(stus) # [&#x27;牛魔王&#x27;, &#x27;红孩儿&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]stus[0:2] = [&#x27;牛魔王&#x27;, &#x27;红孩儿&#x27;, &#x27;二郎神&#x27;, &quot;sda&quot;] # 新元素的个数可以超过旧元素print(stus) # [&#x27;牛魔王&#x27;, &#x27;红孩儿&#x27;, &#x27;二郎神&#x27;, &#x27;sda&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]stus[0:0] = [&#x27;牛魔王&#x27;] # 向索引为0的位置插入元素print(stus) # [&#x27;牛魔王&#x27;, &#x27;牛魔王&#x27;, &#x27;红孩儿&#x27;, &#x27;二郎神&#x27;, &#x27;sda&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]# 当设置了步长时，序列中元素的个数必须和切片中元素的个数一致print(stus[::2]) # [&#x27;牛魔王&#x27;, &#x27;红孩儿&#x27;, &#x27;sda&#x27;, &#x27;蜘蛛精&#x27;]，指定步长，切片中元素个数为4# stus[::2] = [&#x27;牛魔王&#x27;, &#x27;红孩儿&#x27;, &#x27;二郎神&#x27;] # 报错，序列中元素只有3个，ValueError: attempt to assign sequence of size 3 to extended slice of size 4# 通过切片来删除元素del stus[0:2] # 删除头两个元素print(stus) # [&#x27;红孩儿&#x27;, &#x27;二郎神&#x27;, &#x27;sda&#x27;, &#x27;唐僧&#x27;, &#x27;蜘蛛精&#x27;, &#x27;白骨精&#x27;]del stus[::2] # 隔一个删一个print(stus) # [&#x27;二郎神&#x27;, &#x27;唐僧&#x27;, &#x27;白骨精&#x27;]stus[1:3] = [] # 修改位置1和2的元素为空print(stus) # [&#x27;二郎神&#x27;]# 以上操作，只适用于可变序列s = &#x27;hello&#x27;# s[1] = &#x27;a&#x27; # 不可变序列，无法通过索引来修改# 可以通过list()函数将其他的序列转换为lists = list(s)print(s) # [&#x27;h&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;] 列表的方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657stus = [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;]# append() # 向列表的最后添加一个元素stus.append(&#x27;唐僧1&#x27;)print(stus) # [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;唐僧1&#x27;]# insert()# 向列表的指定位置插入一个元素# 参数：# 1.要插入的位置# 2.要插入的元素stus.insert(2, &#x27;唐僧2&#x27;)print(stus) # [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;唐僧2&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;唐僧1&#x27;]# extend()# 使用新的序列来扩展当前序列# 需要一个序列作为参数，它会将该序列中的元素添加到当前列表中stus.extend([&#x27;唐僧3&#x27;, &#x27;白骨精&#x27;]) # 等同于：stus += [&#x27;唐僧3&#x27;,&#x27;白骨精&#x27;]print(stus) # [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;唐僧2&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;唐僧1&#x27;, &#x27;唐僧3&#x27;, &#x27;白骨精&#x27;]# pop()# 根据索引删除并返回被删除的元素result = stus.pop(2)print(result) # 唐僧2print(stus) # [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;唐僧1&#x27;, &#x27;唐僧3&#x27;, &#x27;白骨精&#x27;]result = stus.pop() # 删除最后一个元素print(result) # 白骨精print(stus) # [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;唐僧1&#x27;, &#x27;唐僧3&#x27;]# remove()# 删除指定值的元素，如果相同值的元素有多个，只会删除第一个stus.remove(&#x27;猪八戒&#x27;)print(stus) # [&#x27;孙悟空&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;唐僧1&#x27;, &#x27;唐僧3&#x27;]# reverse()# 用来反转列表stus.reverse()print(stus) # [&#x27;唐僧3&#x27;, &#x27;唐僧1&#x27;, &#x27;唐僧&#x27;, &#x27;沙和尚&#x27;, &#x27;孙悟空&#x27;]# clear()# 清空序列stus.clear()print(stus) # []# sort()# 用来对列表中的元素进行排序，默认是升序排列# 如果需要降序排列，则需要传递一个reverse=True作为参数my_list = list(&#x27;asnbdnbasdabd&#x27;)my_list.sort()print(my_list) # [&#x27;a&#x27;, &#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;b&#x27;, &#x27;b&#x27;, &#x27;d&#x27;, &#x27;d&#x27;, &#x27;d&#x27;, &#x27;n&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;s&#x27;]my_list = [10, 1, 20, 3, 4, 5, 0, -2]my_list.sort()print(my_list) # 升序：[-2, 0, 1, 3, 4, 5, 10, 20]my_list.sort(reverse=True)print(my_list) # 降序：[20, 10, 5, 4, 3, 1, 0, -2] 列表的遍历 12345678910111213141516171819202122232425# 遍历列表，指的就是将列表中的所有元素取出来# 创建列表stus = [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;, &#x27;唐僧&#x27;, &#x27;白骨精&#x27;, &#x27;蜘蛛精&#x27;]# 遍历列表# print(stus[0])# print(stus[1])# print(stus[2])# print(stus[3])# 通过while循环来遍历列表i = 0while i &lt; len(stus): print(stus[i]) i += 1# 通过for循环来遍历列表# 语法：# for 变量 in 序列 :# 代码块# for循环的代码块会执行多次，序列中有几个元素就会执行几次# 每执行一次就会将序列中的一个元素赋值给变量，# 所以我们可以通过变量，来获取列表中的元素for s in stus: print(s) 元组（tuple）123456789101112131415161718192021222324252627282930313233343536373839404142434445my_tuple = () # 创建了一个空元组print(my_tuple, type(my_tuple)) # () &lt;class &#x27;tuple&#x27;&gt;my_tuple = (1, 2, 3, 4, 5) # 创建了一个5个元素的元组# 元组是不可变对象，不能尝试为元组中的元素重新赋值# my_tuple[3] = 10 # TypeError: &#x27;tuple&#x27; object does not support item assignment# print(my_tuple[3])# 当元组不是空元组时，括号可以省略# 如果元组不是空元组，它里边至少要有一个,my_tuple = 10, 20, 30, 40print(my_tuple, type(my_tuple)) # (10, 20, 30, 40) &lt;class &#x27;tuple&#x27;&gt;my_tuple = 40,print(my_tuple, type(my_tuple)) # (40,) &lt;class &#x27;tuple&#x27;&gt;my_tuple = 10, 20, 30, 40# 元组的解包（解构）# 解包指的是将元组当中每一个元素都赋值给一个变量a, b, c, d = my_tupleprint(&quot;a =&quot;, a) # a = 10print(&quot;b =&quot;, b) # b = 20print(&quot;c =&quot;, c) # c = 30print(&quot;d =&quot;, d) # d = 40# 利用元组的解包特性，可以直接交换a和b的值a = 100b = 300a, b = b, a # b, a 就是一个元组，通过解包赋值给 a, bprint(a, b) # 300 100my_tuple = 10, 20, 30, 40# 在对一个元组进行解包时，变量的数量必须和元组中的元素的数量一致# 也可以在变量前边添加一个*，这样变量将会获取元组中所有剩余的元素a, b, *c = my_tuplea, *b, c = my_tuple*a, b, c = my_tuplea, b, *c = [1, 2, 3, 4, 5, 6, 7] # 对列表也可以解包a, b, *c = &#x27;hello world&#x27; # 对字符串也可以解包# 不能同时出现两个或以上的*变量# *a, *b, c = my_tuple # SyntaxError: two starred expressions in assignmentprint(&#x27;a =&#x27;, a) # a = hprint(&#x27;b =&#x27;, b) # b = eprint(&#x27;c =&#x27;, c) # c = [&#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;, &#x27; &#x27;, &#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;l&#x27;, &#x27;d&#x27;] 元组是一个不可变的序列，它的操作的方式基本上和列表是一致的。在操作元组时，可以把元组当成是一个不可变的列表。 一般当我们希望数据不改变时，就使用元组，其余情况都使用列表。 使用 () 来创建元组。 元组是不可变对象，不能尝试为元组中的元素重新赋值。 当元组不是空元组时，括号可以省略。如果元组不是空元组，它里边至少要有一个 ,。 元组的解包（解构）：指的是将元组当中每一个元素都赋值给一个变量。利用元组的解包特性，可以直接交换 a 和 b 的值。说明：不光是元组，列表和字符串都能解包。 在对一个元组进行解包时，变量的数量必须和元组中的元素的数量一致。也可以在变量前边添加一个 *，这样该变量将会获取元组中所有剩余的元素，但不能同时出现两个或以上的带 * 变量。 可变对象说明1234567891011121314151617181920212223# 列表是可变对象a = [1, 2, 3]print(&#x27;修改前：&#x27;, a, id(a)) # 修改前： [1, 2, 3] 1322105181888# 通过索引修改列表的值a[0] = 10print(&#x27;修改后：&#x27;, a, id(a)) # 修改后： [10, 2, 3] 1322105181888# 为变量重新赋值a = [4, 5, 6]print(&#x27;重新赋值：&#x27;, a, id(a)) # 重新赋值： [4, 5, 6] 1322105180928a = [1, 2, 3]b = a # a和b指向同一个对象print(&quot;a&quot;, a, id(a)) # a [1, 2, 3] 2222727763648print(&quot;b&quot;, b, id(b)) # b [1, 2, 3] 2222727763648b[0] = 10print(&quot;a&quot;, a, id(a)) # a [10, 2, 3] 2222727763648print(&quot;b&quot;, b, id(b)) # b [10, 2, 3] 2222727763648b = [10, 2, 3] # b和a指向的不再是同一个对象print(&quot;a&quot;, a, id(a)) # a [10, 2, 3] 2222727763648print(&quot;b&quot;, b, id(b)) # b [10, 2, 3] 2222727762688 每个对象中都保存了三个数据：id（标识），type（类型）和 value（值）。 列表就是一个可变对象，比如 a = [1, 2, 3]。 a[0] = 10：改对象。 这个操作是在通过变量去修改对象的值。 这种操作不会改变变量所指向的对象。 当我们去修改对象时，如果有其他变量也指向了该对象，则修改也会在其他的变量中体现。 a = [4, 5, 6]：改变量。 这个操作是在给变量重新赋值。 这种操作会改变变量所指向的对象。 为一个变量重新赋值时，不会影响其他的变量。 一般只有在为变量赋值时才是修改变量，其余的都是修改对象。 比较符123456a = [1, 2, 3]b = [1, 2, 3]print(a, b) # [1, 2, 3] [1, 2, 3]print(id(a), id(b)) # 2081830016896 2081830015936print(a == b) # a和b的值相等，使用==会返回Trueprint(a is b) # a和b不是同一个对象，内存地址不同，使用is会返回False == 和 != 比较的是对象的值是否相等。 is 和 is not 比较的是对象的 id 是否相等（比较两个对象是否是同一个对象）。 字典（dict）123456789101112131415d = &#123;&#125; # 创建了一个空字典# 创建一个包含有数据的字典d = &#123; &#x27;name&#x27;: &#x27;孙悟空&#x27;, &#x27;age&#x27;: 18, &#x27;gender&#x27;: &#x27;男&#x27;&#125;print(d, type(d)) # &#123;&#x27;name&#x27;: &#x27;孙悟空&#x27;, &#x27;age&#x27;: 18, &#x27;gender&#x27;: &#x27;男&#x27;&#125; &lt;class &#x27;dict&#x27;&gt;# 根据键来获取值print(d[&#x27;name&#x27;], d[&#x27;age&#x27;], d[&#x27;gender&#x27;]) # 孙悟空 18 男# 如果使用了字典中不存在的键，会报错# print(d[&#x27;hello&#x27;]) # KeyError: &#x27;hello&#x27; 字典属于一种新的数据结构，称为映射（mapping）。 字典的作用和列表类似，都是用来存储对象的容器。 列表存储数据的性能很好，但是查询数据的性能的很差。 在字典中每一个元素都有一个唯一的名字，通过这个唯一的名字可以快速的查找到指定的元素。 在查询元素时，字典的效率是非常快的。 在字典中可以保存多个对象，每个对象都会有一个唯一的名字。 这个唯一的名字，我们称其为键（key），通过 key 可以快速的查询 value。 这个对象，我们称其为值（value）。 所以字典，我们也称为键值对（key - value）结构。 每个字典中都可以有多个键值对，而每一个键值对我们称其为一项（item）。 使用 &#123;&#125; 来创建字典。 创建一个包含有数据的字典：&#123;key: value, key: value, key: value&#125;。 字典的键可以是任意的不可变对象（int、str、bool、tuple …），但是一般我们都会使用 str。 字典的值可以是任意对象。 字典的键是不能重复的，如果出现重复，后边的会替换前边的。 字典的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# 创建字典# 方式一：使用&#123;&#125;# 语法：&#123;k1: v1, k2: v2, k3: v3&#125;# 方式二：使用dict()函数来创建字典# 每一个参数都是一个键值对，参数名就是键，参数名就是值（这种方式创建的字典，key都是字符串）d = dict(name=&#x27;孙悟空&#x27;, age=18, gender=&#x27;男&#x27;)# 也可以将一个包含有双值子序列的序列转换为字典# 双值序列：序列中只有两个值，比如[1, 2]，(&#x27;a&#x27;, 3)，&#x27;ab&#x27;# 子序列：如果序列中的元素也是序列，那么我们就称这个元素为子序列，比如[(1, 2), (3, 5)]d = dict([(&#x27;name&#x27;, &#x27;孙悟空&#x27;), (&#x27;age&#x27;, 18)])####################################################################d = dict(name=&#x27;孙悟空&#x27;, age=18, gender=&#x27;男&#x27;)# len()获取字典中键值对的个数print(len(d)) # 3# in 检查字典中是否包含指定的键# not in 检查字典中是否不包含指定的键print(&#x27;hello&#x27; in d) # Falseprint(&#x27;age&#x27; not in d) # False# 获取字典中的值，根据键来获取值# 方式一：语法：d[key]# 通过[]来获取值时，如果键不存在，会抛出异常KeyErrorprint(d[&#x27;age&#x27;]) # 18n = &#x27;name&#x27;print(d[n]) # 孙悟空# 方式二：get(key[, default]) 该方法用来根据键来获取字典中的值# 如果获取的键在字典中不存在，会返回None，不会报错# 也可以指定一个默认值，来作为第二个参数，这样获取不到值时将会返回默认值print(d.get(&#x27;name&#x27;)) # 孙悟空print(d.get(&#x27;hello&#x27;)) # Noneprint(d.get(&#x27;hello&#x27;, &#x27;默认值&#x27;)) # 默认值# 修改字典# d[key] = value 如果key存在则覆盖，不存在则添加d[&#x27;name&#x27;] = &#x27;sunwukong&#x27; # 修改字典的key-valued[&#x27;address&#x27;] = &#x27;花果山&#x27; # 向字典中添加key-value# setdefault(key[, default]) 可以用来向字典中添加key-value# 如果key已经存在于字典中，则返回key的值，不会对字典做任何操作# 如果key不存在，则向字典中添加这个key，并设置valueresult = d.setdefault(&#x27;name&#x27;, &#x27;猪八戒&#x27;)print(&#x27;result =&#x27;, result) # result = sunwukongresult = d.setdefault(&#x27;hello&#x27;, &#x27;猪八戒&#x27;)print(d) # &#123;&#x27;name&#x27;: &#x27;sunwukong&#x27;, &#x27;age&#x27;: 18, &#x27;gender&#x27;: &#x27;男&#x27;, &#x27;address&#x27;: &#x27;花果山&#x27;, &#x27;hello&#x27;: &#x27;猪八戒&#x27;&#125;# update([other])# 将其他的字典中的key-value添加到当前字典中# 如果有重复的key，则后边的会替换到当前的d = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125;d2 = &#123;&#x27;d&#x27;: 4, &#x27;e&#x27;: 5, &#x27;f&#x27;: 6, &#x27;a&#x27;: 7&#125;d.update(d2)print(d) # &#123;&#x27;a&#x27;: 7, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3, &#x27;d&#x27;: 4, &#x27;e&#x27;: 5, &#x27;f&#x27;: 6&#125;# 删除，可以使用del来删除字典中的key-valuedel d[&#x27;a&#x27;]del d[&#x27;b&#x27;]print(d) # &#123;&#x27;c&#x27;: 3, &#x27;d&#x27;: 4, &#x27;e&#x27;: 5, &#x27;f&#x27;: 6&#125;# del d[&#x27;z&#x27;] # z不存在，报错，KeyError: &#x27;z&#x27;# popitem()# 随机删除字典中的一个键值对，一般都会删除最后一个键值对# 删除之后，它会将删除的key-value作为返回值返回# 返回的是一个元组，元组中有两个元素，第一个元素是删除的key，第二个是删除的value# 当使用popitem()删除一个空字典时，会抛出异常 KeyError: &#x27;popitem(): dictionary is empty&#x27;# d.popitem()result = d.popitem()print(result) # (&#x27;f&#x27;, 6)print(d) # &#123;&#x27;c&#x27;: 3, &#x27;d&#x27;: 4, &#x27;e&#x27;: 5&#125;# pop(key[, default])# 根据key删除字典中的key-value# 会将被删除的value返回！# 如果删除不存在的key，会抛出异常# 如果指定了默认值，再删除不存在的key时，不会报错，而是直接返回默认值result = d.pop(&#x27;d&#x27;)print(result) # 4print(d) # &#123;&#x27;c&#x27;: 3, &#x27;e&#x27;: 5&#125;result = d.pop(&#x27;z&#x27;, &#x27;这是默认值&#x27;)print(result) # 这是默认值print(d) # &#123;&#x27;c&#x27;: 3, &#x27;e&#x27;: 5&#125;# clear()用来清空字典d.clear()print(d) # &#123;&#125;# print(&#x27;result =&#x27;,result)# print(d)# copy()# 该方法用于对字典进行浅复制# 复制以后的对象，和原对象是独立的，修改一个不会影响另一个d = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125;d2 = d.copy() # d和d2指向的是两个对象，这两个对象的值相同，id不同print(&#x27;d = &#x27;, d, id(d)) # d = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 1507616638400print(&#x27;d2 = &#x27;, d2, id(d2)) # d2 = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 1507616638464# 注意，浅复制会简单复制对象内部的值，如果值也是一个可变对象，这个可变对象不会被复制# 深复制性能差，使用较少d = &#123;&#x27;a&#x27;: &#123;&#x27;name&#x27;: &#x27;孙悟空&#x27;, &#x27;age&#x27;: 18&#125;, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125;d2 = d.copy()print(&#x27;d = &#x27;, d, id(d)) # d = &#123;&#x27;a&#x27;: &#123;&#x27;name&#x27;: &#x27;孙悟空&#x27;, &#x27;age&#x27;: 18&#125;, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 2538203734848print(&#x27;d2 = &#x27;, d2, id(d2)) # d2 = &#123;&#x27;a&#x27;: &#123;&#x27;name&#x27;: &#x27;孙悟空&#x27;, &#x27;age&#x27;: 18&#125;, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 2538210096768# d的a值也是一个字典，修改d2中a值列表中name的值，d也会受到影响，说明浅复制不修改是可变对象的值d2[&#x27;a&#x27;][&#x27;name&#x27;] = &#x27;猪八戒&#x27;print(&#x27;d = &#x27;, d, id(d)) # d = &#123;&#x27;a&#x27;: &#123;&#x27;name&#x27;: &#x27;猪八戒&#x27;, &#x27;age&#x27;: 18&#125;, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 2538203734848print(&#x27;d2 = &#x27;, d2, id(d2)) # d2 = &#123;&#x27;a&#x27;: &#123;&#x27;name&#x27;: &#x27;猪八戒&#x27;, &#x27;age&#x27;: 18&#125;, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 2538210096768# d的a值是一个列表d = &#123;&#x27;a&#x27;: [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;], &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125;d2 = d.copy()print(&#x27;d = &#x27;, d, id(d)) # d = &#123;&#x27;a&#x27;: [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;], &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 2053189928960print(&#x27;d2 = &#x27;, d2, id(d2)) # d2 = &#123;&#x27;a&#x27;: [&#x27;孙悟空&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;], &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 2053187068736d2[&#x27;a&#x27;][0] = &#x27;唐僧&#x27;print(&#x27;d = &#x27;, d, id(d)) # d = &#123;&#x27;a&#x27;: [&#x27;唐僧&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;], &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 2053189928960print(&#x27;d2 = &#x27;, d2, id(d2)) # d2 = &#123;&#x27;a&#x27;: [&#x27;唐僧&#x27;, &#x27;猪八戒&#x27;, &#x27;沙和尚&#x27;], &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125; 2053187068736 字典的遍历123456789101112131415161718d = &#123;&#x27;name&#x27;: &#x27;孙悟空&#x27;, &#x27;age&#x27;: 18, &#x27;gender&#x27;: &#x27;男&#x27;&#125;# 方式一：keys()# 该方法会返回一个序列，序列中是字典中所有的键for k in d.keys(): print(k, d[k])# 方式二：values()# 该方法会返回一个序列，序列中是字典中所有的值for v in d.values(): print(v)# 方式三：items()# 该方法会返回字典中所有的项，它会返回一个序列，序列中是双值子序列# 双值分别是，字典中的key和valueprint(d.items()) # dict_items([(&#x27;name&#x27;, &#x27;孙悟空&#x27;), (&#x27;age&#x27;, 18), (&#x27;gender&#x27;, &#x27;男&#x27;)])for k, v in d.items(): # 元组解包特性 print(k, &#x27;=&#x27;, v) 集合（set） 集合和列表非常相似。 不同点： 集合中只能存储不可变对象。 集合中存储的对象是无序（不是按照元素的插入顺序保存）。 集合中不能出现重复的元素。 集合的使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 集合的创建# 方式一：使用&#123;&#125;来创建集合s = &#123;10, 3, 5, 1, 2, 1, 2, 3, 1, 1, 1, 1&#125;print(s, type(s)) # &#123;1, 2, 3, 5, 10&#125; &lt;class &#x27;set&#x27;&gt;# s = &#123;[1, 2, 3], [4, 6, 7]&#125; # TypeError: unhashable type: &#x27;list&#x27;，集合中只能存储不可变对象# 方式二：使用set()函数来创建集合s = set() # 空集合只能通过set()来创建，&#123;&#125;创建的是字典# 可以通过set()来将序列和字典转换为集合s = set([1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5]) # 转换列表print(s) # &#123;1, 2, 3, 4, 5&#125;s = set(&#x27;hello&#x27;) # 转换字符串print(s) # &#123;&#x27;e&#x27;, &#x27;h&#x27;, &#x27;o&#x27;, &#x27;l&#x27;&#125;s = set(&#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125;) # 转换字典，使用set()将字典转换为集合时，只会包含字典中的键print(s) # &#123;&#x27;b&#x27;, &#x27;a&#x27;, &#x27;c&#x27;&#125;############################################################################s = &#123;&#x27;a&#x27;, &#x27;b&#x27;, 1, 2, 3, 1&#125;print(s) # &#123;1, 2, 3, &#x27;a&#x27;, &#x27;b&#x27;&#125;# 使用in和not in来检查集合中的元素print(&#x27;a&#x27; in s) # Trueprint(&#x27;b&#x27; not in s) # False# 使用len()来获取集合中元素的数量print(len(s)) # 5# add()向集合中添加元素s.add(10)s.add(30)s.add(1) # 集合中已存在的元素，添加无效print(s) # &#123;1, 2, 3, &#x27;a&#x27;, 10, &#x27;b&#x27;, 30&#125;############################################################################s = &#123;&#x27;a&#x27;, &#x27;b&#x27;, 1, 2, 3&#125;# update()可以将一个集合中的元素添加到当前集合中# update()也可以传递序列或字典作为参数，字典只会添加键倒集合中s2 = set(&#x27;hello&#x27;) # 集合s.update(s2)print(s) # &#123;&#x27;a&#x27;, 2, 3, 1, &#x27;h&#x27;, &#x27;o&#x27;, &#x27;l&#x27;, &#x27;e&#x27;, &#x27;b&#x27;&#125;s = &#123;&#x27;a&#x27;, &#x27;b&#x27;, 1, 2, 3&#125;s.update((10, 20, 30, 40, 50)) # 元组print(s) # &#123;&#x27;a&#x27;, 2, 3, 1, 40, 10, 50, 20, &#x27;b&#x27;, 30&#125;s = &#123;&#x27;a&#x27;, &#x27;b&#x27;, 1, 2, 3&#125;s.update(&#123;10: &#x27;ab&#x27;, 20: &#x27;bc&#x27;, 100: &#x27;cd&#x27;, 1000: &#x27;ef&#x27;&#125;) # 字典print(s) # &#123;&#x27;a&#x27;, 2, 3, 1, 100, 1000, 10, 20, &#x27;b&#x27;&#125;############################################################################s = &#123;&#x27;a&#x27;, &#x27;b&#x27;, 1, 2, 3&#125;# &#123;1, 2, 3, 100, 40, &#x27;o&#x27;, 10, 1000, &#x27;a&#x27;, &#x27;h&#x27;, &#x27;b&#x27;, &#x27;l&#x27;, 20, 50, &#x27;e&#x27;, 30&#125;# pop()随机删除并返回一个集合中的元素result = s.pop()print(result) # 1print(s) # &#123;&#x27;a&#x27;, 3, 2, &#x27;b&#x27;&#125;# remove()删除集合中的指定元素s.remove(&#x27;a&#x27;)print(s) # &#123;2, 3, &#x27;b&#x27;&#125;# clear()清空集合s.clear()print(s) # set()# copy()对集合进行浅复制 集合的运算1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 在对集合做运算时，不会影响原来的集合，而是返回一个运算结果# 创建两个集合s = &#123;1, 2, 3, 4, 5&#125;s2 = &#123;3, 4, 5, 6, 7&#125;# &amp; 交集运算result = s &amp; s2print(result) # &#123;3, 4, 5&#125;# | 并集运算result = s | s2print(result) # &#123;1, 2, 3, 4, 5, 6, 7&#125;# - 差集result = s - s2print(result) # &#123;1, 2&#125;# ^ 异或集，获取两个集合中只出现一次的元素result = s ^ s2print(result) # &#123;1, 2, 6, 7&#125;# &lt;= 检查一个集合是否是另一个集合的子集# 如果a集合中的元素全部都在b集合中出现，那么a集合就是b集合的子集，b集合是a集合超集a = &#123;1, 2, 3&#125;b = &#123;1, 2, 3, 4, 5&#125;result = a &lt;= bprint(result) # Trueresult = &#123;1, 2, 3&#125; &lt;= &#123;1, 2, 3&#125;print(result) # Trueresult = &#123;1, 2, 3, 4, 5&#125; &lt;= &#123;1, 2, 3&#125;print(result) # False# &lt; 检查一个集合是否是另一个集合的真子集# 如果超集b中含有子集a中所有元素，并且b中还有a中没有的元素，则b就是a的真超集，a是b的真子集result = &#123;1, 2, 3&#125; &lt; &#123;1, 2, 3&#125;print(result) # Falseresult = &#123;1, 2, 3&#125; &lt; &#123;1, 2, 3, 4, 5&#125;print(result) # True# &gt;= 检查一个集合是否是另一个的超集result = &#123;1, 2, 3, 4, 5&#125; &gt;= &#123;1, 2, 3, 4, 5&#125;print(result) # True# &gt; 检查一个集合是否是另一个的真超集result = &#123;1, 2, 3, 4, 5&#125; &gt; &#123;1, 2, 3, 4, 5&#125;print(result) # False 函数（function）函数简介12345678910111213141516171819202122232425262728293031323334# 定义一个函数def fn(): print(&#x27;这是我的第一个函数！&#x27;) print(&#x27;hello&#x27;) print(&#x27;今天天气真不错！&#x27;)# 打印fnprint(fn) # &lt;function fn at 0x00000175FD0261F0&gt;print(type(fn)) # &lt;class &#x27;function&#x27;&gt;# fn是函数对象，fn()调用函数# print是函数对象，print()调用函数fn()# 定义一个函数，可以用来求任意两个数的和def sum(): a = 123 b = 456 print(a + b)sum() # 579# 定义函数时指定形参def fn2(a, b): print(a, &quot;+&quot;, b, &quot;=&quot;, a + b)# 调用函数时，来传递实参fn2(10, 20) # 10 + 20 = 30fn2(123, 456) # 123 + 456 = 579 函数也是一个对象，对象是内存中专门用来存储数据的一块区域。 函数可以用来保存一些可执行的代码，并且可以在需要时，对这些语句进行多次的调用。 创建函数： 12def 函数名([形参1, 形参2, ... 形参n]): 代码块 函数名必须要符号标识符的规范（可以包含字母、数字、下划线，但是不能以数字开头）。 函数中保存的代码不会立即执行，需要调用函数代码才会执行。 调用函数： 1函数对象() 定义函数一般都是要实现某种功能的。 函数的参数1234567891011121314# 求任意三个数的乘积def mul(a, b, c): print(a * b * c)mul(1, 2, 3) # 6# 根据不同的用户名显示不同的欢迎信息def welcome(username): print(&#x27;欢迎&#x27;, username, &#x27;光临&#x27;)welcome(&#x27;孙悟空&#x27;) # 欢迎 孙悟空 光临 在定义函数时，可以在函数名后的 () 中定义数量不等的形参，多个形参之间使用 , 隔开。 形参（形式参数），定义形参就相当于在函数内部声明了变量，但是并不赋值。 实参（实际参数）。 如果函数定义时，指定了形参，那么在调用函数时也必须传递实参， 实参将会赋值给对应的形参，简单来说，有几个形参就得传几个实参。 参数的传递方式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# 定义函数，并为形参指定默认值def fn(a=5, b=10, c=20): print(&#x27;a =&#x27;, a) print(&#x27;b =&#x27;, b) print(&#x27;c =&#x27;, c)fn(1, 2, 3)fn(1, 2)fn()# 位置参数fn(1, 2, 3)# 关键字参数fn(b=1, c=2, a=3)print(&#x27;hello&#x27;, end=&#x27;&#x27;)# 位置参数和关键字参数混合使用fn(1, c=30)def fn2(a): print(&#x27;a =&#x27;, a)# 函数在调用时，解析器不会检查实参的类型# 实参可以传递任意类型的对象b = 123fn2(b) # a = 123b = Truefn2(b) # a = Trueb = &#x27;hello&#x27;fn2(b) # a = hellob = Nonefn2(b) # a = Noneb = [1, 2, 3]fn2(b) # a = [1, 2, 3]fn2(fn) # a = &lt;function fn at 0x0000025AB8B561F0&gt;# 类型不检查的缺陷，在传参时需要额外注意def fn3(a, b): print(a + b)# fn3(123, &quot;456&quot;) # TypeError: unsupported operand type(s) for +: &#x27;int&#x27; and &#x27;str&#x27;# 在函数中对形参进行重新赋值，不会影响其他的变量def fn4(a): a = 20 print(&#x27;a =&#x27;, a, id(a)) # a = 20 140708519029120c = 10fn4(c)print(c) # 10# 如果形参指向的是一个对象，当我们通过形参去修改对象时，会影响到所有指向该对象的变量def fn5(a): # a是一个列表，尝试修改列表中的元素 a[0] = 30 print(&#x27;a =&#x27;, a, id(a)) # a = [30, 2, 3] 2056358531968c = [1, 2, 3]fn5(c)print(&#x27;c =&#x27;, c, id(c)) # c = [30, 2, 3] 2056358531968# 通过浅复制，或者切片，实现不修改c本身fn4(c.copy())fn4(c[:]) 定义形参时，可以为形参指定默认值。指定了默认值以后，如果用户传递了参数，则默认值没有任何作用；如果用户没有传递参数，则默认值就会生效。 位置参数：即将对应位置的实参复制给对应位置的形参。 关键字参数：可以不按照形参定义的顺序去传递，而直接根据参数名去传递参数。 位置参数和关键字参数可以混合使用，混合使用时，必须将位置参数写到前面。 函数在调用时，解析器不会检查实参的类型，实参可以传递任意类型的对象。 在函数中对形参进行重新赋值，不会影响其他的变量。 如果形参指向的是一个对象，当我们通过形参去修改对象时，会影响到所有指向该对象的变量。 不定长的参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# 定义一个函数，可以求任意个数字的和def sum(*nums): # 定义一个变量，来保存结果 result = 0 # 遍历元组，并将元组中的数进行累加 for n in nums: result += n print(result)sum(10, 20, 30, 40) # 100sum(10, 20, 30, 40, 50, 60, 70) # 280# *a会接受所有的位置实参，并且会将这些实参统一保存到一个元组中（装包）def fn(*a): print(&quot;a =&quot;, a, type(a))fn(1, 2, 3) # a = (1, 2, 3) &lt;class &#x27;tuple&#x27;&gt;fn(1, 2, 3, 4, 5) # a = (1, 2, 3, 4, 5) &lt;class &#x27;tuple&#x27;&gt;# 带星号的形参只能有一个# 带星号的参数，可以和其他参数配合使用# 下面的函数，第一个参数给a，第二个参数给b，剩下的都保存到c的元组中def fn2(a, b, *c): print(&#x27;a =&#x27;, a) print(&#x27;b =&#x27;, b) print(&#x27;c =&#x27;, c)fn2(1, 2, 3, 4, 5)# 可变参数不是必须写在最后，但是注意，带*的参数后的所有参数，必须以关键字参数的形式传递# 下面的函数，第一个参数给a，剩下的位置参数给b的元组，c必须使用关键字参数def fn3(a, *b, c): print(&#x27;a =&#x27;, a) print(&#x27;b =&#x27;, b) print(&#x27;c =&#x27;, c)fn3(1, 2, 3, 4, c=5)# 下面的函数，所有的位置参数都给a，b和c必须使用关键字参数def fn4(*a, b, c): print(&#x27;a =&#x27;, a) print(&#x27;b =&#x27;, b) print(&#x27;c =&#x27;, c)fn4(1, 2, 3, b=4, c=5)# 如果在形参的开头直接写一个*，则要求我们的所有的参数必须以关键字参数的形式传递def fn5(*, a, b, c): print(&#x27;a =&#x27;, a) print(&#x27;b =&#x27;, b) print(&#x27;c =&#x27;, c)fn5(a=3, b=4, c=5)# *形参只能接收位置参数，而不能接收关键字参数# def fn3(*a) :# print(&#x27;a =&#x27;,a)# **形参可以接收其他的关键字参数，它会将这些参数统一保存到一个字典中# 字典的key就是参数的名字，字典的value就是参数的值# **形参只能有一个，并且必须写在所有参数的最后def fn6(b, c, **a): print(&#x27;a =&#x27;, a, type(a)) print(&#x27;b =&#x27;, b) print(&#x27;c =&#x27;, c)fn6(b=1, c=2, h=3, e=10, f=20)fn6(6, 7, g=1, d=2, h=3, e=10, f=20)print(&#x27;##########################################################&#x27;)# 参数的解包（拆包）def fn7(a, b, c): print(&#x27;a =&#x27;, a) print(&#x27;b =&#x27;, b) print(&#x27;c =&#x27;, c)# 传递实参时，也可以在序列类型的参数前添加星号，这样他会自动将序列中的元素依次作为参数传递给函数# 这里要求序列中元素的个数必须和形参的个数的一致t = (10, 20, 30)fn7(*t)# 通过 **来对一个字典进行解包操作d = &#123;&#x27;a&#x27;: 100, &#x27;b&#x27;: 200, &#x27;c&#x27;: 300&#125;fn7(**d) 在定义函数时，可以在形参前边加上一个 *，这样这个形参将会获取到所有的实参，并将所有的实参保存到一个元组中。 带 * 的形参只能有一个。 带 * 的参数，可以和其他参数配合使用。 可变参数不是必须写在最后，但是注意，带 * 的参数后的所有参数，必须以关键字参数的形式传递，否则报错。 如果在形参的开头直接写一个 *，则要求所有的参数必须以关键字参数的形式传递。 * 形参只能接收位置参数，而不能接收关键字参数。 ** 形参可以接收其他的关键字参数，它会将这些参数统一保存到一个字典中。字典的 key 就是参数的名字，字典的 value 就是参数的值。 ** 形参只能有一个，并且必须写在所有参数的最后。 传递实参时，也可以在序列类型的参数前添加 *，这样会自动将序列中的元素依次作为参数传递给函数，但要求序列中元素的个数必须和形参的个数一致。 如果是字典，通过 ** 来进行解包操作。 函数的返回值12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# return 后边跟什么值，函数就会返回什么值# return 后边可以跟任意的对象，返回值甚至可以是一个函数def fn(): # return &#x27;Hello&#x27; # return [1, 2, 3] # return &#123;&#x27;k&#x27;: &#x27;v&#x27;&#125; def fn1(): print(&#x27;hello&#x27;) return fn1 # 返回值也可以是一个函数r = fn() # 这个函数的执行结果就是它的返回值print(r) # &lt;function fn.&lt;locals&gt;.fn1 at 0x000001F3430BCF70&gt;r() # hello# 如果仅仅写一个return或者不写return，则相当于return None# 在函数中，return后的代码都不会执行，return一旦执行函数自动结束def fn2(): a = 10 return print(&#x27;abc&#x27;) # 不会执行r = fn2()print(r) # Nonedef fn3(): for i in range(5): if i == 3: # break 用来退出当前循环 # continue 用来跳过当次循环 return # return 用来结束函数 print(i) print(&#x27;循环执行完毕！&#x27;)fn3()def fn4(): return 10# fn4 和 fn4()的区别print(fn4) # fn4是函数对象，打印fn4实际是在打印函数对象：&lt;function fn5 at 0x00000229B3CFD670&gt;print(fn4()) # fn4()是在调用函数，打印fn4()实际上是在打印fn4()函数的返回值：10 返回值就是函数执行以后返回的结果，可以通过 return 来指定函数的返回值。 return 后边可以跟任意的对象，甚至可以是一个函数。return 后边跟什么值，函数就会返回什么值。 如果仅仅写一个 return 或者不写 return，则相当于 return None。 在函数中，return 后的代码都不会执行，return 一旦执行函数自动结束。 文档字符串123456789101112131415161718192021222324# help()是Python中的内置函数# 通过help()函数可以查询python中的函数的用法# 语法：help(函数对象)help(print) # 获取print()函数的使用说明# 文档字符串（doc str）# 在定义函数时，可以在函数内部编写文档字符串，文档字符串就是函数的说明# 当我们编写了文档字符串时，就可以通过help()函数来查看函数的说明# 文档字符串非常简单，其实直接在函数的第一行写一个字符串就是文档字符串def fn(a: int, b: bool, c: str = &#x27;hello&#x27;) -&gt; int: # 函数参数后跟着类型，返回值是一个int &quot;&quot;&quot; 这是一个文档字符串的示例 函数的作用：。。。。。。 函数的参数： a，作用，类型，默认值。。。 b，作用，类型，默认值。。。 c，作用，类型，默认值。。。 &quot;&quot;&quot; return 10help(fn) 作用域12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 作用域（scope）# 作用域指的是变量生效的区域b = 20 # 全局变量def fn(): a = 10 # a定义在了函数内部，所以他的作用域就是函数内部，函数外部无法访问 print(&#x27;函数内部：&#x27;, &#x27;a =&#x27;, a) print(&#x27;函数内部：&#x27;, &#x27;b =&#x27;, b)fn()# print(&#x27;函数外部：&#x27;, &#x27;a =&#x27;, a) # NameError: name &#x27;a&#x27; is not definedprint(&#x27;函数外部：&#x27;, &#x27;b =&#x27;, b)# 在Python中一共有两种作用域# 全局作用域# - 全局作用域在程序执行时创建，在程序执行结束时销毁# - 所有函数以外的区域都是全局作用域# - 在全局作用域中定义的变量，都属于全局变量，全局变量可以在程序的任意位置被访问## 函数作用域# - 函数作用域在函数调用时创建，在调用结束时销毁# - 函数每调用一次就会产生一个新的函数作用域# - 在函数作用域中定义的变量，都是局部变量，它只能在函数内部被访问## 变量的查找# - 当我们使用变量时，会优先在当前作用域中寻找该变量，如果有则使用，# 如果没有则继续去上一级作用域中寻找，如果有则使用，# 如果依然没有则继续去上一级作用域中寻找，以此类推# 直到找到全局作用域，依然没有找到，则会抛出异常# NameError: name &#x27;a&#x27; is not defineddef fn1(): def fn2(): print(&#x27;fn3中:&#x27;, &#x27;a =&#x27;, a) fn2()# fn1() # fn1中的嵌套函数fn2，找不到a，报错NameError: name &#x27;a&#x27; is not defineda = 20def fn3(): # a = 10 # 在函数中为变量赋值时，默认都是为局部变量赋值 # 如果希望在函数内部修改全局变量，则需要使用global关键字，来声明变量 global a # 声明在函数内部的使用a是全局变量，此时再去修改a时，就是在修改全局的a a = 10 # 修改全局变量 print(&#x27;函数内部：&#x27;, &#x27;a =&#x27;, a)fn3()print(&#x27;函数外部：&#x27;, &#x27;a =&#x27;, a) # 函数外部： a = 10 命名空间123456789101112131415161718192021222324252627282930313233343536373839# 命名空间（namespace）# 命名空间指的是变量存储的位置，每一个变量都需要存储到指定的命名空间当中# 每一个作用域都会有一个它对应的命名空间# 全局命名空间，用来保存全局变量。函数命名空间用来保存函数中的变量# 命名空间实际上就是一个字典，是一个专门用来存储变量的字典# locals()用来获取当前作用域的命名空间# 如果在全局作用域中调用locals()则获取全局命名空间，如果在函数作用域中调用locals()则获取函数命名空间# 返回的是一个字典scope = locals() # 当前命名空间print(type(scope)) # &lt;class &#x27;dict&#x27;&gt;# 下面两个打印效果相同a = 20print(a) # 20print(scope[&#x27;a&#x27;]) # 20# 向scope中添加一个key-value# scope[&#x27;c&#x27;] = 1000 # 向字典中添加key-value就相当于在全局中创建了一个变量（一般不建议这么做）# print(c) # 1000def fn(): a = 10 scope = locals() # 在函数内部调用locals()会获取到函数的命名空间 print(type(scope)) # &lt;class &#x27;dict&#x27;&gt; # scope[&#x27;b&#x27;] = 90 # 可以通过scope来操作函数的命名空间，但是也是不建议这么做 # print(b) # globals()函数可以用来在任意位置获取全局命名空间 # 函数外面无法获得函数的命名空间 global_scope = globals() print(global_scope[&#x27;a&#x27;]) # 20 # global_scope[&#x27;a&#x27;] = 30 # 不建议这么做 # print(global_scope[&#x27;a&#x27;]) # 30fn() 递归123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128# 10的阶乘n = 10for i in range(1, 10): n *= iprint(n)# 创建一个函数，可以用来求任意数的阶乘def factorial(n): &#x27;&#x27;&#x27; 该函数用来求任意数的阶乘 参数： n 要求阶乘的数字 &#x27;&#x27;&#x27; # 创建一个变量，来保存结果 result = n for i in range(1, n): result *= i return resultprint(factorial(10))# 递归简单理解就是自己去引用自己！# 递归式函数，在函数中自己调用自己！# 下面这个是无穷递归，如果这个函数被调用，程序的内存会溢出，效果类似于死循环# def fn():# fn()# fn()# 递归是解决问题的一种方式，它和循环很像# 它的整体思想是，将一个大问题分解为一个个的小问题，直到问题无法分解时，再去解决问题# 递归式函数的两个要件# 1.基线条件# - 问题可以被分解为的最小问题，当满足基线条件时，递归就不在执行了# 2.递归条件# - 将问题继续分解的条件# 递归和循环类似，基本是可以互相代替的，# 循环编写起来比较容易，阅读起来稍难# 递归编写起来难，但是方便阅读def factorial(n): # 基线条件 判断n是否为1，如果为1则此时不能再继续递归 if n == 1: # 1的阶乘就是1，直接返回1 return 1 # 递归条件 return n * factorial((n - 1))print(factorial(10))# 创建一个函数power，来为任意数字做幂运算 n ** idef power(n, i): &#x27;&#x27;&#x27; power()用来为任意的数字做幂运算 参数： n 要做幂运算的数字 i 做幂运算的次数 &#x27;&#x27;&#x27; # 基线条件 if i == 1: # 求1次幂 return n # 递归条件 return n * power(n, i - 1)print(pow(3, 4))# 创建一个函数，用来检查一个任意的字符串是否是回文字符串，如果是返回True，否则返回False# 回文字符串，字符串从前往后念和从后往前念是一样的# abcba# abcdefgfedcba# 先检查第一个字符和最后一个字符是否一致，如果不一致则不是回文字符串# 如果一致，则看剩余的部分是否是回文字符串# 检查 abcdefgfedcba 是不是回文# 检查 bcdefgfedcb 是不是回文# 检查 cdefgfedc 是不是回文# 检查 defgfed 是不是回文# 检查 efgfe 是不是回文# 检查 fgf 是不是回文# 检查 g 是不是回文def hui_wen(s): &#x27;&#x27;&#x27; 该函数用来检查指定的字符串是否回文字符串，如果是返回True，否则返回False 参数： s：就是要检查的字符串 &#x27;&#x27;&#x27; # 基线条件 if len(s) &lt; 2: # 字符串的长度小于2，则字符串一定是回文 return True elif s[0] != s[-1]: # 第一个字符和最后一个字符不相等，不是回文字符串 return False # 递归条件 return hui_wen(s[1:-1])# def hui_wen(s):# &#x27;&#x27;&#x27;# 该函数用来检查指定的字符串是否回文字符串，如果是返回True，否则返回False# 参数：# s：就是要检查的字符串# &#x27;&#x27;&#x27;# # 基线条件# if len(s) &lt; 2 :# # 字符串的长度小于2，则字符串一定是回文# return True# # 递归条件# return s[0] == s[-1] and hui_wen(s[1:-1])print(hui_wen(&#x27;abba&#x27;))print(hui_wen(&#x27;abcdefgfedcba&#x27;)) 高阶函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 高阶函数# 接收函数作为参数，或者将函数作为返回值的函数是高阶函数# 当我们使用一个函数作为参数时，实际上是将指定的代码传递进了目标函数# 定义一个函数，功能：可以将指定列表中的所有的偶数，保存到一个新的列表中返回# 待提取列表l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]# 常规写法def fn(lst): &#x27;&#x27;&#x27; fn()函数可以将指定列表中的所有偶数提取出来，并保存到一个新列表中返回 参数： lst：指定的要筛选的列表 &#x27;&#x27;&#x27; new_list = [] for n in lst: if n % 2 == 0: new_list.append(n) return new_listprint(fn(l)) # [2, 4, 6, 8, 10]# 高阶函数# 功能1：定义一个函数，用来检查一个任意的数字是否是偶数def fn1(i): if i % 2 == 0: return True return False# 功能2：定义一个函数，用来检查指定的数字是否大于5def fn2(i): if i &gt; 5: return True return False# 功能2：定义一个函数，用来检查一个任意的数字是否能被3整除def fn3(i): return i % 3 == 0# 多功能的高阶函数def fn(func, lst): &#x27;&#x27;&#x27; fn()函数可以将指定列表中的数据按指定函数要求提取出来，并保存到一个新列表中返回 参数： func：指定的提取要求 lst：指定的要筛选的列表 &#x27;&#x27;&#x27; new_list = [] for n in lst: if func(n): new_list.append(n) return new_listprint(fn(fn1, l)) # 获取偶数：[2, 4, 6, 8, 10]print(fn(fn2, l)) # 获取大于5的数：[6, 7, 8, 9, 10]print(fn(fn3, l)) # 获取能被3整除的数：[3, 6, 9]# filter()函数的功能，就如上面自定义的fn()函数# filter()可以从序列中过滤出符合条件的元素，保存到一个新的序列中# 参数：# 1.函数，根据该函数来过滤序列（可迭代的结构）# 2.需要过滤的序列（可迭代的结构）# 返回值：# 过滤后的新序列（可迭代的结构）iterator = filter(fn1, l)# for n in iterator:# print(n)print(list(iterator)) # 返回的是一个可迭代的结构，需要转换成list才能直接打印出来数据print(list(filter(fn2, l)))print(list(filter(fn3, l))) 在 Python 中，函数是一等对象。一等对象一般都会具有如下特点： 对象是在运行时创建的。 能赋值给变量或作为数据结构中的元素。 能作为参数传递。 能作为返回值返回。 高阶函数至少要符合以下两个特点中的一个： 能接收一个或多个函数作为参数。 能将函数作为返回值返回。 当我们使用一个函数作为参数时，实际上是将指定的代码传递进了目标函数。 匿名函数 匿名函数是将一个或多个函数作为参数来接收。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# fn1~fn3是作为参数传递进filter()函数中# 而fn1~fn3实际上只有一个作用，就是作为filter()的参数# filter()调用完毕以后，fn1~fn3就已经没用# 这种情况可以用匿名函数简化# 匿名函数lambda函数表达式（语法糖）# lambda函数表达式专门用来创建一些简单的函数，他是函数创建的又一种方式# 语法：lambda 参数列表 : 返回值# 匿名函数一般都是作为参数使用，其他地方一般不会使用# 待提取列表l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]# 常规写法def fn5(a, b): return a + bprint(fn5(10, 20)) # 常规写法print((lambda a, b: a + b)(10, 20)) # lambad表达式写法fn6 = lambda a, b: a + b # 也可以将匿名函数赋值给一个变量，一般不会这么做print(fn6(10, 20))# filter()函数中可以很方便的使用lambda表达式# 此时，lambda表达式只会使用一次，使用完后内存中自动回收r = filter(lambda i: i % 2 == 0, l)print(list(r)) # [2, 4, 6, 8, 10]print(list(filter(lambda i: i &gt; 5, l))) # [6, 7, 8, 9, 10]# map()函数可以对可迭代对象中的所有元素做指定的操作，然后将其添加到一个新的对象中返回print(list(map(lambda i: i ** 2, l))) # 对列表中的每一个元素求平方，[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]# sort()函数用来对列表中的元素进行排序：# sotr()只能排序列表# 默认是直接比较列表中的元素的大小# 在sort()可以接收一个关键字参数key：# key需要一个函数作为参数，当设置了函数作为参数# 每次都会以列表中的一个元素作为参数来调用该函数# 并且使用函数的返回值来比较元素的大小l = [&#x27;bb&#x27;, &#x27;aaaa&#x27;, &#x27;c&#x27;, &#x27;ddddddddd&#x27;, &#x27;fff&#x27;]l.sort()print(l) # 默认比较：[&#x27;aaaa&#x27;, &#x27;bb&#x27;, &#x27;c&#x27;, &#x27;ddddddddd&#x27;, &#x27;fff&#x27;]l = [&#x27;bb&#x27;, &#x27;aaaa&#x27;, &#x27;c&#x27;, &#x27;ddddddddd&#x27;, &#x27;fff&#x27;]l.sort(key=len)print(l) # 按长度比较：[&#x27;c&#x27;, &#x27;bb&#x27;, &#x27;fff&#x27;, &#x27;aaaa&#x27;, &#x27;ddddddddd&#x27;]l = [2, 5, &#x27;1&#x27;, 3, &#x27;6&#x27;, &#x27;4&#x27;]l.sort(key=int)print(l) # 把每一个元素转换成整形后再比较：[&#x27;1&#x27;, 2, 3, &#x27;4&#x27;, 5, &#x27;6&#x27;]l = [2, 5, &#x27;1&#x27;, 3, &#x27;6&#x27;, &#x27;4&#x27;]l.sort(key=str)print(l) # 把每一个元素转换成字符串后再比较：[&#x27;1&#x27;, 2, 3, &#x27;4&#x27;, 5, &#x27;6&#x27;]# sorted()函数和sort()的用法基本一致，但是sorted()可以对任意的序列进行排序# 并且使用sorted()排序不会影响原来的对象，而是返回一个新对象l = [2, 5, &#x27;1&#x27;, 3, &#x27;6&#x27;, &#x27;4&#x27;] # 排序列表print(&#x27;排序前:&#x27;, l) # 排序前: [2, 5, &#x27;1&#x27;, 3, &#x27;6&#x27;, &#x27;4&#x27;]print(&#x27;排序中:&#x27;, sorted(l, key=int)) # 排序中: [&#x27;1&#x27;, 2, 3, &#x27;4&#x27;, 5, &#x27;6&#x27;]print(&#x27;排序后:&#x27;, l) # 排序后: [2, 5, &#x27;1&#x27;, 3, &#x27;6&#x27;, &#x27;4&#x27;]l = &#x27;123765816742634781&#x27; # 排序字符串print(&#x27;排序前:&#x27;, l) # 排序前: 123765816742634781print(&#x27;排序中:&#x27;, sorted(l, key=int)) # 排序中: [&#x27;1&#x27;, &#x27;1&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;6&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;7&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;8&#x27;]print(&#x27;排序后:&#x27;, l) # 排序后: 123765816742634781 闭包 闭包是将函数作为返回值返回。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 将函数作为返回值返回，也是一种高阶函数# 这种高阶函数我们也称为叫做闭包，通过闭包可以创建一些只有当前函数能访问的变量# 可以将一些私有的数据藏到闭包中def fn(): a = 10 # 函数内部再定义一个函数 def inner(): print(&#x27;我是fn2&#x27;, a) # 将内部函数 inner作为返回值返回 return inner# r是一个函数，是调用fn()后返回的函数# 而且这个函数是在fn()内部定义，并不是全局函数# 所以这个函数总是能访问到fn()函数内的变量r = fn()print(r) # &lt;function fn.&lt;locals&gt;.inner at 0x000001CEC1142430&gt;r() # 我是fn2 10# 求多个数的平均值nums = [50, 30, 20, 10, 77]# 常规写法：sum()用来求一个列表中所有元素的和print(sum(nums) / len(nums)) # 37.4# 如果nums中的数据是变化的，可以使用闭包# 形成闭包的要件# ① 函数嵌套# ② 将内部函数作为返回值返回# ③ 内部函数必须要使用到外部函数的变量def make_averager(): # 创建一个列表，用来保存数值 nums = [] # 创建一个函数，用来计算平均值 def averager(n): # 将n添加到列表中 nums.append(n) # 求平均值 return sum(nums) / len(nums) return averager# 函数返回的是make_averager()中定义的averager()函数# 并创建了一个nums列表，这个列表外界无法访问，只有averager对象可以访问averager = make_averager()print(averager(10)) # 10/1=10print(averager(20)) # (10+20)/2=15print(averager(30)) # (10+20+30)/3=20print(averager(40)) # (10+20+30+40)/4=25 装饰器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134# 创建几个函数def add(a, b): &#x27;&#x27;&#x27; 求任意两个数的和 &#x27;&#x27;&#x27; r = a + b return rdef mul(a, b): &#x27;&#x27;&#x27; 求任意两个数的积 &#x27;&#x27;&#x27; r = a * b return rprint(add(123, 456)) # 579print(mul(10, 20)) # 200# 现在，希望函数可以在计算前，打印开始计算，计算结束后打印计算完毕# 我们可以直接通过修改函数中的代码来完成这个需求，但是会产生以下一些问题# ① 如果要修改的函数过多，修改起来会比较麻烦# ② 并且不方便后期的维护# ③ 并且这样做会违反开闭原则（OCP）# 程序的设计，要求开发对程序的扩展，要关闭对程序的修改# 我们希望在不修改原函数的情况下，来对函数进行扩展def fn(): print(&#x27;我是fn函数....&#x27;)# 只需要根据现有的函数，来创建一个新的函数def fn2(): print(&#x27;函数开始执行~~~&#x27;) fn() print(&#x27;函数执行结束~~~&#x27;)fn2()# 创建新函数，扩展add()def new_add(a, b): print(&#x27;计算开始~~~&#x27;) r = add(a, b) print(&#x27;计算结束~~~&#x27;) return rr = new_add(111, 222)print(r)print(&#x27;##############################################################&#x27;)# 上边的方式，已经可以在不修改源代码的情况下对函数进行扩展了# 但是，这种方式要求我们每扩展一个函数就要手动创建一个新的函数，实在是太麻烦了# 为了解决这个问题，我们创建一个函数，让这个函数可以自动的帮助我们生产函数def begin_end(old): &#x27;&#x27;&#x27; 用来对其他函数进行扩展，使其他函数可以在执行前打印开始执行，执行后打印执行结束 参数： old 要扩展的函数对象 &#x27;&#x27;&#x27; # 创建一个新函数，参数的个数是不确定的，使用*和** def new_function(*args, **kwargs): print(&#x27;开始执行~~~~&#x27;) # 调用被扩展的函数 result = old(*args, **kwargs) print(&#x27;执行结束~~~~&#x27;) # 返回函数的执行结果 return result # 返回新函数 return new_functionf = begin_end(fn) # 包装fn()f2 = begin_end(add) # 包装add()f3 = begin_end(mul) # 包装mul()r = f()print(r)r = f2(123, 456)print(r)r = f3(123, 456)print(r)print(&#x27;##############################################################&#x27;)# 像begin_end()这种函数我们就称它为装饰器# 通过装饰器，可以在不修改原来函数的情况下来对函数进行扩展# 在开发中，我们都是通过装饰器来扩展函数的功能的# 在定义函数时，可以通过@装饰器，来使用指定的装饰器，来装饰当前的函数# 可以同时为一个函数指定多个装饰器，这样函数将会安装从内向外的顺序被装饰def fn3(old): &#x27;&#x27;&#x27; 用来对其他函数进行扩展，使其他函数可以在执行前打印开始执行，执行后打印执行结束 参数： old 要扩展的函数对象 &#x27;&#x27;&#x27; # 创建一个新函数 # *args：old函数中的位置参数(形如：&#x27;a, b&#x27;)，全都存放其中 # **kwargs：old函数中的字典参数(形如：&#x27;a=x, b=y&#x27;)，全部存放其中 def new_function(*args, **kwargs): print(&#x27;fn3装饰~开始执行~~~~&#x27;) # 调用被扩展的函数 result = old(*args, **kwargs) print(&#x27;fn3装饰~执行结束~~~~&#x27;) # 返回函数的执行结果 return result # 返回新函数 return new_function@fn3 # 第一层：fn3装饰@begin_end # ：第二层：begin_end装饰def say_hello(): print(&#x27;大家好~~~&#x27;)say_hello() 对象（object)什么是对象 对象是内存中专门用来存储数据的一块区域。 对象中可以存放各种数据，比如：数字、布尔值、代码。 对象由三部分组成： 对象的标识（id） 对象的类型（type） 对象的值（value） 面向对象（oop） Python 是一门面向对象的编程语言。 所谓的面向对象的语言，简单理解就是语言中的所有操作都是通过对象来进行的。 面向过程的编程的语言： 面向过程指将我们的程序的逻辑分解为一个一个的步骤，通过对每个步骤的抽象，来完成程序。 例子：孩子上学，可能有以下过程。 妈妈起床。 妈妈洗漱。 妈妈做早饭。 妈妈叫孩子起床。 孩子要洗漱。 孩子吃饭。 孩子背着书包上学校。 面向过程的编程思想将一个功能分解为一个一个小的步骤，我们通过完成一个一个的小的步骤来完成一个程序。 这种编程方式，符合我们人类的思维，编写起来相对比较简单。 但是这种方式编写代码的往往只适用于一个功能，如果要在实现别的功能，即使功能相差极小，也往往要重新编写代码，所以它可复用性比较低，并且难于维护 。 面向对象的编程语言： 面向对象的编程语言，关注的是对象，而不关注过程。 对于面向对象的语言来说，一切都是对象。 面向对象的编程思想，将所有的功能统一保存到对应的对象中。比如，妈妈的功能保存到妈妈的对象中，孩子的功能保存到孩子对象中，要使用某个功能，直接找到对应的对象即可。 这种方式编写的代码，比较容易阅读，并且比较易于维护，容易复用。 但是这种方式编写，不太符合常规的思维，编写起来稍微麻烦一点。 简单归纳一下，面向对象的思想： 第一步：创建对象。 第二步：处理对象。 类的简介123456789101112131415161718192021222324252627282930313233343536373839404142a = int(10) # 创建一个int类的实例b = str(&#x27;hello&#x27;) # 创建一个str类的实例print(a, type(a)) # 10 &lt;class &#x27;int&#x27;&gt;print(b, type(b)) # hello &lt;class &#x27;str&#x27;&gt;# 定义一个简单的类# 使用class关键字来定义类，语法和函数很像！# class 类名([父类]):# 代码块# &lt;class &#x27;__main__.MyClass&#x27;&gt;class MyClass(): # 如果没有父类，()可以省略 passprint(MyClass) # &lt;class &#x27;__main__.MyClass&#x27;&gt;# 使用MyClass创建一个对象# 使用类来创建对象，就像调用一个函数一样mc = MyClass() # mc就是通过MyClass创建的对象，mc是MyClass的实例print(mc, type(mc)) # &lt;__main__.MyClass object at 0x000001B009813E50&gt; &lt;class &#x27;__main__.MyClass&#x27;&gt;mc_2 = MyClass()mc_3 = MyClass()mc_4 = MyClass()# mc mc_2 mc_3 mc_4 都是MyClass的实例，他们都是一类对象# isinstance()用来检查一个对象是否是一个类的实例result = isinstance(mc_2, MyClass)print(result) # Trueresult = isinstance(mc_2, str)print(result) # False# 类是一个type类型的对象print(id(MyClass), type(MyClass)) # 1560257906784 &lt;class &#x27;type&#x27;&gt;# 现在我们通过MyClass这个类创建的对象都是一个空对象# 也就是对象中实际上什么都没有，就相当于是一个空的盒子# 可以向对象中添加变量，对象中的变量称为属性# 语法：对象.属性名 = 属性值mc.name = &#x27;孙悟空&#x27;print(mc.name) # 孙悟空mc_2.name = &#x27;猪八戒&#x27;print(mc_2.name) # 猪八戒 我们目前所学习的对象都是 Python 内置的对象。 但是内置对象并不能满足所有的需求，所以我们在开发中经常需要自定义一些对象。 类，简单理解它就相当于一个图纸。在程序中我们需要根据类来创建对象。 类就是对象的图纸！ 我们也称对象是类的实例（instance）。 如果多个对象是通过一个类创建的，我们称这些对象是一类对象。 像 int()，float()，bool()，str()，list()，dict() 等，这些都是类。 a = int(10) # 创建一个int类的实例 等价于 a = 10。 我们自定义的类都需要使用大写字母开头，使用大驼峰命名法（帕斯卡命名法）来对类命名。 类也是一个对象！ 类就是一个用来创建对象的对象！ 类是 type 类型的对象，定义类实际上就是定义了一个 type 类型的对象。 使用类创建对象的流程： 第一步：创建一个变量。 第二步：在内存中创建一个新对象。 第三步：将对象的 id 赋值给变量。 类的定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 尝试定义一个表示人的类class Person: # 在类的代码块中，我们可以定义变量和函数 # 在类中我们所定义的变量，将会成为所有的实例的公共属性 # 所有实例都可以访问这些变量 name = &#x27;swk&#x27; # 公共属性，所有实例都可以访问 # 在类中也可以定义函数，类中的定义的函数，我们称为方法 # 这些方法可以通过该类的所有实例来访问 def say_hello(self): # 方法每次被调用时，解析器都会自动传递第一个实参 # 第一个参数，就是调用方法的对象本身， # 如果是p1调的，则第一个参数就是p1对象 # 如果是p2调的，则第一个参数就是p2对象 # 一般我们都会将这个参数命名为self # say_hello()这个方法，可以显示如下格式的数据： # 你好！我是 xxx # 在方法中不能直接访问类中的属性 print(&#x27;你好！我是 %s&#x27; % self.name) # 类似Java中的this# 创建Person的实例p1 = Person()p2 = Person()# 调用属性：对象.属性名print(p1.name) # swkprint(p2.name) # swk# 调用方法：对象.方法名()# 方法调用和函数调用的区别# 如果是函数调用，则调用时传几个参数，就会有几个实参# 但是如果是方法调用，默认传递一个参数，所以方法中至少要定义一个形参p1.say_hello() # 你好！我是 swkp2.say_hello() # 你好！我是 swk# 修改p1的name属性p1.name = &#x27;猪八戒&#x27;p2.name = &#x27;沙和尚&#x27;print(p1.name)print(p2.name)p1.say_hello() # 你好！我是 猪八戒p2.say_hello() # 你好！我是 沙和尚del p2.name # 删除p2的name属性print(p2.name) # swk 类和对象都是对现实生活中的事物或程序中的内容的抽象。 实际上所有的事物都由两部分构成： 数据（属性） 行为（方法） 在类的代码块中，我们可以定义变量和函数： 变量会成为该类实例的公共属性，所有的该类实例都可以通过 对象.属性名 的形式访问。 函数会成为该类实例的公共方法，所有该类实例都可以通过 对象.方法名() 的形式调用方法。 注意：方法调用时，默认第一个参数由解析器自动传递，所以定义方法时，至少要定义一个形参！ 实例为什么能访问到类中的属性和方法： 类中定义的属性和方法都是公共的，任何该类实例都可以访问。 属性和方法查找的流程： 当我们调用一个对象的属性时，解析器会先在当前对象中寻找是否含有该属性，如果有，则直接返回当前的对象的属性值；如果没有，则去当前对象的类对象中去寻找，如果有，则返回类对象的属性值，如果类对象中依然没有，则报错！ 类对象和实例对象中都可以保存属性（方法）： 如果这个属性（方法）是所有的实例共享的，则应该将其保存到类对象中。 如果这个属性（方法）是某个实例独有，则应该保存到实例对象中。 比如，Person 类中，name 属性每个对象都不同，应该保存到各个实例对象中，而国籍假设都是中国人，是一样的，则应该保存到类对象中。 一般情况下，属性保存到实例对象中，而方法需要保存到类对象中。 对象的初始化12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Person: # 在类中可以定义一些特殊方法（魔术方法） # 特殊方法都是以__开头，__结尾的方法 # 特殊方法不需要我们自己调用，不要尝试去调用特殊方法 # 特殊方法将会在特殊的时刻自动调用 # 学习特殊方法： # 1.特殊方法什么时候调用 # 2.特殊方法有什么作用 # 创建对象的流程 # p1 = Person()的运行流程 # 1.创建一个变量 # 2.在内存中创建一个新对象 # 3.__init__(self)方法执行 # 4.将对象的id赋值给变量 # init会在对象创建以后立刻执行 # init可以用来向新创建的对象中初始化属性 # 调用类创建对象时，类后边的所有参数都会依次传递到init()中 def __init__(self, name): # print(self) # 通过self向新建的对象中初始化属性 self.name = name def say_hello(self): print(&#x27;大家好，我是%s&#x27; % self.name)# 目前来讲，对于Person类来说name是必须的，并且每一个对象中的name属性基本上都是不同# 而我们现在是将name属性在定义为对象以后，手动添加到对象中，这种方式很容易出现错误# 我们希望，在创建对象时，必须设置name属性，如果不设置对象将无法创建# 并且属性的创建应该是自动完成的，而不是在创建对象以后手动完成# p1 = Person()# 手动向对象添加name属性# p1.name = &#x27;孙悟空&#x27;# p2 = Person()# p2.name = &#x27;猪八戒&#x27;# p3 = Person()# p3.name = &#x27;沙和尚&#x27;# p3.say_hello()p1 = Person(&#x27;孙悟空&#x27;)p2 = Person(&#x27;猪八戒&#x27;)p3 = Person(&#x27;沙和尚&#x27;)p4 = Person(&#x27;唐僧&#x27;)# p1.__init__() 不要这么做# print(p1.name)# print(p2.name)# print(p3.name)# print(p4.name)p4.say_hello() 12345678910111213141516171819202122232425262728293031323334353637383940class Dog: &#x27;&#x27;&#x27; 表示狗的类 &#x27;&#x27;&#x27; def __init__(self, name, age, gender, height): self.name = name self.age = age self.gender = gender self.height = height def jiao(self): &#x27;&#x27;&#x27; 狗叫的方法 &#x27;&#x27;&#x27; print(&#x27;汪汪汪~~~&#x27;) def yao(self): &#x27;&#x27;&#x27; 狗咬的方法 &#x27;&#x27;&#x27; print(&#x27;我咬你~~&#x27;) def run(self): print(&#x27;%s 快乐的奔跑着~~&#x27; % self.name)d = Dog(&#x27;小黑&#x27;, 8, &#x27;male&#x27;, 30)print(d.name, d.age, d.gender, d.height)# 目前我们可以直接通过 对象.属性 的方式来修改属性的值，这种方式导致对象中的属性可以随意修改# 非常的不安全，值可以任意修改，不论对错# 现在我们就需要一种方式来增强数据的安全性# 1.属性不能随意修改（我让你改你才能改，不让你改你就不能改）# 2.属性不能修改为任意的值（年龄不能是负数）d.name = &#x27;阿黄&#x27;d.age = -10d.run()print(d.age) 类的基本结构： 12345678910111213141516class 类名([父类]) : 公共的属性... # 对象的初始化方法 def __init__(self,...): ... # 其他的方法 def method_1(self,...): ... def method_2(self,...): ... ... 创建对象的流程，p1 = Person()： 第一步：创建一个变量。 第二步：在内存中创建一个新对象。 第三步：__init__(self) 方法执行。 第四步：将对象的 id 赋值给变量。 封装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 封装是面向对象的三大特性之一# 封装指的是隐藏对象中一些不希望被外部所访问到的属性或方法# 如何隐藏一个对象中的属性？# - 将对象的属性名，修改为一个外部不知道的名字# 如何获取（修改）对象中的属性？# - 需要提供一个getter和setter方法使外部可以访问到属性# - getter 获取对象中的指定属性（get_属性名）# - setter 用来设置对象的指定属性（set_属性名）# 使用封装，确实增加了类的定义的复杂程度，但是它也确保了数据的安全性# 1.隐藏了属性名，使调用者无法随意的修改对象中的属性# 2.增加了getter和setter方法，很好的控制的属性是否是只读的# 如果希望属性是只读的，则可以直接去掉setter方法# 如果希望属性不能被外部访问，则可以直接去掉getter方法# 3.使用setter方法设置属性，可以增加数据的验证，确保数据的值是正确的# 4.使用getter方法获取属性，使用setter方法设置属性# 可以在读取属性和修改属性的同时做一些其他的处理# 5.使用getter方法可以表示一些计算的属性class Dog: &#x27;&#x27;&#x27; 表示狗的类 &#x27;&#x27;&#x27; def __init__(self, name, age): self.hidden_name = name self.hidden_age = age def say_hello(self): print(&#x27;大家好，我是 %s&#x27; % self.hidden_name) def get_name(self): &#x27;&#x27;&#x27; get_name()用来获取对象的name属性 &#x27;&#x27;&#x27; # print(&#x27;用户读取了属性&#x27;) return self.hidden_name def set_name(self, name): # print(&#x27;用户修改了属性&#x27;) self.hidden_name = name def get_age(self): return self.hidden_age def set_age(self, age): if age &gt; 0: self.hidden_age = aged = Dog(&#x27;旺财&#x27;, 8)# d.say_hello()# 调用setter来修改name属性 d.set_name(&#x27;小黑&#x27;)d.set_age(-10)# d.say_hello()print(d.get_age()) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class Rectangle: &#x27;&#x27;&#x27; 表示矩形的类 &#x27;&#x27;&#x27; def __init__(self, width, height): self.hidden_width = width self.hidden_height = height def get_width(self): return self.hidden_width def get_height(self): return self.hidden_height def set_width(self, width): self.hidden_width = width def set_height(self, height): self.hidden_height = height def get_area(self): return self.hidden_width * self.hidden_height# 测试r = Rectangle(5, 2)print(r.get_area()) # 10r.set_width(10)r.set_height(20)print(r.get_area()) # 200# 可以为对象的属性使用双下划线开头，__xxx# 双下划线开头的属性，是对象的隐藏属性，隐藏属性只能在类的内部访问，无法通过对象访问# 其实隐藏属性只不过是Python自动为属性改了一个名字# 实际上是将名字修改为了，_类名__属性名 比如 __name -&gt; _Person__nameclass Person: def __init__(self, name): self.__name = name def get_name(self): return self.__name def set_name(self, name): self.__name = namep = Person(&#x27;孙悟空&#x27;)# print(p.__name) # __开头的属性是隐藏属性，无法通过对象访问print(p._Person__name) # 能直接访问，孙悟空p._Person__name = &#x27;猪八戒&#x27;print(p.get_name()) # 也能直接更改，猪八戒# 上面使用__开头的属性，实际上依然可以在外部访问，所以这种方式我们一般不用# 一般我们会将一些私有属性（不希望被外部访问的属性）以_开头（实际上也可以直接访问和修改）# 一般情况下，使用_开头的属性都是私有属性，没有特殊需要不要修改私有属性class Person: def __init__(self, name): self._name = name def get_name(self): return self._name def set_name(self, name): self._name = namep = Person(&#x27;孙悟空&#x27;)print(p._name) # 能直接访问p._name = &#x27;猪八戒&#x27;print(p._name) # 也能直接修改，猪八戒 123456789101112131415161718192021222324252627282930313233343536class Person: def __init__(self, name, age): self._name = name self._age = age # property装饰器，用来将一个get方法，转换为对象的属性 # 添加为property装饰器以后，我们就可以像调用属性一样使用get方法 # 使用property装饰的方法，必须和属性名是一样的 @property def name(self): print(&#x27;get方法执行了~~~&#x27;) return self._name # setter方法的装饰器：@属性名.setter @name.setter def name(self, name): print(&#x27;setter方法调用了&#x27;) self._name = name @property def age(self): return self._age @age.setter def age(self, age): self._age = agep = Person(&#x27;猪八戒&#x27;, 18)print(p.name, p.age) # 调用的就是装饰器装饰的setter和get方法p.name = &#x27;孙悟空&#x27;p.age = 28print(p.name, p.age) 继承123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 继承# 定义一个类 Animal（动物）# 这个类中需要两个方法：run() sleep()class Animal: def run(self): print(&#x27;动物会跑~~~&#x27;) def sleep(self): print(&#x27;动物睡觉~~~&#x27;) # def bark(self): # print(&#x27;动物嚎叫~~~&#x27;)# 定义一个类 Dog（狗）# 这个类中需要三个方法：run() sleep() bark()# class Dog:# def run(self):# print(&#x27;狗会跑~~~&#x27;)# def sleep(self):# print(&#x27;狗睡觉~~~&#x27;)# def bark(self):# print(&#x27;汪汪汪~~~&#x27;)# 有一个类，能够实现我们需要的大部分功能，但是不能实现全部功能# 如何能让这个类来实现全部的功能呢？# ① 直接修改这个类，在这个类中添加我们需要的功能# - 修改起来会比较麻烦，并且会违反OCP原则# ② 直接创建一个新的类# - 创建一个新的类比较麻烦，并且需要大量的进行复制粘贴，会出现大量的重复性代码# ③ 直接从Animal类中来继承它的属性和方法# - 继承是面向对象三大特性之一# - 通过继承我们可以使一个类获取到其他类中的属性和方法# - 在定义类时，可以在类名后的括号中指定当前类的父类（超类、基类、super）# 子类（衍生类）可以直接继承父类中的所有的属性和方法## 通过继承可以直接让子类获取到父类的方法或属性，避免编写重复性的代码，并且也符合OCP原则# 所以我们经常需要通过继承来对一个类进行扩展class Dog(Animal): def run(self): print(&#x27;狗跑~~~~&#x27;) def bark(self): print(&#x27;汪汪汪~~~&#x27;)class Hashiqi(Dog): def fan_sha(self): print(&#x27;我是一只傻傻的哈士奇&#x27;)d = Dog()d.run() # 狗跑~~~~d.sleep() # 动物睡觉~~~d.bark() # 汪汪汪~~~print(isinstance(d, Dog)) # Trueprint(isinstance(d, Animal)) # Trueh = Hashiqi()h.run() # 狗跑~~~~h.fan_sha() # 我是一只傻傻的哈士奇print(isinstance(h, Hashiqi)) # Trueprint(isinstance(h, Dog)) # Trueprint(isinstance(h, Animal)) # Trueprint(&#x27;######################################&#x27;)# 在创建类时，如果省略了父类，则默认父类为object# object是所有类的父类，所有类都继承自objectclass Person(object): pass# issubclass() 检查一个类是否是另一个类的子类print(issubclass(Animal, Dog)) # Falseprint(issubclass(Animal, object)) # Trueprint(issubclass(Person, object)) # True# isinstance()用来检查一个对象是否是一个类的实例# 如果这个类是这个对象的父类，也会返回True# 所有的对象都是object的实例print(isinstance(print, object)) # True 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Animal: def __init__(self, name): self._name = name def run(self): print(&#x27;动物会跑~~~&#x27;) def sleep(self): print(&#x27;动物睡觉~~~&#x27;) @property def name(self): return self._name @name.setter def name(self, name): self._name = name# 父类中的所有方法都会被子类继承，包括特殊方法，也可以重写特殊方法class Dog(Animal): def __init__(self, name, age): # 希望可以直接调用父类的__init__来初始化父类中定义的属性 # super() 可以用来获取当前类的父类， # 并且通过super()返回对象调用父类方法时，不需要传递self super().__init__(name) self._age = age def run(self): print(&#x27;狗跑~~~~&#x27;) def bark(self): print(&#x27;汪汪汪~~~&#x27;) @property def age(self): return self._age @age.setter def age(self, age): self._age = aged = Dog(&#x27;旺财&#x27;, 18)print(d.name) # 旺财print(d.age) # 18 重写123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 继承# 定义一个类 Animal（动物）# 这个类中需要两个方法：run() sleep()class Animal: def run(self): print(&#x27;动物会跑~~~&#x27;) def sleep(self): print(&#x27;动物睡觉~~~&#x27;)class Dog(Animal): def bark(self): print(&#x27;汪汪汪~~~&#x27;) def run(self): print(&#x27;狗跑~~~~&#x27;)# 如果在子类中如果有和父类同名的方法，则通过子类实例去调用方法时，# 会调用子类的方法而不是父类的方法，这个特点我们成为叫做方法的重写（覆盖，override）# 创建Dog类的实例d = Dog()d.run() # 狗跑~~~~# 当我们调用一个对象的方法时，# 会优先去当前对象中寻找是否具有该方法，如果有则直接调用# 如果没有，则去当前对象的父类中寻找，如果父类中有则直接调用父类中的方法，# 如果没有，则去父类的父类中寻找，以此类推，直到找到object，如果依然没有找到，则报错class A(object): def test(self): print(&#x27;AAA&#x27;)class B(A): def test(self): print(&#x27;BBB&#x27;)class C(B): def test(self): print(&#x27;CCC&#x27;)# 创建一个c的实例c = C()c.test() # CCC 多重继承123456789101112131415161718192021222324252627282930class A(object): def test(self): print(&#x27;AAA&#x27;)class B(object): def test(self): print(&#x27;B中的test()方法~~&#x27;) def test2(self): print(&#x27;BBB&#x27;)# 在Python中是支持多重继承的，也就是我们可以为一个类同时指定多个父类# 可以在类名的()后边添加多个类，来实现多重继承# 多重继承，会使子类同时拥有多个父类，并且会获取到所有父类中的方法# 在开发中没有特殊的情况，应该尽量避免使用多重继承，因为多重继承会让我们的代码过于复杂# 如果多个父类中有同名的方法，则会先在第一个父类中寻找，然后找第二个，然后找第三个。。。# 前边父类的方法会覆盖后边父类的方法class C(A, B): passc = C()c.test() # AAAc.test2() # BBB# 类名.__bases__ 这个属性可以用来获取当前类的所有父类，返回的是一个元组print(B.__bases__) # (&lt;class &#x27;object&#x27;&gt;,)print(C.__bases__) # (&lt;class &#x27;__main__.A&#x27;&gt;, &lt;class &#x27;__main__.B&#x27;&gt;) 多态1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 多态是面向对象的三大特征之一# 多态从字面上理解是多种形态# 狗（狼狗、藏獒、哈士奇、古牧 。。。）# 一个对象可以以不同的形态去呈现# 定义两个类class A: def __init__(self, name): self._name = name @property def name(self): return self._name @name.setter def name(self, name): self._name = nameclass B: def __init__(self, name): self._name = name def __len__(self): return 10 @property def name(self): return self._name @name.setter def name(self, name): self._name = nameclass C: passa = A(&#x27;孙悟空&#x27;)b = B(&#x27;猪八戒&#x27;)c = C()# 定义一个函数# 对于say_hello()这个函数来说，只要对象中含有name属性，它就可以作为参数传递# 这个函数并不会考虑对象的类型，只要有name属性即可 ---&gt; 多态的提现def say_hello(obj): print(&#x27;你好 %s&#x27; % obj.name)# 在say_hello_2中我们做了一个类型检查，也就是只有obj是A类型的对象时，才可以正常使用，# 其他类型的对象都无法使用该函数，这个函数就违反了多态# 违反了多态的函数，只适用于一种类型的对象，无法处理其他类型对象，这样导致函数的适应性非常的差# 注意，像isinstance()这种函数，在开发中一般是不会使用的！（使用这个函数，就表示可能违反了多态）def say_hello_2(obj): # 做类型检查 if isinstance(obj, A): print(&#x27;你好 %s&#x27; % obj.name) # say_hello(b)# say_hello_2(b)# 鸭子类型（多态理论）：# 如果一个东西，走路像鸭子，叫声像鸭子，那么它就是鸭子# len()# 之所以一个对象能通过len()来获取长度，是因为对象中具有一个特殊方法__len__# 换句话说，只要对象中具有__len__特殊方法，就可以通过len()来获取它的长度# 这就是多态的体现l = [1, 2, 3]s = &#x27;hello&#x27;print(len(l)) # 3print(len(s)) # 5print(len(b)) # 10# print(len(c)) # 报错，object of type &#x27;C&#x27; has no len()# 面向对象的三大特征：# 封装# - 确保对象中的数据安全# 继承# - 保证了对象的可扩展性# 多态# - 保证了程序的灵活性 类中的属性和方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# 定义一个类class A(object): # 类属性---所有实例公用的 # 实例属性---每个实例私有的 # 类方法 # 实例方法 # 静态方法 # 类属性，直接在类中定义的属性是类属性 # 类属性可以通过类或类的实例访问到 # 但是类属性只能通过类对象来修改，无法通过实例对象修改 count = 0 # __init__也是实例方法 def __init__(self): # 实例属性，通过实例对象添加的属性属于实例属性 # 实例属性只能通过实例对象来访问和修改，类对象无法访问修改 self.name = &#x27;孙悟空&#x27; # name也是实例属性 # 实例方法 # 在类中定义，以self为第一个参数的方法都是实例方法 # 实例方法在调用时，Python会将调用对象作为self传入 # 实例方法可以通过实例和类去调用 # 当通过实例调用时，会自动将当前调用对象作为self传入 # 当通过类调用时，不会自动传递self，此时我们必须手动传递self def test(self): print(&#x27;这是test方法~~~ &#x27;, self) # 类方法 # 在类内部使用 @classmethod 来修饰的方法属于类方法 # 类方法的第一个参数是cls，也会被自动传递，cls就是当前的类对象 # 类方法和实例方法的区别，实例方法的第一个参数是self，而类方法的第一个参数是cls # 类方法可以通过类去调用，也可以通过实例调用，没有区别 @classmethod def test_2(cls): print(&#x27;这是test_2方法，他是一个类方法~~~ &#x27;, cls) print(cls.count) # 这个访问的是类属性，与实例对象无关 # 静态方法 # 在类中使用 @staticmethod 来修饰的方法属于静态方法 # 静态方法不需要指定任何的默认参数，静态方法可以通过类和实例去调用 # 静态方法，基本上是一个和当前类无关的方法，它只是一个保存到当前类中的函数 # 静态方法一般都是一些工具方法，和当前类无关（建议静态方法不要放到某个类中，或者全部放到一个工具类中） @staticmethod def test_3(): print(&#x27;test_3执行了~~~&#x27;)print(&#x27;A &#x27;, A.count) # 类访问类属性：0a = A()print(&#x27;a &#x27;, a.count) # 类的实例访问类属性：0a.count = 10 # 类的实例无法修改类属性，此操作是给a这个实例对象，添加了一个实例属性countprint(&#x27;A &#x27;, A.count) # 0print(&#x27;a &#x27;, a.count) # 10A.count = 100 # 类可以修改类属性，但不影响类的实例中已存在的同名属性print(&#x27;A &#x27;, A.count) # 100print(&#x27;a &#x27;, a.count) # 10b = A() # b这个实例对象中，没有count实例属性，访问的是A类的属性print(&#x27;b &#x27;, b.count) # 100# print(&#x27;A &#x27;, A.name) # 类无法访问实例属性，AttributeError: type object &#x27;A&#x27; has no attribute &#x27;name&#x27;print(&#x27;a &#x27;, a.name) # 孙悟空# 类和类的实例，都可以访问实例方法a.test() # 等价于 A.test(a)：这是test方法~~~ &lt;__main__.A object at 0x000002631BC28310&gt;# 类和类的实例，都可以访问类方法A.test_2() # 等价于 a.test_2()：这是test_2方法，他是一个类方法~~~ &lt;class &#x27;__main__.A&#x27;&gt;# 静态方法，与类和类的实例无关A.test_3() # test_3执行了~~~a.test_3() # test_3执行了~~~b.test_3() # test_3执行了~~~ 垃圾回收12345678910111213141516171819202122232425# 就像我们生活中会产生垃圾一样，程序在运行过程当中也会产生垃圾# 程序运行过程中产生的垃圾会影响到程序的运行的运行性能，所以这些垃圾必须被及时清理# 没用的东西就是垃圾# 在程序中没有被引用的对象就是垃圾，这种垃圾对象过多以后会影响到程序的运行的性能# 所以我们必须进行及时的垃圾回收，所谓的垃圾回收就是将垃圾对象从内存中删除# 在Python中有自动的垃圾回收机制，它会自动将这些没有被引用的对象删除，# 所以我们不用手动处理垃圾回收class A: def __init__(self): self.name = &#x27;A类&#x27; # del是一个特殊方法，它会在垃圾对象被回收前调用 def __del__(self): print(&#x27;A()对象被回收了~~~&#x27;, self)a = A()print(a.name)# a = None # 将a设置为None，此时没有任何的变量对A()对象进行引用，A()对象变成了垃圾# 变成垃圾的A()对象会被回收，回收前调用__del__()方法# del a # del 会把a变量删除，也会导致A()对象变成垃圾input(&#x27;回车键退出程序...&#x27;) # 程序结束后，A()对象即使还在被a变量引用，仍然会被回收 特殊方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# 特殊方法，也称为魔术方法# 特殊方法都是使用__开头和结尾的# 特殊方法一般不需要我们手动调用，需要在一些特殊情况下自动执行# 定义一个Person类class Person(object): &quot;&quot;&quot;人类&quot;&quot;&quot; def __init__(self, name, age): self.name = name self.age = age # __str__()这个特殊方法会在尝试将对象转换为字符串的时候调用 # 它的作用可以用来指定对象转换为字符串的结果（print函数） def __str__(self): return &#x27;Person [name=%s , age=%d]&#x27; % (self.name, self.age) # __repr__()这个特殊方法会在对当前对象使用repr()函数时调用 # 它的作用是指定对象在‘交互模式’中直接输出的效果 def __repr__(self): return &#x27;Hello, this is repr&#x27; # 重写以下方法，让对象支持比较，以__gt__()为例说明 # object.__lt__(self, other) 小于 &lt; # object.__le__(self, other) 小于等于 &lt;= # object.__eq__(self, other) 等于 == # object.__ne__(self, other) 不等于 != # object.__gt__(self, other) 大于 &gt; # object.__ge__(self, other) 大于等于 &gt;= # __gt__（）会在对象做大于比较的时候调用，该方法的返回值将会作为比较的结果 # 他需要两个参数，一个self表示当前对象，other表示和当前对象比较的对象 # self &gt; other def __gt__(self, other): return self.age &gt; other.age # 以年龄作为比较的指标 # __len__() # 获取对象的长度 # object.__bool__(self) # 可以通过bool来指定对象转换为布尔值的情况 def __bool__(self): return self.age &gt; 17 # 运算的方法 # object.__add__(self, other) # object.__sub__(self, other) # object.__mul__(self, other) # object.__matmul__(self, other) # object.__truediv__(self, other) # object.__floordiv__(self, other) # object.__mod__(self, other) # object.__divmod__(self, other) # object.__pow__(self, other[, modulo]) # object.__lshift__(self, other) # object.__rshift__(self, other) # object.__and__(self, other) # object.__xor__(self, other) # object.__or__(self, other)# 创建两个Person类的实例p1 = Person(&#x27;孙悟空&#x27;, 18)p2 = Person(&#x27;猪八戒&#x27;, 28)# 打印p1# 当我们打印一个对象时，实际上打印的是对象的中特殊方法 __str__()的返回值# print(p1) # 不改写__str__()方法的输出结果：&lt;__main__.Person object at 0x04E95090&gt;print(p1) # 改写__str__()方法后的输出结果：Person [name=孙悟空 , age=18]print(repr(p1)) # Hello, this is repr# 大于比较方法print(p1 &gt; p2) # Falseprint(bool(p1)) # True# 条件不清晰，p1调用的就是__bool__()方法，一般不这样写# if p1:# print(p1.name, &#x27;已经成年了&#x27;)# else:# print(p1.name, &#x27;还未成年了&#x27;) 模块化 简介： 1234567891011121314151617181920212223242526# 模块（module）# 模块化，模块化指将一个完整的程序分解为一个一个小的模块# 通过将模块组合，来搭建出一个完整的程序# 不采用模块化：统一将所有的代码编写到一个文件中# 采用模块化：将程序分别编写到多个文件中# 模块化的优点：# ① 方便开发# ② 方便维护# ③ 模块可以复用！# 在Python中一个py文件就是一个模块，要想创建模块，实际上就是创建一个python文件# 注意：模块名要符号标识符的规范# 在一个模块中引入外部模块：# ① import 模块名 （模块名，就是python文件的名字，注意不要.py后缀）# ② import 模块名 as 模块别名# - 可以引入同一个模块多次，但是模块的实例只会创建一个# - import可以在程序的任意位置调用，但是一般情况下，import语句都会统一写在程序的开头# - 在每一个模块内部都有一个__name__属性，通过这个属性可以获取到模块的名字# - __name__属性值为 __main__的模块是主模块，一个程序中只会有一个主模块# 主模块就是我们直接通过 python 执行的模块（当前程序所在的模块）import test_module as testprint(__name__) # 主模块：__main__print(test.__name__) # 引入的外部模块：test_module m.py： 1234567891011121314151617181920212223242526272829303132# 可以在模块中定义变量，在模块中定义的变量，在引入该模块后，就可以直接使用了a = 10b = 20# 添加了_的变量，只能在模块内部访问，在通过 import * 方式引入时，不会引入_开头的变量_c = 30# 可以在模块中定义函数，同样可以通过模块访问到def test(): print(&#x27;test&#x27;)def test2(): print(&#x27;test2&#x27;)# 也可以定义类class Person: def __init__(self): self.name = &#x27;孙悟空&#x27;# 编写测试代码：# 这部分代码，只有当前模块作为主模块的时候才需要被执行# 而当前模块被其他模块引入时，不需要被执行# 此时，我们就必须要检查当前模块是否是主模块if __name__ == &#x27;__main__&#x27;: test() test2() p = Person() print(p.name) main.py： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import m# 访问模块中的变量：模块名.变量名print(m.a, m.b) # 10 20.# print(m._c) # 此方式可以访问_c属性# 访问模块中的方法：模块名.方法名m.test() # testm.test2() # test2# 访问模块中的类：模块名.类名，创建类的实例p = m.Person()print(p.name) # 孙悟空# 也可以只引入模块中的部分内容# 语法： from 模块名 import 变量, 变量....# from m import Person # 只引入Person# from m import test # 只引入testfrom m import Person, test # 引入多个# 通过上面方式引入后，可以直接使用p1 = Person()print(p1) # &lt;m.Person object at 0x00000115DD088160&gt;test() # test# test2() # test2()没有引入，不能直接使用# from m import * # 引入模块中所有内容，一般不会使用# 当前模块中，会覆盖被引入模块中的同名方法def test2(): print(&#x27;这是主模块中的test2&#x27;)test2() # 这是主模块中的test2# 也可以为引入的变量使用别名# 语法：from 模块名 import 变量 as 别名from m import test2 as new_test2test2() # 这是主模块中的test2new_test2() # test2# from m import *# print(_c) # _c属性无法访问# 总结：# import xxx# import xxx as yyy# from xxx import yyy , zzz , fff# from xxx import *# from xxx import yyy as zz 包 结构： hello/__init__.py： 12def test(): print(&#x27;test&#x27;) hello/a.py： 1c = 30 hello/b.py： 1d = 40 main.py： 123456789101112131415# 包 Package# 包也是一个模块# 当我们模块中代码过多时，或者一个模块需要被分解为多个模块时，这时就需要使用到包# 普通的模块就是一个py文件，而包是一个文件夹# 包中必须要有一个 __init__.py 文件，这个文件中可以包含有包中的主要内容from hello import a, bprint(a.c)print(b.d)# __pycache__ 是模块的缓存文件# .py代码在执行前，需要被解析器先转换为机器码，然后再执行# 所以我们在使用模块（包）时，也需要将模块的代码先转换为机器码，然后再交由计算机执行# 而为了提高程序运行的性能，python会在编译过一次以后，将代码保存到一个缓存文件中# 这样在下次加载这个模块（包）时，就可以不再重新编译而是直接加载缓存中编译好的代码即可 Python 标准库1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# 思想：开箱即用# 为了实现开箱即用的思想，Python中为我们提供了一个模块的标准库# 在这个标准库中，有很多很强大的模块我们可以直接使用，并且标准库会随Python的安装一同安装# sys模块：# 它里面提供了一些变量和函数，使我们可以获取到Python解析器的信息# 或者通过函数来操作Python解析器# 引入sys模块：import sysprint(sys) # &lt;module &#x27;sys&#x27; (built-in)&gt;# sys.argv：# 命令行执行代码时，获取命令行中所包含的参数# 该属性是一个列表，列表中保存了当前命令的所有参数# 参考IDEA中Java程序main()方法模块参数的引入，注意第一个参数print(sys.argv) # [&#x27;D:/JetBrainsWorkSpace/PycharmProjects/main.py&#x27;, &#x27;aaa&#x27;, &#x27;bbb&#x27;]# sys.modules：# 获取当前程序中引入的所有模块# modules是一个字典，字典的key是模块的名字，字典的value是模块对象print(sys.modules) # &#123;&#x27;sys&#x27;: &lt;module &#x27;sys&#x27; (built-in)&gt;, &#x27;builtins&#x27;: &lt;module &#x27;builtins&#x27; (built-in)&gt;, ......&#125;# pprint模块：# print()打印不会格式化数据# 它给我们提供了一个方法pprint()，该方法可以用来对打印的数据做简单的格式化# 引入pprint模块：import pprintpprint.pprint(sys.modules)# sys.path：# 他是一个列表，列表中保存的是模块的搜索路径，不要轻易更改# [&#x27;D:\\\\JetBrainsWorkSpace\\\\PycharmProjects&#x27;,# &#x27;D:\\\\JetBrainsWorkSpace\\\\PycharmProjects&#x27;,# &#x27;D:\\\\Program Files\\\\PyCharm Professional Edition with Anaconda plugin &#x27;# &#x27;2020.1.2\\\\plugins\\\\python\\\\helpers\\\\pycharm_display&#x27;,# &#x27;D:\\\\Program\\\\Miniconda3\\\\python38.zip&#x27;,# &#x27;D:\\\\Program\\\\Miniconda3\\\\DLLs&#x27;,# &#x27;D:\\\\Program\\\\Miniconda3\\\\lib&#x27;,# &#x27;D:\\\\Program\\\\Miniconda3&#x27;,# &#x27;D:\\\\Program\\\\Miniconda3\\\\lib\\\\site-packages&#x27;,# &#x27;D:\\\\Program\\\\Miniconda3\\\\lib\\\\site-packages\\\\win32&#x27;,# &#x27;D:\\\\Program\\\\Miniconda3\\\\lib\\\\site-packages\\\\win32\\\\lib&#x27;,# &#x27;D:\\\\Program\\\\Miniconda3\\\\lib\\\\site-packages\\\\Pythonwin&#x27;,# &#x27;D:\\\\Program Files\\\\PyCharm Professional Edition with Anaconda plugin &#x27;# &#x27;2020.1.2\\\\plugins\\\\python\\\\helpers\\\\pycharm_matplotlib_backend&#x27;]pprint.pprint(sys.path)# sys.platform：# 表示当前Python运行的平台print(sys.platform) # win32# sys.exit()：# 函数用来退出程序# sys.exit(&#x27;程序出现异常，结束！&#x27;) # 后面的print(&#x27;hello&#x27;)语句不再执行# print(&#x27;hello&#x27;)# os模块：# 让我们可以对操作系统进行访问import os# os.environ：# 通过这个属性可以获取到系统的环境变量pprint.pprint(os.environ) # 所有的pprint.pprint(os.environ[&#x27;path&#x27;]) # 只查看path环境变量# os.system()：# 可以用来执行操作系统的命令os.system(&#x27;dir&#x27;) # dir命令os.system(&#x27;notepad&#x27;) # 打开记事本名令 命令行执行代码时的参数： 12PS D:\\JetBrainsWorkSpace\\PycharmProjects&gt; python main.py aaa bbb[&#x27;main.py&#x27;, &#x27;aaa&#x27;, &#x27;bbb&#x27;] 异常和文件异常 程序在运行过程当中，不可避免的会出现一些错误，比如：使用了没有赋值过的变量，使用了不存在的索引，除 0 等。这些错误在程序中，我们称其为异常。 程序运行过程中，一旦出现异常将会导致程序立即终止，异常以后的代码全部都不会执行！ 处理异常 程序运行时出现异常，目的并不是让我们的程序直接终止！Python 是希望在出现异常时，我们可以编写代码来对异常进行处理！ try 语句： 123456789101112try: 代码块（可能出现错误的语句）except 异常类型 as 异常名: 代码块（出现错误以后的处理方式）except 异常类型 as 异常名: 代码块（出现错误以后的处理方式）except 异常类型 as 异常名: 代码块（出现错误以后的处理方式）else: 代码块（没出错时要执行的语句）finally: 代码块（该代码块总会执行） try 是必须的，else 语句有没有都行，except 和 finally 至少有一个。 可以将可能出错的代码放入到 try 语句，这样如果代码没有错误，则会正常执行，如果出现错误，则会执行 expect 子句中的代码，这样我们就可以通过代码来处理异常，避免因为一个异常导致整个程序的终止。 异常的传播1234567891011121314151617def fn(): print(&#x27;Hello fn&#x27;) print(a) print(10 / 0)def fn2(): print(&#x27;Hello fn2&#x27;) fn()def fn3(): print(&#x27;Hello fn3&#x27;) fn2()fn3() 当在函数中出现异常时，如果在函数中对异常进行了处理，则异常不会再继续传播，如果函数中没有对异常进行处理，则异常会继续向函数调用处传播，如果函数调用处处理了异常，则不再传播，如果没有处理则继续向调用处传播，直到传递到全局作用域（主模块），如果依然没有处理，则程序终止，并且显示异常信息。 当程序运行过程中出现异常以后，所有的异常信息会被保存一个专门的异常对象中，而异常传播时，实际上就是异常对象抛给了调用处。比如： ZeroDivisionError 类的对象专门用来表示除 0 的异常。 NameError 类的对象专门用来处理变量错误的异常。 在 Python 为我们提供了多个异常对象。 异常对象1234567891011121314151617181920212223print(&#x27;异常出现前&#x27;)l = []try: # print(c) # l[10] # 1 + &#x27;hello&#x27; print(10 / 0)except NameError: # 如果except后不跟任何的内容，则此时它会捕获到所有的异常 # 如果在except后跟着一个异常的类型，那么此时它只会捕获该类型的异常 print(&#x27;出现 NameError 异常&#x27;)except ZeroDivisionError: print(&#x27;出现 ZeroDivisionError 异常&#x27;)except IndexError: print(&#x27;出现 IndexError 异常&#x27;)# Exception 是所有异常类的父类，所以如果except后跟的是Exception，他也会捕获到所有的异常# 可以在异常类后边跟着一个 as xx 此时xx就是异常对象except Exception as e: # 等同于 except: print(&#x27;未知异常&#x27;, e, type(e))finally: print(&#x27;无论是否出现异常，该子句都会执行&#x27;)print(&#x27;异常出现后&#x27;) 抛出异常12345678910111213141516171819# 也可以自定义异常类，只需要创建一个类继承Exception即可class MyError(Exception): passdef add(a, b): # 如果a和b中有负数，就向调用处抛出异常 if a &lt; 0 or b &lt; 0: # raise用于向外部抛出异常，后边可以跟一个异常类，或异常类的实例 # raise ExceptioUS20190040060A1-20190207.XMLn # 抛出异常的目的，告诉调用者这里调用时出现问题，希望你自己处理一下 # raise Exception(&#x27;两个参数中不能有负数！&#x27;) raise MyError(&#x27;自定义的异常&#x27;) # 也可以通过if else来代替异常的处理，但无法考虑到所有可能出异常的条件，难以控制 return a + bprint(add(-123, 456)) 可以使用 raise 语句来抛出异常，raise 语句后需要跟一个异常类或异常的实例。 文件 通过 Python 程序来对计算机中的各种文件进行增删改查的操作。 操作文件的步骤： 打开文件。 对文件进行各种操作（读、写），然后保存。 关闭文件。 打开文件1234567891011121314151617181920212223242526# open(file, mode=&#x27;r&#x27;, buffering=-1, encoding_=None, errors=None, newline=None, closefd=True, opener=None)# 使用open函数来打开一个文件# 参数：# file 要打开的文件的名字（路径）# 返回值：# 返回一个对象，这个对象就代表了当前打开的文件# 创建一个变量，来保存文件的名字# 如果目标文件和当前文件在同一级目录下，则直接使用文件名即可file_name = &#x27;demo.txt&#x27;# 在windows系统使用路径时，可以使用/来代替 \\# 或者可以使用 \\\\ 来代替 \\# 或者也可以使用原始字符串# file_name = &#x27;hello\\\\demo.txt&#x27;# file_name = r&#x27;hello\\demo.txt&#x27;# 表示路径，可以使用..来返回一级目录# file_name = &#x27;../hello/demo.txt&#x27;# 如果目标文件距离当前文件比较远，此时可以使用绝对路径# 绝对路径应该从磁盘的根目录开始书写# file_name = r&#x27;C:\\Users\\XiSun\\Desktop\\hello.txt&#x27;file_obj = open(file_name) # 打开 file_name 对应的文件print(file_obj) # &lt;_io.TextIOWrapper name=&#x27;demo.txt&#x27; mode=&#x27;r&#x27; encoding=&#x27;cp936&#x27;&gt; 关闭文件123456789101112131415161718192021222324252627282930313233# 打开文件file_name = &#x27;demo.txt&#x27;# 调用open()来打开文件# file_obj = open(file_name)# 当我们获取了文件对象以后，所有的对文件的操作都应该通过对象来进行# 读取文件中的内容：# read()方法，用来读取文件中的内容，它会将内容全部保存为一个字符串返回# content = file_obj.read()# print(content)# 关闭文件：# 调用close()方法来关闭文件# file_obj.close()# 上面的open()和close()是常规写法，可以用下面的方式简化操作# with ... as 语句：with open(file_name) as file_obj: # 在with语句中可以直接使用file_obj来做文件操作 # 此时这个文件只能在with中使用，一旦with结束则文件会自动close() print(file_obj.read())# 进一步完善，添加文件读取异常处理# 文件处理的标准格式：file_name = &#x27;hello.txt&#x27;try: with open(file_name) as file_obj: print(file_obj.read())except FileNotFoundError: print(f&#x27;&#123;file_name&#125; 文件不存在~~&#x27;) 读取文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657file_name = &#x27;demo.txt&#x27;try: # 调用open()来打开一个文件，可以将文件分成两种类型 # 一种，是纯文本文件（使用utf-8等编码编写的文本文件） # 一种，是二进制文件（图片、mp3、ppt等这些文件） # open()打开文件时，默认是以文本文件的形式打开的，但是open()默认的编码为None # 所以处理文本文件时，必须要指定文件的编码 with open(file_name, encoding=&#x27;utf-8&#x27;) as file_obj: # 通过 read() 来读取文件中的内容 # 如果直接调用read()它会将文本文件的所有内容全部都读取出来 # 如果要读取的文件较大的话，会一次性将文件的内容加载到内存中，容易导致内存泄漏 # 所以对于较大的文件，不要直接调用read() # help(file_obj.read)查看帮助信息 # read()可以接收一个size作为参数，该参数用来指定要读取的字符的数量 # 默认值为-1，它会读取文件中的所有字符 # 可以为size指定一个值，这样read()会读取指定数量的字符， # 每一次读取都是从上次读取到位置开始读取的 # 如果字符的数量小于size，则会读取剩余所有的 # 如果已经读取到了文件的最后了，则会返回&#x27;&#x27;空串 # content = file_obj.read(-1) content = file_obj.read(6) content = file_obj.read(6) content = file_obj.read(6) content = file_obj.read(6) # print(content) # print(len(content))except FileNotFoundError: print(f&#x27;&#123;file_name&#125; 这个文件不存在！&#x27;)# 读取大文件的方式file_name = &#x27;demo.txt&#x27;try: with open(file_name, encoding=&#x27;utf-8&#x27;) as file_obj: # 定义一个变量，来保存文件的内容 file_content = &#x27;&#x27; # 定义一个变量，来指定每次读取的大小 chunk = 100 # 创建一个循环来读取文件内容 while True: # 读取chunk大小的内容 content = file_obj.read(chunk) # 检查是否读取到了内容 if not content: # 内容读取完毕，退出循环 break # 输出内容 # print(content, end=&#x27;&#x27;) file_content += contentexcept FileNotFoundError: print(f&#x27;&#123;file_name&#125; 这个文件不存在！&#x27;)print(file_content) 1234567891011121314151617181920212223import pprintfile_name = &#x27;demo.txt&#x27;try: with open(file_name, encoding=&#x27;utf-8&#x27;) as file_obj: # readline()： # 该方法可以用来读取一行内容 # print(file_obj.readline(), end=&#x27;&#x27;) # print()打印自带换行符 # readlines()： # 该方法用于一行一行的读取内容，它会一次性将读取到的内容封装到一个列表中返回 # r = file_obj.readlines() # pprint.pprint(r[0]) # &#x27;aaa\\n&#x27; # pprint.pprint(r[1]) # &#x27;bbb\\n&#x27; # pprint.pprint(r[2]) # &#x27;ccc\\n&#x27; # 简化写法 for t in file_obj: print(t, end=&#x27;&#x27;)except FileNotFoundError: print(f&#x27;&#123;file_name&#125; 这个文件不存在！&#x27;) 写入文件1234567891011121314151617181920212223242526file_name = &#x27;demo1.txt&#x27;# 使用open()打开文件时必须要指定打开文件所要做的操作：读、写、追加# 如果不指定操作类型，则默认是 `读取文件` ，而读取文件时是不能向文件中写入的# r：表示只读的# w：表示是可写的，使用w来写入文件时，如果文件不存在会创建文件，如果文件存在则会截断文件# 截断文件指删除原来文件中的所有内容（覆盖原文件内容）# a：表示追加内容，如果文件不存在会创建文件，如果文件存在则会向文件中追加内容# x：用来新建文件，如果文件不存在则创建，存在则报错# +：为操作符增加功能# r+：即可读又可写，文件不存在会报错# w+：即可写又可读# a+：即可追加又可读# with open(file_name , &#x27;w&#x27; , encoding=&#x27;utf-8&#x27;) as file_obj:# with open(file_name , &#x27;r+&#x27; , encoding=&#x27;utf-8&#x27;) as file_obj:with open(file_name, &#x27;x&#x27;, encoding=&#x27;utf-8&#x27;) as file_obj: # write()来向文件中写入内容 # 如果操作的是一个文本文件的话，则write()需要传递一个字符串作为参数 # 该方法会可以分多次向文件中写入内容 # 写入完成以后，该方法会返回写入的字符的个数 file_obj.write(&#x27;aaa\\n&#x27;) file_obj.write(&#x27;bbb\\n&#x27;) file_obj.write(&#x27;ccc\\n&#x27;) file_obj.write(str(123) + &#x27;123123\\n&#x27;) # 要转换为字符串，且不会自动换行 r = file_obj.write(&#x27;今天天气真不错&#x27;) print(r) # 7 二进制文件操作123456789101112131415161718192021222324252627282930file_name = &#x27;F:/QQMusic/FLOW - Go!!!.flac&#x27;# 读取模式：# t：读取文本文件（默认值）# b：读取二进制文件with open(file_name, &#x27;rb&#x27;) as file_obj: # 读取文本文件时，size是以字符为单位的 # 读取二进制文件时，size是以字节为单位 # print(file_obj.read(100)) # 将读取到的内容写出来 # 定义一个新的文件 new_name = &#x27;aa.flac&#x27; with open(new_name, &#x27;wb&#x27;) as new_obj: # 定义每次读取的大小 chunk = 1024 * 100 # 100 KB while True: # 从已有的对象中读取数据 content = file_obj.read(chunk) # 内容读取完毕，终止循环 if not content: break # 将读取到的数据写入到新对象中 new_obj.write(content) seek() 和 tell()12345678910111213141516171819202122232425262728# 二进制文件with open(&#x27;demo.txt&#x27;, &#x27;rb&#x27;) as file_obj: # print(file_obj.read(30)) # print(file_obj.read(100)) # tell()方法用来查看当前读取的位置 print(&#x27;当前读取到了 --&gt;&#x27;, file_obj.tell()) # seek()方法可以修改当前读取的位置 # seek()需要两个参数： # 第一个：要切换到的位置 # 第二个：计算位置的方式 # 可选值： # 0 从头计算，默认值 # 1 从当前位置计算 # 2 从最后位置开始计算 # file_obj.seek(55) # file_obj.seek(80, 0) # file_obj.seek(70, 1) file_obj.seek(-10, 2) # 从文件的最后往前读10个字符 print(file_obj.read())# 文本文件with open(&#x27;demo.txt&#x27;, &#x27;rt&#x27;, encoding=&#x27;utf-8&#x27;) as file_obj: print(&#x27;当前读取到了 --&gt;&#x27;, file_obj.tell()) file_obj.seek(9) # 从头开始计算，切换到第9个字节开始读取 print(file_obj.read()) 文件的其他操作1234567891011121314151617181920212223242526272829303132333435import osfrom pprint import pprint# os.listdir()：获取指定目录的目录结构# 需要一个路径作为参数，会获取到该路径下的目录结构，默认路径为 . ，即当前目录# 该方法会返回一个列表，目录中的每一个文件（夹）的名字都是列表中的一个元素r = os.listdir()pprint(r) # [&#x27;.idea&#x27;, &#x27;aa.flac&#x27;, &#x27;demo.txt&#x27;, &#x27;hello&#x27;, &#x27;main.py&#x27;, &#x27;__pycache__&#x27;]# os.getcwd()：获取当前所在的目录r = os.getcwd()pprint(r) # &#x27;D:\\\\JetBrainsWorkSpace\\\\PycharmProjects&#x27;# os.chdir()：切换当前所在的目录 作用相当于cd命令# os.chdir(&#x27;c:/&#x27;)# r = os.getcwd()# pprint(r) # &#x27;c:\\\\&#x27;# os.mkdir()；创建目录os.mkdir(&quot;aaa&quot;) # 在当前目录下创建一个名字为 aaa 的目录，如果已存在，则报错# os.rmdir()：删除目录os.rmdir(&#x27;aaa&#x27;) # 在当前目录下删除一个名字为 aaa 的目录，如果不存在，则报错# open()；打开文件open(&#x27;aa.txt&#x27;, &#x27;w&#x27;)# os.remove()：删除文件os.remove(&#x27;aa.flac&#x27;)# os.rename(&#x27;旧名字&#x27;,&#x27;新名字&#x27;)：可以对一个文件进行重命名，也可以用来移动一个文件os.rename(&#x27;hello&#x27;, &#x27;bb.txt&#x27;)# os.rename(&#x27;bb.txt&#x27;, &#x27;c:/users/lilichao/desktop/bb.txt&#x27;)pprint(r) 本文参考https://www.liaoxuefeng.com/wiki/1016959663602400 https://www.bilibili.com/video/BV1hW41197sB?from=search&amp;seid=1852797992981366365&amp;spm_id_from=333.337.0.0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"虚拟机安装","slug":"linux-virtualmachine","date":"2021-08-26T03:14:14.000Z","updated":"2021-08-31T02:21:11.474Z","comments":true,"path":"2021/08/26/linux-virtualmachine/","link":"","permalink":"http://example.com/2021/08/26/linux-virtualmachine/","excerpt":"","text":"VMware 安装 下载地址：https://my.vmware.com/cn/web/vmware/downloads 下载版本：VMware Workstation Pro VMware 15 秘钥：https://www.cnblogs.com/liuqun/p/11737327.html Vmware 创建新的虚拟机，首先配置硬件清单，重要步骤如下： CentOS 安装 准备工作，检查 BIOS 虚拟化支持： 打开任务管理器，进入性能，查看虚拟化是否启用。 若虚拟化未启用，重启电脑，F2 进入 BIOS 模式（不同主板快捷键不同），进入高级模式页面，Advanced —&gt; CPU Configuration，开启虚拟化支持。 下载地址：https://developer.aliyun.com/mirror/centos?spm=a2c6h.13651102.0.0.3e221b11se5c1r 下载版本：dvd 标准安装版。 安装参考：https://blog.csdn.net/qq_44714603/article/details/88829423 Vmware 配置 CentOS 软件，即，向虚拟机插入系统盘： 开启虚拟机，安装系统盘并配置： 语言环境： 日期和时间： 软件选择： 安装位置，自定义磁盘分区，配置 boot，swap 和根目录： 禁用 kdump 设置，如果是正式开发阶段，应该启用 kdump 设置： 网络和主机名： 安全策略： 等上面配置完成之后，开始安装系统盘： 配置 IP 地址和主机名称 VMware： Window 10（即本机）： 如果没有出现 VMnet 8 选项，回到 VMware 的虚拟网络编辑器，点击“更改设置” —&gt; “还原默认设置”，即可。 虚拟机： 切换 root 用户： 12345678 [xisun@centos7 ~]$ su root 密码： [root@centos7 xisun]# - 设置虚拟机 IP 地址： ```sh [root@centos7 xisun]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 ifcfg-ens33 文件原内容： 123456789101112131415TYPE=&quot;Ethernet&quot;PROXY_METHOD=&quot;none&quot;BROWSER_ONLY=&quot;no&quot;BOOTPROTO=&quot;dhcp&quot; # 动态获取IP地址，服务器每次开机时，IP地址可能发生改变DEFROUTE=&quot;yes&quot;IPV4_FAILURE_FATAL=&quot;no&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;IPV6_DEFROUTE=&quot;yes&quot;IPV6_FAILURE_FATAL=&quot;no&quot;IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;NAME=&quot;ens33&quot;UUID=&quot;eb503f88-96af-455d-b8f9-dbda02ca79d4&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot; ifcfg-ens33 文件新内容： 1234567891011121314151617181920TYPE=&quot;Ethernet&quot;PROXY_METHOD=&quot;none&quot;BROWSER_ONLY=&quot;no&quot;BOOTPROTO=&quot;static&quot; # 修改为静态IP地址DEFROUTE=&quot;yes&quot;IPV4_FAILURE_FATAL=&quot;no&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;IPV6_DEFROUTE=&quot;yes&quot;IPV6_FAILURE_FATAL=&quot;no&quot;IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;NAME=&quot;ens33&quot;UUID=&quot;eb503f88-96af-455d-b8f9-dbda02ca79d4&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot;# 新增IPADDR=192.168.10.99 # 静态IP地址，按需自定义GATEWAY=192.168.10.2 # 网关DNS1=192.168.10.2 # 域名解析器 修改虚拟机的主机名称： 1[root@centos7 xisun]# vim /etc/hostname 1centos7 # 主机名称按需求自定义 修改虚拟机主机的名称映射： 1[root@centos7 xisun]# vim /etc/hosts 原文件内容： 12127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 新内容： 1234567891011121314127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# 按需求添加主机名称映射192.168.10.99 centos7192.168.10.100 hadoop100192.168.10.101 hadoop101192.168.10.102 hadoop102192.168.10.103 hadoop103192.168.10.104 hadoop104192.168.10.105 hadoop105192.168.10.106 hadoop106192.168.10.107 hadoop107192.168.10.108 hadoop108 修改 Windows 10 主机的名称映射：C:\\Windows\\System32\\drivers\\etc\\hosts 如果操作系统是 Window7，可以直接修改 hosts 文件；如果操作系统是 Window10，需要先将 hosts 文件拷贝出来，修改保存以后，再覆盖原文件即可。 hosts 文件新增如下主机映射： 12345678910192.168.10.99 centos7192.168.10.100 hadoop100192.168.10.101 hadoop101192.168.10.102 hadoop102192.168.10.103 hadoop103192.168.10.104 hadoop104192.168.10.105 hadoop105192.168.10.106 hadoop106192.168.10.107 hadoop107192.168.10.108 hadoop108 重启： 1[root@centos7 xisun]# reboot 重启之后，以 root 用户重新登陆。 验证虚拟机 IP 地址： 123456789101112131415161718192021222324252627[root@centos7 ~]# ifconfigens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.10.99 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::ac1e:7fe1:a566:2670 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:1c:d5:13 txqueuelen 1000 (Ethernet) RX packets 2033 bytes 2797234 (2.6 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 924 bytes 61834 (60.3 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 48 bytes 4080 (3.9 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 48 bytes 4080 (3.9 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255 ether 52:54:00:97:ed:a7 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 验证是否能连通外网： 123456789101112131415161718[root@centos7 ~]# ping www.baidu.com PING www.a.shifen.com (14.215.177.39) 56(84) bytes of data.64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=1 ttl=128 time=38.4 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=2 ttl=128 time=38.6 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=3 ttl=128 time=38.4 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=4 ttl=128 time=39.3 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=5 ttl=128 time=38.3 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=6 ttl=128 time=38.6 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=7 ttl=128 time=38.3 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=8 ttl=128 time=38.4 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=9 ttl=128 time=38.9 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=10 ttl=128 time=38.3 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=11 ttl=128 time=38.7 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=12 ttl=128 time=38.3 ms^C--- www.a.shifen.com ping statistics ---12 packets transmitted, 12 received, 0% packet loss, time 11035msrtt min/avg/max/mdev = 38.357/38.606/39.328/0.332 ms 查看主机地址： 12[root@centos7 ~]# hostnamecentos7 Xshell 远程连接虚拟机 安装过程略。 远程连接配置： 连接成功： 数据传输：安装 Xftp 工具，或者使用 rz 和 sz 命令。 克隆虚拟机 新配置的 centos7 虚拟机，可以作为一个纯净的虚拟机，在此基础上，克隆出新的虚拟机，在新虚拟机上安装软件，而纯净的虚拟机留作备用。 克隆虚拟机之前，需要正确的关闭虚拟机： 克隆： 常规操作： 移除，此操作只会在 VMware 列表中移除虚拟机，但不会删除磁盘上的虚拟机： 添加，通过打开操作，可以添加磁盘上的虚拟机到 VMware 列表中： 删除，此操作会将磁盘上的虚拟机删除： 修改克隆机的 IP 地址和主机名称 克隆机的信息，和被克隆机相同，需要修改 IP 地址，以及主机名称。 开启克隆机，以 root 用户登录。 修改 IP 地址： 1[root@centos7 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 修改 ifcfg-ens33 文件中的 IPADDR： 12345678910111213141516171819TYPE=&quot;Ethernet&quot;PROXY_METHOD=&quot;none&quot;BROWSER_ONLY=&quot;no&quot;BOOTPROTO=&quot;static&quot;DEFROUTE=&quot;yes&quot;IPV4_FAILURE_FATAL=&quot;no&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;IPV6_DEFROUTE=&quot;yes&quot;IPV6_FAILURE_FATAL=&quot;no&quot;IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;NAME=&quot;ens33&quot;UUID=&quot;eb503f88-96af-455d-b8f9-dbda02ca79d4&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot;IPADDR=192.168.10.100 # 只需要将IP地址按需修改即可GATEWAY=192.168.10.2DNS1=192.168.10.2 修改主机名称： 1[root@centos7 ~]# vim /etc/hostname 1hadoop100 重启： 1[root@centos7 ~]# reboot 查看新的 IP 地址和主机名称： 1234567891011121314151617181920212223242526272829[root@hadoop100 ~]# ifconfigens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.10.100 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::ac1e:7fe1:a566:2670 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:1d:f8:56 txqueuelen 1000 (Ethernet) RX packets 604 bytes 804811 (785.9 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 279 bytes 21418 (20.9 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 48 bytes 4080 (3.9 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 48 bytes 4080 (3.9 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255 ether 52:54:00:97:ed:a7 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@hadoop100 ~]# hostnamehadoop100 JDK 说明 安装的 centos7 模板机，有自带的 JDK，某些情况，需要删除，按照需求自行安装。 123456789101112[xisun@centos7 ~]$ su root密码：[root@centos7 ~]# rpm -qa | grep -i javajavapackages-tools-3.4.1-11.el7.noarchtzdata-java-2019c-1.el7.noarchjava-1.8.0-openjdk-headless-1.8.0.242.b08-1.el7.x86_64java-1.8.0-openjdk-1.8.0.242.b08-1.el7.x86_64java-1.7.0-openjdk-headless-1.7.0.251-2.6.21.1.el7.x86_64python-javapackages-3.4.1-11.el7.noarchjava-1.7.0-openjdk-1.7.0.251-2.6.21.1.el7.x86_64[root@centos7 ~]# rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps[root@centos7 ~]# rpm -qa | grep -i java 本文参考https://www.bilibili.com/video/BV1Qp4y1n7EN 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"Hadoop 入门","slug":"hadoop","date":"2021-08-25T07:53:38.000Z","updated":"2022-01-12T02:43:37.786Z","comments":true,"path":"2021/08/25/hadoop/","link":"","permalink":"http://example.com/2021/08/25/hadoop/","excerpt":"","text":"大数据大数据概念 大数据（Big Data）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 大数据主要解决，海量数据的采集、存储和分析计算问题。 按顺序给出数据存储单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。 1 Byte = 8 bit，1 KB = 1024 Byte，1 MB = 1024 KB，1 GB = 1024 MB，1 TB = 1024 GB，1 PB= 1024 TB 大数据特点 Volume（大量） 截至目前，人类生产的所有印刷材料的数据量是 200 PB，而历史上全人类总共说过的话的数据量大约是 5 EB。当前，典型个人计算机硬盘的容量为 TB 量级，而一些大企业的数据量已经接近 EB 量级。 Velocity（高速） 这是大数据区分于传统数据挖掘的最显著特征。根据 IDC 的“数字宇宙”的报告，预计到 2025 年，全球数据使用量将达到 163 ZB。在如此海量的数据面前，处理数据的效率就是企业的生命。 天猫双十一：2017 年，3 分 01 秒，天猫交易额超过 100 亿；2020 年，96 秒，天猫交易额超过 100 亿。 Variety（多样） 这种类型的多样性也让数据被分为结构化数据和非结构化数据。相对于以往便于存储的以数据库/文本为主的结构化数据，非结构化数据越来越多，包括网络日志、音频、视频、图片、地理位置信息等，这些多类型的数据对数据的处理能力提出了更高要求。 Value（低价值密度） 价值密度的高低与数据总量的大小成反比。比如，在一天的监控视频中，我们只关心宋老师晚上在床上健身那一分钟，如何快速对有价值数据“提纯”，成为目前大数据背景下待解决的难题。 Hadoop 概述Hadoop 是什么 Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构。 主要解决，海量数据的存储和海量数据的分析计算问题。 广义上来说，Hadoop 通常是指一个更广泛的概念 — Hadoop 生态圈。 Hadoop 发展历史 Hadoop 创始人 Doug Cutting，为了实现与 Google 类似的全文搜索功能，他在 Lucene 框架基础上进行优化升级，查询引擎和索引引擎。 2001 年年底，Lucene 成为 Apache 基金会的一个子项目。 对于海量数据的场景，Lucene 框架面对与 Google 同样的困难，存储海量数据困难，检索海量速度慢。 学习和模仿 Google 解决这些问题的办法：微型版 Nutch。 可以说 Google 是 Hadoop 的思想之源（Google 在大数据方面的三篇论文）： GFS —&gt; HDFS MapReduce —&gt; MR BigTable —&gt; HBase 2003 - 2004 年，Google 公开了部分 GFS 和 MapReduce 思想的细节，以此为基础，Doug Cutting 等人用了 2 年业余时间实现了 DFS 和 MapReduce 机制，使 Nutch 性能飙升。 2005 年，Hadoop 作为 Lucene 的子项目 Nutch 的一部分正式引入 Apache 基金会。 2006 年 3 月份，MapReduce 和 Nutch Distributed File System（NDFS）分别被纳入到 Hadoop 项目中，Hadoop 就此正式诞生，标志着大数据时代来临。 名字来源于 Doug Cutting 儿子的玩具大象： Hadoop 三大发行版本 Hadoop 三大发行版本：Apache、Cloudera、Hortonworks。 Apache 版本是最原始（最基础）的版本，对于入门学习最好。— 2006 年 Cloudera 内部集成了很多大数据框架，对应产品 CDH。— 2008 年 Hortonworks 文档较好，对应产品 HDP。— 2011 年 Hortonworks 现在已经被 Cloudera 公司收购，推出新的品牌 CDP。 Apache Hadoop 官网地址：http://hadoop.apache.org 下载地址：https://hadoop.apache.org/releases.html Cloudera Hadoop 官网地址：https://www.cloudera.com/downloads/cdh 下载地址：https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_download.html Hortonworks Hadoop 官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform Hadoop 优势 高可靠性：Hadoop 底层维护多个数据副本，所以即使 Hadoop 某个计算元素或存储出现故障，也不会导致数据的丢失。 高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。 高效性：在 MapReduce 的思想下，Hadoop 是并行工作的，以加快任务处理速度。 高容错性：能够自动将失败的任务重新分配。 Hadoop 组成 Hadoop 1.x 时 代 ，Hadoop 中的 MapReduce 同时处理业务逻辑运算和资源的调度，耦合性较大。 Hadoop 2.x 时代，增加了 Yarn。Yarn 只负责资源的调度，MapReduce 只负责运算。 Hadoop 3.x 时代，在组成上没有变化。 HDFS 架构概述 Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。包含三个模块： NameNode：简称 nn，存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的 DataNode 等。 DataNode：简称 dn，在本地文件系统存储文件块数据，以及块数据的校验和。 Secondary NameNode：简称 2nn，每隔一段时间对 NameNode 元数据备份。 YARN 架构概述 Yet Another Resource Negotiator，简称 YARN ，另一种资源协调者，是 Hadoop 的资源管理器。有两大组件： ResourceManager：简称 RM，整个集群资源（内存、CPU 等）的管理者。 NodeManager：简称 NM，单个节点服务器资源的管理者。每个 NodeManager 上可以有多个 Container。 Container：容器，相当于一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。 ApplicationMaster：简称 AM，单个任务运行的管理者。 客户端 client 可以有多个。 集群上可以运行多个 ApplicationMaster。 MapReduce 架构概述 MapReduce 将计算过程分为两个阶段：Map 和 Reduce。 Map 阶段并行处理输入数据。 Reduce 阶段对 Map 结果进行汇总。 HDFS 、YARN 、MapReduce 三者关系 大数据技术生态体系 Sqoop：Sqoop 是一款开源的工具，主要用于在 Hadoop、Hive 与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如：MySQL，Oracle 等）中的数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。 Flume：Flume 是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume 支持在日志系统中定制各类数据发送方，用于收集数据。 Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统。 Spark：Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。 Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。 Oozie：Oozie 是一个管理 Hadoop 作业（job）的工作流程调度管理系统。 Hbase：HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。 Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的 MapReduce 应用，十分适合数据仓库的统计分析。 ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。 推荐系统框架图 Hadoop 运行环境搭建模板虚拟机环境准备 安装模板虚拟机，IP 地址 192.168.10.100、主机名称 hadoop100、内存 2 G、硬盘 50 G。 主机名称不要起 hadoop，hadoop000 等特殊名称。 开启虚拟机，切换到 root 用户操作下面得命令： 12[xisun@hadoop100 ~]$ su root密码： 确保虚拟机可以正常上网： 12345678910[root@hadoop100 xisun]# ping www.baidu.comPING www.a.shifen.com (14.215.177.39) 56(84) bytes of data.64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=1 ttl=128 time=41.0 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=2 ttl=128 time=40.9 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=3 ttl=128 time=41.3 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=4 ttl=128 time=42.5 ms^C--- www.a.shifen.com ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3007msrtt min/avg/max/mdev = 40.987/41.495/42.582/0.674 ms 安装 epel-release： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@hadoop100 xisun]# yum install -y epel-release已加载插件：fastestmirror, langpacksLoading mirror speeds from cached hostfile * base: mirrors.njupt.edu.cn * extras: mirrors.njupt.edu.cn * updates: mirrors.njupt.edu.cnbase | 3.6 kB 00:00:00 extras | 2.9 kB 00:00:00 updates | 2.9 kB 00:00:00 正在解决依赖关系--&gt; 正在检查事务---&gt; 软件包 epel-release.noarch.0.7-11 将被 安装--&gt; 解决依赖关系完成依赖关系解决=================================================================================================================================================================================================================== Package 架构 版本 源 大小===================================================================================================================================================================================================================正在安装: epel-release noarch 7-11 extras 15 k事务概要===================================================================================================================================================================================================================安装 1 软件包总下载量：15 k安装大小：24 kDownloading packages:警告：/var/cache/yum/x86_64/7/extras/packages/epel-release-7-11.noarch.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: NOKEYepel-release-7-11.noarch.rpm 的公钥尚未安装epel-release-7-11.noarch.rpm | 15 kB 00:00:00 从 file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 检索密钥导入 GPG key 0xF4A80EB5: 用户ID : &quot;CentOS-7 Key (CentOS 7 Official Signing Key) &lt;security@centos.org&gt;&quot; 指纹 : 6341 ab27 53d7 8a78 a7c2 7bb1 24c6 a8a7 f4a8 0eb5 软件包 : centos-release-7-8.2003.0.el7.centos.x86_64 (@anaconda) 来自 : /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7Running transaction checkRunning transaction testTransaction test succeededRunning transaction 正在安装 : epel-release-7-11.noarch 1/1 验证中 : epel-release-7-11.noarch 1/1 已安装: epel-release.noarch 0:7-11 完毕！ Extra Packages for Enterprise Linux 是为“红帽系”的操作系统提供额外的软件包，适用于 RHEL、CentOS 和 Scientific Linux。相当于是一个软件仓库，大多数 rpm 包在官方 repository 中是找不到的。 如果 Linux 安装的是最小系统版，还需要安装如下工具，如果安装的是 Linux 桌面标准版，不需要执行如下操作（本机安装的是桌面版）： net-tool：工具包集合，包含 ifconfig 等命令。 1[root@hadoop100 xisun]# yum install -y net-tools vim：编辑器。 1[root@hadoop100 xisun]# yum install -y vim 关闭防火墙，关闭防火墙开机自启： 查看防火墙状态： 12[root@hadoop100 xisun]# firewall-cmd --staterunning 关闭防火墙： 123[root@hadoop100 xisun]# systemctl stop firewalld.service[root@hadoop100 xisun]# firewall-cmd --statenot running 关闭防火墙开机自启： 123[root@hadoop100 xisun]# systemctl disable firewalld.service Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service. 在企业开发时，通常单个服务器的防火墙是关闭的，公司整体对外访问时会设置非常安全的防火墙。 创建新用户，并修改新用户的密码（可省略）： 12[root@hadoop100 xisun]# useradd xisun[root@hadoop100 xisun]# passwd xisun 生产环境下，应避免使用 root 用户直接操作。 配置刚创建的新用户具有 root 权限，方便后期加 sudo 执行 root 权限的命令： 1[root@hadoop100 xisun]# vim /etc/sudoers 修改 /etc/sudoers 文件，在 %wheel 这行下面添加一行，将新用户 xisun 设置为免密使用 root 权限，如下所示： 1234567891011## Allow root to run any commands anywhere root ALL=(ALL) ALL## Allows members of the &#x27;sys&#x27; group to run networking, software, ## service management apps and more.# %sys ALL = NETWORKING, SOFTWARE, SERVICES, STORAGE, DELEGATING, PROCESSES, LOCATE, DRIVERS## Allows people in group wheel to run all commands%wheel ALL=(ALL) ALLxisun ALL=(ALL) NOPASSWD:ALL xisun 这一行不要直接放到 root 行下面，因为所有用户都属于 wheel 组，如果放在 root 行下面，则是先配置了 xisun 用户在使用 sudo 命令时具有免输入密码功能，但是程序执行到 %wheel 行时，该功能又会被覆盖回需要密码。所以 xisun 这一行要放到 %wheel 这行下面。 从 root 用户退回到 xisun 用户： 12[root@hadoop100 xisun]# exitexit 在 /opt 目录下创建文件夹： 12345678910111213[xisun@hadoop100 ~]$ cd /opt/[xisun@hadoop100 opt]$ sudo mkdir module[xisun@hadoop100 opt]$ sudo mkdir software[xisun@hadoop100 opt]$ ll总用量 0drwxr-xr-x. 2 root root 6 8月 30 22:08 moduledrwxr-xr-x. 2 root root 6 10月 31 2018 rhdrwxr-xr-x. 2 root root 6 8月 30 22:09 software[xisun@hadoop100 opt]$ sudo rm -r rh[xisun@hadoop100 opt]$ ll总用量 0drwxr-xr-x. 2 root root 6 8月 30 22:08 moduledrwxr-xr-x. 2 root root 6 8月 30 22:09 software /opt 目录下，需要使用 sudo 命令才能创建和删除文件夹： 12[xisun@hadoop100 opt]$ mkdir testmkdir: 无法创建目录&quot;test&quot;: 权限不够 修改创建的文件夹所属主和所属组为 xisun 用户： 12345678910[xisun@hadoop100 opt]$ ll总用量 0drwxr-xr-x. 2 root root 6 8月 30 22:08 moduledrwxr-xr-x. 2 root root 6 8月 30 22:09 software[xisun@hadoop100 opt]$ sudo chown xisun:xisun module/[xisun@hadoop100 opt]$ sudo chown xisun:xisun software/[xisun@hadoop100 opt]$ ll总用量 0drwxr-xr-x. 2 xisun xisun 6 8月 30 22:08 moduledrwxr-xr-x. 2 xisun xisun 6 8月 30 22:09 software 卸载虚拟机自带的 JDK，以 root 用户执行： 123456789101112[xisun@hadoop100 opt]$ su root密码：[root@hadoop100 opt]# rpm -qa | grep -i javajavapackages-tools-3.4.1-11.el7.noarchtzdata-java-2019c-1.el7.noarchjava-1.8.0-openjdk-headless-1.8.0.242.b08-1.el7.x86_64java-1.8.0-openjdk-1.8.0.242.b08-1.el7.x86_64java-1.7.0-openjdk-headless-1.7.0.251-2.6.21.1.el7.x86_64python-javapackages-3.4.1-11.el7.noarchjava-1.7.0-openjdk-1.7.0.251-2.6.21.1.el7.x86_64[root@hadoop100 opt]# rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps[root@hadoop100 opt]# rpm -qa | grep -i java 如果你的虚拟机是最小化安装不需要执行这一步。 rpm -qa：查询所安装的所有 rpm 软件包。 grep -i：忽略大小写。 xargs -n1：表示每次只传递一个参数。 rpm -e –nodeps：强制卸载软件。 重启虚拟机： 1[root@hadoop100 opt]# reboot 克隆虚拟机 利用模板机 hadoop100，克隆三台虚拟机：hadoop102，hadoop103，hadoop104。 注意：克隆时，要先关闭 hadoop100。 修改克隆机的 IP 地址和主机名，以 hadoop102 为例，进行说明： 开启 hadoop102，以 root 账户登录。 修改 IP 地址： 1[root@hadoop100 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 修改 ifcfg-ens33 文件中的 IPADDR： 12345678910111213141516171819TYPE=&quot;Ethernet&quot;PROXY_METHOD=&quot;none&quot;BROWSER_ONLY=&quot;no&quot;BOOTPROTO=&quot;static&quot;DEFROUTE=&quot;yes&quot;IPV4_FAILURE_FATAL=&quot;no&quot;IPV6INIT=&quot;yes&quot;IPV6_AUTOCONF=&quot;yes&quot;IPV6_DEFROUTE=&quot;yes&quot;IPV6_FAILURE_FATAL=&quot;no&quot;IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;NAME=&quot;ens33&quot;UUID=&quot;eb503f88-96af-455d-b8f9-dbda02ca79d4&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot;IPADDR=192.168.10.102 # 修改IP地址为192.168.10.102GATEWAY=192.168.10.2DNS1=192.168.10.2 修改主机名： 1[root@hadoop100 ~]# vim /etc/hostname 1hadoop102 重启： 1[root@hadoop100 ~]# reboot 验证 IP 地址和主机名，以及网络是否正常： 12345678910111213141516171819202122232425262728293031323334353637383940[root@hadoop102 ~]# ifconfigens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.10.102 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::ac1e:7fe1:a566:2670 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:c5:1d:96 txqueuelen 1000 (Ethernet) RX packets 1025 bytes 878131 (857.5 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 442 bytes 35254 (34.4 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 48 bytes 4080 (3.9 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 48 bytes 4080 (3.9 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255 ether 52:54:00:97:ed:a7 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@hadoop102 ~]# hostnamehadoop102[root@hadoop102 ~]# ping www.baidu.comPING www.a.shifen.com (14.215.177.39) 56(84) bytes of data.64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=1 ttl=128 time=42.9 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=2 ttl=128 time=42.6 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=3 ttl=128 time=42.7 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=4 ttl=128 time=42.8 ms64 bytes from 14.215.177.39 (14.215.177.39): icmp_seq=5 ttl=128 time=42.8 ms^C--- www.a.shifen.com ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4012msrtt min/avg/max/mdev = 42.689/42.814/42.930/0.276 ms 按照相同的步骤，修改 hadoop103 的 IP 地址为 192.168.10.103，主机名为 hadoop103，hadoop104 的 IP 地址为 192.168.10.104，主机名为 hadoop104，并验证。 安装 JDK 下面步骤以 hadoop102 为例，进行说明。 安装 JDK 前，一定确保提前删除了虚拟机自带的 JDK，此步骤在前面已执行。 1[xisun@hadoop102 ~]$ rpm -qa | grep -i java 1[xisun@hadoop100 ~]$ rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps 12[xisun@hadoop102 ~]$ java -versionbash: java: 未找到命令... 安装 OpenJDK 8。 下载地址：https://openjdk.java.net/，https://openjdk.java.net/install/index.html 使用 root 权限，以命令行安装： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091[xisun@hadoop102 ~]$ sudo yum install -y java-1.8.0-openjdk-devel已加载插件：fastestmirror, langpacksLoading mirror speeds from cached hostfile * base: mirrors.cqu.edu.cn * epel: mirror.sjtu.edu.cn * extras: mirrors.cn99.com * updates: mirrors.cn99.com正在解决依赖关系--&gt; 正在检查事务---&gt; 软件包 java-1.8.0-openjdk-devel.x86_64.1.1.8.0.302.b08-0.el7_9 将被 安装--&gt; 正在处理依赖关系 java-1.8.0-openjdk(x86-64) = 1:1.8.0.302.b08-0.el7_9，它被软件包 1:java-1.8.0-openjdk-devel-1.8.0.302.b08-0.el7_9.x86_64 需要--&gt; 正在处理依赖关系 libjvm.so()(64bit)，它被软件包 1:java-1.8.0-openjdk-devel-1.8.0.302.b08-0.el7_9.x86_64 需要--&gt; 正在处理依赖关系 libjava.so()(64bit)，它被软件包 1:java-1.8.0-openjdk-devel-1.8.0.302.b08-0.el7_9.x86_64 需要--&gt; 正在检查事务---&gt; 软件包 java-1.8.0-openjdk.x86_64.1.1.8.0.302.b08-0.el7_9 将被 安装---&gt; 软件包 java-1.8.0-openjdk-headless.x86_64.1.1.8.0.302.b08-0.el7_9 将被 安装--&gt; 正在处理依赖关系 tzdata-java &gt;= 2021a，它被软件包 1:java-1.8.0-openjdk-headless-1.8.0.302.b08-0.el7_9.x86_64 需要--&gt; 正在处理依赖关系 jpackage-utils，它被软件包 1:java-1.8.0-openjdk-headless-1.8.0.302.b08-0.el7_9.x86_64 需要--&gt; 正在检查事务---&gt; 软件包 javapackages-tools.noarch.0.3.4.1-11.el7 将被 安装--&gt; 正在处理依赖关系 python-javapackages = 3.4.1-11.el7，它被软件包 javapackages-tools-3.4.1-11.el7.noarch 需要---&gt; 软件包 tzdata-java.noarch.0.2021a-1.el7 将被 安装--&gt; 正在检查事务---&gt; 软件包 python-javapackages.noarch.0.3.4.1-11.el7 将被 安装--&gt; 解决依赖关系完成依赖关系解决=================================================================================================================================================================================================================== Package 架构 版本 源 大小===================================================================================================================================================================================================================正在安装: java-1.8.0-openjdk-devel x86_64 1:1.8.0.302.b08-0.el7_9 updates 9.8 M为依赖而安装: java-1.8.0-openjdk x86_64 1:1.8.0.302.b08-0.el7_9 updates 311 k java-1.8.0-openjdk-headless x86_64 1:1.8.0.302.b08-0.el7_9 updates 33 M javapackages-tools noarch 3.4.1-11.el7 base 73 k python-javapackages noarch 3.4.1-11.el7 base 31 k tzdata-java noarch 2021a-1.el7 updates 191 k事务概要===================================================================================================================================================================================================================安装 1 软件包 (+5 依赖软件包)总下载量：43 M安装大小：152 MDownloading packages:(1/6): python-javapackages-3.4.1-11.el7.noarch.rpm | 31 kB 00:00:00 (2/6): java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64.rpm | 311 kB 00:00:00 (3/6): javapackages-tools-3.4.1-11.el7.noarch.rpm | 73 kB 00:00:00 (4/6): tzdata-java-2021a-1.el7.noarch.rpm | 191 kB 00:00:01 (5/6): java-1.8.0-openjdk-headless-1.8.0.302.b08-0.el7_9.x86_64.rpm | 33 MB 00:00:07 (6/6): java-1.8.0-openjdk-devel-1.8.0.302.b08-0.el7_9.x86_64.rpm | 9.8 MB 00:00:13 -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------总计 3.1 MB/s | 43 MB 00:00:13 Running transaction checkRunning transaction testTransaction test succeededRunning transaction警告：RPM 数据库已被非 yum 程序修改。** 发现 9 个已存在的 RPM 数据库问题， &#x27;yum check&#x27; 输出如下：icedtea-web-1.7.1-2.el7_6.x86_64 有缺少的需求 java-1.8.0-openjdkicedtea-web-1.7.1-2.el7_6.x86_64 有缺少的需求 jpackage-utilsicedtea-web-1.7.1-2.el7_6.x86_64 有缺少的需求 jpackage-utilsjline-1.0-8.el7.noarch 有缺少的需求 java &gt;= (&#x27;0&#x27;, &#x27;1.5&#x27;, None)jline-1.0-8.el7.noarch 有缺少的需求 jpackage-utilsrhino-1.7R5-1.el7.noarch 有缺少的需求 jpackage-utilsrhino-1.7R5-1.el7.noarch 有缺少的需求 jpackage-utilstagsoup-1.2.1-8.el7.noarch 有缺少的需求 jpackage-utilstagsoup-1.2.1-8.el7.noarch 有缺少的需求 jpackage-utils &gt;= (&#x27;0&#x27;, &#x27;1.6&#x27;, None) 正在安装 : tzdata-java-2021a-1.el7.noarch 1/6 正在安装 : python-javapackages-3.4.1-11.el7.noarch 2/6 正在安装 : javapackages-tools-3.4.1-11.el7.noarch 3/6 正在安装 : 1:java-1.8.0-openjdk-headless-1.8.0.302.b08-0.el7_9.x86_64 4/6 正在安装 : 1:java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64 5/6 正在安装 : 1:java-1.8.0-openjdk-devel-1.8.0.302.b08-0.el7_9.x86_64 6/6 验证中 : 1:java-1.8.0-openjdk-headless-1.8.0.302.b08-0.el7_9.x86_64 1/6 验证中 : python-javapackages-3.4.1-11.el7.noarch 2/6 验证中 : tzdata-java-2021a-1.el7.noarch 3/6 验证中 : 1:java-1.8.0-openjdk-devel-1.8.0.302.b08-0.el7_9.x86_64 4/6 验证中 : 1:java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64 5/6 验证中 : javapackages-tools-3.4.1-11.el7.noarch 6/6 已安装: java-1.8.0-openjdk-devel.x86_64 1:1.8.0.302.b08-0.el7_9 作为依赖被安装: java-1.8.0-openjdk.x86_64 1:1.8.0.302.b08-0.el7_9 java-1.8.0-openjdk-headless.x86_64 1:1.8.0.302.b08-0.el7_9 javapackages-tools.noarch 0:3.4.1-11.el7 python-javapackages.noarch 0:3.4.1-11.el7 tzdata-java.noarch 0:2021a-1.el7 完毕！ 公司生产环境使用的是 OpenJDK 8，此处保持一致。 验证 JDK 是否安装成功： 1234[xisun@hadoop102 ~]$ java -versionopenjdk version &quot;1.8.0_302&quot;OpenJDK Runtime Environment (build 1.8.0_302-b08)OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode) 配置 JDK 环境变量： 查看 jre/bin 路径： 12[xisun@hadoop102 ~]$ dirname $(readlink $(readlink $(which java)))/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64/jre/bin 新建 /etc/profile.d/my_env.sh 文件，添加 Java 环境变量： 1[xisun@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh 123# JAVA_HOMEexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64export PATH=$PATH:$JAVA_HOME/bin 正常情况下，会将 Java 环境变量添加在 /etc/profile 文件的最后，但该文件有如下设置，因此，可以在 /etc/profile.d 路径下自定义一个以 sh 结尾的文件，能达到同样的效果： source 一下 /etc/profile 文件，让新的环境变量 PATH 生效： 1[xisun@hadoop102 etc]$ source /etc/profile 查看配置是否生效： 12[xisun@hadoop102 etc]$ echo $JAVA_HOME/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64 安装 Hadoop 下面步骤以 hadoop102 为例，进行说明。 下载地址：https://hadoop.apache.org/releases.html 公司生产环境使用的是 hadoop-3.2.1，此处保持一致。 在 opt/software 路径下，使用 wget 命令下载安装包： 12345678910111213141516[xisun@hadoop102 software]$ pwd/opt/software[xisun@hadoop102 software]$ wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz--2021-08-31 11:44:19-- https://archive.apache.org/dist/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz正在解析主机 archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2正在连接 archive.apache.org (archive.apache.org)|138.201.131.134|:443... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度：359196911 (343M) [application/x-gzip]正在保存至: “hadoop-3.2.1.tar.gz”100%[=========================================================================================================================================================================&gt;] 359,196,911 57.8KB/s 用时 57m 32s2021-08-31 12:41:54 (102 KB/s) - 已保存 “hadoop-3.2.1.tar.gz” [359196911/359196911])[xisun@hadoop102 software]$ ll总用量 350780-rw-rw-r--. 1 xisun xisun 359196911 7月 3 2020 hadoop-3.2.1.tar.gz 解压安装包到 /opt/module 路径下面： 1[xisun@hadoop102 software]$ tar -zxvf hadoop-3.2.1.tar.gz -C /opt/module/ 123[xisun@hadoop102 software]$ ll ../module/总用量 0drwxr-xr-x. 9 xisun xisun 149 9月 11 2019 hadoop-3.2.1 将 Hadoop 添加到环境变量： 获取 Hadoop 安装路径： 12[xisun@hadoop102 hadoop-3.2.1]$ pwd/opt/module/hadoop-3.2.1 修改 etc/profile.d/my_env.sh 文件，添加 Hadoop 环境变量： 1[xisun@hadoop102 hadoop-3.2.1]$ sudo vim /etc/profile.d/my_env.sh 1234# HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-3.2.1export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin source 一下 /etc/profile 文件，让新的环境变量 PATH 生效： 1[xisun@hadoop102 hadoop-3.2.1]$ source /etc/profile 查看配置是否生效： 123456789[xisun@hadoop102 hadoop-3.2.1]$ echo $HADOOP_HOME/opt/module/hadoop-3.2.1[xisun@hadoop102 hadoop-3.2.1]$ hadoop versionHadoop 3.2.1Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842Compiled by rohithsharmaks on 2019-09-10T15:56ZCompiled with protoc 2.5.0From source with checksum 776eaf9eee9c0ffc370bcbc1888737This command was run using /opt/module/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar Hadoop 目录结构 查看 Hadoop 的目录结构： 1234567891011121314[xisun@hadoop102 hadoop-3.2.1]$ pwd/opt/module/hadoop-3.2.1[xisun@hadoop102 hadoop-3.2.1]$ ll总用量 180drwxr-xr-x. 2 xisun xisun 203 9月 11 2019 bindrwxr-xr-x. 3 xisun xisun 20 9月 10 2019 etcdrwxr-xr-x. 2 xisun xisun 106 9月 11 2019 includedrwxr-xr-x. 3 xisun xisun 20 9月 11 2019 libdrwxr-xr-x. 4 xisun xisun 288 9月 11 2019 libexec-rw-rw-r--. 1 xisun xisun 150569 9月 10 2019 LICENSE.txt-rw-rw-r--. 1 xisun xisun 22125 9月 10 2019 NOTICE.txt-rw-rw-r--. 1 xisun xisun 1361 9月 10 2019 README.txtdrwxr-xr-x. 3 xisun xisun 4096 9月 10 2019 sbindrwxr-xr-x. 4 xisun xisun 31 9月 11 2019 share 重要目录 bin 目录：存放对 Hadoop 相关服务（hdfs，yarn，mapred）进行操作的脚本。 12[xisun@hadoop102 hadoop-3.2.1]$ ls bin/container-executor hadoop hadoop.cmd hdfs hdfs.cmd mapred mapred.cmd oom-listener test-container-executor yarn yarn.cmd etc 目录：Hadoop 的配置文件目录，存放 Hadoop 的配置文件。 12345678[xisun@hadoop102 hadoop-3.2.1]$ ls etc/hadoop[xisun@hadoop102 hadoop-3.2.1]$ ls etc/hadoop/capacity-scheduler.xml hadoop-env.sh httpfs-env.sh kms-env.sh mapred-env.sh ssl-server.xml.example yarnservice-log4j.propertiesconfiguration.xsl hadoop-metrics2.properties httpfs-log4j.properties kms-log4j.properties mapred-queues.xml.template user_ec_policies.xml.template yarn-site.xmlcontainer-executor.cfg hadoop-policy.xml httpfs-signature.secret kms-site.xml mapred-site.xml workerscore-site.xml hadoop-user-functions.sh.example httpfs-site.xml log4j.properties shellprofile.d yarn-env.cmdhadoop-env.cmd hdfs-site.xml kms-acls.xml mapred-env.cmd ssl-client.xml.example yarn-env.sh lib 目录：存放 Hadoop 的本地库（对数据进行压缩解压缩功能）。 sbin 目录：存放启动或停止 Hadoop 相关服务的脚本。 1234[xisun@hadoop102 hadoop-3.2.1]$ ls sbin/distribute-exclude.sh hadoop-daemons.sh mr-jobhistory-daemon.sh start-all.sh start-dfs.sh start-yarn.sh stop-balancer.sh stop-secure-dns.sh workers.shFederationStateStore httpfs.sh refresh-namenodes.sh start-balancer.sh start-secure-dns.sh stop-all.cmd stop-dfs.cmd stop-yarn.cmd yarn-daemon.shhadoop-daemon.sh kms.sh start-all.cmd start-dfs.cmd start-yarn.cmd stop-all.sh stop-dfs.sh stop-yarn.sh yarn-daemons.sh share 目录：存放 Hadoop 的依赖 JAR 包、文档、和官方案例。 Hadoop 运行模式 Hadoop 官方网站：http://hadoop.apache.org/ Hadoop 运行模式包括：本地模式、伪分布式模式以及完全分布式模式。 本地模式：单机运行，只是用来演示一下官方案例。 数据存储在 Linux 本地，偶尔测试时使用。 伪分布式模式：也是单机运行，但是具备 Hadoop 集群的所有功能，一台服务器模拟一个分布式的环境。 数据存储在 HDFS 上，个别缺钱的公司用来测试，生产环境一般不用。 完全分布式模式：多台服务器组成分布式环境。 数据存储在 HDFS 上，企业生产环境大量使用。 本地模式 以官方 WordCount 进行说明。 第一步：在 hadoop-3.2.1 文件下面创建一个 wcinput 文件夹。 1[xisun@hadoop102 hadoop-3.2.1]$ mkdir wcinput 第二步：在 wcinput 文件下创建一个 word.txt 文件，并输入一些单词做测试。 1[xisun@hadoop102 wcinput]$ vim word.txt 1234hadoop yarnhadoop mapreducexisunxisun 第三步：回到 Hadoop 目录 /opt/module/hadoop-3.2.1。 第四步：执行 wordcount 程序，统计 word.txt 文件中各单词的个数。 1[xisun@hadoop102 hadoop-3.2.1]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount wcinput/ ./wcoutput 本地模式下，输入路径 /wcinput 和输出路径 ./wcoutput 都是本地路径。 注意：结果输出路径 wcoutput 不能已经存在，否则程序会报错 org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/opt/module/hadoop-3.2.1/wcoutput already exists。 第五步：查看结果。 12345678910[xisun@hadoop102 hadoop-3.2.1]$ cd wcoutput/[xisun@hadoop102 wcoutput]$ ll总用量 4-rw-r--r--. 1 xisun xisun 36 8月 31 16:04 part-r-00000-rw-r--r--. 1 xisun xisun 0 8月 31 16:04 _SUCCESS[xisun@hadoop102 wcoutput]$ cat part-r-00000 hadoop 2mapreduce 1xisun 2yarn 1 完全分布式模式虚拟机准备 虚拟机 hadoop102 已准备好，参考前面章节。 在 hadoop103 和 hadoop104 安装 OpenJDK： 1[xisun@hadoop103 ~]$ sudo yum install -y java-1.8.0-openjdk-devel hadoop102，hadoop103 和 hadoop104 三台虚拟机，安装的 JDK 环境和 Hadoop 版本相同，因此，环境变量配置也相同。对于这种情况，不需要在每台虚拟机上再配置环境变量，可以使用 scp 或 rsync 命令等，直接拷贝模板虚拟机 hadoop102 上的配置文件到 hadoop103 和 hadoop104 上。在后面章节，也可以使用集群分发脚本 xsync 拷贝。 如果使用 scp 命令，拷贝 hadoop102 上环境变量配置的 etc/profile.d/my_env.sh 文件到 hadoop103 和 hadoop104 的 etc/profile.d 路径下，方式如下。 方式一，在 hadoop102 上使用 scp 命令，拷贝本地文件到 hadoop103 上： 12345678[xisun@hadoop102 profile.d]$ sudo scp /etc/profile.d/my_env.sh root@hadoop103:/etc/profile.d/The authenticity of host &#x27;hadoop103 (192.168.10.103)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:AOkUHU40E6uekNRiFpZkT4R2gfoE+s9ujdYTZ5e8kwM.ECDSA key fingerprint is MD5:dd:80:45:3e:83:75:92:fe:57:d3:78:fa:af:5a:ca:1b.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;hadoop103,192.168.10.103&#x27; (ECDSA) to the list of known hosts.root@hadoop103&#x27;s password: my_env.sh 方式二，在 hadoop104 上使用 scp 命令，拷贝 hadoop102 上的文件到本地： 12345678[xisun@hadoop104 profile.d]$ sudo scp root@hadoop102:/etc/profile.d/my_env.sh /etc/profile.d/The authenticity of host &#x27;hadoop102 (192.168.10.102)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:AOkUHU40E6uekNRiFpZkT4R2gfoE+s9ujdYTZ5e8kwM.ECDSA key fingerprint is MD5:dd:80:45:3e:83:75:92:fe:57:d3:78:fa:af:5a:ca:1b.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;hadoop102,192.168.10.102&#x27; (ECDSA) to the list of known hosts.root@hadoop102&#x27;s password: my_env.sh 方式三，在 hadoop103 上使用 scp 命令，拷贝 hadoop102 上的文件到 hadoop104 上： 1[xisun@hadoop103 profile.d]$ sudo scp root@hadoop102:/etc/profile.d/my_env.sh root@hadoop104:/etc/profile.d/ 如果使用集群分发脚本 xsync 拷贝，需要注意，my_env.sh 文件是 root 权限的，需要给脚本添加 sudo 命令，同时，xsync 脚本需要补全路径，否则 sudo 识别不出来： 1234567891011121314151617181920212223242526272829[xisun@hadoop102 ~]$ sudo /home/xisun/bin/xsync /etc/profile.d/my_env.sh ==================== hadoop102 ====================root@hadoop102&#x27;s password: root@hadoop102&#x27;s password: sending incremental file listsent 48 bytes received 12 bytes 24.00 bytes/sectotal size is 253 speedup is 4.22==================== hadoop103 ====================root@hadoop103&#x27;s password: root@hadoop103&#x27;s password: sending incremental file listmy_env.shsent 348 bytes received 35 bytes 153.20 bytes/sectotal size is 253 speedup is 0.66==================== hadoop104 ====================The authenticity of host &#x27;hadoop104 (192.168.10.104)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:AOkUHU40E6uekNRiFpZkT4R2gfoE+s9ujdYTZ5e8kwM.ECDSA key fingerprint is MD5:dd:80:45:3e:83:75:92:fe:57:d3:78:fa:af:5a:ca:1b.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;hadoop104,192.168.10.104&#x27; (ECDSA) to the list of known hosts.root@hadoop104&#x27;s password: root@hadoop104&#x27;s password: sending incremental file listmy_env.shsent 348 bytes received 35 bytes 153.20 bytes/sectotal size is 253 speedup is 0.66 注意，需要输入的是各主机 root 用户的密码。 在 hadoop103 和 hadoop104 上 source 一下 /etc/profile 文件，让新的环境变量 PATH 生效： 123[xisun@hadoop103 ~]$ source /etc/profile[xisun@hadoop103 ~]$ echo $JAVA_HOME/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64 123[xisun@hadoop104 ~]$ source /etc/profile[xisun@hadoop104 ~]$ echo $JAVA_HOME/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64 编写集群分发脚本 xsyncscp（secure copy）安全拷贝 scp 可以实现服务器与服务器之间的数据拷贝。（from server1 to server2） 基本语法： 12scp -r $pdir/$fname $user@$host:$pdir/$fname命令 递归 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称 实例：拷贝 hadoop102 上的 Hadoop 安装包到 hadoop103 和 hadoop104 上。 前提：在 hadoop102、hadoop103、hadoop104 三台虚拟机上，都已经创建好 /opt/module 和 /opt/software 两个路径，并且已经把这两个路径的权限修改为 xisun:xisun。 方式一，在 hadoop102 上使用 scp 命令，拷贝本地文件到 hadoop103 上： 1234567[xisun@hadoop102 opt]$ scp -r /opt/module/hadoop-3.2.1/ xisun@hadoop103:/opt/module/The authenticity of host &#x27;hadoop103 (192.168.10.103)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:AOkUHU40E6uekNRiFpZkT4R2gfoE+s9ujdYTZ5e8kwM.ECDSA key fingerprint is MD5:dd:80:45:3e:83:75:92:fe:57:d3:78:fa:af:5a:ca:1b.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;hadoop103,192.168.10.103&#x27; (ECDSA) to the list of known hosts.xisun@hadoop103&#x27;s password: 方式二，在 hadoop104 上使用 scp 命令，拷贝 hadoop102 上的文件到本地： 1234567[xisun@hadoop104 opt]$ scp -r xisun@hadoop102:/opt/module/hadoop-3.2.1/ /opt/module/The authenticity of host &#x27;hadoop102 (192.168.10.102)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:AOkUHU40E6uekNRiFpZkT4R2gfoE+s9ujdYTZ5e8kwM.ECDSA key fingerprint is MD5:dd:80:45:3e:83:75:92:fe:57:d3:78:fa:af:5a:ca:1b.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;hadoop104,192.168.10.102&#x27; (ECDSA) to the list of known hosts.xisun@hadoop102&#x27;s password: 方式三，在 hadoop103 上使用 scp 命令，拷贝 hadoop102 上的文件到 hadoop104 上： 1[xisun@hadoop103 opt]$ scp -r xisun@hadoop102:/opt/module/hadoop-3.2.1/ xisun@hadoop104:/opt/module/ rsync 远程同步工具 rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync 和 scp 的区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更新。scp 是把所有文件都复制过去。 rsync 第一次使用，等同于 scp。 基本语法： 12rsync -av $pdir/$fname $user@$host:$pdir/$fname命令 选项参数 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称 参数说明： 选项 功能 -a 归档拷贝 -v 显示复制过程 实例： 删除 hadoop103 和 hadoop104 上 /opt/module/hadoop-3.2.1 路径下的 wcinput 和 wcoutput 文件夹。 1[xisun@hadoop103 hadoop-3.2.1]$ rm -r wcinput/ wcoutput/ 1[xisun@hadoop104 hadoop-3.2.1]$ rm -r wcinput/ wcoutput/ 同步 hadoop102 上的 /opt/module/hadoop-3.2.1 到 hadoop103 和 hadoop104 上。 方式一，在 hadoop102 上使用 rsync 命令，同步本地文件到 hadoop103 上： 12[xisun@hadoop102 ~]$ rsync -av /opt/module/hadoop-3.2.1/ xisun@hadoop103:/opt/module/hadoop-3.2.1/xisun@hadoop103&#x27;s password: 方式二，在 hadoop104 上使用 rsync 命令，同步 hadoop102 上的文件到本地： 1234567[xisun@hadoop104 ~]$ rsync -av xisun@hadoop102:/opt/module/hadoop-3.2.1/ /opt/module/hadoop-3.2.1/The authenticity of host &#x27;hadoop102 (192.168.10.102)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:AOkUHU40E6uekNRiFpZkT4R2gfoE+s9ujdYTZ5e8kwM.ECDSA key fingerprint is MD5:dd:80:45:3e:83:75:92:fe:57:d3:78:fa:af:5a:ca:1b.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;hadoop102,192.168.10.102&#x27; (ECDSA) to the list of known hosts.xisun@hadoop102&#x27;s password: 方式三，在 hadoop103 上使用 rsync 命令，同步 hadoop102 上的文件到 hadoop104 上： 1[xisun@hadoop103 ~]$ rsync -av xisun@hadoop102:/opt/module/hadoop-3.2.1/ xisun@hadoop104:/opt/module/hadoop-3.2.1/ xsync 集群分发脚本 需求：编写脚本，指定需要同步的文件路径参数，能够循环复制该路径下的所有文件到所有节点的相同路径下。 需求分析： 使用 rsync 命令，实现同步拷贝。 期望脚本格式：xsync 需要同步的文件名称 期望脚本在任何路径都能使用：将脚本放在声明了全局环境变量的路径下。 查看全局变量的路径： 12[xisun@hadoop102 ~]$ echo $PATH/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/xisun/.local/bin:/home/xisun/bin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64/bin 可以看出，在全局变量的路径中，有一个 /home/xisun/bin:，如果把编写的脚本 xsync 放在此路径下，即可在任何路径都能使用。或者，直接把脚本 xsync 所在的路径，配置到全局环境变量中也可以。 脚本实现： 在 /home/xisun 路径下，创建 bin 目录： 1234567891011121314[xisun@hadoop102 ~]$ pwd/home/xisun[xisun@hadoop102 ~]$ mkdir bin[xisun@hadoop102 ~]$ ll总用量 0drwxrwxr-x. 2 xisun xisun 6 9月 1 11:43 bindrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 在 /home/xisun/bin 路径下，创建 xsync 文件，并添加脚本代码： 123[xisun@hadoop102 bin]$ pwd/home/xisun/bin[xisun@hadoop102 bin]$ vim xsync 1234567891011121314151617181920212223242526272829303132#!/bin/bash# 1.判断参数个数，如果参数个数小于1，直接退出，$#是获得参数个数if [ $# -lt 1 ]then echo Not Enough Arguement! exit;fi# 2.遍历集群所有机器for host in hadoop102 hadoop103 hadoop104do echo ==================== $host ==================== # 3.遍历所有目录，挨个发送，$@是脚本的参数，可以有多个 for file in $@ do # 4.判断文件是否存在 if [ -e $file ] then # 5.获取父目录 pdir=$(cd -P $(dirname $file); pwd) # 6.获取当前文件的名称 fname=$(basename $file) # 7.在目标主机上，创建目录 ssh $host &quot;mkdir -p $pdir&quot; # 8.同步拷贝 rsync -av $pdir/$fname $host:$pdir else echo $file does not exists! fi donedone 修改脚本 xsync，使其具有可执行权限： 1234567[xisun@hadoop102 bin]$ ll总用量 4-rw-rw-r--. 1 xisun xisun 908 9月 1 13:33 xsync[xisun@hadoop102 bin]$ chmod 777 xsync [xisun@hadoop102 bin]$ ll总用量 4-rwxrwxrwx. 1 xisun xisun 908 9月 1 13:33 xsync 测试脚本，将 hadoop102 上的 /home/xisun/bin 目录，同步到 hadoop103 和 hadoop104 上： 123456789101112131415161718192021222324252627282930313233[xisun@hadoop102 ~]$ pwd/home/xisun[xisun@hadoop102 ~]$ xsync /home/xisun/bin/==================== hadoop102 ====================The authenticity of host &#x27;hadoop102 (192.168.10.102)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:AOkUHU40E6uekNRiFpZkT4R2gfoE+s9ujdYTZ5e8kwM.ECDSA key fingerprint is MD5:dd:80:45:3e:83:75:92:fe:57:d3:78:fa:af:5a:ca:1b.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;hadoop102,192.168.10.102&#x27; (ECDSA) to the list of known hosts.xisun@hadoop102&#x27;s password: xisun@hadoop102&#x27;s password: sending incremental file listsent 90 bytes received 17 bytes 16.46 bytes/sectotal size is 908 speedup is 8.49==================== hadoop103 ====================xisun@hadoop103&#x27;s password: xisun@hadoop103&#x27;s password: sending incremental file listbin/bin/xsyncsent 1,044 bytes received 39 bytes 196.91 bytes/sectotal size is 908 speedup is 0.84==================== hadoop104 ====================xisun@hadoop104&#x27;s password: xisun@hadoop104&#x27;s password: sending incremental file listbin/bin/xsyncsent 1,044 bytes received 39 bytes 240.67 bytes/sectotal size is 908 speedup is 0.84 ssh 免密登录配置 ssh 命令可以在当前主机上，连接另一台主机，在连接过程中，需要另一台主机的通行密码。 基本语法：ssh 另一台主机的IP地址 ssh 连接时出现 Host key verification failed 的解决方法：输入 yes 并回车。 1Are you sure you want to continue connecting (yes/no)? ssh 连接到另一台主机后，使用 exit 命令可以回到原来的主机： 1234567[xisun@hadoop102 ~]$ ssh hadoop103xisun@hadoop103&#x27;s password: Last login: Tue Aug 31 16:47:43 2021 from 192.168.10.1[xisun@hadoop103 ~]$ exit 登出Connection to hadoop103 closed.[xisun@hadoop102 ~]$ 免密钥配置 免密登录原理： 在 hadoop102 生成公钥和私钥： 12345678910111213141516171819202122232425262728[xisun@hadoop102 .ssh]$ pwd/home/xisun/.ssh[xisun@hadoop102 .ssh]$ ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/home/xisun/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/xisun/.ssh/id_rsa.Your public key has been saved in /home/xisun/.ssh/id_rsa.pub.The key fingerprint is:SHA256:j9G3rd2/lBjPliez4vXcinw0R5CJy8mja0u7wBWgPDU xisun@hadoop102The key&#x27;s randomart image is:+---[RSA 2048]----+| E . o || . o o . + || + .o o . || . . .* .|| S o..o . || . =.. oB +|| + o..o=X.|| ooo.=+Bo|| .++=o+oO|+----[SHA256]-----+[xisun@hadoop102 .ssh]$ ll总用量 12-rw-------. 1 xisun xisun 1675 9月 1 15:59 id_rsa-rw-r--r--. 1 xisun xisun 397 9月 1 15:59 id_rsa.pub-rw-r--r--. 1 xisun xisun 558 9月 1 14:29 known_hosts 在 /home/xisun/.ssh 路径下，执行 ssh-keygen -t rsa 命令，敲三次回车，即可生成两个文件，私钥和公钥。 id_rsa 为 hadoop102 的私钥，id_rsa.pub 为 hadoop102 的公钥。 将 hadoop102 的公钥，拷贝到要免密登录的目标机器上，即 hadoop102，hadoop103 和 hadoop104： 1234567891011121314151617[xisun@hadoop102 ~]$ ssh-copy-id hadoop102/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/home/xisun/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysxisun@hadoop102&#x27;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;hadoop102&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[xisun@hadoop102 ~]$ ssh hadoop102Last login: Wed Sep 1 16:11:59 2021 from hadoop102[xisun@hadoop102 ~]$ exit登出Connection to hadoop102 closed.[xisun@hadoop102 ~]$ 1234567891011121314151617[xisun@hadoop102 .ssh]$ ssh-copy-id hadoop103/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/home/xisun/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysxisun@hadoop103&#x27;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;hadoop103&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[xisun@hadoop102 .ssh]$ ssh hadoop103Last login: Wed Sep 1 15:02:49 2021 from hadoop102[xisun@hadoop103 ~]$ exit登出Connection to hadoop103 closed.[xisun@hadoop102 .ssh]$ 12345678910111213141516171819[xisun@hadoop102 .ssh]$ ssh-copy-id hadoop104/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/home/xisun/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysxisun@hadoop104&#x27;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;hadoop104&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[xisun@hadoop102 .ssh]$ ssh hadoop104Last failed login: Wed Sep 1 14:43:15 CST 2021 from hadoop102 on ssh:nottyThere was 1 failed login attempt since the last successful login.Last login: Tue Aug 31 16:48:06 2021 from 192.168.10.1[xisun@hadoop104 ~]$ exit登出Connection to hadoop104 closed.[xisun@hadoop102 .ssh]$ 按上面同样的步骤，对 hadoop103 和 hadoop104 进行 ssh 免密登录配置。 注意：上面的配置只对 xisun 用户有效，如果希望 root 用户也能免密 ssh 登录，需要切换到 root 账号，做同样的配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[xisun@hadoop102 ~]$ su root密码：[root@hadoop102 xisun]# cd .ssh/[root@hadoop102 .ssh]# pwd/home/xisun/.ssh[root@hadoop102 .ssh]# ll总用量 16-rw-------. 1 xisun xisun 1191 9月 1 16:32 authorized_keys-rw-------. 1 xisun xisun 1675 9月 1 15:59 id_rsa-rw-r--r--. 1 xisun xisun 397 9月 1 15:59 id_rsa.pub-rw-r--r--. 1 xisun xisun 558 9月 1 14:29 known_hosts[root@hadoop102 .ssh]# ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:AQtFVBbgnBrY8u/vIGgxFUxqHtKOYzKFIVRbHNJoMEw root@hadoop102The key&#x27;s randomart image is:+---[RSA 2048]----+|*Eo=B*Bo+. ||.=.*=B = ||o X.+ = . || B = o . ||=.= o S ||oo + . || o . o || . o . || .oo |+----[SHA256]-----+[root@hadoop102 .ssh]# ssh-copy-id hadoop102/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@hadoop102&#x27;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;hadoop102&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@hadoop102 .ssh]# ssh hadoop102Last login: Wed Sep 1 16:41:01 2021[root@hadoop102 ~]# exit登出Connection to hadoop102 closed.[root@hadoop102 .ssh]# ssh-copy-id hadoop103/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@hadoop103&#x27;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;hadoop103&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@hadoop102 .ssh]# ssh hadoop103Last failed login: Wed Sep 1 14:48:11 CST 2021 from hadoop102 on ssh:nottyThere was 1 failed login attempt since the last successful login.Last login: Tue Aug 31 10:02:38 2021exi[root@hadoop103 ~]# exit登出Connection to hadoop103 closed.[root@hadoop102 .ssh]# ssh-copy-id hadoop104/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@hadoop104&#x27;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh &#x27;hadoop104&#x27;&quot;and check to make sure that only the key(s) you wanted were added.[root@hadoop102 .ssh]# ssh hadoop104Last login: Wed Sep 1 16:39:50 2021[root@hadoop104 ~]# exit登出Connection to hadoop104 closed.[root@hadoop102 .ssh]# 配置完免密登录后，测试可以看出，使用 xsync 脚本分发文件时，不需要输入密码： 123456789101112131415161718[xisun@hadoop102 ~]$ xsync a.txt ==================== hadoop102 ====================sending incremental file listsent 58 bytes received 12 bytes 46.67 bytes/sectotal size is 0 speedup is 0.00==================== hadoop103 ====================sending incremental file lista.txtsent 101 bytes received 35 bytes 272.00 bytes/sectotal size is 0 speedup is 0.00==================== hadoop104 ====================sending incremental file lista.txtsent 101 bytes received 35 bytes 272.00 bytes/sectotal size is 0 speedup is 0.00 /home/xisun/.ssh 路径下的文件功能解释： 123456[xisun@hadoop102 .ssh]$ ll总用量 16-rw-------. 1 xisun xisun 397 9月 1 16:12 authorized_keys-rw-------. 1 xisun xisun 1675 9月 1 15:59 id_rsa-rw-r--r--. 1 xisun xisun 397 9月 1 15:59 id_rsa.pub-rw-r--r--. 1 xisun xisun 558 9月 1 14:29 known_hosts 文件名 功能 known_hosts 记录当前主机ssh访问过的计算机的公钥（public key） id_rsa 当前主机生成的私钥 id_rsa.pub 当前主机生成的公钥 authorized_keys 存放授权过的无密登录服务器公钥 集群配置集群部署规划 hadoop102 hadoop103 hadoop104 HDFS NameNodeDataNode DataNode SecondaryNameNodeDataNode YARN NodeManager ResourceManagerNodeManager NodeManager NameNode 和 SecondaryNameNode 都比较消耗内存，不要安装在同一台服务器上。 ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode 配置在同一台服务器上。 配置文件说明 Hadoop 配置文件分两类：默认配置文件和自定义配置文件。只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。 默认配置文件 要获取的默认文件 文件存放在Hadoop的JAR包中的位置 [core-default.xml] hadoop-common-3.2.1.jar/core-default.xml [hdfs-default.xml] hadoop-hdfs-3.2.1.jar/hdfs-default.xml [yarn-default.xml] hadoop-yarn-common-3.2.1.jar/yarn-default.xml [mapred-default.xml] hadoop-mapreduce-client-core-3.2.1.jar/mapred-default.xml core-default.xml： hdfs-default.xml： 需要添加依赖才能看到： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt; yarn-default.xml： mapred-default.xml： 自定义配置文件 Hadoop 的四个默认配置文件，分别对应以下四个自定义配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml。这四个自定义配置文件，存放在 $HADOOP_HOME/etc/hadoop 这个路径上，用户可以根据项目需求重新进行修改配置。 123456789101112131415161718192021222324252627282930313233343536[xisun@hadoop102 hadoop]$ pwd/opt/module/hadoop-3.2.1/etc/hadoop[xisun@hadoop102 hadoop]$ ll总用量 172-rw-r--r--. 1 xisun xisun 8260 9月 11 2019 capacity-scheduler.xml-rw-r--r--. 1 xisun xisun 1335 9月 11 2019 configuration.xsl-rw-r--r--. 1 xisun xisun 1940 9月 11 2019 container-executor.cfg-rw-r--r--. 1 xisun xisun 774 9月 10 2019 core-site.xml-rw-r--r--. 1 xisun xisun 3999 9月 10 2019 hadoop-env.cmd-rw-r--r--. 1 xisun xisun 16235 9月 11 2019 hadoop-env.sh-rw-r--r--. 1 xisun xisun 3321 9月 10 2019 hadoop-metrics2.properties-rw-r--r--. 1 xisun xisun 11392 9月 10 2019 hadoop-policy.xml-rw-r--r--. 1 xisun xisun 3414 9月 10 2019 hadoop-user-functions.sh.example-rw-r--r--. 1 xisun xisun 775 9月 11 2019 hdfs-site.xml-rw-r--r--. 1 xisun xisun 1484 9月 11 2019 httpfs-env.sh-rw-r--r--. 1 xisun xisun 1657 9月 11 2019 httpfs-log4j.properties-rw-r--r--. 1 xisun xisun 21 9月 11 2019 httpfs-signature.secret-rw-r--r--. 1 xisun xisun 620 9月 11 2019 httpfs-site.xml-rw-r--r--. 1 xisun xisun 3518 9月 10 2019 kms-acls.xml-rw-r--r--. 1 xisun xisun 1351 9月 10 2019 kms-env.sh-rw-r--r--. 1 xisun xisun 1860 9月 10 2019 kms-log4j.properties-rw-r--r--. 1 xisun xisun 682 9月 10 2019 kms-site.xml-rw-r--r--. 1 xisun xisun 13326 9月 10 2019 log4j.properties-rw-r--r--. 1 xisun xisun 951 9月 11 2019 mapred-env.cmd-rw-r--r--. 1 xisun xisun 1764 9月 11 2019 mapred-env.sh-rw-r--r--. 1 xisun xisun 4113 9月 11 2019 mapred-queues.xml.template-rw-r--r--. 1 xisun xisun 758 9月 11 2019 mapred-site.xmldrwxr-xr-x. 2 xisun xisun 24 9月 10 2019 shellprofile.d-rw-r--r--. 1 xisun xisun 2316 9月 10 2019 ssl-client.xml.example-rw-r--r--. 1 xisun xisun 2697 9月 10 2019 ssl-server.xml.example-rw-r--r--. 1 xisun xisun 2642 9月 11 2019 user_ec_policies.xml.template-rw-r--r--. 1 xisun xisun 10 9月 10 2019 workers-rw-r--r--. 1 xisun xisun 2250 9月 11 2019 yarn-env.cmd-rw-r--r--. 1 xisun xisun 6056 9月 11 2019 yarn-env.sh-rw-r--r--. 1 xisun xisun 2591 9月 11 2019 yarnservice-log4j.properties-rw-r--r--. 1 xisun xisun 690 9月 11 2019 yarn-site.xml 配置集群 第一步：核心配置文件 — 配置 core-site.xml。 123[xisun@hadoop102 hadoop]$ pwd/opt/module/hadoop-3.2.1/etc/hadoop[xisun@hadoop102 hadoop]$ vim core-site.xml 12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 指定NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop数据的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.2.1/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 第二步：HDFS 配置文件 — 配置 hdfs-site.xml。 123[xisun@hadoop102 hadoop]$ pwd/opt/module/hadoop-3.2.1/etc/hadoop[xisun@hadoop102 hadoop]$ vim hdfs-site.xml 12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- nn web端访问地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;hadoop102:9870&lt;/value&gt; &lt;/property&gt; &lt;!-- 2nn web端访问地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:9868&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 第三步：YARN 配置文件 — 配置 yarn-site.xml。 123[xisun@hadoop102 hadoop]$ pwd/opt/module/hadoop-3.2.1/etc/hadoop[xisun@hadoop102 hadoop]$ vim yarn-site.xml 123456789101112131415161718192021222324252627282930&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;!-- 指定MapReduce走shuffle --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 第四步：MapReduce 配置文件 — 配置 mapred-site.xml。 123[xisun@hadoop102 hadoop]$ pwd/opt/module/hadoop-3.2.1/etc/hadoop[xisun@hadoop102 hadoop]$ vim mapred-site.xml 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 指定MapReduce程序运行在Yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 第五步：在集群上分发在 hadoop102 上配置好的 Hadoop 配置文件。 1234567891011121314151617181920212223242526[xisun@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.2.1/etc/hadoop/==================== hadoop102 ====================sending incremental file listsent 972 bytes received 18 bytes 1,980.00 bytes/sectotal size is 107,791 speedup is 108.88==================== hadoop103 ====================sending incremental file listhadoop/hadoop/core-site.xmlhadoop/hdfs-site.xmlhadoop/mapred-site.xmlhadoop/yarn-site.xmlsent 3,165 bytes received 139 bytes 2,202.67 bytes/sectotal size is 107,791 speedup is 32.62==================== hadoop104 ====================sending incremental file listhadoop/hadoop/core-site.xmlhadoop/hdfs-site.xmlhadoop/mapred-site.xmlhadoop/yarn-site.xmlsent 3,165 bytes received 139 bytes 6,608.00 bytes/sectotal size is 107,791 speedup is 32.62 第六步：到 hadoop103 和 hadoop104 上查看文件分发情况。 1[xisun@hadoop103 ~]$ cat /opt/module/hadoop-3.2.1/etc/hadoop/core-site.xml 1[xisun@hadoop104 ~]$ cat /opt/module/hadoop-3.2.1/etc/hadoop/core-site.xml 群起集群配置 workers 打开 workers 文件，删除默认值 localhost，然后向 workers 文件中添加如下内容： 123[xisun@hadoop102 hadoop]$ pwd/opt/module/hadoop-3.2.1/etc/hadoop[xisun@hadoop102 hadoop]$ vim workers 123hadoop102hadoop103hadoop104 workers 文件中添加的主机名，即是 Hadoop 集群中的各个节点。 注意：workers 文件中添加的内容结尾不允许有空格，文件中不允许有空行。 同步所有节点配置文件： 123456789101112131415161718[xisun@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.2.1/etc/hadoop/workers ==================== hadoop102 ====================sending incremental file listsent 59 bytes received 12 bytes 142.00 bytes/sectotal size is 30 speedup is 0.42==================== hadoop103 ====================sending incremental file listworkerssent 136 bytes received 41 bytes 354.00 bytes/sectotal size is 30 speedup is 0.17==================== hadoop104 ====================sending incremental file listworkerssent 136 bytes received 41 bytes 354.00 bytes/sectotal size is 30 speedup is 0.17 启动集群 第一步：如果集群是第一次启动，需要在配置了 NameNode 的节点（hadoop102）上格式化 NameNode。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798[xisun@hadoop102 ~]$ hdfs namenode -formatWARNING: /opt/module/hadoop-3.2.1/logs does not exist. Creating.2021-09-01 22:39:39,794 INFO namenode.NameNode: STARTUP_MSG: /************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host = hadoop102/192.168.10.102STARTUP_MSG: args = [-format]STARTUP_MSG: version = 3.2.1STARTUP_MSG: classpath = /opt/module/hadoop-3.2.1/etc/hadoop:…… ……STARTUP_MSG: build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by &#x27;rohithsharmaks&#x27; on 2019-09-10T15:56ZSTARTUP_MSG: java = 1.8.0_302************************************************************/2021-09-01 22:39:40,092 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]2021-09-01 22:39:42,475 INFO namenode.NameNode: createNameNode [-format]Formatting using clusterid: CID-ffd49d0a-1e29-4912-91a7-0d6ce7121fda2021-09-01 22:39:49,477 INFO namenode.FSEditLog: Edit logging is async:true2021-09-01 22:39:52,982 INFO namenode.FSNamesystem: KeyProvider: null2021-09-01 22:39:53,002 INFO namenode.FSNamesystem: fsLock is fair: true2021-09-01 22:39:53,037 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false2021-09-01 22:39:53,112 INFO namenode.FSNamesystem: fsOwner = xisun (auth:SIMPLE)2021-09-01 22:39:53,112 INFO namenode.FSNamesystem: supergroup = supergroup2021-09-01 22:39:53,113 INFO namenode.FSNamesystem: isPermissionEnabled = true2021-09-01 22:39:53,113 INFO namenode.FSNamesystem: HA Enabled: false2021-09-01 22:39:54,396 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling2021-09-01 22:39:54,448 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=10002021-09-01 22:39:54,448 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true2021-09-01 22:39:54,617 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.0002021-09-01 22:39:54,618 INFO blockmanagement.BlockManager: The block deletion will start around 2021 九月 01 22:39:542021-09-01 22:39:54,621 INFO util.GSet: Computing capacity for map BlocksMap2021-09-01 22:39:54,621 INFO util.GSet: VM type = 64-bit2021-09-01 22:39:54,654 INFO util.GSet: 2.0% max memory 441 MB = 8.8 MB2021-09-01 22:39:54,654 INFO util.GSet: capacity = 2^20 = 1048576 entries2021-09-01 22:39:54,679 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled2021-09-01 22:39:54,679 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false2021-09-01 22:39:54,707 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS2021-09-01 22:39:54,707 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.99900001287460332021-09-01 22:39:54,708 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 02021-09-01 22:39:54,708 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 300002021-09-01 22:39:54,708 INFO blockmanagement.BlockManager: defaultReplication = 32021-09-01 22:39:54,709 INFO blockmanagement.BlockManager: maxReplication = 5122021-09-01 22:39:54,709 INFO blockmanagement.BlockManager: minReplication = 12021-09-01 22:39:54,709 INFO blockmanagement.BlockManager: maxReplicationStreams = 22021-09-01 22:39:54,709 INFO blockmanagement.BlockManager: redundancyRecheckInterval = 3000ms2021-09-01 22:39:54,709 INFO blockmanagement.BlockManager: encryptDataTransfer = false2021-09-01 22:39:54,709 INFO blockmanagement.BlockManager: maxNumBlocksToLog = 10002021-09-01 22:39:54,794 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=5368709112021-09-01 22:39:54,794 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=167772152021-09-01 22:39:54,794 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=167772152021-09-01 22:39:54,794 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=167772152021-09-01 22:39:54,831 INFO util.GSet: Computing capacity for map INodeMap2021-09-01 22:39:54,832 INFO util.GSet: VM type = 64-bit2021-09-01 22:39:54,832 INFO util.GSet: 1.0% max memory 441 MB = 4.4 MB2021-09-01 22:39:54,832 INFO util.GSet: capacity = 2^19 = 524288 entries2021-09-01 22:39:54,833 INFO namenode.FSDirectory: ACLs enabled? false2021-09-01 22:39:54,834 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true2021-09-01 22:39:54,834 INFO namenode.FSDirectory: XAttrs enabled? true2021-09-01 22:39:54,834 INFO namenode.NameNode: Caching file names occurring more than 10 times2021-09-01 22:39:54,847 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 655362021-09-01 22:39:54,859 INFO snapshot.SnapshotManager: SkipList is disabled2021-09-01 22:39:54,868 INFO util.GSet: Computing capacity for map cachedBlocks2021-09-01 22:39:54,868 INFO util.GSet: VM type = 64-bit2021-09-01 22:39:54,868 INFO util.GSet: 0.25% max memory 441 MB = 1.1 MB2021-09-01 22:39:54,868 INFO util.GSet: capacity = 2^17 = 131072 entries2021-09-01 22:39:54,899 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 102021-09-01 22:39:54,899 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 102021-09-01 22:39:54,899 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,252021-09-01 22:39:54,914 INFO namenode.FSNamesystem: Retry cache on namenode is enabled2021-09-01 22:39:54,914 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis2021-09-01 22:39:54,917 INFO util.GSet: Computing capacity for map NameNodeRetryCache2021-09-01 22:39:54,917 INFO util.GSet: VM type = 64-bit2021-09-01 22:39:54,917 INFO util.GSet: 0.029999999329447746% max memory 441 MB = 135.5 KB2021-09-01 22:39:54,917 INFO util.GSet: capacity = 2^14 = 16384 entries2021-09-01 22:39:55,000 INFO namenode.FSImage: Allocated new BlockPoolId: BP-288566776-192.168.10.102-16305071949792021-09-01 22:39:55,144 INFO common.Storage: Storage directory /opt/module/hadoop-3.2.1/data/dfs/name has been successfully formatted.2021-09-01 22:39:57,641 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/module/hadoop-3.2.1/data/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression2021-09-01 22:39:58,078 INFO namenode.FSImageFormatProtobuf: Image file /opt/module/hadoop-3.2.1/data/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 400 bytes saved in 0 seconds .2021-09-01 22:39:58,126 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 02021-09-01 22:39:58,206 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.2021-09-01 22:39:58,208 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at hadoop102/192.168.10.102************************************************************/[xisun@hadoop102 ~]$ ll /opt/module/hadoop-3.2.1/总用量 180drwxr-xr-x. 2 xisun xisun 203 9月 11 2019 bindrwxrwxr-x. 3 xisun xisun 17 9月 1 22:39 datadrwxr-xr-x. 3 xisun xisun 20 9月 10 2019 etcdrwxr-xr-x. 2 xisun xisun 106 9月 11 2019 includedrwxr-xr-x. 3 xisun xisun 20 9月 11 2019 libdrwxr-xr-x. 4 xisun xisun 288 9月 11 2019 libexec-rw-rw-r--. 1 xisun xisun 150569 9月 10 2019 LICENSE.txtdrwxrwxr-x. 2 xisun xisun 38 9月 1 22:39 logs-rw-rw-r--. 1 xisun xisun 22125 9月 10 2019 NOTICE.txt-rw-rw-r--. 1 xisun xisun 1361 9月 10 2019 README.txtdrwxr-xr-x. 3 xisun xisun 4096 9月 10 2019 sbindrwxr-xr-x. 4 xisun xisun 31 9月 11 2019 sharedrwxrwxr-x. 2 xisun xisun 22 8月 31 16:00 wcinputdrwxr-xr-x. 2 xisun xisun 88 8月 31 16:04 wcoutput 格式化 NameNode 完毕后，会在 Hadoop 安装路径下，生成两个新目录 data 和 logs。 注意：第一次格式化 NameNode 之后，如果再次格式化 NameNode 时，会产生新的集群 id，这会导致与前一次生成的 NameNode 和 DataNode 的集群 id 不一致，集群会找不到已往数据。如果集群在运行过程中发生异常，需要重新格式化 NameNode 的话，一定要先停止 namenode 和 datanode 进程（停止 YARN 和 HDFS），并且删除集群上所有机器的 data 和 logs 目录，然后再进行格式化。最后，重新启动集群。原因如下： NameNode 的版本号： 12345678910111213141516171819202122232425262728293031323334353637383940414243[xisun@hadoop102 ~]$ cd /opt/module/hadoop-3.2.1/data/dfs/[xisun@hadoop102 dfs]$ ll总用量 0drwx------. 3 xisun xisun 40 9月 2 14:05 datadrwxrwxr-x. 3 xisun xisun 40 9月 2 14:05 name[xisun@hadoop102 dfs]$ cd name/current/[xisun@hadoop102 current]$ ll总用量 4196-rw-rw-r--. 1 xisun xisun 42 9月 1 22:59 edits_0000000000000000001-0000000000000000002-rw-rw-r--. 1 xisun xisun 42 9月 1 23:59 edits_0000000000000000003-0000000000000000004-rw-rw-r--. 1 xisun xisun 42 9月 2 00:59 edits_0000000000000000005-0000000000000000006-rw-rw-r--. 1 xisun xisun 42 9月 2 01:59 edits_0000000000000000007-0000000000000000008-rw-rw-r--. 1 xisun xisun 42 9月 2 02:59 edits_0000000000000000009-0000000000000000010-rw-rw-r--. 1 xisun xisun 42 9月 2 03:59 edits_0000000000000000011-0000000000000000012-rw-rw-r--. 1 xisun xisun 42 9月 2 04:59 edits_0000000000000000013-0000000000000000014-rw-rw-r--. 1 xisun xisun 42 9月 2 05:59 edits_0000000000000000015-0000000000000000016-rw-rw-r--. 1 xisun xisun 42 9月 2 06:59 edits_0000000000000000017-0000000000000000018-rw-rw-r--. 1 xisun xisun 42 9月 2 07:59 edits_0000000000000000019-0000000000000000020-rw-rw-r--. 1 xisun xisun 42 9月 2 08:59 edits_0000000000000000021-0000000000000000022-rw-rw-r--. 1 xisun xisun 42 9月 2 09:59 edits_0000000000000000023-0000000000000000024-rw-rw-r--. 1 xisun xisun 1500 9月 2 10:59 edits_0000000000000000025-0000000000000000045-rw-rw-r--. 1 xisun xisun 1048576 9月 2 11:34 edits_0000000000000000046-0000000000000000110-rw-rw-r--. 1 xisun xisun 42 9月 2 11:51 edits_0000000000000000111-0000000000000000112-rw-rw-r--. 1 xisun xisun 8888 9月 2 12:51 edits_0000000000000000113-0000000000000000185-rw-rw-r--. 1 xisun xisun 1048576 9月 2 12:51 edits_0000000000000000186-0000000000000000186-rw-rw-r--. 1 xisun xisun 42 9月 2 13:11 edits_0000000000000000187-0000000000000000188-rw-rw-r--. 1 xisun xisun 1048576 9月 2 13:18 edits_0000000000000000189-0000000000000000271-rw-rw-r--. 1 xisun xisun 42 9月 2 14:06 edits_0000000000000000272-0000000000000000273-rw-rw-r--. 1 xisun xisun 1048576 9月 2 14:10 edits_inprogress_0000000000000000274-rw-rw-r--. 1 xisun xisun 2460 9月 2 13:11 fsimage_0000000000000000188-rw-rw-r--. 1 xisun xisun 62 9月 2 13:11 fsimage_0000000000000000188.md5-rw-rw-r--. 1 xisun xisun 2955 9月 2 14:06 fsimage_0000000000000000273-rw-rw-r--. 1 xisun xisun 62 9月 2 14:06 fsimage_0000000000000000273.md5-rw-rw-r--. 1 xisun xisun 4 9月 2 14:06 seen_txid-rw-rw-r--. 1 xisun xisun 217 9月 1 22:39 VERSION[xisun@hadoop102 current]$ cat VERSION #Wed Sep 01 22:39:55 CST 2021namespaceID=817173371clusterID=CID-ffd49d0a-1e29-4912-91a7-0d6ce7121fdacTime=1630507194979storageType=NAME_NODEblockpoolID=BP-288566776-192.168.10.102-1630507194979layoutVersion=-65 DataNode 的版本号： 123456789101112131415161718[xisun@hadoop102 ~]$ cd /opt/module/hadoop-3.2.1/data/dfs/[xisun@hadoop102 dfs]$ ll总用量 0drwx------. 3 xisun xisun 40 9月 2 14:05 datadrwxrwxr-x. 3 xisun xisun 40 9月 2 14:05 name[xisun@hadoop102 dfs]$ cd data/current/[xisun@hadoop102 current]$ ll总用量 4drwx------. 4 xisun xisun 54 9月 2 14:05 BP-288566776-192.168.10.102-1630507194979-rw-rw-r--. 1 xisun xisun 229 9月 2 14:05 VERSION[xisun@hadoop102 current]$ cat VERSION #Thu Sep 02 14:05:31 CST 2021storageID=DS-7c2f6c8b-80be-44b0-9905-f1621843d7a3clusterID=CID-ffd49d0a-1e29-4912-91a7-0d6ce7121fdacTime=0datanodeUuid=32411bd2-079d-42fb-874a-8e5b492824afstorageType=DATA_NODElayoutVersion=-57 集群正常情况下，DataNode 版本与 NameNode 的版本是对应的，如果重新格式化了 NameNode，NameNode 的版本号发生改变，集群上之前存储的 DataNode 下的数据，版本对应不上新的 NameNode，集群也就无法正常启动。因此，格式化 NameNode 之前，需要先删除集群所有机器的 data 和 logs 目录。 第二步：在 hadoop102 上启动 HDFS。 123456[xisun@hadoop102 ~]$ /opt/module/hadoop-3.2.1/sbin/start-dfs.sh Starting namenodes on [hadoop102]Starting datanodeshadoop103: WARNING: /opt/module/hadoop-3.2.1/logs does not exist. Creating.hadoop104: WARNING: /opt/module/hadoop-3.2.1/logs does not exist. Creating.Starting secondary namenodes [hadoop104] 可以直接使用 start-dfs.sh （脚本存放的位置配置在了环境变量里，因此可以在任何路径使用）启动 HDFS。 在 hadoop102 上查看集群进程启动情况： 1234[xisun@hadoop102 ~]$ jps29008 DataNode28885 NameNode29365 Jps 在 hadoop103 上查看集群进程启动情况： 123[xisun@hadoop103 ~]$ jps23137 DataNode23283 Jps 在 hadoop104 上查看集群进程启动情况： 1234[xisun@hadoop104 ~]$ jps23146 SecondaryNameNode23259 Jps23023 DataNode Web 端查看 HDFS 的 NameNode，浏览器输入 http://192.168.10.102:9870/ 或者 http://hadoop102:9870/（要求 Windows 电脑上配置了 hadoop102 的主机映射）。 第三步：在配置了 ResourceManager 的节点（hadoop103）上启动 YARN。 123[xisun@hadoop103 ~]$ /opt/module/hadoop-3.2.1/sbin/start-yarn.sh Starting resourcemanagerStarting nodemanagers 在 hadoop102 上也可以启动 YARN。 可以直接使用 start-yarn.sh （脚本存放的位置配置在了环境变量里，因此可以在任何路径使用）启动 YARN。 在 hadoop102 上查看集群进程启动情况： 12345[xisun@hadoop102 ~]$ jps29008 DataNode28885 NameNode29671 Jps29563 NodeManager hadoop102 是 NameNode 所在的节点。 在 hadoop103 上查看集群进程启动情况： 12345[xisun@hadoop103 ~]$ jps23137 DataNode23523 ResourceManager23646 NodeManager23998 Jps hadoop103 是 ResourceManager 所在的节点。 在 hadoop104 上查看集群进程启动情况： 12345[xisun@hadoop104 ~]$ jps23602 Jps23480 NodeManager23146 SecondaryNameNode23023 DataNode hadoop104 是 SecondaryNameNode 所在的节点。 Web 端查看 YARN 的 ResourceManager，浏览器输入 http://192.168.10.103:8088/ 或者 http://hadoop103:8088/（要求 Windows 电脑上配置了 hadoop103 的主机映射）。 集群基本测试 上传文件到集群 上传小文件 123[xisun@hadoop102 ~]$ hdfs dfs -mkdir /wcinput[xisun@hadoop102 ~]$ hdfs dfs -put /opt/module/hadoop-3.2.1/wcinput/word.txt /wcinput2021-09-02 10:14:15,604 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false 在 Web 端查看 HDFS，根目录下新建了一个 wcinput 目录，wcinput 目录下也上传了一个 word.txt 文件： 说明：如果需要在 Web 页面上删除 HDFS 上的文件，需要配置 core-site.xml，添加如下配置，然后集群分发 core-site.xml 文件，并重新启动集群。否则没有权限删除。 12345&lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;xisun&lt;/value&gt;&lt;/property&gt; 上传大文件 1234[xisun@hadoop102 ~]$ hdfs dfs -put /opt/software/hadoop-3.2.1.tar.gz /2021-09-02 10:41:41,346 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false2021-09-02 10:43:40,782 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false2021-09-02 10:44:58,477 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false 集群上文件的存储位置 查看 HDFS 文件存储路径 12345678910111213141516171819202122232425262728293031323334[xisun@hadoop102 ~]$ cd /opt/module/hadoop-3.2.1/data/[xisun@hadoop102 data]$ pwd/opt/module/hadoop-3.2.1/data[xisun@hadoop102 data]$ ll总用量 0drwxrwxr-x. 4 xisun xisun 30 9月 1 22:58 dfsdrwxr-xr-x. 5 xisun xisun 57 9月 1 23:15 nm-local-dir[xisun@hadoop102 data]$ cd dfs/[xisun@hadoop102 dfs]$ ll总用量 0drwx------. 3 xisun xisun 40 9月 1 22:58 datadrwxrwxr-x. 3 xisun xisun 40 9月 1 22:58 name[xisun@hadoop102 dfs]$ cd data/[xisun@hadoop102 data]$ ll总用量 4drwxrwxr-x. 3 xisun xisun 70 9月 1 22:58 current-rw-rw-r--. 1 xisun xisun 15 9月 1 22:58 in_use.lock[xisun@hadoop102 data]$ cd current/BP-288566776-192.168.10.102-1630507194979/current/ tmp/ [xisun@hadoop102 data]$ cd current/BP-288566776-192.168.10.102-1630507194979/current/finalized/ rbw/ [xisun@hadoop102 data]$ cd current/BP-288566776-192.168.10.102-1630507194979/current/finalized/subdir0/subdir0/[xisun@hadoop102 subdir0]$ pwd/opt/module/hadoop-3.2.1/data/dfs/data/current/BP-288566776-192.168.10.102-1630507194979/current/finalized/subdir0/subdir0[xisun@hadoop102 subdir0]$ ll总用量 353540-rw-rw-r--. 1 xisun xisun 41 9月 2 10:14 blk_1073741825-rw-rw-r--. 1 xisun xisun 11 9月 2 10:14 blk_1073741825_1001.meta-rw-rw-r--. 1 xisun xisun 134217728 9月 2 10:43 blk_1073741826-rw-rw-r--. 1 xisun xisun 1048583 9月 2 10:43 blk_1073741826_1002.meta-rw-rw-r--. 1 xisun xisun 134217728 9月 2 10:44 blk_1073741827-rw-rw-r--. 1 xisun xisun 1048583 9月 2 10:44 blk_1073741827_1003.meta-rw-rw-r--. 1 xisun xisun 90761455 9月 2 10:45 blk_1073741828-rw-rw-r--. 1 xisun xisun 709083 9月 2 10:45 blk_1073741828_1004.meta 查看 HDFS 在磁盘存储的 word.txt 文件的内容，可以看出，blk_1073741825 就是之前上传的 word.txt 文件。 12345[xisun@hadoop102 subdir0]$ cat blk_1073741825hadoop yarnhadoop mapreducexisunxisun 拼接 HDFS 在磁盘存储的 hadoop-3.2.1.tar.gz 文件的内容，追加到临时的压缩文件中，解压之后可以看出，blk_1073741826，blk_1073741827 和 blk_1073741828 就是之前上传的 hadoop-3.2.1.tar.gz 压缩文件。 123456789101112131415161718192021222324252627282930[xisun@hadoop102 subdir0]$ cat blk_1073741826 &gt;&gt; ~/tmp.tar.gz[xisun@hadoop102 subdir0]$ cat blk_1073741827 &gt;&gt; ~/tmp.tar.gz[xisun@hadoop102 subdir0]$ cat blk_1073741828 &gt;&gt; ~/tmp.tar.gz[xisun@hadoop102 subdir0]$ cd ~[xisun@hadoop102 ~]$ ll总用量 393216drwxrwxr-x. 2 xisun xisun 19 9月 1 13:33 bin-rw-rw-r--. 1 xisun xisun 359196911 9月 2 11:00 tmp.tar.gzdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面[xisun@hadoop102 ~]$ tar -zxf tmp.tar.gz [xisun@hadoop102 ~]$ ll总用量 393216drwxrwxr-x. 2 xisun xisun 19 9月 1 13:33 bindrwxr-xr-x. 9 xisun xisun 149 9月 11 2019 hadoop-3.2.1-rw-rw-r--. 1 xisun xisun 359196911 9月 2 11:00 tmp.tar.gzdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 说明：HDFS 在磁盘上存储的文件，可能是分散的，需要了解分散到哪些文件了，才能追加拼接。 HDFS 上的文件是备份在集群的每一台服务器上的（也就是集群的每一台服务器，都是 DataNode），在 hadoop102，hadoop103 和 hadoop104 上相同的路径下，都有相同的数据备份。 123456789101112[xisun@hadoop102 subdir0]$ pwd/opt/module/hadoop-3.2.1/data/dfs/data/current/BP-288566776-192.168.10.102-1630507194979/current/finalized/subdir0/subdir0[xisun@hadoop102 subdir0]$ ll总用量 353540-rw-rw-r--. 1 xisun xisun 41 9月 2 10:14 blk_1073741825-rw-rw-r--. 1 xisun xisun 11 9月 2 10:14 blk_1073741825_1001.meta-rw-rw-r--. 1 xisun xisun 134217728 9月 2 10:43 blk_1073741826-rw-rw-r--. 1 xisun xisun 1048583 9月 2 10:43 blk_1073741826_1002.meta-rw-rw-r--. 1 xisun xisun 134217728 9月 2 10:44 blk_1073741827-rw-rw-r--. 1 xisun xisun 1048583 9月 2 10:44 blk_1073741827_1003.meta-rw-rw-r--. 1 xisun xisun 90761455 9月 2 10:45 blk_1073741828-rw-rw-r--. 1 xisun xisun 709083 9月 2 10:45 blk_1073741828_1004.meta 123456789101112[xisun@hadoop103 subdir0]$ pwd/opt/module/hadoop-3.2.1/data/dfs/data/current/BP-288566776-192.168.10.102-1630507194979/current/finalized/subdir0/subdir0[xisun@hadoop103 subdir0]$ ll总用量 353540-rw-rw-r--. 1 xisun xisun 41 9月 2 10:14 blk_1073741825-rw-rw-r--. 1 xisun xisun 11 9月 2 10:14 blk_1073741825_1001.meta-rw-rw-r--. 1 xisun xisun 134217728 9月 2 10:43 blk_1073741826-rw-rw-r--. 1 xisun xisun 1048583 9月 2 10:43 blk_1073741826_1002.meta-rw-rw-r--. 1 xisun xisun 134217728 9月 2 10:44 blk_1073741827-rw-rw-r--. 1 xisun xisun 1048583 9月 2 10:44 blk_1073741827_1003.meta-rw-rw-r--. 1 xisun xisun 90761455 9月 2 10:45 blk_1073741828-rw-rw-r--. 1 xisun xisun 709083 9月 2 10:45 blk_1073741828_1004.meta 123456789101112[xisun@hadoop104 subdir0]$ pwd/opt/module/hadoop-3.2.1/data/dfs/data/current/BP-288566776-192.168.10.102-1630507194979/current/finalized/subdir0/subdir0[xisun@hadoop104 subdir0]$ ll总用量 353540-rw-rw-r--. 1 xisun xisun 41 9月 2 10:14 blk_1073741825-rw-rw-r--. 1 xisun xisun 11 9月 2 10:14 blk_1073741825_1001.meta-rw-rw-r--. 1 xisun xisun 134217728 9月 2 10:43 blk_1073741826-rw-rw-r--. 1 xisun xisun 1048583 9月 2 10:43 blk_1073741826_1002.meta-rw-rw-r--. 1 xisun xisun 134217728 9月 2 10:44 blk_1073741827-rw-rw-r--. 1 xisun xisun 1048583 9月 2 10:44 blk_1073741827_1003.meta-rw-rw-r--. 1 xisun xisun 90761455 9月 2 10:45 blk_1073741828-rw-rw-r--. 1 xisun xisun 709083 9月 2 10:45 blk_1073741828_1004.meta 从集群下载文件到本地 12345678910111213141516171819202122232425262728293031[xisun@hadoop102 ~]$ pwd/home/xisun[xisun@hadoop102 ~]$ ll总用量 0drwxrwxr-x. 2 xisun xisun 19 9月 1 13:33 bindrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面[xisun@hadoop102 ~]$ hadoop fs -get /wcinput/word.txt ./2021-09-02 11:24:37,690 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false[xisun@hadoop102 ~]$ ll总用量 4drwxrwxr-x. 2 xisun xisun 19 9月 1 13:33 bin-rw-r--r--. 1 xisun xisun 41 9月 2 11:24 word.txtdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面[xisun@hadoop102 ~]$ cat word.txt hadoop yarnhadoop mapreducexisun 执行 wordcount 程序，查看 YARN 的执行情况。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[xisun@hadoop102 ~]$ hadoop jar /opt/module/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /wcinput /wcoutput2021-09-02 13:15:36,538 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:80322021-09-02 13:15:36,985 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/xisun/.staging/job_1630509320297_00042021-09-02 13:15:37,189 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false2021-09-02 13:15:38,237 INFO input.FileInputFormat: Total input files to process : 12021-09-02 13:15:38,317 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false2021-09-02 13:15:38,407 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false2021-09-02 13:15:38,456 INFO mapreduce.JobSubmitter: number of splits:12021-09-02 13:15:38,608 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false2021-09-02 13:15:38,673 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1630509320297_00042021-09-02 13:15:38,673 INFO mapreduce.JobSubmitter: Executing with tokens: []2021-09-02 13:15:38,887 INFO conf.Configuration: resource-types.xml not found2021-09-02 13:15:38,887 INFO resource.ResourceUtils: Unable to find &#x27;resource-types.xml&#x27;.2021-09-02 13:15:38,964 INFO impl.YarnClientImpl: Submitted application application_1630509320297_00042021-09-02 13:15:39,063 INFO mapreduce.Job: The url to track the job: http://hadoop103:8088/proxy/application_1630509320297_0004/2021-09-02 13:15:39,063 INFO mapreduce.Job: Running job: job_1630509320297_00042021-09-02 13:15:48,877 INFO mapreduce.Job: Job job_1630509320297_0004 running in uber mode : false2021-09-02 13:15:48,879 INFO mapreduce.Job: map 0% reduce 0%2021-09-02 13:17:39,986 INFO mapreduce.Job: map 100% reduce 0%2021-09-02 13:18:46,452 INFO mapreduce.Job: map 100% reduce 100%2021-09-02 13:18:48,469 INFO mapreduce.Job: Job job_1630509320297_0004 completed successfully2021-09-02 13:18:48,743 INFO mapreduce.Job: Counters: 54 File System Counters FILE: Number of bytes read=58 FILE: Number of bytes written=452423 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=144 HDFS: Number of bytes written=36 HDFS: Number of read operations=8 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 HDFS: Number of bytes read erasure-coded=0 Job Counters Launched map tasks=1 Launched reduce tasks=1 Data-local map tasks=1 Total time spent by all maps in occupied slots (ms)=108569 Total time spent by all reduces in occupied slots (ms)=63584 Total time spent by all map tasks (ms)=108569 Total time spent by all reduce tasks (ms)=63584 Total vcore-milliseconds taken by all map tasks=108569 Total vcore-milliseconds taken by all reduce tasks=63584 Total megabyte-milliseconds taken by all map tasks=111174656 Total megabyte-milliseconds taken by all reduce tasks=65110016 Map-Reduce Framework Map input records=4 Map output records=6 Map output bytes=65 Map output materialized bytes=58 Input split bytes=103 Combine input records=6 Combine output records=4 Reduce input groups=4 Reduce shuffle bytes=58 Reduce input records=4 Reduce output records=4 Spilled Records=8 Shuffled Maps =1 Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=11835 CPU time spent (ms)=68260 Physical memory (bytes) snapshot=507523072 Virtual memory (bytes) snapshot=5576531968 Total committed heap usage (bytes)=404226048 Peak Map Physical memory (bytes)=297627648 Peak Map Virtual memory (bytes)=2785103872 Peak Reduce Physical memory (bytes)=209895424 Peak Reduce Virtual memory (bytes)=2791428096 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=41 File Output Format Counters Bytes Written=36 查看执行结果： 执行时异常的排除：在初次执行时，报异常 错误: 找不到或无法加载主类 org.apache.hadoop.mapreduce.v2.app.MRAppMaster，解决方法是停止集群，然后向 yarn-site.xml 配置文件中，添加一个属性值。然后，重新运行 wordcount 程序，即能正常执行。 停止集群： 123456789101112[xisun@hadoop102 ~]$ stop-yarn.sh Stopping nodemanagershadoop103: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9hadoop104: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9hadoop102: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9Stopping resourcemanager[xisun@hadoop102 ~]$ stop-dfs.sh Stopping namenodes on [hadoop102]Stopping datanodesStopping secondary namenodes [hadoop104][xisun@hadoop102 ~]$ jps42220 Jps 配置 yarn-site.xml，添加 yarn.application.classpath 属性值： 123[xisun@hadoop102 hadoop]$ hadoop classpath/opt/module/hadoop-3.2.1/etc/hadoop:/opt/module/hadoop-3.2.1/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.1/share/hadoop/common/*:/opt/module/hadoop-3.2.1/share/hadoop/hdfs:/opt/module/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.1/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.1/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.1/share/hadoop/yarn:/opt/module/hadoop-3.2.1/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.1/share/hadoop/yarn/*[xisun@hadoop102 hadoop]$ vim yarn-site.xml 1234&lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.2.1/etc/hadoop:/opt/module/hadoop-3.2.1/share/hadoop/common/lib/*:/opt/module/hadoop-3.2.1/share/hadoop/common/*:/opt/module/hadoop-3.2.1/share/hadoop/hdfs:/opt/module/hadoop-3.2.1/share/hadoop/hdfs/lib/*:/opt/module/hadoop-3.2.1/share/hadoop/hdfs/*:/opt/module/hadoop-3.2.1/share/hadoop/mapreduce/lib/*:/opt/module/hadoop-3.2.1/share/hadoop/mapreduce/*:/opt/module/hadoop-3.2.1/share/hadoop/yarn:/opt/module/hadoop-3.2.1/share/hadoop/yarn/lib/*:/opt/module/hadoop-3.2.1/share/hadoop/yarn/*&lt;/value&gt;&lt;/property&gt; 分发 yarn-site.xml 到集群所有主机上： 123456789101112131415161718[xisun@hadoop102 ~]$ xsync /opt/module/hadoop-3.2.1/etc/hadoop/yarn-site.xml ==================== hadoop102 ====================sending incremental file listsent 66 bytes received 12 bytes 156.00 bytes/sectotal size is 1,621 speedup is 20.78==================== hadoop103 ====================sending incremental file listyarn-site.xmlsent 1,038 bytes received 47 bytes 434.00 bytes/sectotal size is 1,621 speedup is 1.49==================== hadoop104 ====================sending incremental file listyarn-site.xmlsent 1,038 bytes received 47 bytes 2,170.00 bytes/sectotal size is 1,621 speedup is 1.49 启动集群： 123456789101112[xisun@hadoop102 hadoop]$ start-dfs.sh Starting namenodes on [hadoop102]Starting datanodesStarting secondary namenodes [hadoop104][xisun@hadoop102 hadoop]$ start-yarn.sh Starting resourcemanagerStarting nodemanagers[xisun@hadoop102 hadoop]$ jps42544 NameNode43346 Jps42680 DataNode43177 NodeManager 与本地模式执行 wordcount 一样，HDFS 上不能已经存在 /wcoutput 目录，否则报错。如果已经存在，则需要删除： 12[xisun@hadoop102 ~]$ hdfs dfs -rm -r /wcoutputDeleted /wcoutput 配置历史服务器 为了查看程序的历史运行情况，需要配置一下历史服务器。 第一步：编辑 mapred-site.xml，增加历史服务器的配置。 1[xisun@hadoop102 ~]$ vim /opt/module/hadoop-3.2.1/etc/hadoop/mapred-site.xml 1234567891011&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop102:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器Web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop102:19888&lt;/value&gt;&lt;/property&gt; 第二步：集群分发 mapred-site.xml。 123456789101112131415161718[xisun@hadoop102 ~]$ xsync /opt/module/hadoop-3.2.1/etc/hadoop/mapred-site.xml ==================== hadoop102 ====================sending incremental file listsent 67 bytes received 12 bytes 158.00 bytes/sectotal size is 1,229 speedup is 15.56==================== hadoop103 ====================sending incremental file listmapred-site.xmlsent 647 bytes received 47 bytes 462.67 bytes/sectotal size is 1,229 speedup is 1.77==================== hadoop104 ====================sending incremental file listmapred-site.xmlsent 647 bytes received 47 bytes 1,388.00 bytes/sectotal size is 1,229 speedup is 1.77 第三步：在 hadoop102 上启动历史服务器。 1234567[xisun@hadoop102 ~]$ mapred --daemon start historyserver[xisun@hadoop102 ~]$ jps47605 Jps46106 NodeManager45612 DataNode47564 JobHistoryServer45485 NameNode 第四步：重新运行一次 wordcount（需要先删除 HDFS 上的 /wcoutput），查看任务的历史日志。 配置日志的聚集 日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。 如果不配置日志聚集，在 Web 端查看历史日志时，点击 logs 会无法查看（程序的运行日志，可能会分布在集群的各个机器上）。 日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。 第一步：编辑 yarn-site.xml，添加日志聚集的配置。 1[xisun@hadoop102 ~]$ vim /opt/module/hadoop-3.2.1/etc/hadoop/yarn-site.xml 1234567891011121314151617&lt;!-- 开启日志聚集功能，默认不开启 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置日志聚集服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop102:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置日志保留时间为7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 第二步：集群分发 yarn-site.xml 文件。 123456789101112131415161718[xisun@hadoop102 ~]$ xsync /opt/module/hadoop-3.2.1/etc/hadoop/yarn-site.xml ==================== hadoop102 ====================sending incremental file listsent 66 bytes received 12 bytes 156.00 bytes/sectotal size is 2,115 speedup is 27.12==================== hadoop103 ====================sending incremental file listyarn-site.xmlsent 836 bytes received 53 bytes 1,778.00 bytes/sectotal size is 2,115 speedup is 2.38==================== hadoop104 ====================sending incremental file listyarn-site.xmlsent 836 bytes received 53 bytes 1,778.00 bytes/sectotal size is 2,115 speedup is 2.38 第三步：重新启动 NodeManager 、ResourceManager 和 HistoryServer。 123456789101112131415161718192021[xisun@hadoop102 ~]$ mapred --daemon stop historyserver[xisun@hadoop102 ~]$ stop-yarn.sh Stopping nodemanagershadoop102: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9hadoop104: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9hadoop103: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9Stopping resourcemanager[xisun@hadoop102 ~]$ jps48872 Jps45612 DataNode45485 NameNode[xisun@hadoop102 ~]$ start-yarn.sh Starting resourcemanagerStarting nodemanagers[xisun@hadoop102 ~]$ mapred --daemon start historyserver[xisun@hadoop102 ~]$ jps49696 Jps49636 JobHistoryServer45612 DataNode45485 NameNode49134 NodeManager 第四步：重新运行一次 wordcount（需要先删除 HDFS 上的 /wcoutput），查看任务的历史日志。 集群启动/停止方式总结 各个模块分开启动/停止（配置 ssh 是前提）— 常用 整体启动/停止 HDFS 1[xisun@hadoop102 ~]$ start-dfs.sh 1[xisun@hadoop102 ~]$ stop-dfs.sh 整体启动/停止 YARN 1[xisun@hadoop102 ~]$ start-yarn.sh 1[xisun@hadoop102 ~]$ stop-yarn.sh 各个服务组件逐一启动/停止 分别启动/停止 HDFS 组件 1[xisun@hadoop102 ~]$ hdfs --daemon start namenode/datanode/secondarynamenode 1[xisun@hadoop102 ~]$ hdfs --daemon stop namenode/datanode/secondarynamenode 启动/停止 YARN 1[xisun@hadoop102 ~]$ yarn --daemon start resourcemanager/nodemanager 1[xisun@hadoop102 ~]$ yarn --daemon stop resourcemanager/nodemanager 编写 Hadoop 集群常用脚本 Hadoop 集群启停脚本（包含 HDFS，Yarn 和 Historyserver）：myhadoop.sh 编写脚本： 123[xisun@hadoop102 bin]$ pwd/home/xisun/bin[xisun@hadoop102 bin]$ vim myhadoop.sh 123456789101112131415161718192021222324252627282930313233#!/bin/bashif [ $# -lt 1 ]then echo &quot;No Args Input...&quot; exit ;ficase $1 in&quot;start&quot;) echo &quot; =================== 启动 hadoop集群 ===================&quot; echo &quot; ------------------- 启动 hdfs ---------------&quot; ssh hadoop102 &quot;/opt/module/hadoop-3.2.1/sbin/start-dfs.sh&quot; echo &quot; --------------- 启动 yarn ---------------&quot; ssh hadoop103 &quot;/opt/module/hadoop-3.2.1/sbin/start-yarn.sh&quot; echo &quot; --------------- 启动 historyserver ---------------&quot; ssh hadoop102 &quot;/opt/module/hadoop-3.2.1/bin/mapred --daemon start historyserver&quot;;;&quot;stop&quot;) echo &quot; =================== 关闭 hadoop集群 ===================&quot; echo &quot; --------------- 关闭 historyserver ---------------&quot; ssh hadoop102 &quot;/opt/module/hadoop-3.2.1/bin/mapred --daemon stop historyserver&quot; echo &quot; --------------- 关闭 yarn ---------------&quot; ssh hadoop103 &quot;/opt/module/hadoop-3.2.1/sbin/stop-yarn.sh&quot; echo &quot; --------------- 关闭 hdfs ---------------&quot; ssh hadoop102 &quot;/opt/module/hadoop-3.2.1/sbin/stop-dfs.sh&quot;;;*) echo &quot;Input Args Error...&quot;;;esac 在脚本中，使用全路径，如：/opt/module/hadoop-3.2.1/sbin/start-dfs.sh，不要使用 start-dfs.sh。 赋予脚本执行权限： 1[xisun@hadoop102 bin]$ chmod 777 myhadoop.sh 测试脚本停止集群： 123456789101112[xisun@hadoop102 ~]$ myhadoop.sh stop =================== 关闭 hadoop集群 =================== --------------- 关闭 historyserver --------------- --------------- 关闭 yarn ---------------Stopping nodemanagersStopping resourcemanager --------------- 关闭 hdfs ---------------Stopping namenodes on [hadoop102]Stopping datanodesStopping secondary namenodes [hadoop104][xisun@hadoop102 ~]$ jps51019 Jps 测试脚本启动集群： 12345678910111213141516[xisun@hadoop102 ~]$ myhadoop.sh start =================== 启动 hadoop集群 =================== --------------- 启动 hdfs ---------------Starting namenodes on [hadoop102]Starting datanodesStarting secondary namenodes [hadoop104] --------------- 启动 yarn ---------------Starting resourcemanagerStarting nodemanagers --------------- 启动 historyserver ---------------[xisun@hadoop102 ~]$ jps51666 NodeManager51209 NameNode51339 DataNode51917 Jps51839 JobHistoryServer 查看三台服务器 Java 进程脚本：jpsall 编写脚本： 1234567#!/bin/bashfor host in hadoop102 hadoop103 hadoop104do echo =============== $host =============== ssh $host jpsdone 1234567#!/bin/bashfor host in hadoop102 hadoop103 hadoop104do echo =============== $host =============== ssh $host jpsdone 赋予脚本执行权限： 1[xisun@hadoop102 bin]$ chmod 777 jpsall 测试脚本： 1234567891011121314151617[xisun@hadoop102 ~]$ jpsall =============== hadoop102 ===============51666 NodeManager51209 NameNode51339 DataNode51979 Jps51839 JobHistoryServer=============== hadoop103 ===============40195 Jps39492 DataNode39800 NodeManager39677 ResourceManager=============== hadoop104 ===============40920 NodeManager40809 SecondaryNameNode41081 Jps40699 DataNode 分发 hadoop102 上的 /home/xisun/bin 目录，保证自定义脚本在集群的所有机器上都可以使用。 12345678910111213141516171819202122[xisun@hadoop102 ~]$ xsync /home/xisun/bin/==================== hadoop102 ====================sending incremental file listsent 139 bytes received 17 bytes 312.00 bytes/sectotal size is 2,175 speedup is 13.94==================== hadoop103 ====================sending incremental file listbin/bin/jpsallbin/myhadoop.shsent 1,499 bytes received 58 bytes 3,114.00 bytes/sectotal size is 2,175 speedup is 1.40==================== hadoop104 ====================sending incremental file listbin/bin/jpsallbin/myhadoop.shsent 1,499 bytes received 58 bytes 3,114.00 bytes/sectotal size is 2,175 speedup is 1.40 常用端口号和配置文件说明 常用端口号 端口名称 Hadoop 2.x Hadoop 3.x HDFS NameNode 内部通信端口 8020 / 9000 8020 / 9000 / 9820 HDFS NameNode HTTP UI（对用户的查询端口） 50070 9870 YARN MapReduce 查看执行任务端口 8088 8088 历史服务器通信端口 19888 19888 常用配置文件 Hadoop 2.x：core-site.xml，hdfs-site.xml，yarn-site.xml，mapred-site.xml，slaves。 Hadoop 3.x：core-site.xml，hdfs-site.xml，yarn-site.xml，mapred-site.xml，workers。 集群时间同步 如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期和公网时间进行校准。 如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差，导致集群执行任务时间不同步。 配置过程略。 HDFS 概述HDFS 产生背景及定义HDFS 产生背景 随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS 只是分布式文件管理系统中的一种。 个人电脑上的磁盘，是 NTFS 文件管理系统。 HDFS 定义 HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS 的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。 HDFS 优缺点HDFS 优点 高容错性 数据自动保存多个副本。它通过增加副本的形式，提高容错性。 某一个副本丢失以后，它可以自动恢复。 适合处理大数据 数据规模：能够处理数据规模达到 GB、TB、甚至 PB 级别的数据。 文件规模：能够处理百万规模以上的文件数量，数量相当之大。 可构建在廉价机器上，通过多副本机制，提高可靠性。 HDFS 缺点 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 无法高效的对大量小文件进行存储。 存储大量小文件的话，它会占用 NameNode 大量的内存来存储文件目录和块信息。这样是不可取的，因为 NameNode 的内存总是有限的。 小文件存储的寻址时间会超过读取时间，它违反了 HDFS 的设计目标。 不支持并发写入、文件随机修改。 一个文件只能有一个写，不允许多个线程同时写。 仅支持数据 append（追加），不支持文件的随机修改。 HDFS 组成架构 官方文档：https://hadoop.apache.org/docs/ hadoop-3.2.1 HDFS 文档：https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html HDFS 组成架构： NameNode：nn，就是 Master，它是一个主管、管理者。 管理 HDFS 的名称空间； 配置副本策略； 管理数据块（Block）映射信息； 处理客户端读写请求。 DataNode：就是 Slave。NameNode下达命令，DataNode 执行实际的操作。 存储实际的数据块； 执行数据块的读/写操作。 Client：就是客户端。比如，Web 打开的 Browse Directory 页面就是一个客户端，能够对 HDFS 进行操作。 文件切分。文件上传到 HDFS 的时候，Client 将文件切分成一个一个的 Block，然后进行上传； 与 NameNode 交互，获取文件的位置信息； 与 DataNode 交互，读取或者写入数据； Client 提供一些命令来管理 HDFS，比如 NameNode 格式化； Client 可以通过一些命令来访问 HDFS，比如对 HDFS 增删查改操作。 Secondary NameNode：2nn，并非 NameNode 的热备。当 NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。 辅助 NameNode，分担其工作量，比如定期合并 Fsimage 和 Edits，并推送给 NameNode； 在紧急情况下，可辅助恢复 NameNode（只能恢复一部分数据，无法恢复全部数据）。 HDFS 文件块大小 HDFS 中的文件在物理上是分块存储（Block），块的大小可以通过配置参数 dfs.blocksize 来规定，默认大小在 Hadoop 2.x/3.x 版本中是 128 MB，Hadoop 1.x 版本中是 64 MB。 如果寻址时间约为 10 ms，即查找到集群中的目标 Block 的时间为 10 ms。当寻址时间为传输时间的 1% 时（专家研究），则为最佳状态。因此，传输时间 = 10 ms / 0.01 = 1000 ms = 1 s。而目前磁盘的传输速率普遍为 100 MB/s，因此，Block 大小 = 1 s * 100 MB/s = 100 MB。在计算机系统中，取 1024 的整数倍，即取 Block 大小为 128 MB。如果是固态硬盘，传输速率可以达到 200 ~ 300 MB/s，此时，一般取 Block 大小为 256 MB。 HDFS 的 Block 块的大小设置主要取决于磁盘传输速率。 如果 HDFS 的 Block 块设置的太小，会增加寻址时间，程序一直在找块的开始位置。 如果 HDFS 的 Block 块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间，导致程序在处理这块数据时，会非常慢。 HDFS 的 Shell 操作基本语法 方式一：hadoop fs 具体命令 方式二：hdfs dfs 具体命令 命令大全1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[xisun@hadoop102 ~]$ hadoop fsUsage: hadoop fs [generic options] [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] &lt;path&gt; ...] [-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] [-v] [-x] &lt;path&gt; ...] [-expunge [-immediate]] [-find &lt;path&gt; ... &lt;expression&gt; ...] [-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;] [-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;] [-head &lt;file&gt;] [-help [cmd ...]] [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;] [-test -[defswrz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touch [-a] [-m] [-t TIMESTAMP ] [-c] &lt;path&gt; ...] [-touchz &lt;path&gt; ...] [-truncate [-w] &lt;length&gt; &lt;path&gt; ...] [-usage [cmd ...]]Generic options supported are:-conf &lt;configuration file&gt; specify an application configuration file-D &lt;property=value&gt; define a value for a given property-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.-jt &lt;local|resourcemanager:port&gt; specify a ResourceManager-files &lt;file1,...&gt; specify a comma-separated list of files to be copied to the map reduce cluster-libjars &lt;jar1,...&gt; specify a comma-separated list of jar files to be included in the classpath-archives &lt;archive1,...&gt; specify a comma-separated list of archives to be unarchived on the compute machinesThe general command line syntax is:command [genericOptions] [commandOptions] 常用命令查看帮助 -help：查看命令的使用说明 1234567891011121314[xisun@hadoop102 ~]$ hadoop fs -help rm-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ... : Delete all files that match the specified file pattern. Equivalent to the Unix command &quot;rm &lt;src&gt;&quot; -f If the file does not exist, do not display a diagnostic message or modify the exit status to reflect an error. -[rR] Recursively deletes directories. -skipTrash option bypasses trash, if enabled, and immediately deletes &lt;src&gt;. -safely option requires safety confirmation, if enabled, requires confirmation before deleting large directory with more than &lt;hadoop.shell.delete.limit.num.files&gt; files. Delay is expected when walking over large directory recursively to count the number of files to be deleted before the confirmation. 直接操作 HDFS -ls：显示目录信息。 123456789[xisun@hadoop102 ~]$ hadoop fs -ls /Found 5 items-rw-r--r-- 3 xisun supergroup 359196911 2021-09-02 10:45 /hadoop-3.2.1.tar.gzdrwx------ - xisun supergroup 0 2021-09-02 16:35 /tmpdrwxr-xr-x - xisun supergroup 0 2021-09-02 10:14 /wcinputdrwxr-xr-x - xisun supergroup 0 2021-09-02 16:36 /wcoutput[xisun@hadoop102 ~]$ hadoop fs -ls /wcinputFound 1 items-rw-r--r-- 3 xisun supergroup 41 2021-09-02 10:14 /wcinput/word.txt -cat：显示文件内容。 123456[xisun@hadoop102 ~]$ hadoop fs -cat /wcinput/word.txt2021-09-06 14:07:12,091 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = falsehadoop yarnhadoop mapreducexisunxisun -mkdir：创建路径。 12345678[xisun@hadoop102 ~]$ hadoop fs -mkdir /sanguo[xisun@hadoop102 ~]$ hadoop fs -ls /Found 5 items-rw-r--r-- 3 xisun supergroup 359196911 2021-09-02 10:45 /hadoop-3.2.1.tar.gzdrwxr-xr-x - xisun supergroup 0 2021-09-06 13:35 /sanguodrwx------ - xisun supergroup 0 2021-09-02 16:35 /tmpdrwxr-xr-x - xisun supergroup 0 2021-09-02 10:14 /wcinputdrwxr-xr-x - xisun supergroup 0 2021-09-02 16:36 /wcoutput -cp：从 HDFS 的一个路径拷贝到 HDFS 的另一个路径。 123456[xisun@hadoop102 ~]$ hadoop fs -cp /wcinput/word.txt /sanguo2021-09-06 16:16:26,225 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false2021-09-06 16:16:27,148 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false[xisun@hadoop102 ~]$ hadoop fs -ls /sanguoFound 1 items-rw-r--r-- 3 xisun supergroup 41 2021-09-06 16:16 /sanguo/word.txt -mv：在 HDFS 目录中移动文件或重命名文件。 1234[xisun@hadoop102 ~]$ hadoop fs -mv /sanguo/word.txt /sanguo/sanguo.txt[xisun@hadoop102 ~]$ hadoop fs -ls /sanguoFound 1 items-rw-r--r-- 3 xisun supergroup 41 2021-09-06 16:16 /sanguo/sanguo.txt -tail：显示一个文件的末尾 1 kb 的数据。 123456[xisun@hadoop102 ~]$ hadoop fs -tail /sanguo/sanguo.txt2021-09-06 16:19:45,518 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = falsehadoop yarnhadoop mapreducexisunxisun -rm：删除文件。 12[xisun@hadoop102 ~]$ hadoop fs -rm /sanguo/sanguo.txtDeleted /sanguo/sanguo.txt -rm -r：递归删除目录及目录里面内容。 12345678910[xisun@hadoop102 ~]$ hadoop fs -rm /sanguorm: `/sanguo&#x27;: Is a directory[xisun@hadoop102 ~]$ hadoop fs -rm -r /sanguoDeleted /sanguo[xisun@hadoop102 ~]$ hadoop fs -ls /Found 4 items-rw-r--r-- 3 xisun supergroup 359196911 2021-09-02 10:45 /hadoop-3.2.1.tar.gzdrwx------ - xisun supergroup 0 2021-09-02 16:35 /tmpdrwxr-xr-x - xisun supergroup 0 2021-09-02 10:14 /wcinputdrwxr-xr-x - xisun supergroup 0 2021-09-02 16:36 /wcoutput -du：统计文件夹的大小信息。 1234[xisun@hadoop102 ~]$ hadoop fs -du -s -h /wcinput41 123 /wcinput[xisun@hadoop102 ~]$ hadoop fs -du -h /wcinput41 123 /wcinput/word.txt 41 表示文件大小；123 表示 41 * 3 个副本；/wcinput 表示查看的目录。 -chgrp、-chmod、-chown：与 Linux 文件系统中的用法一样，修改文件所属权限。 1234567891011[xisun@hadoop102 ~]$ hadoop fs -ls /wcinputFound 1 items-rw-r--r-- 3 xisun supergroup 41 2021-09-02 10:14 /wcinput/word.txt[xisun@hadoop102 ~]$ hadoop fs -chmod 666 /wcinput/word.txt[xisun@hadoop102 ~]$ hadoop fs -ls /wcinputFound 1 items-rw-rw-rw- 3 xisun supergroup 41 2021-09-02 10:14 /wcinput/word.txt[xisun@hadoop102 ~]$ hadoop fs -chown xisun:xisun /wcinput/word.txt[xisun@hadoop102 ~]$ hadoop fs -ls /wcinputFound 1 items-rw-rw-rw- 3 xisun xisun 41 2021-09-02 10:14 /wcinput/word.txt -setrep：设置 HDFS 中文件的副本数量。 12[xisun@hadoop102 ~]$ hadoop fs -setrep 10 /wcinput/word.txtReplication 10 set: /wcinput/word.txt 这里设置的副本数只是记录在 NameNode 的元数据中，是否真的会有这么多副本，取决于 DataNode 的数量。因为目前只有 3 台设备，最多也就 3 个副本，只有节点数的增加到 10 台时，副本数才能达到 10。 上传到 HDFS -moveFromLocal：从本地文件系统剪切文件粘贴到 HDFS 路径。 12345678910111213141516171819202122232425262728293031[xisun@hadoop102 ~]$ vim shuguo.txt[xisun@hadoop102 ~]$ ll总用量 4drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-rw-r--. 1 xisun xisun 7 9月 6 16:39 shuguo.txtdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面[xisun@hadoop102 ~]$ hadoop fs -mkdir /sanguo[xisun@hadoop102 ~]$ hadoop fs -ls /sanguo[xisun@hadoop102 ~]$ hadoop fs -moveFromLocal ./shuguo.txt /sanguo2021-09-06 16:40:09,950 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false[xisun@hadoop102 ~]$ hadoop fs -ls /sanguoFound 1 items-rw-r--r-- 3 xisun supergroup 7 2021-09-06 16:40 /sanguo/shuguo.txt[xisun@hadoop102 ~]$ ll总用量 0drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bindrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 -copyFromLocal：从本地文件系统中拷贝文件到 HDFS 路径。 12345678910111213141516171819202122232425262728293031[xisun@hadoop102 ~]$ vim weiguo.txt[xisun@hadoop102 ~]$ ll总用量 4drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-rw-r--. 1 xisun xisun 7 9月 6 16:43 weiguo.txtdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面[xisun@hadoop102 ~]$ hadoop fs -copyFromLocal weiguo.txt /sanguo2021-09-06 16:43:50,298 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false[xisun@hadoop102 ~]$ hadoop fs -ls /sanguoFound 2 items-rw-r--r-- 3 xisun supergroup 7 2021-09-06 16:40 /sanguo/shuguo.txt-rw-r--r-- 3 xisun supergroup 7 2021-09-06 16:43 /sanguo/weiguo.txt[xisun@hadoop102 ~]$ ll总用量 4drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-rw-r--. 1 xisun xisun 7 9月 6 16:43 weiguo.txtdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 -put：等同于 -copyFromLocal，生产环境更习惯用 -put。 12345678910111213141516171819202122232425262728293031323334[xisun@hadoop102 ~]$ vim wuguo.txt[xisun@hadoop102 ~]$ ll总用量 8drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-rw-r--. 1 xisun xisun 7 9月 6 16:43 weiguo.txt-rw-rw-r--. 1 xisun xisun 6 9月 6 16:45 wuguo.txtdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面[xisun@hadoop102 ~]$ hadoop fs -put wuguo.txt /sanguo2021-09-06 16:46:14,881 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false[xisun@hadoop102 ~]$ hadoop fs -ls /sanguoFound 3 items-rw-r--r-- 3 xisun supergroup 7 2021-09-06 16:40 /sanguo/shuguo.txt-rw-r--r-- 3 xisun supergroup 7 2021-09-06 16:43 /sanguo/weiguo.txt-rw-r--r-- 3 xisun supergroup 6 2021-09-06 16:46 /sanguo/wuguo.txt[xisun@hadoop102 ~]$ ll总用量 8drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-rw-r--. 1 xisun xisun 7 9月 6 16:43 weiguo.txt-rw-rw-r--. 1 xisun xisun 6 9月 6 16:45 wuguo.txtdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 -appendToFile：追加一个文件到已经存在的文件末尾。 123456789101112[xisun@hadoop102 ~]$ hadoop fs -cat /sanguo/shuguo.txt2021-09-06 16:47:33,926 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = falseshuguo[xisun@hadoop102 ~]$ vim liubei.txt[xisun@hadoop102 ~]$ cat liubei.txt liubei[xisun@hadoop102 ~]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt2021-09-06 16:48:18,942 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false[xisun@hadoop102 hadoop fs -cat /sanguo/shuguo.txt2021-09-06 16:48:32,423 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = falseshuguoliubei 下载至 HDFS -copyToLocal：从 HDFS 路径拷贝文件到本地文件系统。 1234567891011121314151617181920212223242526[xisun@hadoop102 ~]$ rm liubei.txt weiguo.txt wuguo.txt [xisun@hadoop102 ~]$ ll总用量 0drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bindrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面[xisun@hadoop102 ~]$ hadoop fs -copyToLocal /sanguo/shuguo.txt ./2021-09-06 16:49:58,344 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false[xisun@hadoop102 ~]$ ll总用量 4drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-r--r--. 1 xisun xisun 14 9月 6 16:49 shuguo.txtdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 -get：等同于 -copyToLocal，生产环境更习惯用 -get。 123456789101112131415[xisun@hadoop102 ~]$ hadoop fs -get /sanguo/weiguo.txt ./2021-09-06 16:50:23,583 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false[xisun@hadoop102 ~]$ ll总用量 8drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-r--r--. 1 xisun xisun 14 9月 6 16:49 shuguo.txt-rw-r--r--. 1 xisun xisun 7 9月 6 16:50 weiguo.txtdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 HDFS 的 API 操作客户端环境准备 Windows 系统开发 Hadoop 时，如果是远程连接 Linux 上的 Hadoop 集群，则不需要再下载安装 Windows 版本的 Hadoop。但是，需要在本地配置相关的 Hadoop 变量，主要包括 hadoop.dll 与 winutils.exe 等。 由于 Hadoop 主要基于 Linux 编写，winutil.exe 主要用于模拟 Linux 下的目录环境。当 Hadoop 在 Windows 下运行或调用远程 Hadoop 集群的时候，需要该辅助程序才能运行。winutils.exe 是 Windows 中的二进制文件，适用于不同版本的 Hadoop 系统并构建在 Windows VM 上，该 VM 用以在 Windows 系统中测试 Hadoop/YARN 相关的应用程序。参考：https://blog.csdn.net/HeyShHeyou/article/details/103441110。 相关异常信息： 1234567891011121314151617java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:528) ~[hadoop-common-2.8.4.jar:na] at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:549) ~[hadoop-common-2.8.4.jar:na] at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:572) ~[hadoop-common-2.8.4.jar:na] at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:669) ~[hadoop-common-2.8.4.jar:na] at org.apache.hadoop.util.StringUtils.&lt;clinit&gt;(StringUtils.java:79) [hadoop-common-2.8.4.jar:na] at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555) [hadoop-common-2.8.4.jar:na] at org.apache.hadoop.hbase.HBaseConfiguration.checkDefaultsVersion(HBaseConfiguration.java:66) [hbase-common-2.0.0.jar:2.0.0] at org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources(HBaseConfiguration.java:80) [hbase-common-2.0.0.jar:2.0.0] at org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:94) [hbase-common-2.0.0.jar:2.0.0] at org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl$1.call(ConfigurationFactory.java:49) [phoenix-core-5.0.0-HBase-2.0.jar:5.0.0-HBase-2.0] at org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl$1.call(ConfigurationFactory.java:46) [phoenix-core-5.0.0-HBase-2.0.jar:5.0.0-HBase-2.0] at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:76) [phoenix-core-5.0.0-HBase-2.0.jar:5.0.0-HBase-2.0] at org.apache.phoenix.util.PhoenixContextExecutor.callWithoutPropagation(PhoenixContextExecutor.java:91) [phoenix-core-5.0.0-HBase-2.0.jar:5.0.0-HBase-2.0] at org.apache.phoenix.query.ConfigurationFactory$ConfigurationFactoryImpl.getConfiguration(ConfigurationFactory.java:46) [phoenix-core-5.0.0-HBase-2.0.jar:5.0.0-HBase-2.0] at org.apache.phoenix.jdbc.PhoenixDriver.initializeConnectionCache(PhoenixDriver.java:151) [phoenix-core-5.0.0-HBase-2.0.jar:5.0.0-HBase-2.0] at org.apache.phoenix.jdbc.PhoenixDriver.&lt;init&gt;(PhoenixDriver.java:143) [phoenix-core-5.0.0-HBase-2.0.jar:5.0.0-HBase-2.0] 根据 Linux 上 Hadoop 版本，下载对应的 winutils.exe 版本：https://github.com/cdarlint/winutils，https://github.com/steveloughran/winutils。 配置 HADOOP_HOME 环境变量： 配置 Path 环境变量： 验证 Hadoop 环境变量是否正常。双击 winutils.exe，如果一闪而过，说明正常；如果报如下错误，说明缺少微软运行库（正版系统往往有这个问题）。 Hadoop 环境变量配置好后，可能需要重启 IDEA 或者重启电脑。 项目创建 添加依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.xisun.hadoop&lt;/groupId&gt; &lt;artifactId&gt;xisun-hadoop&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.version&gt;3.6.1&lt;/maven.compiler.version&gt; &lt;maven.compiler.source&gt;$&#123;java.version&#125;&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;$&#123;java.version&#125;&lt;/maven.compiler.target&gt; &lt;hadoop.version&gt;3.2.1&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- 日志相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Hadoop客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven.compiler.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;maven.compiler.source&#125;&lt;/source&gt; &lt;target&gt;$&#123;maven.compiler.target&#125;&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 日志配置： log4j.properties: 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n logback.xml： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;configuration debug=&quot;false&quot; xmlns=&quot;http://ch.qos.logback/xml/ns/logback&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://ch.qos.logback/xml/ns/logback https://raw.githubusercontent.com/enricopulatzo/logback-XSD/master/src/main/xsd/logback.xsd&quot;&gt; &lt;!-- 定义日志文件的存储地址 --&gt; &lt;property name=&quot;logging.path&quot; value=&quot;./&quot;/&gt; &lt;property name=&quot;logging.level&quot; value=&quot;DEBUG&quot;/&gt; &lt;property name=&quot;message.format&quot; value=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n%ex&#123;full, DISPLAY_EX_EVAL&#125;&quot;/&gt; &lt;!-- 控制台输出日志 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义滚动日志 --&gt; &lt;appender name=&quot;RollingAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/reaction-extractor-%d&#123;yyyy-MM-dd&#125;.log&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 异步输出日志 --&gt; &lt;appender name=&quot;ASYNC&quot; class=&quot;ch.qos.logback.classic.AsyncAppender&quot;&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;queueSize&gt;100&lt;/queueSize&gt; &lt;appender-ref ref=&quot;RollingAppender&quot;/&gt; &lt;/appender&gt; &lt;root level=&quot;$&#123;logging.level&#125;&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;appender-ref ref=&quot;ASYNC&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt; 项目结构： 案例实操创建目录 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * @Author XiSun * @Date 2021/9/6 22:46 */@Slf4jpublic class HdfsClient &#123; private static FileSystem fileSystem; static &#123; try &#123; init(); &#125; catch (URISyntaxException | IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; /** * 获取文件系统 * * @throws URISyntaxException * @throws IOException * @throws InterruptedException */ public static void init() throws URISyntaxException, IOException, InterruptedException &#123; // 集群nn的连接地址 URI uri = new URI(&quot;hdfs://hadoop102:8020&quot;); // 创建一个配置文件，按需求自定义配置条件 Configuration configuration = new Configuration(); // 用户 String user = &quot;xisun&quot;; // 获取客户端对象 fileSystem = FileSystem.get(uri, configuration, user); &#125; /** * 关闭资源 */ public static void close() &#123; if (fileSystem != null) &#123; try &#123; fileSystem.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; /** * 创建目录 */ public static void mkdirs() &#123; try &#123; fileSystem.mkdirs(new Path(&quot;/xiyou/huaguoshan/&quot;)); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; close(); &#125; &#125; public static void main(String[] args) &#123; mkdirs(); &#125;&#125; 客户端去操作 HDFS 时，是有一个用户身份的。默认情况下，HDFS 客户端 API 会从采用 Windows 默认用户访问 HDFS，这通常会报权限异常错误。因此，在创建文件系统对象时，一定要配置用户。 控制台输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253542021-09-07 15:35:02.744 [main] DEBUG o.apache.hadoop.metrics2.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)])2021-09-07 15:35:02.750 [main] DEBUG o.apache.hadoop.metrics2.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)])2021-09-07 15:35:02.750 [main] DEBUG o.apache.hadoop.metrics2.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[GetGroups])2021-09-07 15:35:02.751 [main] DEBUG o.apache.hadoop.metrics2.lib.MutableMetricsFactory - field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since startup])2021-09-07 15:35:02.751 [main] DEBUG o.apache.hadoop.metrics2.lib.MutableMetricsFactory - field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, valueName=Time, about=, interval=10, type=DEFAULT, value=[Renewal failures since last successful login])2021-09-07 15:35:02.752 [main] DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl - UgiMetrics, User and group related metrics2021-09-07 15:35:02.761 [main] DEBUG org.apache.hadoop.security.UserGroupInformation - PrivilegedAction as:xisun (auth:SIMPLE) from:org.apache.hadoop.fs.FileSystem.get(FileSystem.java:214)2021-09-07 15:35:03.049 [main] DEBUG org.apache.hadoop.fs.FileSystem - Loading filesystems2021-09-07 15:35:03.065 [main] DEBUG org.apache.hadoop.fs.FileSystem - file:// = class org.apache.hadoop.fs.LocalFileSystem from /D:/java/maven-repo/org/apache/hadoop/hadoop-common/3.2.1/hadoop-common-3.2.1.jar2021-09-07 15:35:03.073 [main] DEBUG org.apache.hadoop.fs.FileSystem - viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /D:/java/maven-repo/org/apache/hadoop/hadoop-common/3.2.1/hadoop-common-3.2.1.jar2021-09-07 15:35:03.076 [main] DEBUG org.apache.hadoop.fs.FileSystem - har:// = class org.apache.hadoop.fs.HarFileSystem from /D:/java/maven-repo/org/apache/hadoop/hadoop-common/3.2.1/hadoop-common-3.2.1.jar2021-09-07 15:35:03.077 [main] DEBUG org.apache.hadoop.fs.FileSystem - http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /D:/java/maven-repo/org/apache/hadoop/hadoop-common/3.2.1/hadoop-common-3.2.1.jar2021-09-07 15:35:03.078 [main] DEBUG org.apache.hadoop.fs.FileSystem - https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /D:/java/maven-repo/org/apache/hadoop/hadoop-common/3.2.1/hadoop-common-3.2.1.jar2021-09-07 15:35:03.090 [main] DEBUG org.apache.hadoop.fs.FileSystem - hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /D:/java/maven-repo/org/apache/hadoop/hadoop-hdfs-client/3.2.1/hadoop-hdfs-client-3.2.1.jar2021-09-07 15:35:03.102 [main] DEBUG org.apache.hadoop.fs.FileSystem - webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /D:/java/maven-repo/org/apache/hadoop/hadoop-hdfs-client/3.2.1/hadoop-hdfs-client-3.2.1.jar2021-09-07 15:35:03.104 [main] DEBUG org.apache.hadoop.fs.FileSystem - swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /D:/java/maven-repo/org/apache/hadoop/hadoop-hdfs-client/3.2.1/hadoop-hdfs-client-3.2.1.jar2021-09-07 15:35:03.104 [main] DEBUG org.apache.hadoop.fs.FileSystem - Looking for FS supporting hdfs2021-09-07 15:35:03.104 [main] DEBUG org.apache.hadoop.fs.FileSystem - looking for configuration option fs.hdfs.impl2021-09-07 15:35:03.120 [main] DEBUG org.apache.hadoop.fs.FileSystem - Looking in service filesystems for implementation class2021-09-07 15:35:03.121 [main] DEBUG org.apache.hadoop.fs.FileSystem - FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem2021-09-07 15:35:03.204 [main] DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf - dfs.client.use.legacy.blockreader.local = false2021-09-07 15:35:03.205 [main] DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf - dfs.client.read.shortcircuit = false2021-09-07 15:35:03.205 [main] DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf - dfs.client.domain.socket.data.traffic = false2021-09-07 15:35:03.205 [main] DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf - dfs.domain.socket.path = 2021-09-07 15:35:03.219 [main] DEBUG org.apache.hadoop.hdfs.DFSClient - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 02021-09-07 15:35:03.240 [main] DEBUG org.apache.hadoop.security.SecurityUtil - Setting hadoop.security.token.service.use_ip to true2021-09-07 15:35:03.254 [main] DEBUG org.apache.hadoop.io.retry.RetryUtils - multipleLinearRandomRetry = null2021-09-07 15:35:03.291 [main] DEBUG org.apache.hadoop.security.Groups - Creating new Groups object2021-09-07 15:35:03.293 [main] DEBUG org.apache.hadoop.util.NativeCodeLoader - Trying to load the custom-built native-hadoop library...2021-09-07 15:35:03.297 [main] DEBUG org.apache.hadoop.util.NativeCodeLoader - Loaded the native-hadoop library2021-09-07 15:35:03.298 [main] DEBUG o.apache.hadoop.security.JniBasedUnixGroupsMapping - Using JniBasedUnixGroupsMapping for Group resolution2021-09-07 15:35:03.298 [main] DEBUG o.a.h.s.JniBasedUnixGroupsMappingWithFallback - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping2021-09-07 15:35:03.335 [main] DEBUG org.apache.hadoop.security.Groups - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=50002021-09-07 15:35:03.347 [main] DEBUG org.apache.hadoop.ipc.Server - rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@3ce1e3092021-09-07 15:35:03.672 [main] DEBUG org.apache.hadoop.ipc.Client - getting client out of cache: org.apache.hadoop.ipc.Client@52bf72b52021-09-07 15:35:04.085 [main] DEBUG org.apache.hadoop.util.PerformanceAdvisory - Both short-circuit local reads and UNIX domain socket are disabled.2021-09-07 15:35:04.092 [main] DEBUG o.a.h.h.p.datatransfer.sasl.DataTransferSaslUtil - DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection2021-09-07 15:35:04.112 [main] DEBUG org.apache.hadoop.hdfs.DFSClient - /xiyou/huaguoshan: masked=&#123; masked: rwxr-xr-x, unmasked: rwxrwxrwx &#125;2021-09-07 15:35:04.151 [main] DEBUG org.apache.hadoop.ipc.Client - The ping interval is 60000 ms.2021-09-07 15:35:04.153 [main] DEBUG org.apache.hadoop.ipc.Client - Connecting to hadoop102/192.168.10.102:80202021-09-07 15:35:04.227 [IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun] DEBUG org.apache.hadoop.ipc.Client - IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun: starting, having connections 12021-09-07 15:35:04.230 [IPC Parameter Sending Thread #0] DEBUG org.apache.hadoop.ipc.Client - IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs2021-09-07 15:35:04.244 [IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun] DEBUG org.apache.hadoop.ipc.Client - IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun got value #02021-09-07 15:35:04.245 [main] DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine - Call: mkdirs took 118ms2021-09-07 15:35:04.248 [main] DEBUG org.apache.hadoop.ipc.Client - stopping client from cache: org.apache.hadoop.ipc.Client@52bf72b52021-09-07 15:35:04.248 [main] DEBUG org.apache.hadoop.ipc.Client - removing client from cache: org.apache.hadoop.ipc.Client@52bf72b52021-09-07 15:35:04.248 [main] DEBUG org.apache.hadoop.ipc.Client - stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@52bf72b52021-09-07 15:35:04.248 [main] DEBUG org.apache.hadoop.ipc.Client - Stopping client2021-09-07 15:35:04.248 [IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun] DEBUG org.apache.hadoop.ipc.Client - IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun: closed2021-09-07 15:35:04.248 [IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun] DEBUG org.apache.hadoop.ipc.Client - IPC Client (956420404) connection to hadoop102/192.168.10.102:8020 from xisun: stopped, remaining connections 02021-09-07 15:35:04.354 [Thread-4] DEBUG org.apache.hadoop.util.ShutdownHookManager - Completed shutdown in 0.001 seconds; Timeouts: 02021-09-07 15:35:04.363 [Thread-4] DEBUG org.apache.hadoop.util.ShutdownHookManager - ShutdownHookManger completed shutdown.Process finished with exit code 0 创建结果： 文件上传 代码实现： 1234567891011121314/** * 上传文件 */public static void copyFromLocalFile() &#123; try &#123; // 参数一：Windows原文件是否删除；参数二：若HDFS目的地文件已存在，是否允许覆盖；参数三：Windows原文件路径；参数四：HDFS目的地路径 fileSystem.copyFromLocalFile(false, true, new Path(&quot;E:\\\\sunwukong.txt&quot;), new Path(&quot;hdfs://hadoop102/xiyou/huaguoshan&quot;)); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; close(); &#125;&#125; 控制台输出： 1232021-09-07 15:46:03.654 [Thread-6] INFO o.a.h.h.p.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = falseProcess finished with exit code 0 上传结果： 可以看出，上传文件，默认备份数是 3，参数的优先级顺序：hdfs-default.xml &lt; hdfs-site.xml &lt; 在项目资源目录下的配置文件 &lt; 代码里面的配置。 项目资源目录下的配置文件： 代码里面的配置： 文件下载 代码实现： 1234567891011121314/** * 下载文件 */public static void copyToLocalFile() &#123; try &#123; // 参数一：HDFS原文件是否删除；参数二：HDFS原文件路径；参数三：Windows目标地址路径；参数四：是否开启本地文件校验，false表示开启 fileSystem.copyToLocalFile(false, new Path(&quot;hdfs://hadoop102/wcinput/word.txt&quot;), new Path(&quot;E:\\\\11\\\\&quot;), false); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; close(); &#125;&#125; 也可以直接下载 HDFS 上的文件路径。 控制台输出： 1232021-09-07 16:06:31.318 [main] INFO o.a.h.h.p.datatransfer.sasl.SaslDataTransferClient - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = falseProcess finished with exit code 0 下载结果： 文件删除 代码实现： 123456789101112131415161718192021/** * 删除文件 */public static void rm() &#123; try &#123; // 参数1：HDFS要删除的路径；参数2：是否递归删除 // 删除文件 fileSystem.delete(new Path(&quot;/hadoop-3.2.1.tar.gz&quot;), false); // 删除空目录 fileSystem.delete(new Path(&quot;/honglou&quot;), false); // 删除非空目录 fileSystem.delete(new Path(&quot;/xiyou&quot;), true); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; close(); &#125;&#125; 文件的更名和移动 代码实现： 123456789101112131415161718192021/** * 文件的更名和移动 */public static void mv() &#123; try &#123; // 参数1：HDFS原文件路径；参数2：HDFS目标文件路径 // 对文件名称的修改 // fileSystem.rename(new Path(&quot;/wcinput/word.txt&quot;), new Path(&quot;/wcinput/words.txt&quot;)); // 文件的移动和更名 // fileSystem.rename(new Path(&quot;/wcinput/words.txt&quot;), new Path(&quot;/wordout.txt&quot;)); // 目录更名 fileSystem.rename(new Path(&quot;/honglou&quot;), new Path(&quot;/hongloumeng&quot;)); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; close(); &#125;&#125; 文件详情查看 代码实现： 12345678910111213141516171819202122232425262728293031/** * 获取文件详细信息 */public static void listFiles() &#123; try &#123; // 获取所有文件信息，第二个参数表示是否递归 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(new Path(&quot;/&quot;), true); // 遍历文件 while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println(&quot;==========&quot; + fileStatus.getPath() + &quot;=========&quot;); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getOwner()); System.out.println(fileStatus.getGroup()); System.out.println(fileStatus.getLen()); System.out.println(fileStatus.getModificationTime()); System.out.println(fileStatus.getReplication()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPath().getName()); // 获取块信息 BlockLocation[] blockLocations = fileStatus.getBlockLocations(); System.out.println(Arrays.toString(blockLocations)); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; close(); &#125;&#125; 控制台输出： 1234567891011121314151617181920212223242526272829303132==========hdfs://hadoop102:8020/sanguoyanyi/shuguo.txt=========rw-r--r--xisunsupergroup1416309181005573134217728shuguo.txt[0,14,hadoop102,hadoop103,hadoop104]==========hdfs://hadoop102:8020/sanguoyanyi/weiguo.txt=========rw-r--r--xisunsupergroup716309178306573134217728weiguo.txt[0,7,hadoop102,hadoop103,hadoop104]==========hdfs://hadoop102:8020/sanguoyanyi/wuguo.txt=========rw-r--r--xisunsupergroup616309179753053134217728wuguo.txt[0,6,hadoop102,hadoop103,hadoop104] Process finished with exit code 0 文件和文件夹类型判断 代码实现： 1234567891011121314151617181920/** * 判断是文件夹还是文件 */public static void fileType() &#123; try &#123; FileStatus[] listStatus = fileSystem.listStatus(new Path(&quot;/&quot;)); for (FileStatus status : listStatus) &#123; if (status.isFile()) &#123; System.out.println(&quot;文件：&quot; + status.getPath().getName()); &#125; else &#123; System.out.println(&quot;目录：&quot; + status.getPath().getName()); &#125; &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; close(); &#125;&#125; 控制台输出： 1234567目录：sanguoyanyi目录：tmp目录：wcinput目录：wcoutput文件：wordout.txtProcess finished with exit code 0 HDFS 的读写流程HDFS 写数据流程 客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。 NameNode 返回是否可以上传。 客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。 NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。 客户端通过 FSDataOutputStream 模块请求向 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。 dn1、dn2、dn3 逐级应答客户端。 客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存），以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 再传给 dn3；dn1 每传一个 Packet 会放入一个应答队列等待应答。 当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务器。（重复执行 3 - 7 步） 网络拓扑 - 节点距离计算 在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。 节点距离：两个节点到达最近的共同祖先的距离总和。 假设有集群 d1 机架 r1 中的节点 n1，则该节点可以表示为 /d1/r1/n1。利用这种标记，这里给出四种距离描述，如下图所示： 节点到其自身的距离为 0，到其所属机架的距离为 1，到其所属集群的距离为 2，到其所属机房的距离为 3。 Distance(/d1/r1/n0, /d1/r1/n0) = 0：同一节点上的进程。/d1/r1/n0 与 /d1/r1/n0 的共同祖先是 n0，因此，距离为 0 + 0 = 0。 Distance(/d1/r1/n1, /d1/r1/n2) = 2：同一机架上的不同节点。/d1/r1/n1 与 /d1/r1/n2 的共同祖先是机架 r1，因此，距离为 1 + 1 = 2。 Distance(/d1/r2/n0, /d1/r3/n2) = 4：同一数据中心不同机架上的节点。/d1/r2/n0 与 /d1/r3/n2 的共同祖先是集群 d1，因此，距离为 2 + 2 = 4。 Distance(/d1/r2/n1, /d2/r4/n1) = 6：不同数据中心的节点。/d1/r2/n1 与 /d2/r4/n1 的共同祖先是机房，因此，距离为 3 + 3 = 6。 再比如下图互联网上的一个计算机群，5 号机和 9 号机的节点距离为 3，2 号机和 10 号机的节点距离也为 3。 机架感知 - 副本存储节点选择 机架感知说明 1For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on the local machine if the writer is on a datanode, otherwise on a random datanode in the same rack as that of the writer, another replica on a node in a different (remote) rack, and the last on a different node in the same remote rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance. 副本存储节点选择 第一个副本，在 Client 所处的节点上，节点距离最近，上传速度最快。如果客户端在集群外，则随机选一个。比如：/d1/r1/n0。 第二个副本，在另一个机架的随机一个节点，以保证数据的可靠性。比如：/d1/r2/n0。 第三个副本，在第二个副本所在机架的随机节点，考虑的是传输效率。比如：/d1/r2/n1。 HDFS 读数据流程 客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过检查权限和查询元数据，找到文件块所在的 DataNode 地址。 挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。 DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。 客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。 NameNode 和 SecondaryNameNodeNN 和 2NN 工作机制 NameNode 中的元数据如何存储的？ 首先，我们做个假设，如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此，需要一个在磁盘中备份元数据的 FsImage。 其次，这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。因此，引入 Edits 文件（此文件只进行追加操作，即只记录每一个请求的过程，不计算结果，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并将请求操作追加到 Edits 中。这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。 但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由 NameNode 节点完成，又会效率过低。因此，引入一个新的节点 SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。 第一阶段：NameNode 启动。 第一次启动 NameNode 格式化后，创建 Fsimage（镜像文件）和 Edits（编辑日志）文件。如果不是第一次启动，直接加载 Fsimage 和 Edits 到内存。 客户端发送对元数据进行增删改的请求。 NameNode 记录操作日志，更新滚动日志。 NameNode 在内存中对元数据进行增删改。 第二阶段：Secondary NameNode 工作。 Secondary NameNode 询问 NameNode 是否需要 CheckPoint，并直接带回 NameNode 是否检查的结果。 Secondary NameNode 请求执行 CheckPoint。 NameNode 滚动正在写的 Edits 日志。 NameNode 将滚动前的镜像文件和编辑日志拷贝到 Secondary NameNode。 Secondary NameNode 加载镜像文件和编辑日志到内存，并合并。 Secondary NameNode 生成新的镜像文件 fsimage.chkpoint。 Secondary NameNode 拷贝 fsimage.chkpoint 到 NameNode。 NameNode 将 fsimage.chkpoint 重新命名成 fsimage。 Fsimage 和 Edits 解析Fsimage 和 Edits 的概念 NameNode 被格式化之后，将在 /opt/module/hadoop-3.1.3/data/tmp/dfs/name/current 路径中产生如下文件： 12345678[xisun@hadoop102 current]$ pwd/opt/module/hadoop-3.2.1/data/dfs/name/current[xisun@hadoop102 current]$ ll总用量 701-rw-rw-r--. 1 xisun xisun 418 9月 8 13:43 fsimage_0000000000000000000-rw-rw-r--. 1 xisun xisun 62 9月 8 13:43 fsimage_0000000000000000000.md5-rw-rw-r--. 1 xisun xisun 4 9月 8 13:43 seen_txid-rw-rw-r--. 1 xisun xisun 217 9月 7 11:44 VERSION Fsimage 文件：HDFS 文件系统元数据的一个永久性的检查点，其中包含 HDFS 文件系统的所有目录和文件 inode 的序列化信息。 Edits 文件：存放 HDFS 文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到 Edits 文件中。 seen_txid 文件：保存的是一个数字，就是最后一个 edits_ 的数字。 每次 NameNode 启动的时候都会将 Fsimage 文件读入内存，加载 Edits 里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成当 NameNode 启动的时候就将 Fsimage 和 Edits 文件进行了合并。 oiv 查看 Fsimage 文件 查看 oiv 和 oev 命令 123456[xisun@hadoop102 current]$ hdfsUsage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]...oev apply the offline edits viewer to an edits fileoiv apply the offline fsimage viewer to an fsimage... 基本语法 1hdfs oiv -p 文件类型 -i 镜像文件名称 -o 转换后文件的输出路径 -o 参数需要制定转换后文件的具体名称，不能只是一个目录。 12345678910[xisun@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000931 -o ~Encountered exception. Exiting: /home/xisun (是一个目录)java.io.FileNotFoundException: /home/xisun (是一个目录) at java.io.FileOutputStream.open0(Native Method) at java.io.FileOutputStream.open(FileOutputStream.java:270) at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213) at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:101) at java.io.PrintStream.&lt;init&gt;(PrintStream.java:248) at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB.run(OfflineImageViewerPB.java:178) at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB.main(OfflineImageViewerPB.java:137) 实例 123456789101112131415161718192021[xisun@hadoop102 current]$ pwd/opt/module/hadoop-3.2.1/data/dfs/name/current[xisun@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000931 -o ~/fsimage.XML2021-09-08 16:35:27,445 INFO offlineImageViewer.FSImageHandler: Loading 6 strings2021-09-08 16:35:29,097 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=5368709112021-09-08 16:35:29,097 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=167772152021-09-08 16:35:29,097 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=167772152021-09-08 16:35:29,097 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215[xisun@hadoop102 current]$ cd ~[xisun@hadoop102 ~]$ ll总用量 28drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-rw-r--. 1 xisun xisun 19241 9月 8 16:35 fsimage.XMLdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 下载文件并查看 1[xisun@hadoop102 ~]$ sz fsimage.XML 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154&lt;?xml version=&quot;1.0&quot;?&gt;&lt;fsimage&gt;&lt;version&gt;&lt;layoutVersion&gt;-65&lt;/layoutVersion&gt;&lt;onDiskVersion&gt;1&lt;/onDiskVersion&gt;&lt;oivRevision&gt;b3cbbb467e22ea829b3808f4b7b01d07e0bf3842&lt;/oivRevision&gt;&lt;/version&gt;&lt;NameSection&gt;&lt;namespaceId&gt;817173371&lt;/namespaceId&gt;&lt;genstampV1&gt;1000&lt;/genstampV1&gt;&lt;genstampV2&gt;1068&lt;/genstampV2&gt;&lt;genstampV1Limit&gt;0&lt;/genstampV1Limit&gt;&lt;lastAllocatedBlockId&gt;1073741891&lt;/lastAllocatedBlockId&gt;&lt;txid&gt;931&lt;/txid&gt;&lt;/NameSection&gt;&lt;ErasureCodingSection&gt;&lt;erasureCodingPolicy&gt;&lt;policyId&gt;1&lt;/policyId&gt;&lt;policyName&gt;RS-6-3-1024k&lt;/policyName&gt;&lt;cellSize&gt;1048576&lt;/cellSize&gt;&lt;policyState&gt;DISABLED&lt;/policyState&gt;&lt;ecSchema&gt;&lt;codecName&gt;rs&lt;/codecName&gt;&lt;dataUnits&gt;6&lt;/dataUnits&gt;&lt;parityUnits&gt;3&lt;/parityUnits&gt;&lt;/ecSchema&gt;&lt;/erasureCodingPolicy&gt;&lt;erasureCodingPolicy&gt;&lt;policyId&gt;2&lt;/policyId&gt;&lt;policyName&gt;RS-3-2-1024k&lt;/policyName&gt;&lt;cellSize&gt;1048576&lt;/cellSize&gt;&lt;policyState&gt;DISABLED&lt;/policyState&gt;&lt;ecSchema&gt;&lt;codecName&gt;rs&lt;/codecName&gt;&lt;dataUnits&gt;3&lt;/dataUnits&gt;&lt;parityUnits&gt;2&lt;/parityUnits&gt;&lt;/ecSchema&gt;&lt;/erasureCodingPolicy&gt;&lt;erasureCodingPolicy&gt;&lt;policyId&gt;3&lt;/policyId&gt;&lt;policyName&gt;RS-LEGACY-6-3-1024k&lt;/policyName&gt;&lt;cellSize&gt;1048576&lt;/cellSize&gt;&lt;policyState&gt;DISABLED&lt;/policyState&gt;&lt;ecSchema&gt;&lt;codecName&gt;rs-legacy&lt;/codecName&gt;&lt;dataUnits&gt;6&lt;/dataUnits&gt;&lt;parityUnits&gt;3&lt;/parityUnits&gt;&lt;/ecSchema&gt;&lt;/erasureCodingPolicy&gt;&lt;erasureCodingPolicy&gt;&lt;policyId&gt;4&lt;/policyId&gt;&lt;policyName&gt;XOR-2-1-1024k&lt;/policyName&gt;&lt;cellSize&gt;1048576&lt;/cellSize&gt;&lt;policyState&gt;DISABLED&lt;/policyState&gt;&lt;ecSchema&gt;&lt;codecName&gt;xor&lt;/codecName&gt;&lt;dataUnits&gt;2&lt;/dataUnits&gt;&lt;parityUnits&gt;1&lt;/parityUnits&gt;&lt;/ecSchema&gt;&lt;/erasureCodingPolicy&gt;&lt;erasureCodingPolicy&gt;&lt;policyId&gt;5&lt;/policyId&gt;&lt;policyName&gt;RS-10-4-1024k&lt;/policyName&gt;&lt;cellSize&gt;1048576&lt;/cellSize&gt;&lt;policyState&gt;DISABLED&lt;/policyState&gt;&lt;ecSchema&gt;&lt;codecName&gt;rs&lt;/codecName&gt;&lt;dataUnits&gt;10&lt;/dataUnits&gt;&lt;parityUnits&gt;4&lt;/parityUnits&gt;&lt;/ecSchema&gt;&lt;/erasureCodingPolicy&gt;&lt;/ErasureCodingSection&gt;&lt;INodeSection&gt;&lt;lastInodeId&gt;16535&lt;/lastInodeId&gt;&lt;numInodes&gt;48&lt;/numInodes&gt;&lt;inode&gt;&lt;id&gt;16385&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;&lt;/name&gt;&lt;mtime&gt;1631005240758&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0755&lt;/permission&gt;&lt;nsquota&gt;9223372036854775807&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16386&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;wcinput&lt;/name&gt;&lt;mtime&gt;1631005111883&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0755&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16387&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;wordout.txt&lt;/name&gt;&lt;replication&gt;10&lt;/replication&gt;&lt;mtime&gt;1630548862351&lt;/mtime&gt;&lt;atime&gt;1631005650890&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:xisun:0666&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741825&lt;/id&gt;&lt;genstamp&gt;1001&lt;/genstamp&gt;&lt;numBytes&gt;41&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16389&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;tmp&lt;/name&gt;&lt;mtime&gt;1630571709992&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0700&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16390&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;hadoop-yarn&lt;/name&gt;&lt;mtime&gt;1630553418947&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0700&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16391&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;staging&lt;/name&gt;&lt;mtime&gt;1630554923848&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0700&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16392&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;xisun&lt;/name&gt;&lt;mtime&gt;1630553418948&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0700&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16393&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;.staging&lt;/name&gt;&lt;mtime&gt;1630571777142&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0700&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16394&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;job_1630509320297_0001&lt;/name&gt;&lt;mtime&gt;1630553421706&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0700&lt;/permission&gt;&lt;xattrs&gt;&lt;xattr&gt;&lt;ns&gt;SYSTEM&lt;/ns&gt;&lt;name&gt;hdfs.erasurecoding.policy&lt;/name&gt;&lt;val&gt;\\0000;\\0000;\\0000;\\000b;replication&lt;/val&gt;&lt;/xattr&gt;&lt;/xattrs&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16395&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job.jar&lt;/name&gt;&lt;replication&gt;10&lt;/replication&gt;&lt;mtime&gt;1630553421192&lt;/mtime&gt;&lt;atime&gt;1630553420729&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741829&lt;/id&gt;&lt;genstamp&gt;1005&lt;/genstamp&gt;&lt;numBytes&gt;316534&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16396&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job.split&lt;/name&gt;&lt;replication&gt;10&lt;/replication&gt;&lt;mtime&gt;1630553421574&lt;/mtime&gt;&lt;atime&gt;1630553421453&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741830&lt;/id&gt;&lt;genstamp&gt;1006&lt;/genstamp&gt;&lt;numBytes&gt;110&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16397&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job.splitmetainfo&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630553421698&lt;/mtime&gt;&lt;atime&gt;1630553421586&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741831&lt;/id&gt;&lt;genstamp&gt;1007&lt;/genstamp&gt;&lt;numBytes&gt;43&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16398&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job.xml&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630553422297&lt;/mtime&gt;&lt;atime&gt;1630553421706&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741832&lt;/id&gt;&lt;genstamp&gt;1008&lt;/genstamp&gt;&lt;numBytes&gt;192215&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16399&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;job_1630509320297_0002&lt;/name&gt;&lt;mtime&gt;1630553679452&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0700&lt;/permission&gt;&lt;xattrs&gt;&lt;xattr&gt;&lt;ns&gt;SYSTEM&lt;/ns&gt;&lt;name&gt;hdfs.erasurecoding.policy&lt;/name&gt;&lt;val&gt;\\0000;\\0000;\\0000;\\000b;replication&lt;/val&gt;&lt;/xattr&gt;&lt;/xattrs&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16400&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job.jar&lt;/name&gt;&lt;replication&gt;10&lt;/replication&gt;&lt;mtime&gt;1630553679124&lt;/mtime&gt;&lt;atime&gt;1630553678890&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741833&lt;/id&gt;&lt;genstamp&gt;1009&lt;/genstamp&gt;&lt;numBytes&gt;316534&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16401&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job.split&lt;/name&gt;&lt;replication&gt;10&lt;/replication&gt;&lt;mtime&gt;1630553679334&lt;/mtime&gt;&lt;atime&gt;1630553679221&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741834&lt;/id&gt;&lt;genstamp&gt;1010&lt;/genstamp&gt;&lt;numBytes&gt;110&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16402&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job.splitmetainfo&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630553679443&lt;/mtime&gt;&lt;atime&gt;1630553679339&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741835&lt;/id&gt;&lt;genstamp&gt;1011&lt;/genstamp&gt;&lt;numBytes&gt;43&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16403&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job.xml&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630553679670&lt;/mtime&gt;&lt;atime&gt;1630553679452&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741836&lt;/id&gt;&lt;genstamp&gt;1012&lt;/genstamp&gt;&lt;numBytes&gt;192215&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16409&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;history&lt;/name&gt;&lt;mtime&gt;1630567073024&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0755&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16410&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;done_intermediate&lt;/name&gt;&lt;mtime&gt;1630554923873&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:1777&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16411&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;xisun&lt;/name&gt;&lt;mtime&gt;1630571854446&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16418&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0003-1630554915644-xisun-word+count-1630554943250-0-0-FAILED-default-1630554928660.jhist&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630554943458&lt;/mtime&gt;&lt;atime&gt;1630554943411&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741844&lt;/id&gt;&lt;genstamp&gt;1020&lt;/genstamp&gt;&lt;numBytes&gt;14969&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16419&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0003_conf.xml&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630554943540&lt;/mtime&gt;&lt;atime&gt;1630554943476&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741845&lt;/id&gt;&lt;genstamp&gt;1021&lt;/genstamp&gt;&lt;numBytes&gt;222769&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16437&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0004-1630559739775-xisun-word+count-1630559924980-1-1-SUCCEEDED-default-1630559746457.jhist&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630559927155&lt;/mtime&gt;&lt;atime&gt;1630559927104&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741854&lt;/id&gt;&lt;genstamp&gt;1030&lt;/genstamp&gt;&lt;numBytes&gt;22803&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16438&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0004_conf.xml&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630559927279&lt;/mtime&gt;&lt;atime&gt;1630559927177&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741855&lt;/id&gt;&lt;genstamp&gt;1031&lt;/genstamp&gt;&lt;numBytes&gt;223439&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16456&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0005-1630562908651-xisun-word+count-1630563047214-1-1-SUCCEEDED-default-1630562921426.jhist&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630563046601&lt;/mtime&gt;&lt;atime&gt;1630563046543&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741864&lt;/id&gt;&lt;genstamp&gt;1040&lt;/genstamp&gt;&lt;numBytes&gt;22770&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16457&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0005_conf.xml&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630563046693&lt;/mtime&gt;&lt;atime&gt;1630563046621&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741865&lt;/id&gt;&lt;genstamp&gt;1041&lt;/genstamp&gt;&lt;numBytes&gt;223439&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16458&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;done&lt;/name&gt;&lt;mtime&gt;1630567231439&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16476&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0006-1630567142285-xisun-word+count-1630567220324-1-1-SUCCEEDED-default-1630567156915.jhist&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630567219772&lt;/mtime&gt;&lt;atime&gt;1630567219669&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741874&lt;/id&gt;&lt;genstamp&gt;1050&lt;/genstamp&gt;&lt;numBytes&gt;22792&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16477&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0006_conf.xml&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630567219913&lt;/mtime&gt;&lt;atime&gt;1630567219802&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741875&lt;/id&gt;&lt;genstamp&gt;1051&lt;/genstamp&gt;&lt;numBytes&gt;223437&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16478&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;2021&lt;/name&gt;&lt;mtime&gt;1630567231440&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16479&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;09&lt;/name&gt;&lt;mtime&gt;1630567231440&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16480&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;02&lt;/name&gt;&lt;mtime&gt;1630567231441&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16481&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;000000&lt;/name&gt;&lt;mtime&gt;1630571854446&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16487&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;logs&lt;/name&gt;&lt;mtime&gt;1630571710042&lt;/mtime&gt;&lt;permission&gt;xisun:xisun:1777&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16488&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;xisun&lt;/name&gt;&lt;mtime&gt;1630571710049&lt;/mtime&gt;&lt;permission&gt;xisun:xisun:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16489&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;logs-tfile&lt;/name&gt;&lt;mtime&gt;1630571710069&lt;/mtime&gt;&lt;permission&gt;xisun:xisun:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16490&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;application_1630509320297_0007&lt;/name&gt;&lt;mtime&gt;1630571783775&lt;/mtime&gt;&lt;permission&gt;xisun:xisun:0770&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16492&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;wcoutput&lt;/name&gt;&lt;mtime&gt;1630571775649&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0755&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16498&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;part-r-00000&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630571775482&lt;/mtime&gt;&lt;atime&gt;1630571775197&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741882&lt;/id&gt;&lt;genstamp&gt;1058&lt;/genstamp&gt;&lt;numBytes&gt;36&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16500&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;_SUCCESS&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630571775653&lt;/mtime&gt;&lt;atime&gt;1630571775649&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16503&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0007-1630571709613-xisun-word+count-1630571776516-1-1-SUCCEEDED-default-1630571719711.jhist&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630571775936&lt;/mtime&gt;&lt;atime&gt;1630656192752&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741884&lt;/id&gt;&lt;genstamp&gt;1060&lt;/genstamp&gt;&lt;numBytes&gt;22765&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16504&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;job_1630509320297_0007_conf.xml&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630571776037&lt;/mtime&gt;&lt;atime&gt;1630571775955&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0770&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741885&lt;/id&gt;&lt;genstamp&gt;1061&lt;/genstamp&gt;&lt;numBytes&gt;223615&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16505&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;hadoop103_43362&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630571783767&lt;/mtime&gt;&lt;atime&gt;1630571783609&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:xisun:0640&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741886&lt;/id&gt;&lt;genstamp&gt;1062&lt;/genstamp&gt;&lt;numBytes&gt;136973&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16508&lt;/id&gt;&lt;type&gt;DIRECTORY&lt;/type&gt;&lt;name&gt;sanguoyanyi&lt;/name&gt;&lt;mtime&gt;1630917975333&lt;/mtime&gt;&lt;permission&gt;xisun:supergroup:0755&lt;/permission&gt;&lt;nsquota&gt;-1&lt;/nsquota&gt;&lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16509&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;shuguo.txt&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630918100557&lt;/mtime&gt;&lt;atime&gt;1630917608900&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741888&lt;/id&gt;&lt;genstamp&gt;1067&lt;/genstamp&gt;&lt;numBytes&gt;14&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16510&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;weiguo.txt&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630917830657&lt;/mtime&gt;&lt;atime&gt;1630917830222&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741889&lt;/id&gt;&lt;genstamp&gt;1065&lt;/genstamp&gt;&lt;numBytes&gt;7&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;inode&gt;&lt;id&gt;16511&lt;/id&gt;&lt;type&gt;FILE&lt;/type&gt;&lt;name&gt;wuguo.txt&lt;/name&gt;&lt;replication&gt;3&lt;/replication&gt;&lt;mtime&gt;1630917975305&lt;/mtime&gt;&lt;atime&gt;1630917974782&lt;/atime&gt;&lt;preferredBlockSize&gt;134217728&lt;/preferredBlockSize&gt;&lt;permission&gt;xisun:supergroup:0644&lt;/permission&gt;&lt;blocks&gt;&lt;block&gt;&lt;id&gt;1073741890&lt;/id&gt;&lt;genstamp&gt;1066&lt;/genstamp&gt;&lt;numBytes&gt;6&lt;/numBytes&gt;&lt;/block&gt;&lt;/blocks&gt;&lt;storagePolicyId&gt;0&lt;/storagePolicyId&gt;&lt;/inode&gt;&lt;/INodeSection&gt;&lt;INodeReferenceSection&gt;&lt;/INodeReferenceSection&gt;&lt;SnapshotSection&gt;&lt;snapshotCounter&gt;0&lt;/snapshotCounter&gt;&lt;numSnapshots&gt;0&lt;/numSnapshots&gt;&lt;/SnapshotSection&gt;&lt;INodeDirectorySection&gt;&lt;directory&gt;&lt;parent&gt;16385&lt;/parent&gt;&lt;child&gt;16508&lt;/child&gt;&lt;child&gt;16389&lt;/child&gt;&lt;child&gt;16386&lt;/child&gt;&lt;child&gt;16492&lt;/child&gt;&lt;child&gt;16387&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16389&lt;/parent&gt;&lt;child&gt;16390&lt;/child&gt;&lt;child&gt;16487&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16390&lt;/parent&gt;&lt;child&gt;16391&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16391&lt;/parent&gt;&lt;child&gt;16409&lt;/child&gt;&lt;child&gt;16392&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16392&lt;/parent&gt;&lt;child&gt;16393&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16393&lt;/parent&gt;&lt;child&gt;16394&lt;/child&gt;&lt;child&gt;16399&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16394&lt;/parent&gt;&lt;child&gt;16395&lt;/child&gt;&lt;child&gt;16396&lt;/child&gt;&lt;child&gt;16397&lt;/child&gt;&lt;child&gt;16398&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16399&lt;/parent&gt;&lt;child&gt;16400&lt;/child&gt;&lt;child&gt;16401&lt;/child&gt;&lt;child&gt;16402&lt;/child&gt;&lt;child&gt;16403&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16409&lt;/parent&gt;&lt;child&gt;16458&lt;/child&gt;&lt;child&gt;16410&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16410&lt;/parent&gt;&lt;child&gt;16411&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16458&lt;/parent&gt;&lt;child&gt;16478&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16478&lt;/parent&gt;&lt;child&gt;16479&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16479&lt;/parent&gt;&lt;child&gt;16480&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16480&lt;/parent&gt;&lt;child&gt;16481&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16481&lt;/parent&gt;&lt;child&gt;16418&lt;/child&gt;&lt;child&gt;16419&lt;/child&gt;&lt;child&gt;16437&lt;/child&gt;&lt;child&gt;16438&lt;/child&gt;&lt;child&gt;16456&lt;/child&gt;&lt;child&gt;16457&lt;/child&gt;&lt;child&gt;16476&lt;/child&gt;&lt;child&gt;16477&lt;/child&gt;&lt;child&gt;16503&lt;/child&gt;&lt;child&gt;16504&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16487&lt;/parent&gt;&lt;child&gt;16488&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16488&lt;/parent&gt;&lt;child&gt;16489&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16489&lt;/parent&gt;&lt;child&gt;16490&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16490&lt;/parent&gt;&lt;child&gt;16505&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16492&lt;/parent&gt;&lt;child&gt;16500&lt;/child&gt;&lt;child&gt;16498&lt;/child&gt;&lt;/directory&gt;&lt;directory&gt;&lt;parent&gt;16508&lt;/parent&gt;&lt;child&gt;16509&lt;/child&gt;&lt;child&gt;16510&lt;/child&gt;&lt;child&gt;16511&lt;/child&gt;&lt;/directory&gt;&lt;/INodeDirectorySection&gt;&lt;FileUnderConstructionSection&gt;&lt;/FileUnderConstructionSection&gt;&lt;SecretManagerSection&gt;&lt;currentId&gt;0&lt;/currentId&gt;&lt;tokenSequenceNumber&gt;0&lt;/tokenSequenceNumber&gt;&lt;numDelegationKeys&gt;0&lt;/numDelegationKeys&gt;&lt;numTokens&gt;0&lt;/numTokens&gt;&lt;/SecretManagerSection&gt;&lt;CacheManagerSection&gt;&lt;nextDirectiveId&gt;1&lt;/nextDirectiveId&gt;&lt;numDirectives&gt;0&lt;/numDirectives&gt;&lt;numPools&gt;0&lt;/numPools&gt;&lt;/CacheManagerSection&gt;&lt;/fsimage&gt; Fsimage 文件中保存了 HDFS 上存储的文件的信息，包括父节点和子节点的关系。 Fsimage 文件中没有记录 Block 块所对应的 DataNode 信息，这是因为，在集群启动后，会要求 DataNode 上报数据块信息，并间隔一段时间后再次上报。 oev 查看 Edits 文件 基本语法 1hdfs oev -p 文件类型 -i 编辑日志名称 -o 转换后文件的输出路径 实例 12345678910111213141516[xisun@hadoop102 ~]$ cd /opt/module/hadoop-3.2.1/data/dfs/name/current[xisun@hadoop102 current]$ hdfs oev -p XML -i edits_inprogress_0000000000000000936 -o ~/edits.XML[xisun@hadoop102 current]$ cd ~[xisun@hadoop102 ~]$ ll总用量 24drwxrwxr-x. 2 xisun xisun 52 9月 2 23:15 bin-rw-rw-r--. 1 xisun xisun 221 9月 8 16:46 edits.XML-rw-rw-r--. 1 xisun xisun 19241 9月 8 16:35 fsimage.XMLdrwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 公共drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 模板drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 视频drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 图片drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 文档drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 下载drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 音乐drwxr-xr-x. 2 xisun xisun 6 8月 28 23:56 桌面 下载文件并查看 1[xisun@hadoop102 current]$ sz ~/edits.xml 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;&lt;EDITS&gt; &lt;EDITS_VERSION&gt;-65&lt;/EDITS_VERSION&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;936&lt;/TXID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_DELETE&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;937&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;PATH&gt;/sanguoyanyi&lt;/PATH&gt; &lt;TIMESTAMP&gt;1631092478280&lt;/TIMESTAMP&gt; &lt;RPC_CLIENTID&gt;db87d83d-dba1-4362-add1-ff9cbdbceea0&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;3&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt;&lt;/EDITS&gt; NameNode 如何确定下次开机启动的时候合并哪些 Edits：假设当前 fsimage 最大编号为 935，则会将所有编号大于 935 的 Edits 合并。 CheckPoint 时间设置 CheckPoint 是指 SecondaryNameNode 向 nn 执行的操作。 通常情况下，SecondaryNameNode 每隔一小时执行一次。间隔时间在 hdfs-default.xml 文件中进行配置： 123456789&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600s&lt;/value&gt; &lt;description&gt; The number of seconds between two periodic checkpoints. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval. &lt;/description&gt;&lt;/property&gt; 另外，SecondaryNameNode 每隔一分钟检查一次 NameNode 的操作次数，当操作次数达到 1 百万时，SecondaryNameNode 执行一次 CheckPoint。间隔时间和操作次数在 hdfs-default.xml 文件中进行配置： 123456789101112131415161718&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every &#x27;dfs.namenode.checkpoint.txns&#x27; transactions, regardless of whether &#x27;dfs.namenode.checkpoint.period&#x27; has expired. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60s&lt;/value&gt; &lt;description&gt;The SecondaryNameNode and CheckpointNode will poll the NameNode every &#x27;dfs.namenode.checkpoint.check.period&#x27; seconds to query the number of uncheckpointed transactions. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval. &lt;/description&gt;&lt;/property&gt; DataNodeDataNode 工作机制 一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据，包括数据块的长度，块数据的校验和，以及时间戳。 12345[xisun@hadoop102 ~]$ cd /opt/module/hadoop-3.2.1/data/dfs/data/current/BP-288566776-192.168.10.102-1630507194979/current/finalized/subdir0/subdir0[xisun@hadoop102 subdir0]$ ll总用量 52-rw-rw-r--. 1 xisun xisun 41 9月 2 10:14 blk_1073741825-rw-rw-r--. 1 xisun xisun 11 9月 2 10:14 blk_1073741825_1001.meta .meta 文件存储的是元数据，另一个文件存储的是数据本身。 DataNode 启动后向 NameNode 注册，通过后，再周期性（6 小时）的向 NameNode 上报所有的块信息。 DataNode 扫描自己节点块信息列表的时间，默认 6 小时。在 hdfs-default.xml 文件中配置： 123456789&lt;property&gt; &lt;name&gt;dfs.datanode.directoryscan.interval&lt;/name&gt; &lt;value&gt;21600s&lt;/value&gt; &lt;description&gt;Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval. &lt;/description&gt;&lt;/property&gt; DataNode 向 NameNode 汇报当前节点块信息的时间间隔，默认 6 小时。在 hdfs-default.xml 文件中配置： 12345&lt;property&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; &lt;value&gt;21600000&lt;/value&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;&lt;/property&gt; 可以看出，DataNode 每隔 6 小时自查一次，自查完成后，立即向 NameNode 上报一次当前节点的信息。如果服务器中存在某些服务器性能差，可以设置自查和上报的时间间隔短一点。 心跳是每 3 秒一次，心跳返回结果带有 NameNode 给该 DataNode 的命令，比如复制块数据到另一台机器，或删除某个数据块。如果超过 10 分钟没有收到某个 DataNode 的心跳，则认为该节点不可用，NameNode 也就不会再和该 DataNode 有数据交互。 集群运行中可以安全加入和退出一些机器。 数据完整性 思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理，如果 DataNode 节点上的数据损坏了，却没有发现，也很危险，那么如何解决呢？ 如下是 DataNode 节点保证数据完整性的方法： 当 DataNode 读取 Block 的时候，它会计算 CheckSum。 如果计算后的 CheckSum，与 Block 创建时的值不一样，说明 Block 已经损坏。 然后，Client 读取其他 DataNode 上的 Block。 DataNode 会在其文件创建后周期验证 CheckSum。 常见的校验算法有： crc（32），md5（128），sha1（160）。 奇偶校验位方法，是检验算法中极其简单的一种。原始数据封装时，查看待传输数据中 1 的个数，如果为偶数，则在数据后添加一个校验位，值为 0，如果为奇数，则校验位值为 1。当接收到数据后，对数据进行重新计算，查看数据中 1 的个数，验证其是否与传输前的校验位相同。如果在数据传输过程中，导致数据发生了错误，接收到的数据校验位与原始数据的不同，则认为数据损坏，不再使用。 奇偶校验法，并不能保证一定检验出数据传输过程中可能出现的错误。crc 校验算法会更复杂，也更可靠，但原理上是相同的。 掉线时限参数设置 如果 DataNode 进程死亡或者网络故障，造成 DataNode 无法与 NameNode 通信； 此时，NameNode 不会立即把该 DataNode 节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。 HDFS 默认的超时时长为 10 分钟 + 30 秒。（实际上，这就是 DataNode 工作机制中，DataNode 向 NameNode 发送心跳的超时时间） 如果定义超时时间为 TimeOut，则超时时长的计算公式为： 1TimeOut = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval 在 hdfs-default.xml 文件中，dfs.namenode.heartbeat.recheck-interval 默认为 300000 毫秒（5 分钟），dfs.heartbeat.interval 默认为 3 秒。 12345678910111213141516171819202122&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;description&gt; This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3s&lt;/value&gt; &lt;description&gt; Determines datanode heartbeat interval in seconds. Can use the following suffix (case insensitive): ms(millis), s(sec), m(min), h(hour), d(day) to specify the time (such as 2s, 2m, 1h, etc.). Or provide complete number in seconds (such as 30 for 30 seconds). &lt;/description&gt;&lt;/property&gt; YARN 资源调度器 YARN 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序。 YARN 基础架构 YARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件构成。 ResourceManager（RM）的主要作用如下： 处理客户端请求。 监控 NodeManager。 启动或监控 ApplicationMaster。 资源的分配与调度。 NodeManager（NM）的主要作用如下： 管理单个节点上的资源。 处理来自 ResourceManager 的命令。 处理来自 ApplicationMaster 的命令。 ApplicationMaster（AM）的主要作用如下： 为应用程序申请资源并分配给内部的任务。 任务的监控与容错。 Container 的主要作用如下： Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。 YARN 工作机制 MapReduce 程序（任务分为 MapTask 和 ReduceTask）提交到客户端所在的节点，程序执行后创建一个 YarnRunner。 YarnRunner 向 ResourceManager 申请一个 Application。 ResourceManager 将该应用程序的资源提交路径返回给 YarnRunner。 该程序将运行所需资源（split 切片信息，xml 配置文件，JAR 包等）提交到 HDFS 上。 程序运行所需的资源提交到 HDFS 完毕后，再向 ResourceManager 申请运行 ApplicationMaster。 ResourceManager 将客户端（用户）的请求初始化成一个 Task。并将 Task 以及其他客户端的请求所初始化的 Task，都放到一个任务调度队列中（Apache Hadoop 默认使用容量调度器）。 集群中的一个空闲的 NodeManager，从任务调度队列中领取到 Task 任务。 然后，该 NodeManager 创建一个容器 Container，并启动一个 ApplicationMaster 进程。 Container 从 HDFS 上拷贝程序运行所需的资源到本地。 ApplicationMaster 进程拿到 Job 的切片信息之后，再向 ResourceManager 申请运行 MapTask 的资源（假设 MapTask 有两个任务）。 ResourceManager 将运行 MapTask 的任务分配给集群另外两个 NodeManager（也有可能是同一个 NodeManager 的两个 Container 上），这两个 NodeManager 分别领取任务并创建容器，并拷贝任务运行所需的 JAR 包等资源。 MapReduce 向两个接收到任务的 NodeManager 发送程序启动脚本，然后这两个 NodeManager 分别启动 MapTask，各自开启一个 YarnChild 子进程，之后，MapTask 开始执行，对数据分区排序。 ApplicationMaster 进程等待所有的 MapTask 运行完毕后，再向 ResourceManager 申请容器，运行 ReduceTask。这一步与运行 MapTask 的操作相同。 ReduceTask 启动时，向 MapTask 获取相应分区的数据，然后执行任务。 当程序运行完毕后，MapReduce 会向 ResourceManager 申请注销任务执行过程中所使用的资源。 作业提交全过程 HDFS、YARN、MapReduce 三者关系 作业提交过程之 YARN 作业提交过程之 HDFS &amp; MapReduce 作业提交全过程详解 作业提交： 第 1 步：Client 调用 job.waitForCompletion 方法，向整个集群提交 MR 作业。 第 2 步：Client 向 RM 申请一个作业 id。 第 3 步：RM 给 Client 返回该 Job 资源的 HDFS 提交路径和作业 id。 第 4 步：Client 提交 JAR 包、切片信息和配置文件到指定的 HDFS 资源提交路径。 第 5 步：Client 提交完资源后，向 RM 申请运行 AM。 作业初始化： 第 6 步：当 RM 收到 Client 的请求后，将该 Job 添加到容量调度器中。 第 7 步：某一个空闲的 NM 领取到该 Job。 第 8 步：该 NM 创建 Container，并启动 AM 进程。 第 9 步：该 Container 下载 Client 提交到 HDFS 的资源到本地。 任务分配： 第 10 步：AM 向 RM 申请执行多个 MapTask 任务的资源。 第 11 步：RM 将运行 MapTask 的任务分配给相应的 NM，这些 NM 分别领取任务并创建 Container。 任务运行： 第 12 步：MR 向接收到任务的 NM 发送程序启动脚本，这些 NM 分别启动 MapTask，MapTask 对数据分区排序。 第 13 步：AM 等待所有 MapTask 运行完毕后，再向 RM 申请容器，运行 ReduceTask。 第 14 步：ReduceTask 向 MapTask 获取相应分区的数据。 第 15 步：程序运行完毕后，MR 向 RM 申请注销自己。 进度和状态更新： YARN 中的任务将其进度和状态（包括 counter）返回给应用管理器, 客户端每秒（通过 mapreduce.client.progressmonitor.pollinterval 设置）向应用管理器请求进度更新，展示给用户。 作业完成： 除了向应用管理器请求作业进度外，客户端每 5 秒都会通过调用 waitForCompletion() 来检查作业是否完成。时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后，应用管理器和 Container 会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。 YARN 调度器和调度算法 目前，Hadoop 提供的任务队列调度器主要有三种：先进先出调度器（FIFO）、容量调度器（Capacity Scheduler）和公平调度器（Fair Scheduler）。 Apache Hadoop-3.2.1 默认的资源调度器是 Capacity Scheduler，在 yarn-default.xml 文件中，有具体的配置： 12345&lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; Cloudera Hadoop 默认使用的调度器是 Fair Scheduler。 先进先出调度器（FIFO） FIFO 调度器（First In First Out）：单队列，根据提交作业的先后顺序，先来的先服务。 优点：简单易懂。 缺点：不支持多队列，生产环境很少使用。 容量调度器（Capacity Scheduler） Capacity Scheduler 是 Yahoo 开发的多用户调度器。 容量调度器的特点 多队列：每个队列可配置一定的资源量，每个队列采用 FIFO 调度策略。 容量保证：管理员可为每个队列设置资源最低保证和资源使用上限。 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列（非强制回收）。 多租户：支持多用户共享集群和多应用程序同时运行。为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。比如，queueC 队列中，有 ss 和 cls 两个用户提交的作业，调度器可以限制每一个用户所占的资源量为自身资源的 50%。 容量调度器的队列的资源分配方式 FIFO 策略，默认。 具体的资源分配过程： 第一步：队列资源分配。 从 root 开始，使用深度优先算法，优先选择资源利用率最低（已使用的资源量/队列分配的资源容量）的队列分配资源。 第二步：作业资源分配。 默认按照提交作业的优先级和提交时间顺序分配资源。 第三步：容器资源分配。 按照容器的优先级分配资源。 如果容器的优先级相同，按照数据本地性原则（节点距离最小的）： 任务和数据在同一节点。 任务和数据在同一机架。 任务和数据不在同一节点也不在同一机架。 DRF 策略 容量调度器的资源分配算法实例 公平调度器（Fair Scheduler ） Fair Schedulere 是 Facebook 开发的多用户调度器。 公平调度器的特点 与容量调度器的相同点 多队列：支持多队列多作业。 容量保证：管理员可为每个队列设置资源最低保证和资源使用上限。 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。 多租户：支持多用户共享集群和多应用程序同时运行。为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。 与容量调度器的不同点 核心调度策略不同： 容量调度器：优先选择资源利用率低的队列。 公平调度器：优先选择对资源的缺额比例大的。 每个队列可以单独设置资源的分配方式不同： 容量调度器：FIFO、DRF。 公平调度器：FIFO、FAIR、DRF。 公平调度器的缺额概念 公平调度器设计目标是：在时间尺度上，所有作业获得公平的资源。在某一时刻，一个作业应获取的资源和它实际获取的资源的差距叫“缺额”。 公平调度器会优先为缺额大的作业分配资源。 公平调度器的队列的资源分配方式 FIFO 策略 公平调度器每个队列资源分配策略如果选择 FIFO 的话，此时公平调度器相当于上面讲过的容量调度器。 Fair 策略，默认。 Fair 策略是一种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到 1/2 的资源；如果三个应用程序同时运行，则每个应用程序可得到 1/3 的资源。 具体的资源分配过程和容量调度器一致： 选择队列 选择作业 选择容器 注意：以上三步，每一步都是按照公平策略来分配资源。 实际最小资源份额：mindshare = Min(资源需求量, 配置的最小资源) 是否饥饿：isNeedy = 资源使用量 &lt; mindshare 资源分配比：minShareRatio = 资源使用量 / Max（mindshare, 1) 资源使用权重比：useToWeightRatio = 资源使用量 / 权重 假设一个任务，其资源需求量为 4，配置的最小资源为 2，现在的资源使用量为 1，权重为 8。则 mindshare 为 2，因为资源使用量小于 mindshare，所以该任务处于饥饿状态。另外，其 minShareRatio 的值为 1/2，useToWeightRatio 值为 1/6。同理，计算出其他任务的这些状态值，按上图流程，决定资源优先分配到哪个任务。 DRF 策略 DRF（Dominant Resource Fairness），前面说的资源，都是单一标准，例如只考虑内存（也是 YARN 默认的情况）。但是很多时候资源有很多种，例如内存，CPU，网络带宽等，这样就很难衡量两个应用应该分配的资源比例。 在 YARN 中，用 DRF 来决定如何调度： 假设集群一共有 100 CPU 和 10 T 内存，而应用 A 需要（2 CPU，300 GB 内存），应用 B 需要（6 CPU，100 GB 内存）。则两个应用分别需要 A（2% CPU，3% 内存）和 B（6% CPU，1% 内存）的资源，这就意味着 A 是内存主导的，B 是 CPU 主导的，针对这种情况，我们可以选择 DRF 策略对不同应用进行不同资源（CPU 和内存）的一个不同比例的限制。 公平调度器的资源分配算法实例 队列资源分配 需求：集群总资源 100，有三个队列，对资源的需求分别是：queueA —&gt; 20，queueB —&gt; 50，queueC —&gt; 30。 第一次算：100 / 3 = 33.33，即每个队列应得资源 33.33。 queueA：分 33.33 —&gt; 多 13.33。 queueB：分 33.33 —&gt; 少 16.67。 queueC：分 33.33 —&gt; 多 3.33。 第二次算：(13.33 + 3.33) / 1 = 16.66，各队列空闲资源的总和，与需要资源的队列数的比值。 queueA：分 20。 queueB：分 33.33 + 16.66 = 50。 queueC：分 30。 作业资源分配 不加权（关注点是 job 的个数） 需求：有一条队列总资源 12 个，有 4 个 job，对资源的需求分别是：job1 —&gt; 1，job2 —&gt; 2，job3 —&gt; 6，job4 —&gt; 5。 第一次算：12 / 4 = 3。 job1：分 3 —&gt; 多 2 个。 job2：分 3 —&gt; 多 1 个。 job3：分 3 —&gt; 少 3 个。 job4：分 3 —&gt; 少 2 个。 第二次算：3 / 2 = 1.5 job1：分 1 —&gt; 最终：1。 job2：分 2 —&gt; 最终：2。 job3：分 3 —&gt; 少 3 个 —&gt; 分 1.5 —&gt; 最终：4.5。 job4：分 3 —&gt; 少 2 个 —&gt; 分 1.5 —&gt; 最终：4.5。 job3 和 job4 的资源都不够，继续等待，运行过程中，有资源释放出来时，继续按平均计算。 第 n 次算：一直算到没有空闲资源。 加权（ 关注点是 job 的权重） 需求：有一条队列总资源 16 个，有 4 个 job，对资源的需求分别是：job1 —&gt; 4，job2 —&gt; 2，job3 —&gt;10，job4 —&gt; 4，每个 job 的权重为：job1 —&gt; 5，job2 —&gt; 8，job3 —&gt; 1，job4 —&gt; 2。 第一次算：16 / (5 + 8 + 1 + 2) = 1。 job1：分 5 * 1 —&gt; 多 1 个。 job2：分 8 * 1 —&gt; 多 6 个。 job3：分 1 * 1 —&gt; 少 9 个。 job4：分 2 * 1 —&gt; 少 2 个。 第二次算：7 / (1 + 2) = 7/3。 job1：分 4。 job2：分 2。 job3：分 1 —&gt; 少 9 个 —&gt; 分 1 * 7/3 ≈ 2.33 —&gt; 少 6.67 个。 job4：分 2 —&gt; 少 2 个 —&gt; 分 2 * 7/3 ≈ 4.66 —&gt; 多 2.66 个。 第三次算：2.66 / 1 = 2.66。 job1：分 4 —&gt; 最终：4。 job2：分 2 —&gt; 最终：2。 job3：分 1 —&gt; 少 9 个 —&gt; 分 1 * 7/3 ≈ 2.33 —&gt; 少 6.67 个 —&gt; 分 1 * 2.66 —&gt; 最终：6。 job4：分 4 —&gt; 最终：4。 job3 资源不够，继续等待，运行过程中，有资源释放出来时，继续按平均计算。 第 n 次算：一直算到没有空闲资源。 YARN 常用命令 YARN 状态的查询，除了可以在 http://hadoop103:8088/ 页面查看外，还可以通过命令操作。 yarn node 查看节点 列出所有节点 1[xisun@hadoop102 hadoop]$ yarn node -list -all yarn queue 查看队列 查询队列信息 1[xisun@hadoop102 hadoop]$ yarn queue -status &lt;QueueName&gt; yarn application 查看任务 列出所有 Application 1[xisun@hadoop102 hadoop]$ yarn application -list 根据 Application 的状态查询 1[xisun@hadoop102 hadoop]$ yarn application -list -appStates [option] 状态参数可选项有：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED。 Kill Application 1[xisun@hadoop102 hadoop]$ yarn application -kill [ApplicationId] yarn container 查看容器 列出所有 Container 1[xisun@hadoop102 hadoop]$ yarn container -list &lt;ApplicationId&gt; 打印 Container 状态 1[xisun@hadoop102 hadoop]$ yarn container -status &lt;ContainerId&gt; 只有在任务运行的过程中，才能看到 Container 的状态。如果任务运行结束了，Container 也就被回收了。 yarn logs 查看日志 查询 Application 日志 1[xisun@hadoop102 hadoop]$ yarn logs -applicationId &lt;ApplicationId&gt; 查询 Container 日志 1[xisun@hadoop102 hadoop]$ yarn logs -applicationId &lt;ApplicationId&gt; -containerId &lt;ContainerId&gt; yarn applicationattempt 查看尝试运行的任务 列出所有 Application 尝试的列表 1[xisun@hadoop102 hadoop]$ yarn applicationattempt -list &lt;ApplicationId&gt; 打印 ApplicationAttemp 状态 1[xisun@hadoop102 hadoop]$ yarn applicationattempt -status &lt;ApplicationAttemptId&gt; yarn rmadmin 更新配置 更新队列配置 1[xisun@hadoop102 hadoop]$ yarn rmadmin -refreshQueues 如果修改了队列的相关信息，不需要重启 YARN，执行此命令即可生效。 YARN 生产环境核心参数 ResourceManager 相关 yarn.resourcemanager.scheduler.class：配置调度器，Apache Hadoop 默认使用容量调度器。 yarn.resourcemanager.scheduler.client.thread-count：ResourceManager 处理调度器请求的线程数量，默认 50。 NodeManager 相关 yarn.nodemanager.resource.detect-hardware-capabilities：是否让 YARN 自己检测硬件进行配置，默认 false。 yarn.nodemanager.resource.count-logical-processors-as-cores：是否将虚拟核数当作 CPU 核数，默认 false。 虚拟 CPU 和物理 CPU。如果集群中的多台服务器，CPU 性能差距较大，可以考虑设置此参数为 true。 yarn.nodemanager.resource.pcores-vcores-multiplier：虚拟核数和物理核数乘数，例如：4 核 8 线程，该参数就应设为 2。默认为 1.0。如果上一个参数设置为 true，此参数按实际配置。 yarn.nodemanager.resource.memory-mb：NodeManager 使用的内存，默认 8 G。 yarn.nodemanager.resource.system-reserved-memory-mb：NodeManager 为系统保留多少内存（系统内存与使用内存的差值）。与上一个参数，二者配置一个即可。 yarn.nodemanager.resource.cpu-vcores：NodeManager 使用的 CPU 核数，默认 8 个。 yarn.nodemanager.pmem-check-enabled：是否开启物理内存检查限制 Container，默认打开。 一种安全机制：如果 NodeManager 使用的内存，即将超过其可以使用的内存量时，触发报警机制，防止 NodeManager 占用虚拟机正常运行时的内存，导致系统崩溃。 yarn.nodemanager.vmem-check-enabled：是否开启虚拟内存检查限制 Container，默认打开。 yarn.nodemanager.vmem-pmem-ratio：虚拟内存与物理内存的比例，默认 2.1。 Container 相关 yarn.scheduler.minimum-allocation-mb：容器最小内存，默认 1 G。 yarn.scheduler.maximum-allocation-mb：容器最大内存，默认 8 G。不能超过 NodeManager 能使用的内存。 yarn.scheduler.minimum-allocation-vcores：容器最小 CPU 核数，默认 1 个。 yarn.scheduler.maximum-allocation-vcores：容器最大 CPU 核数，默认 4 个。 YARN 案例实操 调整下列参数之前尽量拍摄 Linux 快照，否则后续的案例，还需要重新准备集群。 YARN 生产环境核心参数配置案例 需求：从 1 G 数据中，统计每个单词出现的次数。服务器 3 台，每台配置 4 G 内存，4 核 CPU，4 线程。 本文参考https://www.bilibili.com/video/BV1Qp4y1n7EN 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://example.com/tags/hadoop/"}]},{"title":"movie","slug":"movie","date":"2021-08-25T07:19:54.000Z","updated":"2021-08-25T07:20:37.037Z","comments":true,"path":"2021/08/25/movie/","link":"","permalink":"http://example.com/2021/08/25/movie/","excerpt":"","text":"a那山那人那狗 岁月无声 夜莺","categories":[],"tags":[{"name":"movie","slug":"movie","permalink":"http://example.com/tags/movie/"}]},{"title":"记录一次 Flink 与 Kafka 结合的项目经验","slug":"flink-kafka","date":"2021-08-12T06:55:52.000Z","updated":"2021-08-27T05:17:29.834Z","comments":true,"path":"2021/08/12/flink-kafka/","link":"","permalink":"http://example.com/2021/08/12/flink-kafka/","excerpt":"","text":"项目主体结构 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class FlinkKafkaJob3 &#123; public static void main(String[] args) throws Exception &#123; // 获取参数 ParameterTool parameterTool = ParameterTool.fromArgs(args); String consumerTopic = parameterTool.get(&quot;consumerTopic&quot;); String producerTopic = parameterTool.get(&quot;producerTopic&quot;); String specialTopic = parameterTool.get(&quot;specialTopic&quot;); String ifEarliestOffset = parameterTool.get(&quot;ifEarliestOffset&quot;); String ifSpecialOffset = parameterTool.get(&quot;ifSpecialOffset&quot;); System.out.println(&quot;consumerTopic: &quot; + consumerTopic + &quot;, producerTopic: &quot; + producerTopic + &quot;, specialTopic: &quot; + specialTopic + &quot;, ifEarliestOffset: &quot; + ifEarliestOffset + &quot;, ifSpecialOffset: &quot; + ifSpecialOffset); // 创建流环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); /*env.enableCheckpointing(30 * 1000); env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); env.getCheckpointConfig().setCheckpointTimeout(60000L); env.getCheckpointConfig().setMinPauseBetweenCheckpoints(60000L); env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, Time.minutes(1)));*/ // 定义kafka消费者 Properties consumerProp = ConsumerProperties.getProps(); FlinkKafkaConsumer&lt;ConsumerRecord&lt;String, String&gt;&gt; consumer = new FlinkKafkaConsumer&lt;&gt;( consumerTopic, new ConsumerStringDeserializationSchema(), consumerProp); if (&quot;yes&quot;.equals(ifEarliestOffset)) &#123; consumer.setStartFromEarliest(); &#125; else if (&quot;yes&quot;.equals(ifSpecialOffset)) &#123; String partitions = parameterTool.get(&quot;partition&quot;); String offsets = parameterTool.get(&quot;offset&quot;); if (partitions != null &amp;&amp; offsets != null) &#123; Map&lt;KafkaTopicPartition, Long&gt; specificOffsets = new HashMap&lt;&gt;(16); String[] split = partitions.split(&quot;,&quot;); String[] split1 = offsets.split(&quot;,&quot;); for (int i = 0; i &lt; split.length; i++) &#123; KafkaTopicPartition kafkaTopicPartition = new KafkaTopicPartition(consumerTopic, Integer.parseInt(split[i])); specificOffsets.put(kafkaTopicPartition, Long.parseLong(split1[i])); &#125; consumer.setStartFromSpecificOffsets(specificOffsets); &#125; &#125; // 定义kafka生产者 Properties producerProp = ProducerProperties.getProps(); FlinkKafkaProducer&lt;String&gt; producer = new FlinkKafkaProducer&lt;&gt;( producerTopic, new ProducerStringSerializationSchema(producerTopic), producerProp, FlinkKafkaProducer.Semantic.EXACTLY_ONCE); producer.setWriteTimestampToKafka(true); // Flink任务链 env.addSource(consumer) .map((MapFunction&lt;ConsumerRecord&lt;String, String&gt;, PatentMessage&gt;) consumerRecord -&gt; &#123; String topic = consumerRecord.topic(); int partition = consumerRecord.partition(); long offset = consumerRecord.offset(); String key = consumerRecord.key(); String value = consumerRecord.value(); JSONObject kafkaMessage = new JSONObject(); kafkaMessage.put(&quot;topic&quot;, topic); kafkaMessage.put(&quot;partition&quot;, partition); kafkaMessage.put(&quot;offset&quot;, offset); kafkaMessage.put(&quot;key&quot;, key); return new PatentMessage(topic, partition, offset, key, value, null, kafkaMessage); &#125;) .filter(new FilterLargerOperator(specialTopic)) .map((MapFunction&lt;PatentMessage, PatentMessage&gt;) patentMessage -&gt; &#123; String value = patentMessage.getValue(); if (patentMessage.getKey().endsWith(PatentXmlTags.SGM)) &#123; value = PatentContentUtil.transferSgmPatentContent(value); &#125; Document document = Utils.buildXmlFromString(value); patentMessage.setDocument(document); return patentMessage; &#125;) .filter(new FilterBadOperator(specialTopic)) .flatMap(new RichFlatMapOperator(specialTopic)) // 有的时候，lambda表达式无法获悉返回值参数类型，需要指定，could not be determined automatically .returns(Types.STRING) .addSink(producer); // 执行任务 env.execute(&quot;patent reaction extractor loader&quot;); &#125;&#125; 打包命令：mvn clean package -DskipTests。 集群配置 主机地址： 1234567891011121314151617181920212223242526272829303132(base) [hadoop@client ~]$ ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:14:8b:4e brd ff:ff:ff:ff:ff:ff inet 192.168.2.106/24 brd 192.168.2.255 scope global noprefixroute dynamic ens192 valid_lft 40466sec preferred_lft 40466sec inet6 fe80::fda6:9e98:3fce:8b74/64 scope link noprefixroute valid_lft forever preferred_lft forever(base) [hadoop@client ~]$ ifconfigens192: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.2.106 netmask 255.255.255.0 broadcast 192.168.2.255 inet6 fe80::fda6:9e98:3fce:8b74 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:14:8b:4e txqueuelen 1000 (Ethernet) RX packets 2326369660 bytes 961932882940 (895.8 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1614468546 bytes 1124856260518 (1.0 TiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 34513 bytes 3083025 (2.9 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 34513 bytes 3083025 (2.9 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 主机名：hadoopclient。 Flink 版本： 123456789(base) [hadoop@client flink-1.11.1]$ pwd/opt/flink-1.11.1(base) [hadoop@client flink-1.11.1]$ flink -vSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/opt/flink-1.11.1/lib/logback-classic-1.2.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]Version: 1.11.1, Commit ID: 7eb514a Hadoop 版本： 12(base) [hadoop@client hadoop-3.2.1]$ pwd/opt/hadoop-3.2.1 Hadoop 节点： 123456789101112131415161718192021(base) [hadoop@client ~]$ cd .ssh/(base) [hadoop@client .ssh]$ pwd/home/hadoop/.ssh(base) [hadoop@client .ssh]$ lltotal 24-rw-------. 1 hadoop hadoop 5921 Sep 3 2020 authorized_keys-rw-------. 1 hadoop hadoop 1675 Sep 3 2020 id_rsa-rw-r--r--. 1 hadoop hadoop 395 Sep 3 2020 id_rsa.pub-rw-r--r--. 1 hadoop hadoop 7446 Sep 9 2020 known_hosts(base) [hadoop@client .ssh]$ cat known_hosts datanode1,192.168.2.188datanode3,192.168.2.143datanode2,192.168.2.199node1,192.168.1.61node2,192.168.1.62node3,192.168.1.63node4,192.168.1.64node5,192.168.1.65node6,192.168.1.66node7,192.168.1.67node8,192.168.1.68 known_hosts 文件内容有删改。 datanode1 ~ 3 主要为存储节点，node1 ~ 8 主要为计算节点。 主机与节点间的切换： 123456789101112131415161718192021222324(base) [hadoop@client ~]$ ssh node1Last login: Mon Aug 23 15:58:50 2021 from node2[hadoop@node1 ~]$ ll /opt/total 0drwxrwxr-x. 6 hadoop hadoop 99 Feb 28 2020 apache-maven-3.6.3drwxr-xr-x. 10 hadoop hadoop 156 Sep 3 2020 flink-1.11.1drwxr-xr-x. 12 hadoop hadoop 185 Sep 23 2020 hadoop-3.2.1drwxrwxr-x. 7 hadoop hadoop 146 Feb 13 2020 zookeeper-3.5.6[hadoop@node1 ~]$ ssh node6Last login: Mon Oct 19 14:19:59 2020 from node5[hadoop@node6 ~]$ ll /opt/total 0drwxrwxr-x. 6 hadoop hadoop 99 Aug 30 2020 apache-maven-3.6.3drwxr-xr-x. 10 hadoop hadoop 156 Sep 3 2020 flink-1.11.1drwxr-xr-x. 12 hadoop hadoop 185 Sep 23 2020 hadoop-3.2.1drwxrwxr-x. 7 hadoop hadoop 146 Aug 30 2020 zookeeper-3.5.6[hadoop@node6 ~]$ ssh hadoopclientLast login: Mon Aug 23 16:06:40 2021 from 192.168.1.1(base) [hadoop@client ~]$ ll /opt/total 0drwxrwxr-x. 6 hadoop hadoop 99 Aug 30 2020 apache-maven-3.6.3drwxr-xr-x. 10 hadoop hadoop 186 Aug 23 15:37 flink-1.11.1drwxr-xr-x. 12 hadoop hadoop 185 Sep 23 2020 hadoop-3.2.1drwxrwxr-x. 7 hadoop hadoop 146 Aug 30 2020 zookeeper-3.5.6 Kafka 操作命令 查看 Kafka topic 列表命令，返回 topic 名字列表： 1234567(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --list__consumer_offsets__transaction_stateextractor-logextractor-patentextractor-patent-exceptionextractor-result 创建 Kafka topic 命令： 12(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181,hadoopdatanode2:2181,hadoopdatanode3:2181 --create --partitions 12 --replication-factor 2 --topic extractor-patentCreated topic extractor-patent. 查看 Kafka 指定 topic 的详情命令，返回该 topic 的 partition 数量、replica 因子以及每个 partition 的 leader、replica 信息： 1234567891011121314(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --describe --topic extractor-patentTopic: extractor-patent PartitionCount: 12 ReplicationFactor: 2 Configs: Topic: extractor-patent Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1,2 Topic: extractor-patent Partition: 1 Leader: 2 Replicas: 2,0 Isr: 2,0 Topic: extractor-patent Partition: 2 Leader: 0 Replicas: 0,1 Isr: 0,1 Topic: extractor-patent Partition: 3 Leader: 1 Replicas: 1,0 Isr: 1,0 Topic: extractor-patent Partition: 4 Leader: 2 Replicas: 2,1 Isr: 2,1 Topic: extractor-patent Partition: 5 Leader: 0 Replicas: 0,2 Isr: 0,2 Topic: extractor-patent Partition: 6 Leader: 1 Replicas: 1,2 Isr: 1,2 Topic: extractor-patent Partition: 7 Leader: 2 Replicas: 2,0 Isr: 2,0 Topic: extractor-patent Partition: 8 Leader: 0 Replicas: 0,1 Isr: 0,1 Topic: extractor-patent Partition: 9 Leader: 1 Replicas: 1,0 Isr: 1,0 Topic: extractor-patent Partition: 10 Leader: 2 Replicas: 2,1 Isr: 2,1 Topic: extractor-patent Partition: 11 Leader: 0 Replicas: 0,2 Isr: 0,2 查看 Kafka 指定 topic 各 partition 的 offset 信息命令，–time 参数为 -1 时，表示各分区最大的 offset，为 -2 时，表示各分区最小的 offset： 12345678910111213(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list hadoopdatanode1:9092 --time -1 --topic extractor-patentextractor-patent:0:364004extractor-patent:1:364109extractor-patent:2:363695extractor-patent:3:364158extractor-patent:4:363723extractor-patent:5:363860extractor-patent:6:363821extractor-patent:7:365197extractor-patent:8:364364extractor-patent:9:364092extractor-patent:10:365772extractor-patent:11:364990 删除 Kafka topic 命令： 123(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --delete -topic extractor-patentsTopic extractor-patents is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true. 查看 Kafka consumer group 命令，返回 consumer group 名字列表 (新版信息保存在 broker 中，老版信息保存在 zookeeper 中，二者命令不同)： 12345(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --listextractor-patent-consumerresult-consumerlog-consumertimeout-consumer 老版命令：~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --zookeeper hadoopdatanode1:2181 --list。 查看 Kafka 指定 consumer group 的详情命令，返回 consumer group 对应的 topic 信息、当前消费的 offset、总 offset、剩余待消费 offset 等信息： 1234567891011121314151617(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --describe --group extractor-patent-consumerConsumer group &#x27;extractor-patent-consumer&#x27; has no active members.GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-IDextractor-patent-consumer extractor-patent 5 262567 363860 101293 - - -extractor-patent-consumer extractor-patent 6 267012 363821 96809 - - -extractor-patent-consumer extractor-patent 3 262809 364158 101349 - - -extractor-patent-consumer extractor-patent 4 262506 363723 101217 - - -extractor-patent-consumer extractor-patent 9 266548 364092 97544 - - -extractor-patent-consumer extractor-patent 10 266975 365772 98797 - - -extractor-patent-consumer extractor-patent 7 264665 365197 100532 - - -extractor-patent-consumer extractor-patent 8 270991 364364 93373 - - -extractor-patent-consumer extractor-patent 11 256111 364990 108879 - - -extractor-patent-consumer extractor-patent 1 267012 364109 97097 - - -extractor-patent-consumer extractor-patent 2 264837 363695 98858 - - -extractor-patent-consumer extractor-patent 0 262513 364004 101491 - - - 重置 Kafka 指定 consumer group 消费的 topic 的 offset 命令： 1(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --reset-offsets -to-offset 0 --execute --topic patent-app --group extractor-patent-consumer 删除 Kafka 指定 consumer group 命令： 1(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --delete --group extractor-patent-consumer 命令行消费 Kafka 指定 topic 的内容命令： 12# 从头开始消费(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --from-beginning --topic extractor-log &gt; extractor.log 12# 从头开始消费前10条消息，并显示key(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --from-beginning --max-messages 10 --property print.key=true --topic extractor-log 12# 从指定分区、指定offset开始消费(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --partition 0 --offset 219000 --topic extractor-log 12# 从尾开始消费，必须指定分区(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --partition 0 --offset latest --topic extractor-log 更多命令参数查看 help： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --helpThis tool helps to create, delete, describe, or change a topic.Option Description ------ ----------- --alter Alter the number of partitions, replica assignment, and/or configuration for the topic. --at-min-isr-partitions if set when describing topics, only show partitions whose isr count is equal to the configured minimum. Not supported with the --zookeeper option. --bootstrap-server &lt;String: server to REQUIRED: The Kafka server to connect connect to&gt; to. In case of providing this, a direct Zookeeper connection won&#x27;t be required. --command-config &lt;String: command Property file containing configs to be config property file&gt; passed to Admin Client. This is used only with --bootstrap-server option for describing and altering broker configs. --config &lt;String: name=value&gt; A topic configuration override for the topic being created or altered.The following is a list of valid configurations: cleanup.policy compression.type delete.retention.ms file.delete.delay.ms flush.messages flush.ms follower.replication.throttled. replicas index.interval.bytes leader.replication.throttled.replicas max.compaction.lag.ms max.message.bytes message.downconversion.enable message.format.version message.timestamp.difference.max.ms message.timestamp.type min.cleanable.dirty.ratio min.compaction.lag.ms min.insync.replicas preallocate retention.bytes retention.ms segment.bytes segment.index.bytes segment.jitter.ms segment.ms unclean.leader.election.enable See the Kafka documentation for full details on the topic configs.It is supported only in combination with -- create if --bootstrap-server option is used. --create Create a new topic. --delete Delete a topic --delete-config &lt;String: name&gt; A topic configuration override to be removed for an existing topic (see the list of configurations under the --config option). Not supported with the --bootstrap-server option. --describe List details for the given topics. --disable-rack-aware Disable rack aware replica assignment --exclude-internal exclude internal topics when running list or describe command. The internal topics will be listed by default --force Suppress console prompts --help Print usage information. --if-exists if set when altering or deleting or describing topics, the action will only execute if the topic exists. --if-not-exists if set when creating topics, the action will only execute if the topic does not already exist. --list List all available topics. --partitions &lt;Integer: # of partitions&gt; The number of partitions for the topic being created or altered (WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected). If not supplied for create, defaults to the cluster default. --replica-assignment &lt;String: A list of manual partition-to-broker broker_id_for_part1_replica1 : assignments for the topic being broker_id_for_part1_replica2 , created or altered. broker_id_for_part2_replica1 : broker_id_for_part2_replica2 , ...&gt; --replication-factor &lt;Integer: The replication factor for each replication factor&gt; partition in the topic being created. If not supplied, defaults to the cluster default. --topic &lt;String: topic&gt; The topic to create, alter, describe or delete. It also accepts a regular expression, except for --create option. Put topic name in double quotes and use the &#x27;\\&#x27; prefix to escape regular expression symbols; e. g. &quot;test\\.topic&quot;. --topics-with-overrides if set when describing topics, only show topics that have overridden configs --unavailable-partitions if set when describing topics, only show partitions whose leader is not available --under-min-isr-partitions if set when describing topics, only show partitions whose isr count is less than the configured minimum. Not supported with the --zookeeper option. --under-replicated-partitions if set when describing topics, only show under replicated partitions --version Display Kafka version. --zookeeper &lt;String: hosts&gt; DEPRECATED, The connection string for the zookeeper connection in the form host:port. Multiple hosts can be given to allow fail-over. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --helpMissing required argument &quot;[bootstrap-server]&quot;Option Description ------ ----------- --all-groups Apply to all consumer groups. --all-topics Consider all topics assigned to a group in the `reset-offsets` process.--bootstrap-server &lt;String: server to REQUIRED: The server(s) to connect to. connect to&gt; --by-duration &lt;String: duration&gt; Reset offsets to offset by duration from current timestamp. Format: &#x27;PnDTnHnMnS&#x27; --command-config &lt;String: command Property file containing configs to be config property file&gt; passed to Admin Client and Consumer. --delete Pass in groups to delete topic partition offsets and ownership information over the entire consumer group. For instance --group g1 -- group g2 --delete-offsets Delete offsets of consumer group. Supports one consumer group at the time, and multiple topics. --describe Describe consumer group and list offset lag (number of messages not yet processed) related to given group. --dry-run Only show results without executing changes on Consumer Groups. Supported operations: reset-offsets. --execute Execute operation. Supported operations: reset-offsets. --export Export operation execution to a CSV file. Supported operations: reset- offsets. --from-file &lt;String: path to CSV file&gt; Reset offsets to values defined in CSV file. --group &lt;String: consumer group&gt; The consumer group we wish to act on. --help Print usage information. --list List all consumer groups. --members Describe members of the group. This option may be used with &#x27;--describe&#x27; and &#x27;--bootstrap-server&#x27; options only. Example: --bootstrap-server localhost: 9092 --describe --group group1 -- members --offsets Describe the group and list all topic partitions in the group along with their offset lag. This is the default sub-action of and may be used with &#x27;--describe&#x27; and &#x27;-- bootstrap-server&#x27; options only. Example: --bootstrap-server localhost: 9092 --describe --group group1 -- offsets --reset-offsets Reset offsets of consumer group. Supports one consumer group at the time, and instances should be inactive Has 2 execution options: --dry-run (the default) to plan which offsets to reset, and --execute to update the offsets. Additionally, the -- export option is used to export the results to a CSV format. You must choose one of the following reset specifications: --to-datetime, --by-period, --to-earliest, --to- latest, --shift-by, --from-file, -- to-current. To define the scope use --all-topics or --topic. One scope must be specified unless you use &#x27;--from- file&#x27;. --shift-by &lt;Long: number-of-offsets&gt; Reset offsets shifting current offset by &#x27;n&#x27;, where &#x27;n&#x27; can be positive or negative. --state [String] When specified with &#x27;--describe&#x27;, includes the state of the group. Example: --bootstrap-server localhost: 9092 --describe --group group1 -- state When specified with &#x27;--list&#x27;, it displays the state of all groups. It can also be used to list groups with specific states. Example: --bootstrap-server localhost: 9092 --list --state stable,empty This option may be used with &#x27;-- describe&#x27;, &#x27;--list&#x27; and &#x27;--bootstrap- server&#x27; options only. --timeout &lt;Long: timeout (ms)&gt; The timeout that can be set for some use cases. For example, it can be used when describing the group to specify the maximum amount of time in milliseconds to wait before the group stabilizes (when the group is just created, or is going through some changes). (default: 5000) --to-current Reset offsets to current offset. --to-datetime &lt;String: datetime&gt; Reset offsets to offset from datetime. Format: &#x27;YYYY-MM-DDTHH:mm:SS.sss&#x27; --to-earliest Reset offsets to earliest offset. --to-latest Reset offsets to latest offset. --to-offset &lt;Long: offset&gt; Reset offsets to a specific offset. --topic &lt;String: topic&gt; The topic whose consumer group information should be deleted or topic whose should be included in the reset offset process. In `reset- offsets` case, partitions can be specified using this format: `topic1: 0,1,2`, where 0,1,2 are the partition to be included in the process. Reset-offsets also supports multiple topic inputs. --verbose Provide additional information, if any, when describing the group. This option may be used with &#x27;-- offsets&#x27;/&#x27;--members&#x27;/&#x27;--state&#x27; and &#x27;--bootstrap-server&#x27; options only. Example: --bootstrap-server localhost: 9092 --describe --group group1 -- members --verbose --version Display Kafka version. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576(base) [hadoop@client patent-extractor]$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --helpThis tool helps to read data from Kafka topics and outputs it to standard output.Option Description ------ ----------- --bootstrap-server &lt;String: server to REQUIRED: The server(s) to connect to. connect to&gt; --consumer-property &lt;String: A mechanism to pass user-defined consumer_prop&gt; properties in the form key=value to the consumer. --consumer.config &lt;String: config file&gt; Consumer config properties file. Note that [consumer-property] takes precedence over this config. --enable-systest-events Log lifecycle events of the consumer in addition to logging consumed messages. (This is specific for system tests.) --formatter &lt;String: class&gt; The name of a class to use for formatting kafka messages for display. (default: kafka.tools. DefaultMessageFormatter) --from-beginning If the consumer does not already have an established offset to consume from, start with the earliest message present in the log rather than the latest message. --group &lt;String: consumer group id&gt; The consumer group id of the consumer. --help Print usage information. --isolation-level &lt;String&gt; Set to read_committed in order to filter out transactional messages which are not committed. Set to read_uncommitted to read all messages. (default: read_uncommitted)--key-deserializer &lt;String: deserializer for key&gt; --max-messages &lt;Integer: num_messages&gt; The maximum number of messages to consume before exiting. If not set, consumption is continual. --offset &lt;String: consume offset&gt; The offset id to consume from (a non- negative number), or &#x27;earliest&#x27; which means from beginning, or &#x27;latest&#x27; which means from end (default: latest) --partition &lt;Integer: partition&gt; The partition to consume from. Consumption starts from the end of the partition unless &#x27;--offset&#x27; is specified. --property &lt;String: prop&gt; The properties to initialize the message formatter. Default properties include: print.timestamp=true|false print.key=true|false print.value=true|false key.separator=&lt;key.separator&gt; line.separator=&lt;line.separator&gt; key.deserializer=&lt;key.deserializer&gt; value.deserializer=&lt;value. deserializer&gt; Users can also pass in customized properties for their formatter; more specifically, users can pass in properties keyed with &#x27;key. deserializer.&#x27; and &#x27;value. deserializer.&#x27; prefixes to configure their deserializers. --skip-message-on-error If there is an error when processing a message, skip it instead of halt. --timeout-ms &lt;Integer: timeout_ms&gt; If specified, exit if no message is available for consumption for the specified interval. --topic &lt;String: topic&gt; The topic id to consume on. --value-deserializer &lt;String: deserializer for values&gt; --version Display Kafka version. --whitelist &lt;String: whitelist&gt; Regular expression specifying whitelist of topics to include for consumption. Flink 任务提交 提交任务所需的 jar 包，放置于主机上： 12345(base) [hadoop@client patent-extractor]$ pwd/data/patent/official/patent-extractor(base) [hadoop@client patent-extractor]$ lltotal 349140-rw-rw-r-- 1 hadoop hadoop 357510953 Aug 20 12:01 reaction-extractor-1.0-SNAPSHOT.jar 提交任务的模式：yarn-cluster。 提交任务的命令： 1flink run -m yarn-cluster -p 12 -d reaction-extractor-1.0-SNAPSHOT.jar --consumerTopic extractor-patent --producerTopic extractor-result --specialTopic extractor-patent-exception 1flink run -m yarn-cluster -p 12 -d reaction-extractor-1.0-SNAPSHOT.jar --consumerTopic extractor-patent --producerTopic extractor-result --specialTopic extractor-patent-timeout --ifEarliestOffset yes 12flink run -m yarn-cluster -p 12 -d reaction-extractor-1.0-SNAPSHOT.jar --consumerTopic extractor-patent --producerTopic extractor-result --specialTopic extractor-patent-timeout --ifSpecialOffset yes --partition 0,1,2,3,4,5,6,7,8,9,10,11 --offset 273413,277780,268447,274226,273855,273795,278069,275783,284768,277734,281195,265928 YARN 查询命令 查看yarn上面的资源使用情况命令，ctrl+c退出： 12345678910(base) [hadoop@client official]$ yarn topYARN top - 22:52:46, up 266d, 4:43, 0 active users, queue(s): rootNodeManager(s): 11 total, 11 active, 0 unhealthy, 0 decommissioned, 0 lost, 0 rebootedQueue(s) Applications: 1 running, 167 submitted, 0 pending, 39 completed, 127 killed, 0 failedQueue(s) Mem(GB): 382 available, 146 allocated, 0 pending, 0 reservedQueue(s) VCores: 163 available, 13 allocated, 0 pending, 0 reservedQueue(s) Containers: 13 allocated, 0 pending, 0 reserved APPLICATIONID USER TYPE QUEUE PRIOR #CONT #RCONT VCORES RVCORES MEM RMEM VCORESECS MEMSECS %PROGR TIME NAME application_1606730935892_0168 hadoop apache flink default 0 13 0 13 0 146G 0G 68363 767752 100.00 00:01:27 Flink per-job cluster 查看 YARN 上运行的任务列表命令，如果集群有 krb 认证的话，需要先 kinit，认证后可以看到所有正在运行的任务： 1234(base) [hadoop@client official]$ yarn application -listTotal number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1 Application-Id Application-Name Application-Type User Queue State Final-State Progress Tracking-URLapplication_1606730935892_0168 Flink per-job cluster Apache Flink hadoop default RUNNING UNDEFINED 100% http://node3:36312 查看 YARN 上运行的指定状态的任务列表命令： 1234(base) [hadoop@client official]$ yarn application -list -appStates RUNNINGTotal number of applications (application-types: [], states: [RUNNING] and tags: []):1 Application-Id Application-Name Application-Type User Queue State Final-State Progress Tracking-URLapplication_1606730935892_0168 Flink per-job cluster Apache Flink hadoop default RUNNING UNDEFINED 100% http://node3:36312 查看 YARN 指定任务的状态信息命令： 1yarn application -status &lt;applicationId&gt; 123456789101112131415161718192021222324(base) [hadoop@client official]$ yarn application -status application_1606730935892_0168Application Report : Application-Id : application_1606730935892_0168 Application-Name : Flink per-job cluster Application-Type : Apache Flink User : hadoop Queue : default Application Priority : 0 Start-Time : 1629725098333 Finish-Time : 0 Progress : 100% State : RUNNING Final-State : UNDEFINED Tracking-URL : http://node3:36312 RPC Port : 36312 AM Host : node3 Aggregate Resource Allocation : 829449970 MB-seconds, 72126 vcore-seconds Aggregate Resource Preempted : 0 MB-seconds, 0 vcore-seconds Log Aggregation Status : NOT_START Diagnostics : Unmanaged Application : false Application Node Label Expression : &lt;Not set&gt; AM container Node Label Expression : &lt;DEFAULT_PARTITION&gt; TimeoutType : LIFETIME ExpiryTime : UNLIMITED RemainingTime : -1seconds 查看 YARN 指定 application 任务日志命令，可以选择输出到本地文件： 1yarn logs -applicationId &lt;applicationId&gt; &gt; yarn.log kill yarn application 命令： 1yarn application -kill &lt;applicationId&gt; kill yarn job 命令： 1yarn job -kill &lt;jobId&gt;","categories":[],"tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}]},{"title":"MySQL 入门","slug":"db-mysql","date":"2021-06-28T02:36:17.000Z","updated":"2021-07-05T06:56:02.410Z","comments":true,"path":"2021/06/28/db-mysql/","link":"","permalink":"http://example.com/2021/06/28/db-mysql/","excerpt":"","text":"MySQL 安装Windows 上安装 MySQL 下载： 地址：https://dev.mysql.com/downloads/ 安装： 产品选择： Developer Default：开发者默认，安装 MySQL 开发所需的所有产品。 Server only：服务器，只安装 MySQL 服务器产品。 Client only：客户端，只安装没有服务器的 MySQL 客户端产品。 Full：完全，安装所有包含的 MySQL 产品和功能。 Custom：手动，手动选择系统上应安装的产品。 跳过 Check Requirements： 进度，安装需要一段时间： 待配置项，接下来会一项接一项的进行配置。当一项配置完后，会回到此页面，并显示该项 Configuration Complete，然后进行下一项： 端口： root 用户密码 (此密码不能忘)： 可以在本页面添加其他用户，并赋予权限和登录密码。 配置 MySQL 在 Windows 系统中的名字，以及是否选择开机启动 MySQL 服务： 应用配置： 安装日志，可以查看端口和安装位置等信息： 安装完成： 双击运行之前下载的安装包，能看到所安装的产品。 添加环境变量： 新建 MASQL_HOME： 添加 path： 测试： 123456789101112131415C:\\Users\\XiSun&gt;mysql -u root -pEnter password: ***************Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 30Server version: 8.0.25 MySQL Community Server - GPLCopyright (c) 2000, 2021, Oracle and/or its affiliates.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; 本文参考https://www.runoob.com/mysql/mysql-install.html https://www.runoob.com/w3cnote/windows10-mysql-installer.html 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://example.com/tags/database/"}]},{"title":"JDBC 入门","slug":"db-jdbc","date":"2021-06-27T05:53:15.000Z","updated":"2021-07-06T06:32:32.718Z","comments":true,"path":"2021/06/27/db-jdbc/","link":"","permalink":"http://example.com/2021/06/27/db-jdbc/","excerpt":"","text":"JDBC 概述数据的持久化 持久化 (persistence)：把数据保存到可掉电式存储设备中以供之后使用。大多数情况下，特别是企业级应用，数据持久化意味着将内存中的数据保存到硬盘上加以 “固化”，而持久化的实现过程大多通过各种关系数据库来完成。 持久化的主要应用是将内存中的数据存储在关系型数据库中，当然也可以存储在磁盘文件、XML 数据文件中。 Java 中的数据存储技术 在 Java 中，数据库存取技术可分为如下几类： JDBC 直接访问数据库。 JDO (Java Data Object ) 技术。 第三方 O/R 工具，如 Hibernate，Mybatis 等。 JDBC 是 Java 访问数据库的基石，JDO、Hibernate、MyBatis 等只是更好的封装了 JDBC。 JDBC 介绍 JDBC (Java Database Connectivity) 是一个独立于特定数据库管理系统、通用的 SQL 数据库存取和操作的公共接口 (一组 API)，定义了用来访问数据库的标准 Java 类库，然后 java.sql 和 javax.sql 这些 Java 的 API，可以使用这些类库以一种标准的方法、方便地访问数据库资源。 JDBC 为访问不同的数据库提供了一种统一的途径，为开发者屏蔽了一些细节问题。 JDBC 的目标是使 Java 程序员使用 JDBC 可以连接任何提供了 JDBC 驱动程序的数据库系统，这样就使得程序员无需对特定的数据库系统的特点有过多的了解，从而大大简化和加快了开发过程。 如果没有 JDBC，那么 Java 程序访问数据库时是这样的： 有了 JDBC，Java 程序访问数据库时是这样的： 总结如下： JDBC 体系结构 JDBC 接口 (API) 包括两个层次： 面向应用的 API：Java API，抽象接口，供应用程序开发人员使用 (连接数据库，执行 SQL 语句，获得结果)。 面向数据库的 API：Java Driver API，供开发商开发数据库驱动程序用。 面向接口编程： JDBC 是 sun 公司提供一套用于数据库操作的接口，Java 程序员只需要面向这套接口编程即可。 不同的数据库厂商，需要针对这套接口，提供不同实现。不同的实现的集合，即为不同数据库的驱动。 JDBC 程序编写步骤 ODBC (Open Database Connectivity，开放式数据库连接)，是微软在 Windows 平台下推出的。使用者在程序中只需要调用 ODBC API，由 ODBC 驱动程序将调用转换成为对特定的数据库的调用请求。 获取数据库连接要素一：Driver 驱动 java.sql.Driver 接口是所有 JDBC 驱动程序需要实现的接口。这个接口是提供给数据库厂商使用的，不同数据库厂商提供不同的实现。 在程序中不需要直接去访问实现了 Driver 接口的类，而是由驱动程序管理器类 (java.sql.DriverManager) 去调用这些 Driver 实现。 第一步：Maven 添加驱动依赖。 MySQL： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.25&lt;/version&gt;&lt;/dependency&gt; PostgreSQL： 12345&lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;42.2.10&lt;/version&gt;&lt;/dependency&gt; 第二步：加载驱动。加载 JDBC 驱动需调用 Class 类的静态方法 forName()，并向其传递要加载的 JDBC 驱动的类名。 MySQL： 1Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;); com.mysql.jdbc.Driver 已被舍弃。 PostgreSQL： 1Class.forName(&quot;org.postgresql.Driver&quot;); 第三步：注册驱动。DriverManager 类是驱动程序管理器类，负责管理驱动程序。 DriverManager 类使用 registerDriver() 注册驱动。 通常不用显式调用 DriverManager 类的 registerDriver() 来注册驱动程序类的实例，因为 Driver 接口的驱动程序类 (即实现类) 都包含了一个静态代码块，在这个静态代码块中，会调用 DriverManager 类 的 registerDriver() 来注册自身的一个实例。下图是 MySQL 的 Driver 实现类的源码： 1234567891011121314151617package com.mysql.cj.jdbc;import java.sql.DriverManager;import java.sql.SQLException;public class Driver extends NonRegisteringDriver implements java.sql.Driver &#123; public Driver() throws SQLException &#123; &#125; static &#123; try &#123; DriverManager.registerDriver(new Driver()); &#125; catch (SQLException var1) &#123; throw new RuntimeException(&quot;Can&#x27;t register driver!&quot;); &#125; &#125;&#125; 要素二：URL JDBC URL 用于标识一个被注册的驱动程序，驱动程序管理器通过这个 URL 选择正确的驱动程序，从而建立到数据库的连接。 JDBC URL 的标准由三部分组成，各部分间用冒号分隔。 格式：协议:子协议:子名称 协议：JDBC URL 中的协议总是 jdbc。 子协议：子协议用于标识一个数据库驱动程序。 子名称：一种标识数据库的方法。子名称可以依不同的子协议而变化，用子名称的目的是为定位数据库提供足够的信息。包含主机名 (对应服务端的 ip 地址)，端口号和数据库名。 举例： 几种常用数据库的 JDBC URL MySQL 的连接 URL 的编写方式： jdbc:mysql://主机名称:mysql服务端口号/数据库名称?参数=值&amp;参数=值 jdbc:mysql://localhost:3306/atguigu jdbc:mysql://localhost:3306/atguigu?useUnicode=true&amp;characterEncoding=utf8 如果 JDBC 程序与服务器端的字符集不一致，会导致乱码，此时，可以通过参数指定服务器端的字符集。 jdbc:mysql://localhost:3306/atguigu?user=root&amp;password=123456 Oracle 9i 的连接 URL 的编写方式： jdbc:oracle:thin:@主机名称:oracle服务端口号:数据库名称 jdbc:oracle:thin:@localhost:1521:atguigu SQLServer 的连接 URL 的编写方式： jdbc:sqlserver://主机名称:sqlserver服务端口号:DatabaseName=数据库名称 jdbc:sqlserver://localhost:1433:DatabaseName=atguigu 要素三：用户名和密码 user 和 password，可以用 属性名=属性值 的方式告诉数据库。 可以调用 DriverManager 类的 getConnection() 方法建立到数据库的连接。 数据库连接方式举例 连接方式一 123456789101112131415161718192021public void testConnection1() &#123; try &#123; // 1.提供java.sql.Driver接口实现类的对象 Driver driver = null; driver = new com.mysql.cj.jdbc.Driver(); // 2.提供url，指明具体操作的数据 String url = &quot;jdbc:mysql://localhost:3306/test&quot;; // 3.提供Properties的对象，指明用户名和密码 Properties info = new Properties(); info.setProperty(&quot;user&quot;, &quot;root&quot;); info.setProperty(&quot;password&quot;, &quot;abc123&quot;); // 4.调用driver的connect()，获取连接 Connection connection = driver.connect(url, info); System.out.println(connection); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;&#125; 说明：上述代码中显式出现了第三方数据库的 API。 连接方式二 12345678910111213141516171819202122public void testConnection2() &#123; try &#123; // 1.实例化Driver String className = &quot;com.mysql.cj.jdbc.Driver&quot;; Class&lt;?&gt; clazz = Class.forName(className); Driver driver = (Driver) clazz.newInstance(); // 2.提供url，指明具体操作的数据 String url = &quot;jdbc:mysql://localhost:3306/test&quot;; // 3.提供Properties的对象，指明用户名和密码 Properties info = new Properties(); info.setProperty(&quot;user&quot;, &quot;root&quot;); info.setProperty(&quot;password&quot;, &quot;abc123&quot;); // 4.调用driver的connect()，获取连接 Connection connection = driver.connect(url, info); System.out.println(connection); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 说明：相较于方式一，这里使用反射实例化 Driver，不在代码中体现第三方数据库的 API，体现了面向接口编程思想。 连接方式三 12345678910111213141516171819202122public void testConnection3() &#123; try &#123; // 1.数据库连接的4个基本要素： String url = &quot;jdbc:mysql://localhost:3306/test&quot;; String user = &quot;root&quot;; String password = &quot;abc123&quot;; String driverName = &quot;com.mysql.cj.jdbc.Driver&quot;; // 2.实例化Driver Class&lt;?&gt; clazz = Class.forName(driverName); Driver driver = (Driver) clazz.newInstance(); // 3.注册驱动 DriverManager.registerDriver(driver); // 4.获取连接 Connection connection = DriverManager.getConnection(url, user, password); System.out.println(connection); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 说明：使用 DriverManager 实现数据库的连接。体会获取连接必要的 4 个基本要素。 连接方式四 1234567891011121314151617181920212223242526272829303132public void testConnection4() &#123; try &#123; // 1.数据库连接的4个基本要素： String url = &quot;jdbc:mysql://localhost:3306/test&quot;; String user = &quot;root&quot;; String password = &quot;abc123&quot;; String driverName = &quot;com.mysql.cj.jdbc.Driver&quot;; // 2.加载驱动(实例化Driver和注册驱动) Class.forName(driverName); // Driver driver = (Driver) clazz.newInstance(); // 3.注册驱动 // DriverManager.registerDriver(driver); /* 可以注释掉上述代码的原因，是因为在mysql的Driver类中声明有(其他数据库的Driver类有类似代码)： static &#123; try &#123; DriverManager.registerDriver(new Driver()); &#125; catch (SQLException var1) &#123; throw new RuntimeException(&quot;Can&#x27;t register driver!&quot;); &#125; &#125; */ // 4.获取连接 Connection connection = DriverManager.getConnection(url, user, password); System.out.println(connection); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 说明：不必显式的注册驱动。因为在 DriverManager 的源码中已经存在静态代码块，实现了驱动的注册。 连接方式五 (最终版) 1234567891011121314151617181920@Testpublic void testConnection5() throws Exception &#123; // 1.加载配置文件 InputStream is = ConnectionTest.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties pros = new Properties(); pros.load(is); // 2.读取配置信息 String user = pros.getProperty(&quot;user&quot;); String password = pros.getProperty(&quot;password&quot;); String url = pros.getProperty(&quot;url&quot;); String driverClass = pros.getProperty(&quot;driverClass&quot;); // 3.加载驱动 Class.forName(driverClass); // 4.获取连接 Connection connection = DriverManager.getConnection(url,user,password); System.out.println(connection);&#125; 配置文件 jdbc.properties 内容如下： 1234user=rootpassword=abc123url=jdbc:mysql://localhost:3306/testdriverClass=com.mysql.cj.jdbc.Driver 说明：使用配置文件的方式保存配置信息，在代码中加载配置文件。 使用配置文件的好处： 实现了代码和数据的分离，如果需要修改配置信息，直接在配置文件中修改，不需要深入代码。 如果修改了配置信息，省去重新编译、打包的过程。 使用 PreparedStatement 实现 CRUD 操作Java 操作和访问数据库 数据库连接被用于向数据库服务器发送命令和 SQL 语句，并接受数据库服务器返回的结果。其实一个数据库连接就是一个 Socket 连接。 在 java.sql 包中有 3 个接口分别定义了对数据库的调用的不同方式： Statement：用于执行静态 SQL 语句并返回它所生成结果的对象。 PrepatedStatement：SQL 语句被预编译并存储在此对象中，可以使用此对象多次高效地执行该语句。 CallableStatement：用于执行 SQL 存储过程。 Java 与 SQL 对应数据类型转换表 Java 类型 SQL 类型 boolean BIT byte TINYINT short SMALLINT int INTEGER long BIGINT String CHAR,VARCHAR,LONGVARCHAR byte array BINARY , VAR BINARY java.sql.Date DATE java.sql.Time TIME java.sql.Timestamp TIMESTAMP 使用 Statement 操作数据表的弊端 通过调用 Connection 对象的 createStatement() 创建该对象。该对象用于执行静态的 SQL 语句，并且返回执行结果。 Statement 接口中定义了下列方法用于执行 SQL 语句： 12345// 执行更新操作INSERT、UPDATE、DELETEint executeUpdate(String sql) throws SQLException;// 执行查询操作SELECTResultSet executeQuery(String sql) throws SQLException; 但是，使用 Statement 操作数据表存在弊端： 问题一：存在拼串操作，繁琐。 问题二：存在 SQL 注入问题。 SQL 注入是利用某些系统没有对用户输入的数据进行充分的检查，而在用户输入的数据中注入非法的 SQL 语句段或命令，如：SELECT user, password FROM user_table WHERE user=&#39;a&#39; OR 1 = &#39; AND password = &#39; OR &#39;1&#39; = &#39;1&#39;)，从而利用系统的 SQL 引擎完成恶意行为的做法。 对于 Java 而言，要防范 SQL 注入，只要用 PreparedStatement (从 Statement 扩展而来) 取代 Statement 就可以了。 代码演示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105public class StatementTest &#123; // 使用Statement实现对数据表的查询操作 public static &lt;T&gt; T get(String sql, Class&lt;T&gt; clazz) &#123; Connection connection = null; Statement statement = null; ResultSet resultSet = null; try &#123; // 1.加载配置文件 InputStream is = StatementTest.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties properties = new Properties(); properties.load(is); // 2.读取配置信息 String user = properties.getProperty(&quot;user&quot;); String password = properties.getProperty(&quot;password&quot;); String url = properties.getProperty(&quot;url&quot;); String driverClass = properties.getProperty(&quot;driverClass&quot;); // 3.加载驱动 Class.forName(driverClass); // 4.获取连接 connection = DriverManager.getConnection(url, user, password); statement = connection.createStatement(); resultSet = statement.executeQuery(sql); // 获取结果集的元数据 ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); // 获取结果集的列数 int columnCount = resultSetMetaData.getColumnCount(); if (resultSet.next()) &#123; T t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) &#123; // 1.获取列的名称 // String columnName = resultSetMetaData.getColumnName(i+1); // 1.获取列的别名 String columnName = resultSetMetaData.getColumnLabel(i + 1); // 2. 根据列名获取对应数据表中的数据 Object columnVal = resultSet.getObject(columnName); // 3. 将数据表中得到的数据，封装进对象 Field field = clazz.getDeclaredField(columnName); field.setAccessible(true); field.set(t, columnVal); &#125; return t; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭资源 if (resultSet != null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (statement != null) &#123; try &#123; statement.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (connection != null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return null; &#125; public static void main(String[] args) &#123; // 使用Statement的弊端：需要拼写sql语句，并且存在SQL注入的问题 // 如何避免出现sql注入：只要用 PreparedStatement(从Statement扩展而来) 取代 Statement Scanner scanner = new Scanner(System.in); System.out.print(&quot;请输入用户名：&quot;); String user = scanner.nextLine(); System.out.print(&quot;请输入密码：&quot;); String password = scanner.nextLine(); // SELECT user,password FROM user_table WHERE user = &#x27;1&#x27; or &#x27; AND password = &#x27;=1 or &#x27;1&#x27; = &#x27;1&#x27; String sql = &quot;SELECT user,password FROM user_table WHERE user = &#x27;&quot; + user + &quot;&#x27; AND password = &#x27;&quot; + password + &quot;&#x27;&quot;; User returnUser = get(sql, User.class); if (returnUser != null) &#123; System.out.println(&quot;登录成功&quot;); &#125; else &#123; System.out.println(&quot;用户名不存在或密码错误&quot;); &#125; &#125;&#125; 改进： PreparedStatement的使用PreparedStatement介绍 通过调用 Connection 对象的 preparedStatement(String sql) 获取 PreparedStatement 对象。 PreparedStatement 接口是 Statement 的子接口，它表示一条预编译过的 SQL 语句。 PreparedStatement 对象所代表的 SQL 语句中的参数用问号 (?) 来表示，调用 PreparedStatement 对象的 setXxx() (Xxx 表示数据类型) 来设置这些参数。setXxx() 有两个参数，第一个参数是要设置的 SQL 语句中的问号参数的索引 (从 1 开始)，第二个是设置的 SQL 语句中的该索引位置对应参数的值。 PreparedStatement 与 Statement 的对比 代码的可读性和可维护性。 PreparedStatement 通过预编译，可以防止 SQL 注入 (占位符的位置只是参数，SQL 的语意，在预编译时已经完成)。 PreparedStatement 可以操作 Blob 的数据，而 Statement 做不到。 PreparedStatement 能最大可能提高性能： DBServer 会对预编译语句提供性能优化。因为预编译语句有可能被重复调用，所以语句在被 DBServer 的编译器编译后的执行代码被缓存下来，那么下次调用时只要是相同的预编译语句就不需要编译，只要将参数直接传入编译过的语句执行代码中就会得到执行。 在 Statement 语句中，即使是相同操作，但因为数据内容不一样，所以整个语句本身不能匹配，没有缓存语句的意义。事实是没有数据库会对普通语句编译后的执行代码缓存。这样，每执行一次都要对传入的语句编译一次。 对于新的 SQL 语句，需要经过语法检查，语义检查，翻译成二进制命令等操作，PreparedStatement 的语句是预编译的，会被缓存，就不再需要经过前面的那几个操作，从而提高效率。 使用 PreparedStatement 实现增、删、改操作 常规： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class PreparedStatementUpdateTest &#123; // 向customers表中添加一条记录 public static void testInsert() &#123; Connection connection = null; PreparedStatement preparedStatement = null; try &#123; // 1.读取配置文件中的4个基本信息 InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties properties = new Properties(); properties.load(is); String user = properties.getProperty(&quot;user&quot;); String password = properties.getProperty(&quot;password&quot;); String url = properties.getProperty(&quot;url&quot;); String driverClass = properties.getProperty(&quot;driverClass&quot;); // 2.加载驱动 Class.forName(driverClass); // 3.获取连接 connection = DriverManager.getConnection(url, user, password); // 4.预编译sql语句，返回PreparedStatement的实例 String sql = &quot;insert into customers(name, email, birth) values(?, ?, ?)&quot;;// ?: 占位符 preparedStatement = connection.prepareStatement(sql); // 5.填充占位符 preparedStatement.setString(1, &quot;哪吒&quot;); preparedStatement.setString(2, &quot;nezha@gmail.com&quot;); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); java.util.Date date = sdf.parse(&quot;1000-01-01&quot;); preparedStatement.setDate(3, new Date(date.getTime()));// java.util.Date与java.sql.Date转换 // 6.执行操作 preparedStatement.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 7.资源的关闭 try &#123; if (preparedStatement != null) &#123; preparedStatement.close(); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; if (connection != null) &#123; connection.close(); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; testInsert(); &#125;&#125; 通用： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class JDBCUtils &#123; // 使用throws抛出异常，在真正用到Connection的地方，统一使用try/catch，防止获取连接时出现异常，导致Connection为空但代码继续执行 public static Connection getConnection() throws Exception &#123; // 1.读取配置文件中的4个基本信息 InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); Properties properties = new Properties(); properties.load(is); String user = properties.getProperty(&quot;user&quot;); String password = properties.getProperty(&quot;password&quot;); String url = properties.getProperty(&quot;url&quot;); String driverClass = properties.getProperty(&quot;driverClass&quot;); // 2.加载驱动 Class.forName(driverClass); // 3.获取连接 Connection connection = DriverManager.getConnection(url, user, password); return connection; &#125; public static void closeResource(Connection connection, Statement statement) &#123; if (statement != null) &#123; try &#123; statement.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (connection != null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void closeResource(Connection connection, Statement statement, ResultSet resultSet) &#123; if (resultSet != null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (statement != null) &#123; try &#123; statement.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (connection != null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class PreparedStatementUpdateTest &#123; // 通用的增删改操作 public static void update(String sql, Object... args) &#123;// sql中占位符的个数与可变形参的长度相同！ Connection connection = null; PreparedStatement preparedStatement = null; try &#123; // 1.获取数据库的连接 connection = JDBCUtils.getConnection(); // 2.预编译sql语句，返回PreparedStatement的实例 preparedStatement = connection.prepareStatement(sql); // 3.填充占位符 for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; // 4.执行 /* * 方式一，preparedStatement.execute()： * 如果执行的是查询操作，有返回结果，则此方法返回true; * 如果执行的是增、删、改操作，没有返回结果，则此方法返回false。 */ // preparedStatement.execute(); /* * 方式二，preparedStatement.executeUpdate()： * 返回sql语句执行过后，对数据库影响的行数，可以根据返回值，判断增、删、查操作的结果 */ preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 5.资源的关闭 JDBCUtils.closeResource(connection, preparedStatement); &#125; &#125; public static void main(String[] args) &#123; // String sql = &quot;delete from customers where id = ?&quot;; // update(sql,3); // order是数据库关键字，作为表名，为防止错误，添加``符号 String sql = &quot;update `order` set order_name = ? where order_id = ?&quot;; update(sql, &quot;DD&quot;, &quot;2&quot;); &#125;&#125; 使用 PreparedStatement 实现查询操作 常规： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/* * ORM编程思想---Object Relational Mapping * 一个数据表对应一个Java类 * 表中的一条记录对应Java类的一个对象 * 表中的一个字段对应Java类的一个属性 */public class Customer &#123; private int id; private String name; private String email; private Date birth;// java.sql.Date public Customer() &#123; &#125; public Customer(int id, String name, String email, Date birth) &#123; this.id = id; this.name = name; this.email = email; this.birth = birth; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; @Override public String toString() &#123; return &quot;Customer [id=&quot; + id + &quot;, name=&quot; + name + &quot;, email=&quot; + email + &quot;, birth=&quot; + birth + &quot;]&quot;; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 针对于customers表的查询操作 */public class CustomerForQuery &#123; public static void testQuery1() &#123; Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; connection = JDBCUtils.getConnection(); String sql = &quot;select id, name, email, birth from customers where id = ?&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setObject(1, 1); // 执行，并返回结果集 resultSet = preparedStatement.executeQuery(); // 处理结果集 // next()：判断结果集的下一条是否有数据，如果有数据返回true，并且指针下移； // 如果没有数据返回false，指针不会下移。 if (resultSet.next()) &#123; // 获取当前这条数据的各个字段值 int id = resultSet.getInt(1); String name = resultSet.getString(2); String email = resultSet.getString(3); Date birth = resultSet.getDate(4); // 方式一： // System.out.println(&quot;id = &quot; + id + &quot;,name = &quot; + name + &quot;,email = &quot; + email + &quot;,birth = &quot; + birth); // 方式二： // Object[] data = new Object[]&#123;id, name, email, birth&#125;; // 方式三：将数据封装为一个对象（推荐） Customer customer = new Customer(id, name, email, birth); System.out.println(customer); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭资源 JDBCUtils.closeResource(connection, preparedStatement, resultSet); &#125; &#125; public static void main(String[] args) &#123; testQuery1(); &#125;&#125; 通用： 类的属性和表的字段相同： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 针对于customers表的查询操作 */public class CustomerForQuery &#123; /** * 针对customers表的通用的查询操作 */ public static Customer queryForCustomers(String sql, Object... args) &#123; Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; connection = JDBCUtils.getConnection(); preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); // 获取结果集的元数据：ResultSetMetaData ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); // 通过ResultSetMetaData获取结果集中的列数 int columnCount = resultSetMetaData.getColumnCount(); // 只返回一条数据q2w if (resultSet.next()) &#123; Customer customer = new Customer(); // 处理结果集一行数据中的每一个列 for (int i = 0; i &lt; columnCount; i++) &#123; // 获取列值 Object columnValue = resultSet.getObject(i + 1); // 获取每个列的列名 // String columnName = resultSetMetaData.getColumnName(i + 1); String columnLabel = resultSetMetaData.getColumnLabel(i + 1); // 通过反射：给customer对象指定的columnName属性，赋值为columnValue Field field = Customer.class.getDeclaredField(columnLabel); field.setAccessible(true); field.set(customer, columnValue); &#125; return customer; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, preparedStatement, resultSet); &#125; return null; &#125; public static void main(String[] args) &#123; String sql = &quot;select id, name, email, birth from customers where id = ?&quot;; Customer customer = queryForCustomers(sql, 13); System.out.println(customer); sql = &quot;select id, name, email, birth from customers where id = ? and name = ?&quot;; Customer customer1 = queryForCustomers(sql, 10, &quot;杰杰&quot;); System.out.println(customer1); &#125;&#125; 类的属性和表的字段不同： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Order &#123; private int orderId; private String orderName; private Date orderDate;// java.sql.Date public Order() &#123; super(); &#125; public Order(int orderId, String orderName, Date orderDate) &#123; super(); this.orderId = orderId; this.orderName = orderName; this.orderDate = orderDate; &#125; public int getOrderId() &#123; return orderId; &#125; public void setOrderId(int orderId) &#123; this.orderId = orderId; &#125; public String getOrderName() &#123; return orderName; &#125; public void setOrderName(String orderName) &#123; this.orderName = orderName; &#125; public Date getOrderDate() &#123; return orderDate; &#125; public void setOrderDate(Date orderDate) &#123; this.orderDate = orderDate; &#125; @Override public String toString() &#123; return &quot;Order [orderId=&quot; + orderId + &quot;, orderName=&quot; + orderName + &quot;, orderDate=&quot; + orderDate + &quot;]&quot;; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 针对order表的通用的查询操作 */public class OrderForQuery &#123; /* * 针对于表的字段名与类的属性名不相同的情况： * 1.在声明sql时，使用类的属性名来命名字段的别名 * 2.使用ResultSetMetaData时，需要使用getColumnLabel()来替换getColumnName()获取列的别名 * 说明：如果sql中没有给字段起别名，getColumnLabel()获取的就是列名 */ /** * 针对于order表的通用的查询操作 */ public static Order orderForQuery(String sql, Object... args) &#123; Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; connection = JDBCUtils.getConnection(); preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; // 执行，获取结果集 resultSet = preparedStatement.executeQuery(); // 获取结果集的元数据 ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); // 获取列数 int columnCount = resultSetMetaData.getColumnCount(); if (resultSet.next()) &#123; Order order = new Order(); for (int i = 0; i &lt; columnCount; i++) &#123; // 获取每个列的列值 Object columnValue = resultSet.getObject(i + 1); // 通过ResultSetMetaData // 获取列的列名：getColumnName()---不推荐使用 // 获取列的别名：getColumnLabel() // String columnName = resultSetMetaData.getColumnName(i + 1); String columnLabel = resultSetMetaData.getColumnLabel(i + 1); // 通过反射，将对象指定名columnName的属性赋值为指定的值columnValue Field field = Order.class.getDeclaredField(columnLabel); field.setAccessible(true); field.set(order, columnValue); &#125; return order; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, preparedStatement, resultSet); &#125; return null; &#125; public static void main(String[] args) &#123; String sql = &quot;select order_id orderId, order_name orderName, order_date orderDate from `order` where order_id = ?&quot;; Order order = orderForQuery(sql, 1); System.out.println(order); &#125;&#125; 不同表： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class PreparedStatementQueryTest &#123; // 返回一个 public static &lt;T&gt; T getInstance(Class&lt;T&gt; clazz, String sql, Object... args) &#123; Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; connection = JDBCUtils.getConnection(); preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); // 获取结果集的元数据 ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); // 通过ResultSetMetaData获取结果集中的列数 int columnCount = resultSetMetaData.getColumnCount(); if (resultSet.next()) &#123; T t = clazz.newInstance(); // 处理结果集一行数据中的每一个列 for (int i = 0; i &lt; columnCount; i++) &#123; // 获取列值 Object columnValue = resultSet.getObject(i + 1); // 获取每个列的别名 String columnLabel = resultSetMetaData.getColumnLabel(i + 1); // 通过反射，给t对象指定的columnName属性，赋值为columnValue Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columnValue); &#125; return t; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, preparedStatement, resultSet); &#125; return null; &#125; // 返回一个集合 public static &lt;T&gt; List&lt;T&gt; getForList(Class&lt;T&gt; clazz, String sql, Object... args) &#123; Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; connection = JDBCUtils.getConnection(); preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); // 获取结果集的元数据 ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); // 通过ResultSetMetaData获取结果集中的列数 int columnCount = resultSetMetaData.getColumnCount(); // 创建集合对象 List&lt;T&gt; list = new ArrayList&lt;&gt;(); while (resultSet.next()) &#123; T t = clazz.newInstance(); // 处理结果集一行数据中的每一个列: 给t对象指定的属性赋值 for (int i = 0; i &lt; columnCount; i++) &#123; // 获取列值 Object columnValue = resultSet.getObject(i + 1); // 获取每个列的别名 String columnLabel = resultSetMetaData.getColumnLabel(i + 1); // 给t对象指定的columnName属性，赋值为columnValue：通过反射 Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columnValue); &#125; list.add(t); &#125; return list; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, preparedStatement, resultSet); &#125; return null; &#125; public static void main(String[] args) &#123; String sql = &quot;select id, name, email, birth from customers where id = ?&quot;; Customer customer = getInstance(Customer.class, sql, 12); System.out.println(customer); String sql1 = &quot;select order_id orderId, order_name orderName from `order` where order_id = ?&quot;; Order order = getInstance(Order.class, sql1, 1); System.out.println(order); String sql2 = &quot;select id, name, email, birth from customers where id &lt; ?&quot;; List&lt;Customer&gt; list = getForList(Customer.class, sql2, 12); list.forEach(System.out::println); String sql3 = &quot;select order_id orderId, order_name orderName from `order`&quot;; List&lt;Order&gt; orderList = getForList(Order.class, sql3); orderList.forEach(System.out::println); &#125;&#125; ResultSet 与 ResultSetMetaData 的说明ResultSet 查询数据库时，需要调用 PreparedStatement 的 executeQuery()，查询结果是一个 ResultSet 对象。 ResultSet 对象以逻辑表格的形式封装了执行数据库操作的结果集，ResultSet 接口由数据库厂商提供实现。 ResultSet 返回的实际上就是一张数据表。有一个指针指向数据表的第一条记录的前面。 ResultSet 对象维护了一个指向当前数据行的游标，初始的时候，游标在第一行之前，可以通过 ResultSet 对象的 next() 移动到下一行。调用 next() 时会检测下一行是否有效，若有效，该方法返回 true，且指针下移。相当于 Iterator 对象的 hasNext() 和 next() 两个方法的结合体。 当指针指向一行时，可以通过调用 getXxx(int index) 或 getXxx(int columnName) 获取每一列的值。 例如：getInt(1)，getString(&quot;name&quot;)。 注意：Java 与数据库交互涉及到的相关 Java API 中的索引都从 1 开始。 ResultSetMetaData ResultSetMetaData 是描述 ResultSet 的元数据，可用于获取 ResultSet 对象中列的类型和属性信息。 通过 ResultSet 的 getMetaData() 获取 ResultSetMetaData。 常用方法： getColumnCount()：返回当前 ResultSet 对象中的列数。 getColumnName(int column)：获取指定列的名称，不推荐使用。 getColumnLabel(int column)：获取指定列的别名，如果没有别名，则返回该列的名称。 getColumnTypeName(int column)：检索指定列的数据库特定的类型名称。 getColumnDisplaySize(int column)：指示指定列的最大标准宽度，以字符为单位。 isNullable(int column)：指示指定列中的值是否可以为 null。 isAutoIncrement(int column)：指示是否自动为指定列进行编号，这样这些列仍然是只读的。 资源的释放 释放 ResultSet，Statement，Connection。 数据库连接 (Connection) 是非常稀有的资源，用完后必须马上释放，如果 Connection 不能及时正确的关闭将导致系统宕机。Connection 的使用原则是尽量晚创建，尽量早的释放。 应该在 finally 中关闭，保证及时其他代码出现异常，资源也一定能被关闭。 JDBC API 使用小结 两种思想： 面向接口编程的思想 ORM 思想 (Object Relational Mapping) 一个数据表对应一个 Java 类。 表中的一条记录对应 Java 类的一个对象。 表中的一个字段对应 Java 类的一个属性。 SQL 应结合列名和表的属性名来写，必要时需要起别名。 两种技术： JDBC 结果集的元数据 ResultSetMetaData。 获取列数：getColumnCount() 获取列的别名：getColumnLabel() 通过反射，创建指定类的对象，获取指定的属性并赋值。 操作 BLOB 类型字段MySQL BLOB 类型 MySQL 中，BLOB 是一个二进制大型对象，是一个可以存储大量数据的容器，它能容纳不同大小的数据。 插入 BLOB 类型的数据必须使用 PreparedStatement，因为 BLOB 类型的数据无法使用字符串拼接来写。 MySQL 的四种 BLOB 类型 (除了在存储的最大信息量上不同外，它们是等同的)： 实际使用中根据需要存入的数据大小定义不同的 BLOB 类型。 需要注意的是：如果存储的文件过大，数据库的性能会下降。 如果在指定了相关的 BLOB 类型以后，还报错：xxx too large，原因是文件大小超过默认存储。 1com.mysql.cj.jdbc.exceptions.PacketTooBigException: Packet for query is too large (6,218,041 &gt; 4,194,304). You can change this value on the server by setting the &#x27;max_allowed_packet&#x27; variable. 在 MySQL 的安装目录下，查找 my.ini 文件，并添加如下的配置参数： max_allowed_packet=16M，设置存储大小。注意：修改了 my.ini 文件之后，需要重新启动 MySQL 服务。 1max_allowed_packet=16M Windows 系统下，在服务中，右键点击属性，可以查找到 my.ini 文件的位置： 在 cmd 中，也可以查看 max_allowed_packet 配置： 12345678910111213141516171819202122232425C:\\Users\\XiSun&gt;mysql -u root -pEnter password: ***************Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 8Server version: 8.0.25 MySQL Community Server - GPLCopyright (c) 2000, 2021, Oracle and/or its affiliates.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; show VARIABLES like &#x27;%max_allowed_packet%&#x27;;+---------------------------+------------+| Variable_name | Value |+---------------------------+------------+| max_allowed_packet | 16777216 || mysqlx_max_allowed_packet | 67108864 || slave_max_allowed_packet | 1073741824 |+---------------------------+------------+3 rows in set, 1 warning (0.01 sec)mysql&gt; 设置：set global max_allowed_packet = 2*1024*1024*10; 退出 MySQL：quit 重启：service mysqld restart 数据表中插入 BLOB 类型字段123456789101112131415161718192021222324252627282930313233public class BlobTest &#123; // 向数据表customers中插入Blob类型的字段 public static void testInsert() &#123; Connection connection = null; PreparedStatement preparedStatement = null; FileInputStream fis = null; try &#123; connection = JDBCUtils.getConnection(); String sql = &quot;insert into customers(name, email, birth, photo) values(?, ?, ?, ?)&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setObject(1, &quot;袁浩&quot;); preparedStatement.setObject(2, &quot;yuan@qq.com&quot;); preparedStatement.setObject(3, &quot;1992-09-08&quot;); fis = new FileInputStream(new File(&quot;E:/test.png&quot;)); preparedStatement.setBlob(4, fis); preparedStatement.execute(); &#125; catch (Exception exception) &#123; exception.printStackTrace(); &#125; finally &#123; try &#123; if (fis != null) &#123; fis.close(); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; JDBCUtils.closeResource(connection, preparedStatement); &#125; &#125;&#125; 数据表中修改 BLOB 类型字段12345678910111213141516171819202122232425262728293031public class BlobTest &#123; // 向数据表customers中修改Blob类型的字段 public static void testUpdate() &#123; Connection connection = null; PreparedStatement preparedStatement = null; FileInputStream fis = null; try &#123; connection = JDBCUtils.getConnection(); String sql = &quot;update customers set photo = ? where id = ?&quot;; preparedStatement = connection.prepareStatement(sql); fis = new FileInputStream(new File(&quot;test.png&quot;)); preparedStatement.setBlob(1, fis); preparedStatement.setObject(2, 20); preparedStatement.execute(); &#125; catch (Exception exception) &#123; exception.printStackTrace(); &#125; finally &#123; try &#123; if (fis != null) &#123; fis.close(); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; JDBCUtils.closeResource(connection, preparedStatement); &#125; &#125;&#125; 数据表中读取 BLOB 类型字段123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class BlobTest &#123; // 查询数据表customers中Blob类型的字段 public static void testQuery() &#123; Connection connection = null; PreparedStatement preparedStatement = null; InputStream is = null; FileOutputStream fos = null; ResultSet resultSet = null; try &#123; connection = JDBCUtils.getConnection(); String sql = &quot;select id, name, email, birth, photo from customers where id = ?&quot;; preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, 20); resultSet = preparedStatement.executeQuery(); if (resultSet.next()) &#123; // 方式一： // int id = resultSet.getInt(1); // String name = resultSet.getString(2); // String email = resultSet.getString(3); // Date birth = resultSet.getDate(4); // 方式二： int id = resultSet.getInt(&quot;id&quot;); String name = resultSet.getString(&quot;name&quot;); String email = resultSet.getString(&quot;email&quot;); Date birth = resultSet.getDate(&quot;birth&quot;); Customer customer = new Customer(id, name, email, birth); System.out.println(customer); // Blob类型的字段需要下载下来，以文件的方式保存在本地 Blob photo = resultSet.getBlob(&quot;photo&quot;); is = photo.getBinaryStream(); fos = new FileOutputStream(&quot;test2.jpg&quot;); byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fos != null) &#123; fos.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; if (is != null) &#123; is.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; JDBCUtils.closeResource(connection, preparedStatement, resultSet); &#125; &#125;&#125; 批量处理批量执行 SQL 语句 当需要成批插入或者更新记录时，可以采用 JDBC 的批量更新机制，这一机制允许多条语句一次性提交给数据库批量处理。通常情况下比单独提交处理更有效率 JDBC 的批量处理语句包括下面三个方法： addBatch(String)：添加需要批量处理的 SQL 语句或是参数。 executeBatch()：执行批量处理语句。 clearBatch()：清空缓存的数据。 通常我们会遇到两种批量执行 SQL 语句的情况： 多条 SQL 语句的批量处理。 一条 SQL 语句的批量传参。 高效的批量插入 举例：向数据表中插入 20000 条数据。 数据库中提供一个 goods 表作为测试。创建如下： 1234CREATE TABLE goods(id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20)); 12345# 查询goods表总条目数SELECT COUNT(*) FROM goods;# 清空goods表TRUNCATE TABLE goods; 实现层次一123456789101112131415161718192021222324252627282930/* * 使用PreparedStatement实现批量数据的操作 * * update、delete本身就具有批量操作的效果。 * 此时的批量操作，主要指的是批量插入。使用PreparedStatement如何实现更高效的批量插入？ * * 功能：向goods表中插入20000条数据 */public class InsertTest &#123; // 批量插入的方式一：使用Statement public static void testInsert1() &#123; Connection connection = null; Statement statement = null; try &#123; long start = System.currentTimeMillis(); connection = JDBCUtils.getConnection(); statement = connection.createStatement(); for (int i = 1; i &lt;= 20000; i++) &#123; String sql = &quot;insert into goods(name) values(&#x27;name_&quot; + i + &quot;&#x27;)&quot;; statement.execute(sql); &#125; long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start));// 20000: 35856 &#125; catch (Exception exception) &#123; exception.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, statement); &#125; &#125;&#125; 实现层次二12345678910111213141516171819202122232425262728293031/* * 使用PreparedStatement实现批量数据的操作 * * update、delete本身就具有批量操作的效果。 * 此时的批量操作，主要指的是批量插入。使用PreparedStatement如何实现更高效的批量插入？ * * 功能：向goods表中插入20000条数据 */public class InsertTest &#123; // 批量插入的方式二：使用PreparedStatement public static void testInsert2() &#123; Connection connection = null; PreparedStatement preparedStatement = null; try &#123; long start = System.currentTimeMillis(); connection = JDBCUtils.getConnection(); String sql = &quot;insert into goods(name) values(?)&quot;; preparedStatement = connection.prepareStatement(sql); for (int i = 1; i &lt;= 20000; i++) &#123; preparedStatement.setObject(1, &quot;name_&quot; + i); preparedStatement.execute(); &#125; long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start));// 20000：36488 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, preparedStatement); &#125; &#125;&#125; 实现层次三1234567891011121314151617181920212223242526272829303132333435363738394041424344/* * 使用PreparedStatement实现批量数据的操作 * * update、delete本身就具有批量操作的效果。 * 此时的批量操作，主要指的是批量插入。使用PreparedStatement如何实现更高效的批量插入？ * * 功能：向goods表中插入20000条数据 */public class InsertTest &#123; /* * 批量插入的方式三： * 1.addBatch()、executeBatch()、clearBatch() * 2.MySQL服务器默认是关闭批处理的，需要通过一个参数，让MySQL开启批处理的支持。 * 在配置文件的url后面添加参数：?rewriteBatchedStatements=true * 3.MySQL驱动要求5.1.37及以上版本 */ public static void testInsert3() &#123; Connection connection = null; PreparedStatement preparedStatement = null; try &#123; long start = System.currentTimeMillis(); connection = JDBCUtils.getConnection(); String sql = &quot;insert into goods(name) values(?)&quot;; preparedStatement = connection.prepareStatement(sql); for (int i = 1; i &lt;= 1000000; i++) &#123; preparedStatement.setObject(1, &quot;name_&quot; + i); // 1.&quot;攒&quot;sql preparedStatement.addBatch(); if (i % 500 == 0) &#123; // 2.执行batch preparedStatement.executeBatch(); // 3.清空batch preparedStatement.clearBatch(); &#125; &#125; long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start));// 20000: 1130 &#125; catch (Exception e) &#123; // 1000000: 13560 e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, preparedStatement); &#125; &#125;&#125; url 格式：url=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=true 实现层次四123456789101112131415161718192021222324252627282930313233343536373839404142434445/* * 使用PreparedStatement实现批量数据的操作 * * update、delete本身就具有批量操作的效果。 * 此时的批量操作，主要指的是批量插入。使用PreparedStatement如何实现更高效的批量插入？ * * 功能：向goods表中插入20000条数据 */public class InsertTest &#123; /* * 批量插入的方式四：设置连接不允许自动提交数据(在方式三的基础上实现) * 使用Connection的setAutoCommit(false)/commit() */ public static void testInsert4() &#123; Connection connection = null; PreparedStatement ps = null; try &#123; long start = System.currentTimeMillis(); connection = JDBCUtils.getConnection(); // 先设置不允许自动提交数据 connection.setAutoCommit(false); String sql = &quot;insert into goods(name) values(?)&quot;; ps = connection.prepareStatement(sql); for (int i = 1; i &lt;= 1000000; i++) &#123; ps.setObject(1, &quot;name_&quot; + i); // 1.&quot;攒&quot;sql ps.addBatch(); if (i % 500 == 0) &#123; // 2.执行batch ps.executeBatch(); // 3.清空batch ps.clearBatch(); &#125; &#125; // 然后手动提交数据 connection.commit(); long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start));// 20000: 1094 &#125; catch (Exception e) &#123; // 1000000: 10129 e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, ps); &#125; &#125;&#125; 每次向数据库 commit 操作时，也会耗时，可以等全部数据处理完后，再统一 commit。 数据库事务数据库事务介绍 事务：一组逻辑操作单元，使数据从一种状态变换到另一种状态。 一组逻辑操作单元：指一个或多个 DML 操作。 事务处理 (事务操作)：保证所有事务都作为一个工作单元来执行，即使出现了故障，都不能改变这种执行方式。当在一个事务中执行多个操作时，要么所有的事务都被提交 (commit)，那么这些修改就永久地保存下来；要么数据库管理系统将放弃所作的所有修改，整个事务回滚 (rollback) 到最初状态。 为确保数据库中数据的一致性，数据的操纵应当是离散的成组的逻辑单元：当它全部完成时，数据的一致性可以保持，而当这个单元中的一部分操作失败，整个事务应全部视为错误，所有从起始点以后的操作应全部回退到开始状态。 JDBC 事务处理 数据一旦提交 (commit)，就不可回滚。 数据什么时候会提交： DDL 操作，一旦执行，都会自动提交。注意：set autocommit = false 对 DDL 操作无效。 DDL：CREATE、ALTER、DROP、TRUNCATE、COMMENT、RENAME。 DML 操作，默认情况下，一旦执行，就会自动提交。可以通过 set autocommit = false 的方式取消 DML 操作的自动提交。 DML：SELECT、INSERT、UPDATE、DELETE、MERGE、CALL、EXPLAIN PLAN、LOCK TABLE。 当一个连接对象被创建时，默认情况下是自动提交事务：每次执行一个 SQL 语句时，如果执行成功，就会向数据库自动提交，而不能回滚。 关闭数据库连接时，数据也会自动的提交。如果多个操作，每个操作使用的是自己单独的连接，则无法保证事务。即同一个事务的多个操作必须在同一个连接下。 JDBC 程序中为了让多个 SQL 语句作为一个事务执行： 第一步：调用 Connection 对象的 **setAutoCommit(false);**，取消自动提交事务； 第二步：在所有的 SQL 语句都成功执行后，调用 **commit();**，提交事务； 第三步：当出现异常时，调用 **rollback();**，回滚事务。 当一个事务操作结束之后，若此时 Connection 没有被关闭，还可能被重复使用，则应该调用 Connection 的 setAutoCommit(true)，恢复其自动提交状态 。尤其是在使用数据库连接池技术时，在执行 close() 关闭资源之前，建议恢复自动提交状态。 案例：用户 AA 向用户 BB 转账 100。 未考虑事务的情况： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class TransactionTest &#123; /* * 针对于数据表user_table来说： * AA用户给BB用户转账100 * * update user_table set balance = balance - 100 where user = &#x27;AA&#x27;; * update user_table set balance = balance + 100 where user = &#x27;BB&#x27;; */ // 通用的增删改操作---version 1.0 // ******************未考虑数据库事务情况下的转账操作************************** public static int update(String sql, Object... args) &#123; Connection connection = null; PreparedStatement preparedStatement = null; try &#123; // 1.获取数据库的连接 connection = JDBCUtils.getConnection(); // 2.预编译sql语句，返回PreparedStatement的实例 preparedStatement = connection.prepareStatement(sql); // 3.填充占位符 for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; // 4.执行 return preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 5.资源的关闭 JDBCUtils.closeResource(connection, preparedStatement); &#125; return 0; &#125; public static void main(String[] args) &#123; String sql1 = &quot;update user_table set balance = balance - 100 where user = ?&quot;; update(sql1, &quot;AA&quot;); // 模拟网络异常 System.out.println(10 / 0); String sql2 = &quot;update user_table set balance = balance + 100 where user = ?&quot;; update(sql2, &quot;BB&quot;); System.out.println(&quot;转账成功&quot;); &#125;&#125; 上述代码，当出现网络异常时，AA 用户资产会少 100，但 BB 用户资产不会变化，转账过程发生错误。 考虑事务的情况： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105/* * 1.什么叫数据库事务？ * 事务：一组逻辑操作单元,使数据从一种状态变换到另一种状态。 * &gt; 一组逻辑操作单元：一个或多个DML操作。 * * 2.事务处理的原则：保证所有事务都作为一个工作单元来执行，即使出现了故障，都不能改变这种执行方式。 * 当在一个事务中执行多个操作时，要么所有的事务都被提交(commit)，那么这些修改就永久地保存 * 下来；要么数据库管理系统将放弃所作的所有修改，整个事务回滚(rollback)到最初状态。 * * 3.数据一旦提交，就不可回滚 * * 4.哪些操作会导致数据的自动提交？ * &gt;DDL操作一旦执行，都会自动提交。 * &gt;set autocommit = false 对DDL操作失效 * &gt;DML默认情况下，一旦执行，就会自动提交。 * &gt;我们可以通过set autocommit = false的方式取消DML操作的自动提交。 * &gt;默认在关闭连接时，会自动的提交数据 */public class TransactionTest &#123; /* * 针对于数据表user_table来说： * AA用户给BB用户转账100 * * update user_table set balance = balance - 100 where user = &#x27;AA&#x27;; * update user_table set balance = balance + 100 where user = &#x27;BB&#x27;; */ // 通用的增删改操作---version 2.0(考虑事务) public static int update(Connection conn, String sql, Object... args) &#123; PreparedStatement ps = null; try &#123; // 1.预编译sql语句，返回PreparedStatement的实例 ps = conn.prepareStatement(sql); // 2.填充占位符 for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; // 3.执行 return ps.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.资源的关闭 if (ps != null) &#123; try &#123; ps.close(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; &#125; return 0; &#125; public static void main(String[] args) &#123; // ********************考虑数据库事务后的转账操作********************* Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); System.out.println(connection.getAutoCommit());// 默认为true // 1.取消数据的自动提交 connection.setAutoCommit(false); String sql1 = &quot;update user_table set balance = balance - 100 where user = ?&quot;; update(connection, sql1, &quot;AA&quot;); // 模拟网络异常 System.out.println(10 / 0); String sql2 = &quot;update user_table set balance = balance + 100 where user = ?&quot;; update(connection, sql2, &quot;BB&quot;); System.out.println(&quot;转账成功&quot;); // 2.提交数据 connection.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); // 3.回滚数据 if (connection != null) &#123; try &#123; connection.rollback(); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; finally &#123; if (connection != null) &#123; // 关闭之前修改其为自动提交数据，主要针对于使用数据库连接池的使用 try &#123; connection.setAutoCommit(true); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; connection.close(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 上述代码，是一个完整的事务操作，AA 和 BB 用户，要么资产都发生变化，要么资产都不发生变化。 事务的 ACID 属性 原子性 (Atomicity) 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性 (Consistency) 事务必须使数据库从一个一致性状态变换到另外一个一致性状态。 隔离性 (Isolation) 事务的隔离性是指一个事务的执行不能被其他事务干扰，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 参考多线程对共享数据的处理。 持久性 (Durability) 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来的其他操作和数据库故障不应该对其有任何影响。 数据库的并发问题 对于同时运行的多个事务，当这些事务访问数据库中相同的数据时，如果没有采取必要的隔离机制，就会导致各种并发问题： 脏读：对于两个事务 T1 和 T2，T1 读取了已经被 T2 更新但还没有被提交的字段。之后，若 T2 回滚，那么 T1 读取的内容就是临时且无效的。 不可重复读：对于两个事务 T1 和 T2，T1 读取了一个字段，然后 T2 更新了该字段。之后，T1 再次读取同一个字段，值会不一样。 幻读：对于两个事务 T1 和 T2，T1 从一个表中读取了一个字段，然后 T2 在该表中插入了一些新的行。之后, 如果 T1 再次读取同一个表，就会多出几行。 数据库事务的隔离性：数据库系统必须具有隔离并发运行各个事务的能力，使它们不会相互影响，从而避免各种并发问题。 一个事务与其他事务隔离的程度称为隔离级别。数据库规定了多种事务隔离级别，不同隔离级别对应不同的干扰程度，隔离级别越高，数据一致性就越好，但并发性越弱。 四种隔离级别 数据库提供了 4 种事务隔离级别： Oracle 支持 2 种事务隔离级别：READ COMMITED 和 SERIALIZABLE。 Oracle 默认的事务隔离级别为 READ COMMITED。 MySQL 支持 4 种事务隔离级别。MySQL 默认的事务隔离级别为 REPEATABLE READ。 在开发中，要保证隔离级别至少为 READ COMMITED。如果数据库本身能够满足，不需要在代码中设置，否则，应该在代码中显式设置事物的隔离级别。 在 MySQL 中设置隔离级别 每启动一个 MySQL 程序，就会获得一个单独的数据库连接。每个数据库连接都有一个全局变量 @@transaction_isolation，表示当前的事务隔离级别。 查看当前的隔离级别： 1SELECT @@transaction_isolation; 旧版本是 @@tx_isolation。 设置当前用户 MySQL 连接的隔离级别: 1set transaction isolation level read committed; 设置数据库系统的全局的隔离级别： 1set global transaction isolation level read committed; 设置隔离级别之后，需要重新连接 MySQL。 补充操作： 创建 MySQL 数据库用户： 1create user tom identified by &#39;abc123&#39;; 授予权限： 12345# 授予通过网络方式登录的tom用户，对所有库所有表的全部权限，密码设为abc123grant all privileges on *.* to tom@&#39;%&#39; identified by &#39;abc123&#39;; # 给tom用户使用本地命令行方式，授予test这个库下的所有表的插删改查的权限。grant select, insert, delete, update on test.* to tom@localhost identified by &#39;abc123&#39;; 在 JDBC 中设置隔离级别123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170public class TransactionTest &#123; // 通用的增删改操作---version 2.0(考虑事务) public static int update(Connection conn, String sql, Object... args) &#123; PreparedStatement ps = null; try &#123; // 1.预编译sql语句，返回PreparedStatement的实例 ps = conn.prepareStatement(sql); // 2.填充占位符 for (int i = 0; i &lt; args.length; i++) &#123; ps.setObject(i + 1, args[i]); &#125; // 3.执行 return ps.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.资源的关闭 if (ps != null) &#123; try &#123; ps.close(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; &#125; return 0; &#125; // 通用的查询操作，用于返回数据表中的一条记录---version 2.0(考虑事务) public static &lt;T&gt; T getInstance(Connection connection, Class&lt;T&gt; clazz, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); // 获取结果集的元数据 :ResultSetMetaData ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); // 通过ResultSetMetaData获取结果集中的列数 int columnCount = resultSetMetaData.getColumnCount(); if (resultSet.next()) &#123; T t = clazz.newInstance(); // 处理结果集一行数据中的每一个列 for (int i = 0; i &lt; columnCount; i++) &#123; // 获取列值 Object columValue = resultSet.getObject(i + 1); // 获取每个列的列名 String columnLabel = resultSetMetaData.getColumnLabel(i + 1); // 给t对象指定的columnName属性，赋值为columValue：通过反射 Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columValue); &#125; return t; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (resultSet != null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; if (preparedStatement != null) &#123; try &#123; preparedStatement.close(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; &#125; return null; &#125; // 模拟隔离级别测试：查询 public static void testTransactionSelect() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); // 获取当前连接的隔离级别，默认：TRANSACTION_REPEATABLE_READ System.out.println(connection.getTransactionIsolation()); // 设置数据库的隔离级别 connection.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED); // 取消自动提交数据 connection.setAutoCommit(false); Thread.sleep(5000); String sql = &quot;select user, password, balance from user_table where user = ?&quot;; User user = getInstance(connection, User.class, sql, &quot;CC&quot;); System.out.println(user); &#125; catch (Exception exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (connection != null) &#123; try &#123; connection.close(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; &#125; &#125; // 模拟隔离级别测试：更新 public static void testTransactionUpdate() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); // 取消自动提交数据 connection.setAutoCommit(false); String sql = &quot;update user_table set balance = ? where user = ?&quot;; System.out.println(&quot;开始修改&quot;); update(connection, sql, 5000, &quot;CC&quot;); Thread.sleep(15000); System.out.println(&quot;修改结束&quot;); connection.commit(); &#125; catch (Exception exception) &#123; exception.printStackTrace(); if (connection != null) &#123; try &#123; connection.rollback(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; &#125; finally &#123; if (connection != null) &#123; try &#123; connection.setAutoCommit(true); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; connection.close(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; Runnable update = () -&gt; &#123; try &#123; testTransactionUpdate(); &#125; catch (Exception exception) &#123; exception.printStackTrace(); &#125; &#125;; new Thread(update).start(); Runnable select = () -&gt; &#123; try &#123; testTransactionSelect(); &#125; catch (Exception exception) &#123; exception.printStackTrace(); &#125; &#125;; new Thread(select).start(); &#125;&#125; DAO 及相关实现类 DAO：Data Access Object，访问数据信息的类和接口，包括了对数据的 CRUD (Create、Retrival、Update、Delete) 操作，但不包含任何业务相关的信息。有时也称作 BaseDAO。 作用：为了实现功能的模块化，更有利于代码的维护和升级。 下面是尚硅谷 JavaWeb 阶段书城项目中 DAO 使用的体现： 层次结构： 基础实现 Page.java： 12345678910111213141516171819202122232425262728293031323334353637383940public class Page&lt;T&gt; &#123; public static final int PAGE_SIZE = 4; // 每页显示的记录数 private int pageNo; // 当前页 private List&lt;T&gt; list; // 每页查到的记录存放的集合 // private int totalPageNo; // 总页数，通过计算得到 private int totalRecord; // 总记录数，通过查询数据库得到 public List&lt;T&gt; getList() &#123; return list; &#125; public void setList(List&lt;T&gt; list) &#123; this.list = list; &#125; public int getPageNo() &#123; return pageNo; &#125; public void setPageNo(int pageNo) &#123; this.pageNo = pageNo; &#125; public int getTotalRecord() &#123; return totalRecord; &#125; public void setTotalRecord(int totalRecord) &#123; this.totalRecord = totalRecord; &#125; @Override public String toString() &#123; return &quot;Page&#123;&quot; + &quot;list=&quot; + list + &quot;, pageNo=&quot; + pageNo + &quot;, totalRecord=&quot; + totalRecord + &#x27;&#125;&#x27;; &#125;&#125; Customer.java： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Customer &#123; private int id; private String name; private String email; private Date birth; public Customer() &#123; &#125; public Customer(int id, String name, String email, Date birth) &#123; this.id = id; this.name = name; this.email = email; this.birth = birth; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; @Override public String toString() &#123; return &quot;Customer [id=&quot; + id + &quot;, name=&quot; + name + &quot;, email=&quot; + email + &quot;, birth=&quot; + birth + &quot;]&quot;; &#125;&#125; BaseDao.java： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/* * DAO: Data(base) Access Object * 封装了针对于数据表的通用的操作，声明为abstact类，不能被实例化 */public abstract class BaseDao &#123; // 通用的增删改操作---version 2.0(考虑上事务) public int update(Connection connection, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; return preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, preparedStatement); &#125; return 0; &#125; // 通用的查询操作，用于返回数据表中的一条记录---version 2.0(考虑上事务) public &lt;T&gt; T getInstance(Connection connection, Class&lt;T&gt; clazz, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); int columnCount = resultSetMetaData.getColumnCount(); if (resultSet.next()) &#123; T t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) &#123; Object columnValue = resultSet.getObject(i + 1); String columnLabel = resultSetMetaData.getColumnLabel(i + 1); Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columnValue); &#125; return t; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, preparedStatement, resultSet); &#125; return null; &#125; // 通用的查询操作，用于返回数据表中的多条记录构成的集合---version 2.0(考虑事务) public &lt;T&gt; List&lt;T&gt; getForList(Connection connection, Class&lt;T&gt; clazz, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); int columnCount = resultSetMetaData.getColumnCount(); ArrayList&lt;T&gt; list = new ArrayList&lt;T&gt;(); while (resultSet.next()) &#123; T t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) &#123; Object columnValue = resultSet.getObject(i + 1); String columnLabel = resultSetMetaData.getColumnLabel(i + 1); Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columnValue); &#125; list.add(t); &#125; return list; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, preparedStatement, resultSet); &#125; return null; &#125; // 用于查询特殊值的通用的方法，比如：select count(*) from test; public &lt;E&gt; E getValue(Connection connection, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); if (resultSet.next()) &#123; return (E) resultSet.getObject(1); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, preparedStatement, resultSet); &#125; return null; &#125;&#125; BaseDao 中未指定泛型，getInstance() 和 getForList() 两个方法是泛型方法，每个子类在实现这两个方法时，都需要传入一个对应的 Class 参数，实际上这个参数是可以省略的。 CustomerDao.java： 1234567891011121314151617181920212223242526272829303132333435363738394041/* * 此接口用于规范针对于customers表的常用操作 */public interface CustomerDao &#123; // 将customer对象添加到数据库中 void insert(Connection connection, Customer customer); // 针对指定的id，删除表中的一条记录 void deleteById(Connection connection, int id); // 针对内存中的customer对象，去修改数据表中指定的记录 void update(Connection connection, Customer customer); // 针对指定的id查询得到对应的customer对象 Customer getCustomerById(Connection connection, int id); // 查询表中的所有记录构成的集合 List&lt;Customer&gt; getAll(Connection connection); // 返回数据表中的数据的条目数 Long getCount(Connection connection); // 返回数据表中最大的生日 Date getMaxBirth(Connection connection); /** * 获取带分页的Customer信息 * * @param page：是只包含了用户输入的pageNo属性的Page对象 * @return 返回的Page对象是包含了所有属性的Page对象 */ Page&lt;Customer&gt; getPageCustomers(Connection connection, Page&lt;Customer&gt; page); /** * 获取带分页和生日范围的Customer信息 * * @param page：是只包含了用户输入的pageNo属性的Page对象 * @return 返回的Page对象是包含了所有属性的Page对象 */ Page&lt;Customer&gt; getPageCustomersByBirth(Connection connection, Page&lt;Customer&gt; page, Date minBirth, Date maxBirth);&#125; CustomerDaoImpl.java： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class CustomerDaoImpl extends BaseDao implements CustomerDao &#123; @Override public void insert(Connection connection, Customer customer) &#123; String sql = &quot;insert into customers(name, email, birth) values(?, ?, ?)&quot;; update(connection, sql, customer.getName(), customer.getEmail(), customer.getBirth()); &#125; @Override public void deleteById(Connection connection, int id) &#123; String sql = &quot;delete from customers where id = ?&quot;; update(connection, sql, id); &#125; @Override public void update(Connection connection, Customer customer) &#123; String sql = &quot;update customers set name = ?, email = ?, birth = ? where id = ?&quot;; update(connection, sql, customer.getName(), customer.getEmail(), customer.getBirth(), customer.getId()); &#125; @Override public Customer getCustomerById(Connection connection, int id) &#123; String sql = &quot;select id, name, email, birth from customers where id = ?&quot;; return getInstance(connection, Customer.class, sql, id); &#125; @Override public List&lt;Customer&gt; getAll(Connection connection) &#123; String sql = &quot;select id, name, email, birth from customers&quot;; return getForList(connection, Customer.class, sql); &#125; @Override public Long getCount(Connection connection) &#123; String sql = &quot;select count(*) from customers&quot;; return getValue(connection, sql); &#125; @Override public Date getMaxBirth(Connection connection) &#123; String sql = &quot;select max(birth) from customers&quot;; return getValue(connection, sql); &#125; @Override public Page&lt;Customer&gt; getPageCustomers(Connection connection, Page&lt;Customer&gt; page) &#123; // 获取数据库中Customer的总数 String sql = &quot;select count(*) from customers&quot;; // 调用BaseDao中获取一个单一值的方法 long totalRecord = (long) getValue(connection, sql); // 将总数设置都page对象中 page.setTotalRecord((int) totalRecord); // 获取当前页中的记录存放的List String sql2 = &quot;select id, name, email, birth from customers limit ?, ?&quot;; // 调用BaseDao中获取一个集合的方法 List&lt;Customer&gt; beanList = getForList(connection, sql2, (page.getPageNo() - 1) * Page.PAGE_SIZE, Page.PAGE_SIZE); // 将这个List设置到page对象中 page.setList(beanList); return page; &#125; @Override public Page&lt;Customer&gt; getPageCustomersByBirth(Connection connection, Page&lt;Customer&gt; page, Date minBirth, Date maxBirth) &#123; // 获取数据库中Customer的总数 String sql = &quot;select count(*) from customers where birth between ? and ?&quot;; // 调用BaseDao中获取一个单一值的方法 long totalRecord = getValue(connection, sql, minBirth, maxBirth); // 将总数设置都page对象中 page.setTotalRecord((int) totalRecord); // 获取当前页中的记录存放的List String sql2 = &quot;select id, name, email, birth from customers where birth between ? and ? limit ?, ?&quot;; // 调用BaseDao中获取一个集合的方法 List&lt;Customer&gt; beanList = getForList(connection, sql2, minBirth, maxBirth, (page.getPageNo() - 1) * Page.PAGE_SIZE, Page.PAGE_SIZE); // 将这个List设置到page对象中 page.setList(beanList); return page; &#125;&#125; CustomerDaoImpl 实现父类 BaseDao 的 getInstance() 和 getForList() 方法，需要传入 Customer.class 参数。 CustomerDaoImplTest.java： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138class CustomerDaoImplTest &#123; private final CustomerDaoImpl dao = new CustomerDaoImpl(); @Test public void testInsert() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); Customer customer = new Customer(1, &quot;于小飞&quot;, &quot;xiaofei@126.com&quot;, new Date(43534646435L)); dao.insert(connection, customer); System.out.println(&quot;添加成功&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125; @Test public void testDeleteById() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); dao.deleteById(connection, 22); System.out.println(&quot;删除成功&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125; @Test public void testUpdate() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); Customer customer = new Customer(18, &quot;贝多芬&quot;, &quot;beiduofen@126.com&quot;, new Date(453465656L)); dao.update(connection, customer); System.out.println(&quot;修改成功&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125; @Test public void testGetCustomerById() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); Customer customer = dao.getCustomerById(connection, 20); System.out.println(customer); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125; @Test public void testGetAll() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); List&lt;Customer&gt; list = dao.getAll(connection); list.forEach(System.out::println); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125; @Test public void testGetCount() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); Long count = dao.getCount(connection); System.out.println(&quot;表中的记录数为：&quot; + count); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125; @Test public void testGetMaxBirth() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); Date maxBirth = dao.getMaxBirth(connection); System.out.println(&quot;最大的生日为：&quot; + maxBirth); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125; @Test public void testGetPageCustomers() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); Page&lt;Customer&gt; page = new Page&lt;&gt;(); page.setPageNo(2);// 查询第二页 page = dao.getPageCustomers(connection, page); System.out.println(page); &#125; catch (Exception exception) &#123; exception.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125; @Test public void testGetPageCustomersByBirth() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getConnection(); Page&lt;Customer&gt; page = new Page&lt;&gt;(); page.setPageNo(1);// 查询第二页 SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); java.util.Date minDate = sdf.parse(&quot;1984-01-01&quot;); java.util.Date maxDate = sdf.parse(&quot;2010-12-31&quot;); page = dao.getPageCustomersByBirth(connection, page, new Date(minDate.getTime()), new Date(maxDate.getTime())); System.out.println(page); &#125; catch (Exception exception) &#123; exception.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(connection, null); &#125; &#125;&#125; 升级 在 BaseDao 中添加泛型，同步修改 CustomerDaoImpl 的相应代码： BaseDao.java： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134/* * DAO: Data(base) Access Object * 封装了针对于数据表的通用的操作，声明为abstact类，不能被实例化 */public abstract class BaseDao&lt;T&gt; &#123; // 在构造器中，或者代码块中赋值，clazz就是当前类的父类的泛型参数 // 比如当前类为CustomerDaoImpl，clazz就是它的父类BaseDao&lt;Customer&gt;的泛型参数 private Class&lt;T&gt; clazz; // 方式一：获取T的Class对象，获取泛型的类型，泛型是在被子类继承时才确定 public BaseDao() &#123; // 获取子类的类型 Class clazz = this.getClass(); // 获取父类的类型 // getGenericSuperclass()用来获取当前类的父类的类型 // ParameterizedType表示的是带泛型的类型 ParameterizedType parameterizedType = (ParameterizedType) clazz.getGenericSuperclass(); // 获取具体的泛型类型 getActualTypeArguments获取具体的泛型的类型 // 这个方法会返回一个Type的数组 Type[] types = parameterizedType.getActualTypeArguments(); // 获取具体的泛型的类型· this.type = (Class&lt;T&gt;) types[0]; &#125; // 方式二： /*&#123; Type genericSuperclass = this.getClass().getGenericSuperclass(); ParameterizedType parameterizedType = (ParameterizedType) genericSuperclass; Type[] actualTypeArguments = parameterizedType.getActualTypeArguments();// 获取了父类的泛型参数 clazz = (Class&lt;T&gt;) actualTypeArguments[0];// 泛型的第一个参数 &#125;*/ // 通用的增删改操作---version 2.0(考虑上事务) public int update(Connection connection, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; return preparedStatement.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, preparedStatement); &#125; return 0; &#125; // 通用的查询操作，用于返回数据表中的一条记录---version 2.0(考虑上事务) public T getInstance(Connection connection, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); int columnCount = resultSetMetaData.getColumnCount(); if (resultSet.next()) &#123; T t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) &#123; Object columnValue = resultSet.getObject(i + 1); String columnLabel = resultSetMetaData.getColumnLabel(i + 1); Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columnValue); &#125; return t; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, preparedStatement, resultSet); &#125; return null; &#125; // 通用的查询操作，用于返回数据表中的多条记录构成的集合---version 2.0(考虑事务) public List&lt;T&gt; getForList(Connection connection, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); int columnCount = resultSetMetaData.getColumnCount(); ArrayList&lt;T&gt; list = new ArrayList&lt;T&gt;(); while (resultSet.next()) &#123; T t = clazz.newInstance(); for (int i = 0; i &lt; columnCount; i++) &#123; Object columnValue = resultSet.getObject(i + 1); String columnLabel = resultSetMetaData.getColumnLabel(i + 1); Field field = clazz.getDeclaredField(columnLabel); field.setAccessible(true); field.set(t, columnValue); &#125; list.add(t); &#125; return list; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, preparedStatement, resultSet); &#125; return null; &#125; // 用于查询特殊值的通用的方法，比如：select count(*) from test; public &lt;E&gt; E getValue(Connection connection, String sql, Object... args) &#123; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; preparedStatement = connection.prepareStatement(sql); for (int i = 0; i &lt; args.length; i++) &#123; preparedStatement.setObject(i + 1, args[i]); &#125; resultSet = preparedStatement.executeQuery(); if (resultSet.next()) &#123; return (E) resultSet.getObject(1); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResource(null, preparedStatement, resultSet); &#125; return null; &#125;&#125; BaseDao 中添加了泛型参数，每一个子类在继承时，直接添加对应的 Class，这样在方法中，就不需要再单独传入 Class 参数。 CustomerDaoImpl.java： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class CustomerDaoImpl extends BaseDao&lt;Customer&gt; implements CustomerDao &#123; @Override public void insert(Connection connection, Customer customer) &#123; String sql = &quot;insert into customers(name, email, birth) values(?, ?, ?)&quot;; update(connection, sql, customer.getName(), customer.getEmail(), customer.getBirth()); &#125; @Override public void deleteById(Connection connection, int id) &#123; String sql = &quot;delete from customers where id = ?&quot;; update(connection, sql, id); &#125; @Override public void update(Connection connection, Customer customer) &#123; String sql = &quot;update customers set name = ?, email = ?, birth = ? where id = ?&quot;; update(connection, sql, customer.getName(), customer.getEmail(), customer.getBirth(), customer.getId()); &#125; @Override public Customer getCustomerById(Connection connection, int id) &#123; String sql = &quot;select id, name, email, birth from customers where id = ?&quot;; return getInstance(connection, sql, id); &#125; @Override public List&lt;Customer&gt; getAll(Connection connection) &#123; String sql = &quot;select id, name, email, birth from customers&quot;; return getForList(connection, sql); &#125; @Override public Long getCount(Connection connection) &#123; String sql = &quot;select count(*) from customers&quot;; return getValue(connection, sql); &#125; @Override public Date getMaxBirth(Connection connection) &#123; String sql = &quot;select max(birth) from customers&quot;; return getValue(connection, sql); &#125;&#125; 数据库连接池JDBC 数据库连接池的必要性 在使用开发基于数据库的 Web 程序时，传统的模式基本是按以下步骤： 在主程序 (如 servlet、beans) 中建立数据库连接； 进行 SQL 操作； 断开数据库连接。 这种模式开发，存在的问题: 普通的 JDBC 数据库连接使用 DriverManager 来获取，每次向数据库建立连接的时候都要将 Connection 加载到内存中，再验证用户名和密码 (需要耗费 0.05s～1s 的时间)。当需要数据库连接的时候，就向数据库要求一个，执行完成后再断开连接。这样的方式将会消耗大量的资源和时间。数据库的连接资源并没有得到很好的重复利用。若同时有几百人甚至几千人在线，频繁的进行数据库连接操作将占用很多的系统资源，严重的甚至会造成服务器的崩溃。 对于每一次数据库连接，使用完后都得断开。否则，如果程序出现异常而未能关闭，将会导致数据库系统中的内存泄漏，最终将导致重启数据库。 这种开发不能控制被创建的连接对象数，系统资源会被毫无顾及的分配出去，如连接过多，也可能导致内存泄漏，服务器崩溃。 数据库连接池技术 为解决传统开发中的数据库连接问题，可以采用数据库连接池技术。 数据库连接池的基本思想：就是为数据库连接建立一个”缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从”缓冲池”中取出一个，使用完毕之后再放回去。 数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。 数据库连接池在初始化时，会创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。 工作原理： 数据库连接池技术的优点： 资源重用 由于数据库连接得以重用，避免了频繁创建、释放连接所引起的大量性能开销。在减少系统消耗的基础上，另一方面也增加了系统运行环境的平稳性。 更快的系统反应速度 数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于连接池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接，避免了数据库连接初始化和释放过程的时间开销，从而减少了系统的响应时间。 新的资源分配手段 对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接池的配置，实现某一应用最大可用数据库连接数的限制，避免某一应用独占所有的数据库资源。 统一的连接管理，避免数据库连接泄漏 在较为完善的数据库连接池实现中，可根据预先的占用超时设定，强制回收被占用连接，从而避免了常规数据库连接操作中可能出现的资源泄露。 多种开源的数据库连接池 JDBC 的数据库连接池使用 javax.sql.DataSource 来表示，DataSource 是一个接口，该接口通常由服务器 (如 Weblogic，WebSphere，Tomcat 等) 提供实现，也有一些开源组织提供实现： DBCP：是 Apache 提供的数据库连接池。Tomcat 服务器自带 DBCP 数据库连接池。速度相对 C3P0 较快，但因自身存在 BUG，Hibernate3 已不再提供支持。 C3P0：是一个开源组织提供的一个数据库连接池，速度相对较慢，稳定性还可以。Hibernate 官方推荐使用。 Proxool：是 sourceforge 下的一个开源项目数据库连接池，有监控连接池状态的功能，稳定性较 C3P0 差一点。 BoneCP：是一个开源组织提供的数据库连接池，速度快。 Druid：是阿里提供的数据库连接池，据说是集DBCP 、C3P0 、Proxool 优点于一身的数据库连接池，但是速度不确定是否有BoneCP快 DataSource 通常被称为数据源，它包含连接池和连接池管理两个部分，习惯上也经常把 DataSource 称为连接池。 DataSource 用来取代 DriverManager 来获取 Connection，获取速度快，同时可以大幅度提高数据库访问速度。 特别注意： 数据源和数据库连接不同，数据源无需创建多个，它是产生数据库连接的工厂，因此，整个应用只需要一个数据源即可。 当数据库访问结束后，程序还是像以前一样关闭数据库连接：connection.close();，但 connection.close() 并没有关闭数据库的物理连接，它仅仅把数据库连接释放，将其归还给了数据库连接池。 C3P0 数据库连接池 Maven 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt;&lt;/dependency&gt; 获取连接方式一 123456789101112131415161718192021222324252627282930public class C3P0Test &#123; // 方式一：不推荐 private ComboPooledDataSource cpds = new ComboPooledDataSource(); &#123; try &#123; // 创建c3p0数据库连接池 cpds.setDriverClass(&quot;com.mysql.cj.jdbc.Driver&quot;); cpds.setJdbcUrl(&quot;jdbc:mysql://localhost:3306/test&quot;); cpds.setUser(&quot;root&quot;); cpds.setPassword(&quot;abc123&quot;); // 通过设置相关的参数，对数据库连接池进行管理 cpds.setInitialPoolSize(10);// 设置初始时数据库连接池中的连接数 // ...... &#125; catch (PropertyVetoException e) &#123; e.printStackTrace(); &#125; &#125; public Connection getC3P0Connection() throws Exception &#123; Connection connection = cpds.getConnection(); System.out.println(connection); // 销毁c3p0数据库连接池的方法，了解，不要轻易使用 // DataSources.destroy(cpds); return connection; &#125;&#125; 获取连接方式二 12345678910public class C3P0Test &#123; // 方式二：使用配置文件，推荐 private final DataSource cpds = new ComboPooledDataSource(&quot;helloc3p0&quot;); public Connection getC3P0Connection() throws SQLException &#123; Connection connection = cpds.getConnection(); System.out.println(connection); return connection; &#125;&#125; 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;c3p0-config&gt; &lt;named-config name=&quot;helloc3p0&quot;&gt; &lt;!-- 获取连接的4个基本信息 --&gt; &lt;property name=&quot;driverClass&quot;&gt;com.mysql.cj.jdbc.Driver&lt;/property&gt; &lt;!-- 如果连接本地3306端口，可以简写为：jdbc:mysql:///test --&gt; &lt;property name=&quot;jdbcUrl&quot;&gt;jdbc:mysql://localhost:3306/test&lt;/property&gt; &lt;property name=&quot;user&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;password&quot;&gt;abc123&lt;/property&gt; &lt;!-- 涉及到数据库连接池的管理的常用相关属性的设置 --&gt; &lt;!-- 若数据库中连接数不足时，一次向数据库服务器申请多少个连接 --&gt; &lt;property name=&quot;acquireIncrement&quot;&gt;5&lt;/property&gt; &lt;!-- 初始化数据库连接池时连接的数量 --&gt; &lt;property name=&quot;initialPoolSize&quot;&gt;5&lt;/property&gt; &lt;!-- 数据库连接池中的最小的数据库连接数 --&gt; &lt;property name=&quot;minPoolSize&quot;&gt;5&lt;/property&gt; &lt;!-- 数据库连接池中的最大的数据库连接数 --&gt; &lt;property name=&quot;maxPoolSize&quot;&gt;10&lt;/property&gt; &lt;!-- C3P0数据库连接池可以维护的Statement的个数 --&gt; &lt;property name=&quot;maxStatements&quot;&gt;20&lt;/property&gt; &lt;!-- 每个连接同时可以使用的Statement对象的个数 --&gt; &lt;property name=&quot;maxStatementsPerConnection&quot;&gt;5&lt;/property&gt; &lt;/named-config&gt;&lt;/c3p0-config&gt; c3p0-config.xml 配置文件，在 resourcs 目录下新建。 DBCP 数据库连接池 DBCP 是 Apache 软件基金组织下的开源连接池实现，该连接池依赖该组织下的另一个开源系统：Common-pool。如需使用该连接池实现，应在系统中增加如下两个 jar 文件： Commons-dbcp.jar：连接池的实现。 Commons-pool.jar：连接池实现的依赖库。 Tomcat 的连接池正是采用该连接池来实现的。该数据库连接池既可以与应用服务器整合使用，也可由应用程序独立使用。 配置属性说明： 属性 默认值 说明 initialSize 0 连接池启动时创建的初始化连接数量 maxActive 8 连接池中可同时连接的最大的连接数 maxIdle 8 连接池中最大的空闲的连接数，超过的空闲连接将被释放，如果设置为负数表示不限制 minIdle 0 连接池中最小的空闲的连接数，低于这个数量会被创建新的连接。该参数越接近maxIdle，性能越好，因为连接的创建和销毁，都是需要消耗资源的；但是不能太大。 maxWait 无限制 最大等待时间，当没有可用连接时，连接池等待连接释放的最大时间，超过该时间限制会抛出异常，如果设置-1表示无限等待 poolPreparedStatements false 开启池的Statement是否prepared maxOpenPreparedStatements 无限制 开启池的prepared 后的同时最大连接数 minEvictableIdleTimeMillis 连接池中连接，在时间段内一直空闲， 被逐出连接池的时间 removeAbandonedTimeout 300 超过时间限制，回收没有用(废弃)的连接 removeAbandoned false 超过removeAbandonedTimeout时间后，是否进 行没用连接（废弃）的回收 Maven 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;commons-dbcp&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt;&lt;/dependency&gt; 获取连接方式一 12345678910111213141516171819202122232425262728public class DBCPTest &#123; // 方式一：不推荐 private final BasicDataSource source = new BasicDataSource(); &#123; // 设置基本信息 source.setDriverClassName(&quot;com.mysql.cj.jdbc.Driver&quot;); source.setUrl(&quot;jdbc:mysql:///test&quot;); source.setUsername(&quot;root&quot;); source.setPassword(&quot;abc123&quot;); // 设置其他涉及数据库连接池管理的相关属性： source.setInitialSize(10); source.setMaxActive(10); // ...... &#125; public Connection getDbcpConnection() &#123; Connection connection = null; try &#123; connection = source.getConnection(); System.out.println(connection); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; return connection; &#125;&#125; 获取连接方式二 123456789101112131415161718192021222324public class DBCPTest &#123; // 方式二：使用配置文件，推荐 private DataSource source = null; &#123; try &#123; Properties properties = new Properties(); // 方式1： // FileInputStream is = new FileInputStream(new File(&quot;dbcp.properties&quot;)); // 方式2： InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;dbcp.properties&quot;); properties.load(is); source = BasicDataSourceFactory.createDataSource(properties); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public Connection getDbcpConnection() throws Exception &#123; Connection connection = source.getConnection(); System.out.println(connection); return connection; &#125;&#125; 12345driverClassName=com.mysql.cj.jdbc.Driverurl=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=true&amp;useServerPrepStmts=falseusername=rootpassword=abc123initialSize=10 dbcp.properties 配置文件，在 resourcs 目录下新建。 Druid (德鲁伊) 数据库连接池 Druid是阿里巴巴开源平台上一个数据库连接池实现，它结合了C3P0、DBCP、Proxool等DB池的优点，同时加入了日志监控，可以很好的监控DB池连接和SQL的执行情况，可以说是针对监控而生的DB连接池，可以说是目前最好的连接池之一。 详细配置参数： 配置 缺省 说明 name 配置这个属性的意义在于，如果存在多个数据源，监控的时候可以通过名字来区分开来。 如果没有配置，将会生成一个名字，格式是：”DataSource-” + System.identityHashCode(this) url 连接数据库的url，不同数据库不一样。例如：mysql : jdbc:mysql://10.20.153.104:3306/druid2 oracle : jdbc:oracle:thin:@10.20.149.85:1521:ocnauto username 连接数据库的用户名 password 连接数据库的密码。如果你不希望密码直接写在配置文件中，可以使用ConfigFilter。详细看这里：https://github.com/alibaba/druid/wiki/%E4%BD%BF%E7%94%A8ConfigFilter driverClassName 根据url自动识别 这一项可配可不配，如果不配置druid会根据url自动识别dbType，然后选择相应的driverClassName(建议配置下) initialSize 0 初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时 maxActive 8 最大连接池数量 maxIdle 8 已经不再使用，配置了也没效果 minIdle 最小连接池数量 maxWait 获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。 poolPreparedStatements false 是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql下建议关闭。 maxOpenPreparedStatements -1 要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题，可以把这个数值配置大一些，比如说100 validationQuery 用来检测连接是否有效的sql，要求是一个查询语句。如果validationQuery为null，testOnBorrow、testOnReturn、testWhileIdle都不会其作用。 testOnBorrow true 申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 testOnReturn false 归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能 testWhileIdle false 建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。 timeBetweenEvictionRunsMillis 有两个含义： 1)Destroy线程会检测连接的间隔时间2)testWhileIdle的判断依据，详细看testWhileIdle属性的说明 numTestsPerEvictionRun 不再使用，一个DruidDataSource只支持一个EvictionRun minEvictableIdleTimeMillis connectionInitSqls 物理连接初始化的时候执行的sql exceptionSorter 根据dbType自动识别 当数据库抛出一些不可恢复的异常时，抛弃连接 filters 属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有：监控统计用的filter：stat；日志用的filter：log4j；防御sql注入的filter：wall。 proxyFilters 类型是List，如果同时配置了filters和proxyFilters，是组合关系，并非替换关系 Maven 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt;&lt;/dependency&gt; 获取连接方式一 1234567891011121314151617181920212223public class DruidTest &#123; // 方式一：不推荐 private DruidDataSource dataSource = null; &#123; // 设置基本信息 dataSource = new DruidDataSource(); dataSource.setDriverClassName(&quot;com.mysql.cj.jdbc.Driver&quot;); dataSource.setUrl(&quot;jdbc:mysql:///test&quot;); dataSource.setUsername(&quot;root&quot;); dataSource.setPassword(&quot;abc123&quot;); // 设置其他涉及数据库连接池管理的相关属性： dataSource.setInitialSize(10); // ...... &#125; public Connection getDruidConnection() throws Exception &#123; Connection connection = dataSource.getConnection(); System.out.println(connection); return connection; &#125;&#125; 获取连接方式二 123456789101112131415161718192021public class DruidTest &#123; // 方式二：使用配置文件，推荐 private DataSource dataSource = null; &#123; try &#123; Properties properties = new Properties(); InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;druid.properties&quot;); properties.load(is); dataSource = DruidDataSourceFactory.createDataSource(properties); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public Connection getDruidConnection() throws Exception &#123; Connection connection = dataSource.getConnection(); System.out.println(connection); return connection; &#125;&#125; 12345678driverClassName=com.mysql.cj.jdbc.Driverurl=jdbc:mysql://localhost:3306/test?rewriteBatchedStatements=trueusername=rootpassword=abc123initialSize=10maxActive=20maxWait=1000filters=wall druid.properties 配置文件，在 resourcs 目录下新建。 Apache-DBUtils 实现 CRUD 操作Apache-DBUtils 简介 commons-dbutils 是 Apache 组织提供的一个开源 JDBC工具类库，它是对 JDBC 的简单封装，学习成本极低，并且使用 DBUtils 能极大简化 JDBC 编码的工作量，同时也不会影响程序的性能。 API 介绍： org.apache.commons.dbutils.DbUtils 工具类。 org.apache.commons.dbutils.QueryRunner 提供数据库操作的一系列重载的 update() 和 query() 操作。 org.apache.commons.dbutils.ResultSetHandler 此接口用于处理数据库查询操作得到的结果集。不同的结果集的情形，由其不同的子类来实现。 Maven 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;commons-dbutils&lt;/groupId&gt; &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt; &lt;version&gt;1.7&lt;/version&gt;&lt;/dependency&gt; 主要 API 的使用DbUtils DbUtils ：提供如关闭连接、装载 JDBC 驱动程序等常规工作的工具类，里面的所有方法都是静态的。 主要方法如下： public static void close(…) throws java.sql.SQLException： DbUtils 类提供了三个重载的关闭方法。这些方法检查所提供的参数是不是 null，如果不是的话，它们就关闭 Connection、Statement 和 ResultSet。 public static void closeQuietly(…)：这一类方法不仅能在 Connection、Statement 和 ResultSet 为 null 情况下避免关闭，还能隐藏一些在程序中抛出的 SQLEeception。 public static void commitAndClose(Connection connection)throws SQLException： 用来提交连接的事务，然后关闭连接。 public static void commitAndCloseQuietly(Connection connection)： 用来提交连接，然后关闭连接，并且在关闭连接时不抛出 SQL 异常。 public static void rollback(Connection connection)throws SQLException：允许 connection 为 null，因为方法内部做了判断。 public static void rollbackAndClose(Connection connection)throws SQLException rollbackAndCloseQuietly(Connection connection) public static boolean loadDriver(java.lang.String driverClassName)：这一方装载并注册 JDBC 驱动程序，如果成功就返回 true。使用该方法，你不需要捕捉 ClassNotFoundException 异常。 QueryRunner类 该类简单化了 SQL 查询，它与 ResultSetHandler 组合在一起使用可以完成大部分的数据库操作，能够大大减少编码量。 QueryRunner 类提供了两个构造器： 默认的构造器。 需要一个 javax.sql.DataSource 来作参数的构造器。 QueryRunner类的主要方法： 更新 public int update(Connection connection, String sql, Object... params) throws SQLException：用来执行一个更新 (插入、更新或删除) 操作。 插入 public &lt;T&gt; T insert(Connection connection, String sql, ResultSetHandler&lt;T&gt; rsh, Object... params) throws SQLException：只支持 INSERT 语句，其中，rsh：The handler used to create the result object from the ResultSet of auto-generated keys。返回值：An object generated by the handler，即自动生成的键值。 批处理 public int[] batch(Connection connection, String sql, Object[][] params)throws SQLException： INSERT，UPDATE，DELETE 语句。 public &lt;T&gt; T insertBatch(Connection connection, String sql, ResultSetHandler&lt;T&gt; rsh, Object[][] params)throws SQLException：只支持 INSERT 语句。 查询 public Object query(Connection connection, String sql, ResultSetHandler rsh, Object... params) throws SQLException：执行一个查询操作，在这个查询中，对象数组中的每个元素值被用来作为查询语句的置换参数。该方法会自行处理 PreparedStatement 和 ResultSet 的创建和关闭。 ResultSetHandler 接口及实现类 该接口用于处理 java.sql.ResultSet，将数据按要求转换为另一种形式。 ResultSetHandler 接口提供了一个单独的方法：Object handle (java.sql.ResultSet rs)。 接口的主要实现类： ArrayHandler：把结果集中的第一行数据转成对象数组。 ArrayListHandler：把结果集中的每一行数据都转成一个数组，再存放到 List 中。 BeanHandler：将结果集中的第一行数据封装到一个对应的 JavaBean 实例中。 BeanListHandler：将结果集中的每一行数据都封装到一个对应的 JavaBean 实例中，再存放到 List 中。 ColumnListHandler：将结果集中某一列的数据存放到 List 中。 KeyedHandler(name)：将结果集中的每一行数据都封装到一个 Map 里，再把这些 Map 再存到一个 Map 里，其 key 为指定的 key。 MapHandler：将结果集中的第一行数据封装到一个 Map 里，key 是列名，value 是对应的值。 MapListHandler：将结果集中的每一行数据都封装到一个 Map 里，再存放到 List 中。 ScalarHandler：查询单个值对象。 JDBC 总结 pom.xml： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.xisun&lt;/groupId&gt; &lt;artifactId&gt;xisun-jdbc&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt; &lt;version&gt;5.6.2&lt;/version&gt; &lt;scope&gt;chemicalStructure&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-dbcp&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-dbutils&lt;/groupId&gt; &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt; &lt;version&gt;1.7&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; JDBCUtils.java： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155public class JDBCUtils &#123; /** * @Description 常规方式获取数据库的连接 */ public static Connection getCommonConnection() throws Exception &#123; // 1.读取配置文件中的4个基本信息 Properties properties = new Properties(); InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;); properties.load(is); String driverClass = properties.getProperty(&quot;driverClass&quot;); String url = properties.getProperty(&quot;url&quot;); String user = properties.getProperty(&quot;user&quot;); String password = properties.getProperty(&quot;password&quot;); // 2.加载驱动 Class.forName(driverClass); // 3.获取连接 return DriverManager.getConnection(url, user, password); &#125; /** * @Description 使用C3P0数据库连接池技术获取数据库连接 */ // 创建一个C3P0数据库连接池，数据库连接池只需提供一个即可 private static final ComboPooledDataSource CPDS = new ComboPooledDataSource(&quot;hellc3p0&quot;); public static Connection getC3P0Connection() throws SQLException &#123; return CPDS.getConnection(); &#125; /** * @Description 使用DBCP数据库连接池技术获取数据库连接 */ // 创建一个DBCP数据库连接池，数据库连接池只需提供一个即可 private static DataSource dbcpSource; static &#123; try &#123; Properties properties = new Properties(); InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;dbcp.properties&quot;); properties.load(is); dbcpSource = BasicDataSourceFactory.createDataSource(properties); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static Connection getDbcpConnection() throws Exception &#123; return dbcpSource.getConnection(); &#125; /** * @Description 使用Druid数据库连接池技术获取数据库连接 */ // 创建一个Druid数据库连接池，数据库连接池只需提供一个即可 private static DataSource druidSource; static &#123; try &#123; Properties properties = new Properties(); InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;druid.properties&quot;); properties.load(is); druidSource = DruidDataSourceFactory.createDataSource(properties); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static Connection getDruidConnection() throws SQLException &#123; return druidSource.getConnection(); &#125; /** * @Description 连接回滚 */ public static void rollBackConnection(Connection connection) &#123; if (connection != null) &#123; try &#123; connection.rollback(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; // DbUtils工具类提供的回滚操作 /*DbUtils.rollback(connection); DbUtils.rollbackAndClose(connection); DbUtils.rollbackAndCloseQuietly(connection);*/ &#125; /** * @Description 常规方式，实现资源的关闭 */ public static void closeResource(Connection connection, Statement statement, ResultSet resultSet) &#123; if (resultSet != null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException throwables) &#123; throwables.printStackTrace(); &#125; &#125; if (statement != null) &#123; try &#123; statement.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (connection != null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * @Description 使用DbUtils工具类，实现资源的关闭 */ public static void closeResourceByDbUtils(Connection connection, Statement statement, ResultSet resultSet) &#123; // 方式一 /*try &#123; DbUtils.close(resultSet); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; DbUtils.close(statement); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; DbUtils.close(connection); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;*/ // 方式二 DbUtils.closeQuietly(resultSet); DbUtils.closeQuietly(statement); DbUtils.closeQuietly(connection); &#125;&#125; resources： jdbc.properties： 1234driverClass=com.mysql.cj.jdbc.Driverurl=jdbc:mysql://localhost:3306/testuser=rootpassword=abc123 c3p0-config.xml： 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;c3p0-config&gt; &lt;named-config name=&quot;helloc3p0&quot;&gt; &lt;!-- 获取连接的4个基本信息 --&gt; &lt;property name=&quot;driverClass&quot;&gt;com.mysql.cj.jdbc.Driver&lt;/property&gt; &lt;!-- 如果连接本地3306端口，可以简写为：jdbc:mysql:///test --&gt; &lt;property name=&quot;jdbcUrl&quot;&gt;jdbc:mysql://localhost:3306/test&lt;/property&gt; &lt;property name=&quot;user&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;password&quot;&gt;abc123&lt;/property&gt; &lt;!-- 涉及到数据库连接池的管理的常用相关属性的设置 --&gt; &lt;!-- 若数据库中连接数不足时，一次向数据库服务器申请多少个连接 --&gt; &lt;property name=&quot;acquireIncrement&quot;&gt;5&lt;/property&gt; &lt;!-- 初始化数据库连接池时连接的数量 --&gt; &lt;property name=&quot;initialPoolSize&quot;&gt;5&lt;/property&gt; &lt;!-- 数据库连接池中的最小的数据库连接数 --&gt; &lt;property name=&quot;minPoolSize&quot;&gt;5&lt;/property&gt; &lt;!-- 数据库连接池中的最大的数据库连接数 --&gt; &lt;property name=&quot;maxPoolSize&quot;&gt;10&lt;/property&gt; &lt;!-- C3P0数据库连接池可以维护的Statement的个数 --&gt; &lt;property name=&quot;maxStatements&quot;&gt;20&lt;/property&gt; &lt;!-- 每个连接同时可以使用的Statement对象的个数 --&gt; &lt;property name=&quot;maxStatementsPerConnection&quot;&gt;5&lt;/property&gt; &lt;/named-config&gt;&lt;/c3p0-config&gt; dbcp.properties： 12345driverClassName=com.mysql.cj.jdbc.Driverurl=jdbc:mysql:///test?rewriteBatchedStatements=true&amp;useServerPrepStmts=falseusername=rootpassword=abc123initialSize=10 druid.properties： 12345678driverClassName=com.mysql.cj.jdbc.Driverurl=jdbc:mysql:///test?rewriteBatchedStatements=trueusername=rootpassword=abc123initialSize=10maxActive=20maxWait=1000filters=wall ?rewriteBatchedStatements=true：开启 MySQL 批处理的支持。 QueryRunnerTest.java： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282public class QueryRunnerTest &#123; /** * @Description 测试插入 */ public void testInsert() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;insert into customers(name, email, birth) values(?, ?, ?)&quot;; int insertCount = runner.update(connection, sql, &quot;cc&quot;, &quot;cc@126.com&quot;, &quot;1997-09-08&quot;); System.out.println(&quot;添加了&quot; + insertCount + &quot;条记录&quot;); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 测试删除 */ public void testDelete() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;delete from customers where id &lt; ?&quot;; int deleteCount = runner.update(connection, sql, 26); System.out.println(&quot;删除了&quot; + deleteCount + &quot;条记录&quot;); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 测试更新 */ public void testUpdate() &#123; Connection connection = null; FileInputStream fis = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;update customers set name = ?, email = ?, birth = ?, photo = ? where id = ?&quot;; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); java.util.Date date = sdf.parse(&quot;1997-05-21&quot;); // 处理BLOB类型字段数据 fis = new FileInputStream(new File(&quot;E:/test.png&quot;)); int updateCount = runner.update(connection, sql, &quot;zhangsan&quot;, &quot;zs@163.com&quot;, new Date(date.getTime()), fis, 26); System.out.println(&quot;更新了&quot; + updateCount + &quot;条记录&quot;); &#125; catch (SQLException | ParseException | FileNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 测试查询 * BeanHander：ResultSetHandler接口的实现类，用于封装表中的一条记录。 */ public void testQueryForBean() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;select id, name, email, birth from customers where id = ?&quot;; BeanHandler&lt;Customer&gt; handler = new BeanHandler&lt;&gt;(Customer.class); Customer customer = runner.query(connection, sql, handler, 26); System.out.println(customer); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 测试查询 * BeanListHandler：ResultSetHandler接口的实现类，用于封装表中的多条记录构成的集合。 */ public void testQueryForBeanList() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;select id, name, email, birth from customers where id &lt; ?&quot;; BeanListHandler&lt;Customer&gt; handler = new BeanListHandler&lt;&gt;(Customer.class); List&lt;Customer&gt; list = runner.query(connection, sql, handler, 28); list.forEach(System.out::println); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 测试查询 * MapHander：ResultSetHandler接口的实现类，对应表中的一条记录。 * 将字段及相应字段的值作为Map中的key和value */ public void testQueryForMap() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;select id, name, email, birth from customers where id = ?&quot;; MapHandler handler = new MapHandler(); Map&lt;String, Object&gt; map = runner.query(connection, sql, handler, 26); System.out.println(map); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 测试查询 * MapListHander：ResultSetHandler接口的实现类，对应表中的多条记录。 * 将字段及相应字段的值作为Map中的key和value，再将每一个Map添加到List中 */ public void testQueryForMapList() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;select id, name, email, birth from customers where id &lt; ?&quot;; MapListHandler handler = new MapListHandler(); List&lt;Map&lt;String, Object&gt;&gt; list = runner.query(connection, sql, handler, 28); list.forEach(System.out::println); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 测试查询 * ScalarHandler：用于查询特殊值。 * 类似于最大的，最小的，平均的，总和，个数相关的数据。 */ public void testQueryForSpecialValue1() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;select count(*) from customers&quot;; ScalarHandler&lt;Long&gt; handler = new ScalarHandler&lt;&gt;(); Long count = runner.query(connection, sql, handler); System.out.println(count); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; public void testQueryForSpecialValue2() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;select max(birth) from customers&quot;; ScalarHandler&lt;Date&gt; handler = new ScalarHandler&lt;&gt;(); Date maxBirth = runner.query(connection, sql, handler); System.out.println(maxBirth); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 自定义ResultSetHandler的实现类 */ public void testQueryPersonal() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); QueryRunner runner = new QueryRunner(); String sql = &quot;select id, name, email, birth, photo from customers where id = ?&quot;; // 匿名内部类 ResultSetHandler&lt;Customer&gt; handler = new ResultSetHandler&lt;Customer&gt;() &#123; @Override public Customer handle(ResultSet resultSet) throws SQLException &#123; if (resultSet.next()) &#123; int id = resultSet.getInt(&quot;id&quot;); String name = resultSet.getString(&quot;name&quot;); String email = resultSet.getString(&quot;email&quot;); Date birth = resultSet.getDate(&quot;birth&quot;); // 处理BLOB类型字段 InputStream is = null; FileOutputStream fos = null; try &#123; Blob photo = resultSet.getBlob(&quot;photo&quot;); is = photo.getBinaryStream(); fos = new FileOutputStream(&quot;test2.jpg&quot;); byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; if (is != null) &#123; try &#123; is.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; return new Customer(id, name, email, birth); &#125; return null; &#125; &#125;; Customer customer = runner.query(connection, sql, handler, 26); System.out.println(customer); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125; /** * @Description 测试事务操作 */ public void testTransaction() &#123; Connection connection = null; try &#123; connection = JDBCUtils.getDruidConnection(); // 1.取消数据的自动提交 connection.setAutoCommit(false); QueryRunner runner = new QueryRunner(); String sql1 = &quot;update user_table set balance = balance - 100 where user = ?&quot;; runner.update(connection, sql1, &quot;AA&quot;); // 模拟网络异常 System.out.println(10 / 0); String sql2 = &quot;update user_table set balance = balance + 100 where user = ?&quot;; runner.update(connection, sql2, &quot;BB&quot;); System.out.println(&quot;转账成功&quot;); // 2.提交数据 connection.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); // 3.回滚数据 JDBCUtils.rollBackConnection(connection); System.out.println(&quot;转账失败&quot;); &#125; finally &#123; JDBCUtils.closeResourceByDbUtils(connection, null, null); &#125; &#125;&#125; 本文参考https://www.bilibili.com/video/BV1eJ411c7rf 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"database","slug":"database","permalink":"http://example.com/tags/database/"}]},{"title":"Spring Boot 入门","slug":"spring-boot","date":"2021-06-12T07:31:58.000Z","updated":"2021-08-24T09:34:01.029Z","comments":true,"path":"2021/06/12/spring-boot/","link":"","permalink":"http://example.com/2021/06/12/spring-boot/","excerpt":"","text":"Spring Boot 简介 官网：https://spring.io/projects/spring-boot 文档：https://spring.io/projects/spring-boot#learn 查看各版本的新特性：https://github.com/spring-projects/spring-boot/wiki#release-notes Spring Boot 的作用 Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”. Spring Boot 能快速创建出生产级别的 Spring 应用。 Spring Boot 的优点 Create stand-alone Spring applications 创建独立的 Spring 应用。 Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files) 内嵌 web 服务器。 Provide opinionated ‘starter’ dependencies to simplify your build configuration 自动 starter 依赖，简化构建配置。 Automatically configure Spring and 3rd party libraries whenever possible 自动配置 Spring 以及第三方功能。 Provide production-ready features such as metrics, health checks, and externalized configuration 提供生产级别的监控、健康检查及外部化配置。 Absolutely no code generation and no requirement for XML configuration 无代码生成、无需编写 XML。 Spring Boot 的缺点 人称版本帝，迭代快，需要时刻关注变化。 封装太深，内部原理复杂，不容易精通。 Spring Boot 2 入门系统要求 Java 8 +： 1234PS C:\\Users\\XiSun&gt; java -versionopenjdk version &quot;1.8.0_222&quot;OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_222-b10)OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.222-b10, mixed mode) Maven 3.5 +： 123456PS C:\\Users\\XiSun&gt; mvn -vApache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)Maven home: D:\\Program Files\\Maven\\apache-maven-3.6.3\\bin\\..Java version: 1.8.0_222, vendor: AdoptOpenJDK, runtime: D:\\Program Files\\AdoptOpenJDK\\jdk-8.0.222.10-hotspot\\jreDefault locale: zh_CN, platform encoding: GBKOS name: &quot;windows 10&quot;, version: &quot;10.0&quot;, arch: &quot;amd64&quot;, family: &quot;windows&quot; Maven setting.xml 的设置： 1234567891011121314151617181920212223&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 说明：添加上面的配置后，项目中每次 Maven 更新依赖时，不会改变 Compiler 的版本。如果针对单个项目配置，则在该项目的 pom.xml 文件中添加： 123456789101112131415161718192021&lt;properties&gt; &lt;app.main.class&gt;cn.matgene.reaction.extractor.FlinkKafkaJob&lt;/app.main.class&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.version&gt;3.6.1&lt;/maven.compiler.version&gt; &lt;maven.compiler.source&gt;$&#123;java.version&#125;&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;$&#123;java.version&#125;&lt;/maven.compiler.target&gt; &lt;/properties&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven.compiler.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;maven.compiler.source&#125;&lt;/source&gt; &lt;target&gt;$&#123;maven.compiler.target&#125;&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt;&lt;/build&gt; HelloWorld 需求：浏览器发送 /hello请求，服务器响应 Hello, Spring Boot 2!。 参考：https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html#getting-started.first-application 创建 Maven 工程，并添加 parent 依赖： 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.xisun&lt;/groupId&gt; &lt;artifactId&gt;springboot-helloworld&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;/parent&gt;&lt;/project&gt; parent 节点为手动添加。 引入 web 相关依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 创建主程序： 1234567891011/** * @Author XiSun * @Date 2021/6/20 15:03 * @Description 主程序类 */@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class, args); &#125;&#125; 业务层： 123456789101112/** * @Author XiSun * @Date 2021/6/20 15:17 */@Controllerpublic class HelloController &#123; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String hello() &#123; return &quot;Hello, Spring Boot 2!&quot;; &#125;&#125; 运行 MainApplication.class 的 main 方法，启动程序，在浏览器输入地址 http://localhost:8080/hello，查看结果： 1234567891011121314151617181920 . ____ _ __ _ _ /\\\\ / ___&#x27;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) &#x27; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.5.1)2021-06-20 15:37:47.623 INFO 14268 --- [ main] cn.xisun.web.MainApplication : Starting MainApplication using Java 1.8.0_222 on DESKTOP-OJKMETJ with PID 14268 (D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-springboot\\target\\classes started by XiSun in D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-springboot)2021-06-20 15:37:47.627 INFO 14268 --- [ main] cn.xisun.web.MainApplication : No active profile set, falling back to default profiles: default2021-06-20 15:37:48.380 INFO 14268 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http)2021-06-20 15:37:48.386 INFO 14268 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]2021-06-20 15:37:48.386 INFO 14268 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.46]2021-06-20 15:37:48.438 INFO 14268 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext2021-06-20 15:37:48.439 INFO 14268 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 760 ms2021-06-20 15:37:48.674 INFO 14268 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path &#x27;&#x27;2021-06-20 15:37:48.681 INFO 14268 --- [ main] cn.xisun.web.MainApplication : Started MainApplication in 1.374 seconds (JVM running for 2.301)2021-06-20 15:37:59.504 INFO 14268 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet &#x27;dispatcherServlet&#x27;2021-06-20 15:37:59.504 INFO 14268 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet &#x27;dispatcherServlet&#x27;2021-06-20 15:37:59.504 INFO 14268 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 0 ms 简化配置： 参考：https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html#application-properties 在 resources 目录下新建 application.properties 文件，项目中的一些配置可在此文件中进行修改。 如，修改 tomcat 端口： 1server.port=8888 简化部署： 添加 spring-boot-maven-plugin： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 打包： 123456789101112131415161718192021222324252627282930313233343536373839404142D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-springboot&gt;mvn clean package -DskipTests[INFO] Scanning for projects...[INFO][INFO] -------------------&lt; cn.xisun:springboot-helloworld &gt;-------------------[INFO] Building springboot-helloworld 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO][INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ springboot-helloworld ---[INFO] Deleting D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-springboot\\target[INFO][INFO] --- maven-resources-plugin:3.2.0:resources (default-resources) @ springboot-helloworld ---[INFO] Using &#x27;UTF-8&#x27; encoding to copy filtered resources.[INFO] Using &#x27;UTF-8&#x27; encoding to copy filtered properties files.[INFO] Copying 1 resource[INFO] Copying 0 resource[INFO][INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ springboot-helloworld ---[INFO] Changes detected - recompiling the module![INFO] Compiling 2 source files to D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-springboot\\target\\classes[INFO][INFO] --- maven-resources-plugin:3.2.0:testResources (default-testResources) @ springboot-helloworld ---[INFO] Using &#x27;UTF-8&#x27; encoding to copy filtered resources.[INFO] Using &#x27;UTF-8&#x27; encoding to copy filtered properties files.[INFO] skip non existing resourceDirectory D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-springboot\\src\\test\\resources[INFO][INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ springboot-helloworld ---[INFO] Changes detected - recompiling the module![INFO][INFO] --- maven-surefire-plugin:2.22.2:test (default-test) @ springboot-helloworld ---[INFO] Tests are skipped.[INFO][INFO] --- maven-jar-plugin:3.2.0:jar (default-jar) @ springboot-helloworld ---[INFO] Building jar: D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-springboot\\target\\springboot-helloworld-1.0-SNAPSHOT.jar[INFO][INFO] --- spring-boot-maven-plugin:2.5.1:repackage (repackage) @ springboot-helloworld ---[INFO] Replacing main artifact with repackaged archive[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.890 s[INFO] Finished at: 2021-06-20T16:47:43+08:00[INFO] ------------------------------------------------------------------------ Spring Boot 的特点依赖管理 Spring Boot 项目，都会添加一个 parent 依赖 spring-boot-starter-parent： 12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt;&lt;/parent&gt; 父项目一般都是做依赖管理的，后续在项目中添加的依赖，其版本号和父项目 version 一致，不需要再单独指定。 spring-boot-starter-parent 有自己的父项目 spring-boot-dependencies，在该项目中几乎声明了所有开发中常用的依赖的版本号，这个版本号一般适应当前项目对应的版本。这是自动版本仲裁机制。 12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt;&lt;/parent&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199&lt;properties&gt; &lt;activemq.version&gt;5.16.2&lt;/activemq.version&gt; &lt;antlr2.version&gt;2.7.7&lt;/antlr2.version&gt; &lt;appengine-sdk.version&gt;1.9.89&lt;/appengine-sdk.version&gt; &lt;artemis.version&gt;2.17.0&lt;/artemis.version&gt; &lt;aspectj.version&gt;1.9.6&lt;/aspectj.version&gt; &lt;assertj.version&gt;3.19.0&lt;/assertj.version&gt; &lt;atomikos.version&gt;4.0.6&lt;/atomikos.version&gt; &lt;awaitility.version&gt;4.0.3&lt;/awaitility.version&gt; &lt;build-helper-maven-plugin.version&gt;3.2.0&lt;/build-helper-maven-plugin.version&gt; &lt;byte-buddy.version&gt;1.10.22&lt;/byte-buddy.version&gt; &lt;caffeine.version&gt;2.9.1&lt;/caffeine.version&gt; &lt;cassandra-driver.version&gt;4.11.1&lt;/cassandra-driver.version&gt; &lt;classmate.version&gt;1.5.1&lt;/classmate.version&gt; &lt;commons-codec.version&gt;1.15&lt;/commons-codec.version&gt; &lt;commons-dbcp2.version&gt;2.8.0&lt;/commons-dbcp2.version&gt; &lt;commons-lang3.version&gt;3.12.0&lt;/commons-lang3.version&gt; &lt;commons-pool.version&gt;1.6&lt;/commons-pool.version&gt; &lt;commons-pool2.version&gt;2.9.0&lt;/commons-pool2.version&gt; &lt;couchbase-client.version&gt;3.1.6&lt;/couchbase-client.version&gt; &lt;db2-jdbc.version&gt;11.5.5.0&lt;/db2-jdbc.version&gt; &lt;dependency-management-plugin.version&gt;1.0.11.RELEASE&lt;/dependency-management-plugin.version&gt; &lt;derby.version&gt;10.14.2.0&lt;/derby.version&gt; &lt;dropwizard-metrics.version&gt;4.1.22&lt;/dropwizard-metrics.version&gt; &lt;ehcache.version&gt;2.10.9.2&lt;/ehcache.version&gt; &lt;ehcache3.version&gt;3.9.4&lt;/ehcache3.version&gt; &lt;elasticsearch.version&gt;7.12.1&lt;/elasticsearch.version&gt; &lt;embedded-mongo.version&gt;3.0.0&lt;/embedded-mongo.version&gt; &lt;flyway.version&gt;7.7.3&lt;/flyway.version&gt; &lt;freemarker.version&gt;2.3.31&lt;/freemarker.version&gt; &lt;git-commit-id-plugin.version&gt;4.0.5&lt;/git-commit-id-plugin.version&gt; &lt;glassfish-el.version&gt;3.0.3&lt;/glassfish-el.version&gt; &lt;glassfish-jaxb.version&gt;2.3.4&lt;/glassfish-jaxb.version&gt; &lt;groovy.version&gt;3.0.8&lt;/groovy.version&gt; &lt;gson.version&gt;2.8.7&lt;/gson.version&gt; &lt;h2.version&gt;1.4.200&lt;/h2.version&gt; &lt;hamcrest.version&gt;2.2&lt;/hamcrest.version&gt; &lt;hazelcast.version&gt;4.1.3&lt;/hazelcast.version&gt; &lt;hazelcast-hibernate5.version&gt;2.2.0&lt;/hazelcast-hibernate5.version&gt; &lt;hibernate.version&gt;5.4.32.Final&lt;/hibernate.version&gt; &lt;hibernate-validator.version&gt;6.2.0.Final&lt;/hibernate-validator.version&gt; &lt;hikaricp.version&gt;4.0.3&lt;/hikaricp.version&gt; &lt;hsqldb.version&gt;2.5.2&lt;/hsqldb.version&gt; &lt;htmlunit.version&gt;2.49.1&lt;/htmlunit.version&gt; &lt;httpasyncclient.version&gt;4.1.4&lt;/httpasyncclient.version&gt; &lt;httpclient.version&gt;4.5.13&lt;/httpclient.version&gt; &lt;httpclient5.version&gt;5.0.4&lt;/httpclient5.version&gt; &lt;httpcore.version&gt;4.4.14&lt;/httpcore.version&gt; &lt;httpcore5.version&gt;5.1.1&lt;/httpcore5.version&gt; &lt;infinispan.version&gt;12.1.4.Final&lt;/infinispan.version&gt; &lt;influxdb-java.version&gt;2.21&lt;/influxdb-java.version&gt; &lt;jackson-bom.version&gt;2.12.3&lt;/jackson-bom.version&gt; &lt;jakarta-activation.version&gt;1.2.2&lt;/jakarta-activation.version&gt; &lt;jakarta-annotation.version&gt;1.3.5&lt;/jakarta-annotation.version&gt; &lt;jakarta-jms.version&gt;2.0.3&lt;/jakarta-jms.version&gt; &lt;jakarta-json.version&gt;1.1.6&lt;/jakarta-json.version&gt; &lt;jakarta-json-bind.version&gt;1.0.2&lt;/jakarta-json-bind.version&gt; &lt;jakarta-mail.version&gt;1.6.7&lt;/jakarta-mail.version&gt; &lt;jakarta-persistence.version&gt;2.2.3&lt;/jakarta-persistence.version&gt; &lt;jakarta-servlet.version&gt;4.0.4&lt;/jakarta-servlet.version&gt; &lt;jakarta-servlet-jsp-jstl.version&gt;1.2.7&lt;/jakarta-servlet-jsp-jstl.version&gt; &lt;jakarta-transaction.version&gt;1.3.3&lt;/jakarta-transaction.version&gt; &lt;jakarta-validation.version&gt;2.0.2&lt;/jakarta-validation.version&gt; &lt;jakarta-websocket.version&gt;1.1.2&lt;/jakarta-websocket.version&gt; &lt;jakarta-ws-rs.version&gt;2.1.6&lt;/jakarta-ws-rs.version&gt; &lt;jakarta-xml-bind.version&gt;2.3.3&lt;/jakarta-xml-bind.version&gt; &lt;jakarta-xml-soap.version&gt;1.4.2&lt;/jakarta-xml-soap.version&gt; &lt;jakarta-xml-ws.version&gt;2.3.3&lt;/jakarta-xml-ws.version&gt; &lt;janino.version&gt;3.1.4&lt;/janino.version&gt; &lt;javax-activation.version&gt;1.2.0&lt;/javax-activation.version&gt; &lt;javax-annotation.version&gt;1.3.2&lt;/javax-annotation.version&gt; &lt;javax-cache.version&gt;1.1.1&lt;/javax-cache.version&gt; &lt;javax-jaxb.version&gt;2.3.1&lt;/javax-jaxb.version&gt; &lt;javax-jaxws.version&gt;2.3.1&lt;/javax-jaxws.version&gt; &lt;javax-jms.version&gt;2.0.1&lt;/javax-jms.version&gt; &lt;javax-json.version&gt;1.1.4&lt;/javax-json.version&gt; &lt;javax-jsonb.version&gt;1.0&lt;/javax-jsonb.version&gt; &lt;javax-mail.version&gt;1.6.2&lt;/javax-mail.version&gt; &lt;javax-money.version&gt;1.1&lt;/javax-money.version&gt; &lt;javax-persistence.version&gt;2.2&lt;/javax-persistence.version&gt; &lt;javax-transaction.version&gt;1.3&lt;/javax-transaction.version&gt; &lt;javax-validation.version&gt;2.0.1.Final&lt;/javax-validation.version&gt; &lt;javax-websocket.version&gt;1.1&lt;/javax-websocket.version&gt; &lt;jaxen.version&gt;1.2.0&lt;/jaxen.version&gt; &lt;jaybird.version&gt;4.0.3.java8&lt;/jaybird.version&gt; &lt;jboss-logging.version&gt;3.4.2.Final&lt;/jboss-logging.version&gt; &lt;jboss-transaction-spi.version&gt;7.6.1.Final&lt;/jboss-transaction-spi.version&gt; &lt;jdom2.version&gt;2.0.6&lt;/jdom2.version&gt; &lt;jedis.version&gt;3.6.0&lt;/jedis.version&gt; &lt;jersey.version&gt;2.33&lt;/jersey.version&gt; &lt;jetty-el.version&gt;9.0.29&lt;/jetty-el.version&gt; &lt;jetty-jsp.version&gt;2.2.0.v201112011158&lt;/jetty-jsp.version&gt; &lt;jetty-reactive-httpclient.version&gt;1.1.9&lt;/jetty-reactive-httpclient.version&gt; &lt;jetty.version&gt;9.4.42.v20210604&lt;/jetty.version&gt; &lt;jmustache.version&gt;1.15&lt;/jmustache.version&gt; &lt;johnzon.version&gt;1.2.13&lt;/johnzon.version&gt; &lt;jolokia.version&gt;1.6.2&lt;/jolokia.version&gt; &lt;jooq.version&gt;3.14.11&lt;/jooq.version&gt; &lt;json-path.version&gt;2.5.0&lt;/json-path.version&gt; &lt;json-smart.version&gt;2.4.7&lt;/json-smart.version&gt; &lt;jsonassert.version&gt;1.5.0&lt;/jsonassert.version&gt; &lt;jstl.version&gt;1.2&lt;/jstl.version&gt; &lt;jtds.version&gt;1.3.1&lt;/jtds.version&gt; &lt;junit.version&gt;4.13.2&lt;/junit.version&gt; &lt;junit-jupiter.version&gt;5.7.2&lt;/junit-jupiter.version&gt; &lt;kafka.version&gt;2.7.1&lt;/kafka.version&gt; &lt;kotlin.version&gt;1.5.10&lt;/kotlin.version&gt; &lt;kotlin-coroutines.version&gt;1.5.0&lt;/kotlin-coroutines.version&gt; &lt;lettuce.version&gt;6.1.2.RELEASE&lt;/lettuce.version&gt; &lt;liquibase.version&gt;4.3.5&lt;/liquibase.version&gt; &lt;log4j2.version&gt;2.14.1&lt;/log4j2.version&gt; &lt;logback.version&gt;1.2.3&lt;/logback.version&gt; &lt;lombok.version&gt;1.18.20&lt;/lombok.version&gt; &lt;mariadb.version&gt;2.7.3&lt;/mariadb.version&gt; &lt;maven-antrun-plugin.version&gt;1.8&lt;/maven-antrun-plugin.version&gt; &lt;maven-assembly-plugin.version&gt;3.3.0&lt;/maven-assembly-plugin.version&gt; &lt;maven-clean-plugin.version&gt;3.1.0&lt;/maven-clean-plugin.version&gt; &lt;maven-compiler-plugin.version&gt;3.8.1&lt;/maven-compiler-plugin.version&gt; &lt;maven-dependency-plugin.version&gt;3.1.2&lt;/maven-dependency-plugin.version&gt; &lt;maven-deploy-plugin.version&gt;2.8.2&lt;/maven-deploy-plugin.version&gt; &lt;maven-enforcer-plugin.version&gt;3.0.0-M3&lt;/maven-enforcer-plugin.version&gt; &lt;maven-failsafe-plugin.version&gt;2.22.2&lt;/maven-failsafe-plugin.version&gt; &lt;maven-help-plugin.version&gt;3.2.0&lt;/maven-help-plugin.version&gt; &lt;maven-install-plugin.version&gt;2.5.2&lt;/maven-install-plugin.version&gt; &lt;maven-invoker-plugin.version&gt;3.2.2&lt;/maven-invoker-plugin.version&gt; &lt;maven-jar-plugin.version&gt;3.2.0&lt;/maven-jar-plugin.version&gt; &lt;maven-javadoc-plugin.version&gt;3.2.0&lt;/maven-javadoc-plugin.version&gt; &lt;maven-resources-plugin.version&gt;3.2.0&lt;/maven-resources-plugin.version&gt; &lt;maven-shade-plugin.version&gt;3.2.4&lt;/maven-shade-plugin.version&gt; &lt;maven-source-plugin.version&gt;3.2.1&lt;/maven-source-plugin.version&gt; &lt;maven-surefire-plugin.version&gt;2.22.2&lt;/maven-surefire-plugin.version&gt; &lt;maven-war-plugin.version&gt;3.3.1&lt;/maven-war-plugin.version&gt; &lt;micrometer.version&gt;1.7.0&lt;/micrometer.version&gt; &lt;mimepull.version&gt;1.9.14&lt;/mimepull.version&gt; &lt;mockito.version&gt;3.9.0&lt;/mockito.version&gt; &lt;mongodb.version&gt;4.2.3&lt;/mongodb.version&gt; &lt;mssql-jdbc.version&gt;9.2.1.jre8&lt;/mssql-jdbc.version&gt; &lt;mysql.version&gt;8.0.25&lt;/mysql.version&gt; &lt;nekohtml.version&gt;1.9.22&lt;/nekohtml.version&gt; &lt;neo4j-java-driver.version&gt;4.2.6&lt;/neo4j-java-driver.version&gt; &lt;netty.version&gt;4.1.65.Final&lt;/netty.version&gt; &lt;netty-tcnative.version&gt;2.0.39.Final&lt;/netty-tcnative.version&gt; &lt;oauth2-oidc-sdk.version&gt;9.3.3&lt;/oauth2-oidc-sdk.version&gt; &lt;nimbus-jose-jwt.version&gt;9.8.1&lt;/nimbus-jose-jwt.version&gt; &lt;ojdbc.version&gt;19.3.0.0&lt;/ojdbc.version&gt; &lt;okhttp3.version&gt;3.14.9&lt;/okhttp3.version&gt; &lt;oracle-database.version&gt;21.1.0.0&lt;/oracle-database.version&gt; &lt;pooled-jms.version&gt;1.2.2&lt;/pooled-jms.version&gt; &lt;postgresql.version&gt;42.2.20&lt;/postgresql.version&gt; &lt;prometheus-pushgateway.version&gt;0.10.0&lt;/prometheus-pushgateway.version&gt; &lt;quartz.version&gt;2.3.2&lt;/quartz.version&gt; &lt;querydsl.version&gt;4.4.0&lt;/querydsl.version&gt; &lt;r2dbc-bom.version&gt;Arabba-SR10&lt;/r2dbc-bom.version&gt; &lt;rabbit-amqp-client.version&gt;5.12.0&lt;/rabbit-amqp-client.version&gt; &lt;reactive-streams.version&gt;1.0.3&lt;/reactive-streams.version&gt; &lt;reactor-bom.version&gt;2020.0.7&lt;/reactor-bom.version&gt; &lt;rest-assured.version&gt;4.3.3&lt;/rest-assured.version&gt; &lt;rsocket.version&gt;1.1.1&lt;/rsocket.version&gt; &lt;rxjava.version&gt;1.3.8&lt;/rxjava.version&gt; &lt;rxjava-adapter.version&gt;1.2.1&lt;/rxjava-adapter.version&gt; &lt;rxjava2.version&gt;2.2.21&lt;/rxjava2.version&gt; &lt;saaj-impl.version&gt;1.5.3&lt;/saaj-impl.version&gt; &lt;selenium.version&gt;3.141.59&lt;/selenium.version&gt; &lt;selenium-htmlunit.version&gt;2.49.1&lt;/selenium-htmlunit.version&gt; &lt;sendgrid.version&gt;4.7.2&lt;/sendgrid.version&gt; &lt;servlet-api.version&gt;4.0.1&lt;/servlet-api.version&gt; &lt;slf4j.version&gt;1.7.30&lt;/slf4j.version&gt; &lt;snakeyaml.version&gt;1.28&lt;/snakeyaml.version&gt; &lt;solr.version&gt;8.8.2&lt;/solr.version&gt; &lt;spring-amqp.version&gt;2.3.8&lt;/spring-amqp.version&gt; &lt;spring-batch.version&gt;4.3.3&lt;/spring-batch.version&gt; &lt;spring-data-bom.version&gt;2021.0.1&lt;/spring-data-bom.version&gt; &lt;spring-framework.version&gt;5.3.8&lt;/spring-framework.version&gt; &lt;spring-hateoas.version&gt;1.3.1&lt;/spring-hateoas.version&gt; &lt;spring-integration.version&gt;5.5.0&lt;/spring-integration.version&gt; &lt;spring-kafka.version&gt;2.7.2&lt;/spring-kafka.version&gt; &lt;spring-ldap.version&gt;2.3.4.RELEASE&lt;/spring-ldap.version&gt; &lt;spring-restdocs.version&gt;2.0.5.RELEASE&lt;/spring-restdocs.version&gt; &lt;spring-retry.version&gt;1.3.1&lt;/spring-retry.version&gt; &lt;spring-security.version&gt;5.5.0&lt;/spring-security.version&gt; &lt;spring-session-bom.version&gt;2021.0.0&lt;/spring-session-bom.version&gt; &lt;spring-ws.version&gt;3.1.1&lt;/spring-ws.version&gt; &lt;sqlite-jdbc.version&gt;3.34.0&lt;/sqlite-jdbc.version&gt; &lt;sun-mail.version&gt;1.6.7&lt;/sun-mail.version&gt; &lt;thymeleaf.version&gt;3.0.12.RELEASE&lt;/thymeleaf.version&gt; &lt;thymeleaf-extras-data-attribute.version&gt;2.0.1&lt;/thymeleaf-extras-data-attribute.version&gt; &lt;thymeleaf-extras-java8time.version&gt;3.0.4.RELEASE&lt;/thymeleaf-extras-java8time.version&gt; &lt;thymeleaf-extras-springsecurity.version&gt;3.0.4.RELEASE&lt;/thymeleaf-extras-springsecurity.version&gt; &lt;thymeleaf-layout-dialect.version&gt;2.5.3&lt;/thymeleaf-layout-dialect.version&gt; &lt;tomcat.version&gt;9.0.46&lt;/tomcat.version&gt; &lt;unboundid-ldapsdk.version&gt;4.0.14&lt;/unboundid-ldapsdk.version&gt; &lt;undertow.version&gt;2.2.8.Final&lt;/undertow.version&gt; &lt;versions-maven-plugin.version&gt;2.8.1&lt;/versions-maven-plugin.version&gt; &lt;webjars-hal-browser.version&gt;3325375&lt;/webjars-hal-browser.version&gt; &lt;webjars-locator-core.version&gt;0.46&lt;/webjars-locator-core.version&gt; &lt;wsdl4j.version&gt;1.6.3&lt;/wsdl4j.version&gt; &lt;xml-maven-plugin.version&gt;1.0.2&lt;/xml-maven-plugin.version&gt; &lt;xmlunit2.version&gt;2.8.2&lt;/xmlunit2.version&gt;&lt;/properties&gt; 通过 spring-boot-dependencies，可以查看适应当前版本的其他依赖的 version。 场景启动器： 参考：https://docs.spring.io/spring-boot/docs/current/reference/html/using.html#using.build-systems.starters 场景启动器表示的是实现某种功能时，所需要的一组常规的依赖，当引入这个启动器后，会自动添加这一组依赖。比如 spring-boot-start-web： Spring 官方的启动器命名规则为 spring-boot-start-*，* 代表的就是某种场景。 自定义的第三方启动器，命名规则一般为 thirdpartyproject-spring-boot-starter。 所有场景启动器最底层的依赖： 123456&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 自动配置 比如，引入 spring-boot-start-web 启动器时，会自动引入 Tomcat、SpringMVC 的相关依赖，并配置好。也会自动配好 Web 的常见功能，如：字符编码问题。 默认的包结构： 主程序所在包及其下面的所有子包里面的组件都会被默认扫描进来，无需自行设置包扫描。 如果想要改变扫描路径，使用 **@SpringBootApplication(scanBasePackages=&quot;cn.xisun&quot;)**。 123456@SpringBootApplication(scanBasePackages = &quot;cn.xisun&quot;)public class MainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class, args); &#125;&#125; @SpringBootApplication 注解等同于 @SpringBootConfiguration，@EnableAutoConfiguration 和 @ComponentScan，复写此三个注解，然后使用 @ComponentScan 也可以重新指定扫码路径。 12345678@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(&quot;cn.xisun&quot;)public class MainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class, args); &#125;&#125; 各种配置拥有默认值： 默认配置最终都是映射到某个类上，如：MultipartProperties。 配置文件的值最终会绑定每个类上，这个类会在容器中会创建对象。 在 application.properties 文件内可以修改各种配置的默认值。 按需加载所有自动配置项： 引入了一个场景启动器后，这个场景的自动配置才会开启。 Spring Boot 所有的自动配置功能，都在 spring-boot-autoconfigure 包里面。 Spring Boot 的容器功能添加组件 新建 User 类和 Pet 类，用于测试： 12345678910111213141516171819202122232425262728293031package cn.xisun.web.bean;/** * @Author XiSun * @Date 2021/6/23 16:28 */public class Pet &#123; private String name; public Pet() &#123; &#125; public Pet(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Pet&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package cn.xisun.web.bean;/** * @Author XiSun * @Date 2021/6/23 15:23 */public class User &#123; private String name; private int age; private Pet pet; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public Pet getPet() &#123; return pet; &#125; public void setPet(Pet pet) &#123; this.pet = pet; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, pet=&quot; + pet + &#x27;&#125;&#x27;; &#125;&#125; @Configuration 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @Author XiSun * @Date 2021/6/23 15:24 * @Description 1.@Configuration注解标识当前类是一个配置类，作用等同于Spring的配置文件 * 2.@Configuration注解标识的配置类本身也是一个组件 * 3.配置类里可以使用@Bean注解，标注在方法上给容器注册组件，组件是单实例的 * 4.@Configuration注解有一个proxyBeanMethods属性，表示是否代理配置类中Bean的方法，默认为true，即代理 */@Configuration(proxyBeanMethods = false)public class MyConfig &#123; /** * 使用@Bean注解给容器中注册组件 * * @return 以方法名作为组件的id，返回类型就是组件的类型，返回的值，就是组件在容器中的实例 */ @Bean public User user01() &#123; User zhangsan = new User(&quot;zhangsan&quot;, 18); /* * user01组件依赖了tom组件： * 如果proxyBeanMethods = true，user01组件依赖的tom组件，就是容器中注册的那个 * 如果proxyBeanMethods = false，user01组件依赖的tom组件，是新建的，与容器中注册的那个无关 */ zhangsan.setPet(tomcatPet()); return zhangsan; &#125; /** * @return 可以重新指定组件的id */ @Bean(&quot;lisi&quot;) public User user02() &#123; return new User(&quot;lisi&quot;, 19); &#125; @Bean(&quot;tom&quot;) public Pet tomcatPet() &#123; return new Pet(&quot;tomcat&quot;); &#125; /** * 使用@Scope(&quot;prototype&quot;)注解，指定注册的组件是多实例的，默认情况是单实例 * * @return 每次从容器中获得的tom1组件，都不相同 */ @Bean(&quot;tom1&quot;) @Scope(&quot;prototype&quot;) public Pet tomcatPet1() &#123; return new Pet(&quot;tomcat2&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * @Author XiSun * @Date 2021/6/20 15:03 * @Description 主程序类 */@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; // 1.返回IOC容器 ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); // 2.查看容器内的所有组件 String[] beanDefinitionNames = run.getBeanDefinitionNames(); for (String beanDefinitionName : beanDefinitionNames) &#123; System.out.println(beanDefinitionName); &#125; // 3.从容器中获取配置类本身的组件 MyConfig myConfig = run.getBean(MyConfig.class); System.out.println(myConfig); // 4.从容器中获取配置类中注册的组件，每次获取的实例都相同 User user01 = run.getBean(&quot;user01&quot;, User.class); User user011 = run.getBean(&quot;user01&quot;, User.class); System.out.println(user01); System.out.println(&quot;单例? &quot; + (user01 == user011)); User lisi = run.getBean(&quot;lisi&quot;, User.class); System.out.println(lisi); /* * 5.通过配置类的方法获取实例 * @Configuration(proxyBeanMethods = true)： * 此时，配置类是一个MyConfig$$EnhancerBySpringCGLIB$$70400c34@1517f633对象(CGLIB代理对象) * 在执行方法前，SpringBoot总会检查要获取的组件是否在容器中已存在，若存在，直接返回该组件---保持容器中组件单实例 * Full模式：外部无论对配置类中的这个组件的注册方法调用多少次，获取的都是之前已经注册在容器中的单实例对象，即user和user1总是相等 * 组件依赖必须使用Full模式 * @Configuration(proxyBeanMethods = false)： * 此时，配置类是一个MyConfig@644abb8f对象(普通对象) * 在执行方法前，SpringBoot不会检查要获取的组件是否在容器中已存在 * Lite模式：外部对配置类中的这个组件的注册方法的每一次调用，都会获得一个新的实例，即user和user1总是不等 */ User user = myConfig.user01(); User user1 = myConfig.user01(); System.out.println(user == user1); // 根据proxyBeanMethods的属性为true或false，可以看出user01的pet属性，与容器中的tom组件是否相同 Pet tom = run.getBean(&quot;tom&quot;, Pet.class); System.out.println(&quot;用户的宠物：&quot; + (user01.getPet() == tom)); // tom1组件是多实例的，tom1对象和tom2对象不相同 Pet tom1 = run.getBean(&quot;tom1&quot;, Pet.class); Pet tom2 = run.getBean(&quot;tom1&quot;, Pet.class); System.out.println(tom1 == tom2); &#125;&#125; @Configuration 标注在类上，表明该类是一个配置类，作用等同于 Spring 的 xml 配置文件中的 &lt;beans&gt; 标签，如下所示： 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;user01&quot; class=&quot;cn.xisun.web.bean.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;zhangsan&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;18&quot;/&gt; &lt;property name=&quot;pet&quot; ref=&quot;tom&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;lisi&quot; class=&quot;cn.xisun.web.bean.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;lisi&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;19&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;tom&quot; class=&quot;cn.xisun.web.bean.Pet&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;tomcat&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 根据 @Configuration 注解的 proxyBeanMethods 属性值： false：Lite 模式。当配置类组件之间无依赖关系时，用 Lite 模式可以减少判断，加速容器启动过程。 true：Full 模式。当配置类组件之间有依赖关系时，配置类里的 Bean 方法会被调用，为了得到之前容器中注册的单实例组件，需要使用 Full 模式。 组件依赖必须使用 Full 模式。 @ComponentScan：指定扫描的包，默认扫码主程序所在包及其下面的所有子包。 @Import：给容器中自动创建出指定类型的组件，并且，默认组件的名字是全类名。 12345678910111213@Configuration@Import(&#123;User.class, ThrowableToStringArray.class&#125;)public class MyConfig &#123; @Bean public User user01() &#123; return new User(&quot;zhangsan&quot;, 18); &#125; @Bean(&quot;lisi&quot;) public User user02() &#123; return new User(&quot;lisi&quot;, 19); &#125;&#125; 12345678910111213141516171819202122@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; // 1.返回IOC容器 ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); // 2.按User类型获取容器中注册的实例 String[] beanNamesForType = run.getBeanNamesForType(User.class); for (String bean : beanNamesForType) &#123; System.out.println(bean); &#125; ThrowableToStringArray bean = run.getBean(ThrowableToStringArray.class); System.out.println(bean); &#125;&#125;输出结果： cn.xisun.web.bean.User // 全类名 user01 // 容器中注册的 lisi // 容器中注册的 ch.qos.logback.core.helpers.ThrowableToStringArray@312afbc7 // 全类名 @Bean、@Component、@Controller、@Service、@Repository。 @Conditional：条件装配，当满足 @Conditional 指定的条件时，则进行组件注入。 @Conditional 注解有多个派生注解，每一个派生注解都代表一种条件。 @ConditionalOnBean：当容器中存在指定的 Bean 时。 @ConditionalOnMissingBean：当容器中不存在指定的 Bean 时。 @ConditionalOnClass：当容器中存在指定的类时。 @ConditionalOnMissingClass：当容器中不存在指定的类时。 @ConditionalOnJava：当指定的 Java 版本时。 @ConditionalOnResource：当指定资源存在时。 注意：配置类中定义的组件，是按照从上到下的顺序依次注册的，在使用类似 @ConditionalOnBean 这样的条件装配注解时，需要注意组件的定义顺序。在这样的情况下，在配置类上使用条件装配注解时，需要额外注意。 tom 组件在 user01 组件上面定义： 123456789101112131415@Configurationpublic class MyConfig &#123; @Bean(&quot;tom&quot;) public Pet tomcatPet() &#123; return new Pet(&quot;tomcat&quot;); &#125; @Bean @ConditionalOnBean(name = &quot;tom&quot;) public User user01() &#123; User zhangsan = new User(&quot;zhangsan&quot;, 18); zhangsan.setPet(tomcatPet()); return zhangsan; &#125;&#125; 1234567891011121314@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); boolean tom = run.containsBean(&quot;tom&quot;); boolean user01 = run.containsBean(&quot;user01&quot;); System.out.println(&quot;容器中存在tom？&quot; + tom); System.out.println(&quot;容器中存在user01？&quot; + user01); &#125;&#125;输出结果： 容器中存在tom？true 容器中存在user01？true tom 组件在 user01 组件下面定义： 123456789101112131415@Configurationpublic class MyConfig &#123; @Bean @ConditionalOnBean(name = &quot;tom&quot;) public User user01() &#123; User zhangsan = new User(&quot;zhangsan&quot;, 18); zhangsan.setPet(tomcatPet()); return zhangsan; &#125; @Bean(&quot;tom&quot;) public Pet tomcatPet() &#123; return new Pet(&quot;tomcat&quot;); &#125;&#125; 1234567891011121314@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); boolean tom = run.containsBean(&quot;tom&quot;); boolean user01 = run.containsBean(&quot;user01&quot;); System.out.println(&quot;容器中存在tom？&quot; + tom); System.out.println(&quot;容器中存在user01？&quot; + user01); &#125;&#125;输出结果： 容器中存在tom？true 容器中存在user01？false 原生配置文件引入 @ImportResource：导入 Spring 的配置文件，使用在主类上，或者任一配置类上。当旧项目更新，并存在很多配置文件时，会很有用处。 oldBeans.xml： 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;wangwu&quot; class=&quot;cn.xisun.web.bean.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;wangwu&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;20&quot;/&gt; &lt;property name=&quot;pet&quot; ref=&quot;jerry&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;jerry&quot; class=&quot;cn.xisun.web.bean.Pet&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;jerry&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; MainApplication.java： 123456789101112131415@SpringBootApplication@ImportResource(&quot;classpath:oldBeans.xml&quot;)public class MainApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); boolean wangwu = run.containsBean(&quot;wangwu&quot;); boolean jerry = run.containsBean(&quot;jerry&quot;); System.out.println(&quot;容器中存在jerry？&quot; + jerry); System.out.println(&quot;容器中存在wangwu？&quot; + wangwu); &#125;&#125;输出结果： 容器中存在jerry？true 容器中存在wangwu？true 配置绑定 application.properties 文件： 123server.port=8080mycar.brand=BMWmycar.price=200000.0 待封装的 JavaBean： 1234567891011121314151617181920212223242526272829public class Car &#123; private String brand; private Double price; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand = brand; &#125; public Double getPrice() &#123; return price; &#125; public void setPrice(Double price) &#123; this.price = price; &#125; @Override public String toString() &#123; return &quot;Car&#123;&quot; + &quot;brand=&#x27;&quot; + brand + &#x27;\\&#x27;&#x27; + &quot;, price=&quot; + price + &#x27;&#125;&#x27;; &#125;&#125; 自定义的类和配置文件绑定一般没有提示，Car 类上会出现以下提示，需要添加 spring-boot-configuration-processo 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 该依赖只在开发时提供帮助，因此在打包 jar 包时，应该排除： 123456789101112131415161718&lt;!-- 打包插件 --&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;!-- 打包时排除依赖 --&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 从 application.properties 文件中读取内容，并且把它封装到 JavaBean 中的普通写法： 12345678910111213public static void getProperties() throws IOException &#123; Properties properties = new Properties(); InputStream is = ClassLoader.getSystemClassLoader().getResourceAsStream(&quot;application.properties&quot;); properties.load(is); // 得到配置文件中的值 Enumeration&lt;?&gt; enumeration = properties.propertyNames(); while (enumeration.hasMoreElements()) &#123; String strKey = (String) enumeration.nextElement(); String strValue = properties.getProperty(strKey); System.out.println(strKey + &quot;=&quot; + strValue); // 封装到JavaBean的操作 &#125;&#125; 方式一：在需绑定的 JavaBean 上，添加 @Component 和 @ConfigurationProperties 注解。 12345678910111213141516171819202122232425262728293031323334353637383940/** * @Author XiSun * @Date 2021/7/9 21:58 * 1.使用@Component注解将JavaBean注册到容器中，只有容器中的组件才能 * 拥有SpringBoot提供的功能，这是前提； * 2.使用@ConfigurationProperties注解，将配置文件和JavaBean绑定， * prefix属性指定配置文件中需绑定的值的前缀； * 3.JavaBean的属性名，需和配置文件中对应值前缀后的值相同。 */@Component@ConfigurationProperties(prefix = &quot;mycar&quot;)public class Car &#123; private String brand; private Double price; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand = brand; &#125; public Double getPrice() &#123; return price; &#125; public void setPrice(Double price) &#123; this.price = price; &#125; @Override public String toString() &#123; return &quot;Car&#123;&quot; + &quot;brand=&#x27;&quot; + brand + &#x27;\\&#x27;&#x27; + &quot;, price=&quot; + price + &#x27;&#125;&#x27;; &#125;&#125; 方式二：在需绑定的 JavaBean 上，添加 @ConfigurationProperties 注解，在配置类上添加 @EnableConfigurationProperties 注解。 12345678910111213141516171819202122232425262728293031323334353637/** * @Author XiSun * @Date 2021/7/9 21:58 * 1.使用@ConfigurationProperties注解，将配置文件和JavaBean绑定， * prefix属性指定配置文件中需绑定的值的前缀； * 2.JavaBean的属性名，需和配置文件中对应值前缀后的值相同。 */@ConfigurationProperties(prefix = &quot;mycar&quot;)public class Car &#123; private String brand; private Double price; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand = brand; &#125; public Double getPrice() &#123; return price; &#125; public void setPrice(Double price) &#123; this.price = price; &#125; @Override public String toString() &#123; return &quot;Car&#123;&quot; + &quot;brand=&#x27;&quot; + brand + &#x27;\\&#x27;&#x27; + &quot;, price=&quot; + price + &#x27;&#125;&#x27;; &#125;&#125; 123456789101112131415161718192021/** * 1.使用@EnableConfigurationProperties注解，开启待装配的JavaBean的配置绑定功能， * 同时，将该JavaBean这个组件自动注入到容器中； * 2.JavaBean上不需要使用@Component注解，某些时候，比如JavaBean是第三方依赖包中的 * 类，这个特点会很重要。 */@Configuration@EnableConfigurationProperties(&#123;Car.class&#125;)public class MyConfig &#123; @Bean public User user01() &#123; User zhangsan = new User(&quot;zhangsan&quot;, 18); zhangsan.setPet(tomcatPet()); return zhangsan; &#125; @Bean(&quot;tom&quot;) public Pet tomcatPet() &#123; return new Pet(&quot;tomcat&quot;); &#125;&#125; 主类测试： 12345678910111213141516171819202122232425262728/** * @Author XiSun * @Date 2021/6/20 15:03 * @Description 主程序类 */@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); // 获取容器中的Car类型的组件 String[] beanNamesForType = run.getBeanNamesForType(Car.class); for (String beanName : beanNamesForType) &#123; System.out.println(beanName); &#125; Car car = run.getBean(&quot;car&quot;, Car.class); System.out.println(car); &#125;&#125;方式一输出结果： car Car&#123;brand=&#x27;BMW&#x27;, price=200000.0&#125;方式二输出结果： mycar-cn.xisun.web.bean.Car Car&#123;brand=&#x27;BMW&#x27;, price=200000.0&#125; 对于方式一，注册到容器中的组件名，就是 JavaBean 类名的首字母小写。 对于方式二，注册到容器中的组件名，有所不同，为前缀加 JavaBean 全类名。 Controller 中获取： 123456789101112131415161718192021/** * @Author XiSun * @Date 2021/6/20 15:17 */@Controllerpublic class HelloController &#123; @Autowired private Car car; @RequestMapping(&quot;/car&quot;) @ResponseBody public Car car() &#123; return car; &#125; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String hello() &#123; return &quot;Hello, Spring Boot 2!&quot;; &#125;&#125; Spring Boot 的自动配置原理入门引导加载自动配置类 主类： 123456@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class, args); &#125;&#125; @SpringBootApplication： 123456789@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123;&#125; @SpringBootConfiguration：是 @Configuration 的派生注解，表明当前主类实际上也是一个配置类。 @ComponentScan：指定扫描的包，默认为当前主类所在包及其子包。 @EnableAutoConfiguration： 1234567@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123;&#125; @AutoConfigurationPackage： 123456@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Import(AutoConfigurationPackages.Registrar.class)public @interface AutoConfigurationPackage &#123;&#125; @Import(AutoConfigurationPackages.Registrar.class)：向容器中注册了一个 AutoConfigurationPackages.Registrar.class 组件。 1234567891011121314151617/** * &#123;@link ImportBeanDefinitionRegistrar&#125; to store the base package from the importing * configuration. */static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports &#123; @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) &#123; register(registry, new PackageImports(metadata).getPackageNames().toArray(new String[0])); &#125; @Override public Set&lt;Object&gt; determineImports(AnnotationMetadata metadata) &#123; return Collections.singleton(new PackageImports(metadata)); &#125;&#125; new PackageImports(metadata).getPackageNames()：拿到元注解所包含的包信息，实际上就是主类所在的包，如 cn.xisun.web。 register() 的功能，也就是将主类所在包下的所有组件，批量注册到容器中。这也就是默认包路径为主类所在包的原因。 @Import(AutoConfigurationImportSelector.class)：向容器中注册了一个 AutoConfigurationImportSelector.class 组件，执行如下方法。 12345678@Overridepublic String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());&#125; getAutoConfigurationEntry(annotationMetadata)：向容器中批量注册一些组件。 1234567891011121314protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return EMPTY_ENTRY; &#125; AnnotationAttributes attributes = getAttributes(annotationMetadata); List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = getConfigurationClassFilter().filter(configurations); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions);&#125; getCandidateConfigurations(annotationMetadata, attributes);：获取所有待批量注册的组件。 1234567protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you &quot; + &quot;are using a custom packaging, make sure that file is correct.&quot;); return configurations;&#125; SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader());：具体通过 SpringFactoriesLoader 工厂加载所有的组件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * The location to look for factories. * &lt;p&gt;Can be present in multiple JAR files. */public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;;public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryType, @Nullable ClassLoader classLoader) &#123; ClassLoader classLoaderToUse = classLoader; if (classLoaderToUse == null) &#123; classLoaderToUse = SpringFactoriesLoader.class.getClassLoader(); &#125; String factoryTypeName = factoryType.getName(); return loadSpringFactories(classLoaderToUse).getOrDefault(factoryTypeName, Collections.emptyList());&#125;private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(ClassLoader classLoader) &#123; Map&lt;String, List&lt;String&gt;&gt; result = cache.get(classLoader); if (result != null) &#123; return result; &#125; result = new HashMap&lt;&gt;(); try &#123; // 在此处，加载项目里 Enumeration&lt;URL&gt; urls = classLoader.getResources(FACTORIES_RESOURCE_LOCATION); while (urls.hasMoreElements()) &#123; URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123; String factoryTypeName = ((String) entry.getKey()).trim(); String[] factoryImplementationNames = StringUtils.commaDelimitedListToStringArray((String) entry.getValue()); for (String factoryImplementationName : factoryImplementationNames) &#123; result.computeIfAbsent(factoryTypeName, key -&gt; new ArrayList&lt;&gt;()) .add(factoryImplementationName.trim()); &#125; &#125; &#125; // Replace all lists with unmodifiable lists containing unique elements result.replaceAll((factoryType, implementations) -&gt; implementations.stream().distinct() .collect(Collectors.collectingAndThen(Collectors.toList(), Collections::unmodifiableList))); cache.put(classLoader, result); &#125; catch (IOException ex) &#123; throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; + FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex); &#125; return result;&#125; classLoader.getResources(FACTORIES_RESOURCE_LOCATION);：此方法扫描项目内各 jar 包的 META-INF/spring.factories 路径内声明的资源。主要看 spring-boot-autoconfigure-2.5.1.jar 包下的 spring.factories 文件，该文件内声明了 131 个需要自动注册的组件，当 Spring Boot 启动时，就会向容器中注册这些声明的组件： 按需开启自动配置项 在上面的分析中，Spring Boot 在启动时，默认会加载 131 个自动配置的组件。但在实际启动时，各 xxxxAutoConfiguration 组件，会根据 @Conditional 注解，即按照条件装配规则，实现按需配置。 例如，org.springframework.boot.autoconfigure.aop.AopAutoConfiguration： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Configuration(proxyBeanMethods = false)@ConditionalOnProperty(prefix = &quot;spring.aop&quot;, name = &quot;auto&quot;, havingValue = &quot;true&quot;, matchIfMissing = true)public class AopAutoConfiguration &#123; /** * 当org.aspectj.weaver.Advice.class文件存在时，AspectJAutoProxyingConfiguration生效 */ @Configuration(proxyBeanMethods = false) @ConditionalOnClass(Advice.class) static class AspectJAutoProxyingConfiguration &#123; @Configuration(proxyBeanMethods = false) @EnableAspectJAutoProxy(proxyTargetClass = false) @ConditionalOnProperty(prefix = &quot;spring.aop&quot;, name = &quot;proxy-target-class&quot;, havingValue = &quot;false&quot;) static class JdkDynamicAutoProxyConfiguration &#123; &#125; @Configuration(proxyBeanMethods = false) @EnableAspectJAutoProxy(proxyTargetClass = true) @ConditionalOnProperty(prefix = &quot;spring.aop&quot;, name = &quot;proxy-target-class&quot;, havingValue = &quot;true&quot;, matchIfMissing = true) static class CglibAutoProxyConfiguration &#123; &#125; &#125; /** * 当org.aspectj.weaver.Advice.class文件不存在，且配置文件中spring.aop.proxy-target-class属性值为true(默认为true)时， * ClassProxyingConfiguration生效 */ @Configuration(proxyBeanMethods = false) @ConditionalOnMissingClass(&quot;org.aspectj.weaver.Advice&quot;) @ConditionalOnProperty(prefix = &quot;spring.aop&quot;, name = &quot;proxy-target-class&quot;, havingValue = &quot;true&quot;, matchIfMissing = true) static class ClassProxyingConfiguration &#123; @Bean static BeanFactoryPostProcessor forceAutoProxyCreatorToUseClassProxying() &#123; return (beanFactory) -&gt; &#123; if (beanFactory instanceof BeanDefinitionRegistry) &#123; BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; AopConfigUtils.registerAutoProxyCreatorIfNecessary(registry); AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); &#125; &#125;; &#125; &#125;&#125; @ConditionalOnProperty(prefix = &quot;spring.aop&quot;, name = &quot;auto&quot;, havingValue = &quot;true&quot;, matchIfMissing = true)：当配置文件中配置了 spring.aop.auto 属性，且值为 true 时，AopAutoConfiguration 生效。默认情况下，即使没有配置此属性，也认为其生效。 可以看出，当导入 aop 依赖时，会注册 AspectJAutoProxyingConfiguration 配置类，否则，注册 ClassProxyingConfiguration 配置类，且后者是 Spring Boot 默认开启的一个简单的 aop 功能。 例如，org.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE)// 当前配置类的配置顺序@Configuration(proxyBeanMethods = false)@ConditionalOnWebApplication(type = Type.SERVLET)// 当项目是一个原生的Web Servlet应用时@ConditionalOnClass(DispatcherServlet.class)// 当容器中存在DispatcherServlet.class时@AutoConfigureAfter(ServletWebServerFactoryAutoConfiguration.class)// 在ServletWebServerFactoryAutoConfiguration后配置public class DispatcherServletAutoConfiguration &#123; /** * The bean name for a DispatcherServlet that will be mapped to the root URL &quot;/&quot;. */ public static final String DEFAULT_DISPATCHER_SERVLET_BEAN_NAME = &quot;dispatcherServlet&quot;; /** * The bean name for a ServletRegistrationBean for the DispatcherServlet &quot;/&quot;. */ public static final String DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME = &quot;dispatcherServletRegistration&quot;; @Configuration(proxyBeanMethods = false) @Conditional(DefaultDispatcherServletCondition.class) @ConditionalOnClass(ServletRegistration.class)// 当容器中存在ServletRegistration.class时 @EnableConfigurationProperties(WebMvcProperties.class)// 开启WebMvcProperties类的配置绑定功能，并注册到容器中 protected static class DispatcherServletConfiguration &#123; @Bean(name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)// 注册DispatcherServlet组件到容器中，名字为dispatcherServlet public DispatcherServlet dispatcherServlet(WebMvcProperties webMvcProperties) &#123; DispatcherServlet dispatcherServlet = new DispatcherServlet();// 新建了一个DispatcherServlet对象 dispatcherServlet.setDispatchOptionsRequest(webMvcProperties.isDispatchOptionsRequest()); dispatcherServlet.setDispatchTraceRequest(webMvcProperties.isDispatchTraceRequest()); dispatcherServlet.setThrowExceptionIfNoHandlerFound(webMvcProperties.isThrowExceptionIfNoHandlerFound()); dispatcherServlet.setPublishEvents(webMvcProperties.isPublishRequestHandledEvents()); dispatcherServlet.setEnableLoggingRequestDetails(webMvcProperties.isLogRequestDetails()); return dispatcherServlet; &#125; @Bean// 注册MultipartResolver组件到容器中，即文件上传解析器 @ConditionalOnBean(MultipartResolver.class)// 当容器中存在MultipartResolver.class时 // 当容器中没有name为multipartResolver的MultipartResolver对象时 @ConditionalOnMissingBean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) // 用@Bean标注的方法传入的对象参数，会从容器中找一个该参数所属类型的对象，并赋值 public MultipartResolver multipartResolver(MultipartResolver resolver) &#123; // 因为容器中有MultipartResolver的对象，所以resolver参数会自动绑定该对象 // 此方法的作用是，防止有些用户配置的文件上传解析器不符合规范： // 将用户自己配置的文件上传解析器重新注册给容器，并重命名为multipartResolver(方法名) // (Spring Boot种的文件上传解析器的名字，就叫multipartResolver) // Detect if the user has created a MultipartResolver but named it incorrectly return resolver; &#125; &#125; @Configuration(proxyBeanMethods = false) @Conditional(DispatcherServletRegistrationCondition.class) @ConditionalOnClass(ServletRegistration.class) @EnableConfigurationProperties(WebMvcProperties.class) @Import(DispatcherServletConfiguration.class) protected static class DispatcherServletRegistrationConfiguration &#123; @Bean(name = DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME) @ConditionalOnBean(value = DispatcherServlet.class, name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME) public DispatcherServletRegistrationBean dispatcherServletRegistration(DispatcherServlet dispatcherServlet, WebMvcProperties webMvcProperties, ObjectProvider&lt;MultipartConfigElement&gt; multipartConfig) &#123; DispatcherServletRegistrationBean registration = new DispatcherServletRegistrationBean(dispatcherServlet, webMvcProperties.getServlet().getPath()); registration.setName(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME); registration.setLoadOnStartup(webMvcProperties.getServlet().getLoadOnStartup()); multipartConfig.ifAvailable(registration::setMultipartConfig); return registration; &#125; &#125; @Order(Ordered.LOWEST_PRECEDENCE - 10) private static class DefaultDispatcherServletCondition extends SpringBootCondition &#123; @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; ConditionMessage.Builder message = ConditionMessage.forCondition(&quot;Default DispatcherServlet&quot;); ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); List&lt;String&gt; dispatchServletBeans = Arrays .asList(beanFactory.getBeanNamesForType(DispatcherServlet.class, false, false)); if (dispatchServletBeans.contains(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)) &#123; return ConditionOutcome .noMatch(message.found(&quot;dispatcher servlet bean&quot;).items(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)); &#125; if (beanFactory.containsBean(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)) &#123; return ConditionOutcome.noMatch( message.found(&quot;non dispatcher servlet bean&quot;).items(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)); &#125; if (dispatchServletBeans.isEmpty()) &#123; return ConditionOutcome.match(message.didNotFind(&quot;dispatcher servlet beans&quot;).atAll()); &#125; return ConditionOutcome.match(message.found(&quot;dispatcher servlet bean&quot;, &quot;dispatcher servlet beans&quot;) .items(Style.QUOTE, dispatchServletBeans) .append(&quot;and none is named &quot; + DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)); &#125; &#125; @Order(Ordered.LOWEST_PRECEDENCE - 10) private static class DispatcherServletRegistrationCondition extends SpringBootCondition &#123; @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); ConditionOutcome outcome = checkDefaultDispatcherName(beanFactory); if (!outcome.isMatch()) &#123; return outcome; &#125; return checkServletRegistration(beanFactory); &#125; private ConditionOutcome checkDefaultDispatcherName(ConfigurableListableBeanFactory beanFactory) &#123; boolean containsDispatcherBean = beanFactory.containsBean(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME); if (!containsDispatcherBean) &#123; return ConditionOutcome.match(); &#125; List&lt;String&gt; servlets = Arrays .asList(beanFactory.getBeanNamesForType(DispatcherServlet.class, false, false)); if (!servlets.contains(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)) &#123; return ConditionOutcome.noMatch( startMessage().found(&quot;non dispatcher servlet&quot;).items(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)); &#125; return ConditionOutcome.match(); &#125; private ConditionOutcome checkServletRegistration(ConfigurableListableBeanFactory beanFactory) &#123; ConditionMessage.Builder message = startMessage(); List&lt;String&gt; registrations = Arrays .asList(beanFactory.getBeanNamesForType(ServletRegistrationBean.class, false, false)); boolean containsDispatcherRegistrationBean = beanFactory .containsBean(DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME); if (registrations.isEmpty()) &#123; if (containsDispatcherRegistrationBean) &#123; return ConditionOutcome.noMatch(message.found(&quot;non servlet registration bean&quot;) .items(DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME)); &#125; return ConditionOutcome.match(message.didNotFind(&quot;servlet registration bean&quot;).atAll()); &#125; if (registrations.contains(DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME)) &#123; return ConditionOutcome.noMatch(message.found(&quot;servlet registration bean&quot;) .items(DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME)); &#125; if (containsDispatcherRegistrationBean) &#123; return ConditionOutcome.noMatch(message.found(&quot;non servlet registration bean&quot;) .items(DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME)); &#125; return ConditionOutcome.match(message.found(&quot;servlet registration beans&quot;).items(Style.QUOTE, registrations) .append(&quot;and none is named &quot; + DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME)); &#125; private ConditionMessage.Builder startMessage() &#123; return ConditionMessage.forCondition(&quot;DispatcherServlet Registration&quot;); &#125; &#125;&#125; @ConditionalOnWebApplication(type = Type.SERVLET)：Spring Boot 支持两种类型的 Web 应用开发，一种是响应式，一种是原生 Servlet。响应式 Web 开发导入 spring-boot-starter-webflux 依赖，原生 Servlet Web 开发导入 spring-boot-starter-web 依赖。 @ConditionalOnClass(DispatcherServlet.class)：在主类中可以验证项目中存在 DispatcherServlet 类。 123456789@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args); String[] beanNamesForType = run.getBeanNamesForType(DispatcherServlet.class); System.out.println(beanNamesForType.length);// 1 &#125;&#125; 例如，org.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Configuration(proxyBeanMethods = false)@EnableConfigurationProperties(ServerProperties.class)// 开启ServerProperties类的配置绑定功能，并注册到容器中@ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.SERVLET)// 当项目是一个原生的Web Servlet应用时@ConditionalOnClass(CharacterEncodingFilter.class)// 当容器中存在CharacterEncodingFilter.class时// 当配置文件中server.servlet.encoding属性值为enabled(默认为true)时@ConditionalOnProperty(prefix = &quot;server.servlet.encoding&quot;, value = &quot;enabled&quot;, matchIfMissing = true)public class HttpEncodingAutoConfiguration &#123; private final Encoding properties; public HttpEncodingAutoConfiguration(ServerProperties properties) &#123; this.properties = properties.getServlet().getEncoding(); &#125; /** * 向容器中注册一个CharacterEncodingFilter组件，此组件就是解决Spring Boot收到的请求出现乱码的问题 */ @Bean @ConditionalOnMissingBean// 当容器中没有这个Bean时才配置，即用户未配置时，Spring Boot才主动配置一个 public CharacterEncodingFilter characterEncodingFilter() &#123; CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(Encoding.Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(Encoding.Type.RESPONSE)); return filter; &#125; @Bean public LocaleCharsetMappingsCustomizer localeCharsetMappingsCustomizer() &#123; return new LocaleCharsetMappingsCustomizer(this.properties); &#125; static class LocaleCharsetMappingsCustomizer implements WebServerFactoryCustomizer&lt;ConfigurableServletWebServerFactory&gt;, Ordered &#123; private final Encoding properties; LocaleCharsetMappingsCustomizer(Encoding properties) &#123; this.properties = properties; &#125; @Override public void customize(ConfigurableServletWebServerFactory factory) &#123; if (this.properties.getMapping() != null) &#123; factory.setLocaleCharsetMappings(this.properties.getMapping()); &#125; &#125; @Override public int getOrder() &#123; return 0; &#125; &#125;&#125; HttpEncodingAutoConfiguration 配置类会防止 Spring Boot 乱码。 测试： 12345678@Controllerpublic class HelloController &#123; @RequestMapping(&quot;/helloWho&quot;) @ResponseBody public String helloWho(@RequestParam(&quot;name&quot;) String name) &#123; return &quot;Hello, &quot; + name + &quot;!&quot;; &#125;&#125; 修改默认配置 一般来说，Spring Boot 默认会在底层配好所有需要的组件，但是如果用户自己配置了，就会以用户配置的优先。 以 CharacterEncodingFilter 为例，如果用户希望按自己的需求进行配置，可以在配置类中自行添加： 12345678@Configurationpublic class MyConfig &#123; @Bean public CharacterEncodingFilter characterEncodingFilter() &#123; // filter的实现代码 return null; &#125;&#125; 从前面对 CharacterEncodingFilter 的分析可以看出，当用户自己配置了 CharacterEncodingFilter 的实例时，Spring Boot 就不会再配置。 总结 Spring Boot 先加载所有的自动配置类，即 xxxxxAutoConfiguration.class。 每个自动配置类按照条件进行生效。xxxxxAutoConfiguration.class 在配置时，会从对应的 xxxxxProperties.class 中取值，而 xxxxxProperties.class 会和配置文件中对应的值进行绑定。比如： 1234567891011121314151617@Configuration(proxyBeanMethods = false)@Conditional(DefaultDispatcherServletCondition.class)@ConditionalOnClass(ServletRegistration.class)@EnableConfigurationProperties(WebMvcProperties.class)// WebMvcProperties.class与配置文件绑定protected static class DispatcherServletConfiguration &#123; @Bean(name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME) public DispatcherServlet dispatcherServlet(WebMvcProperties webMvcProperties) &#123;// 从容器中的webMvcProperties组件取值 DispatcherServlet dispatcherServlet = new DispatcherServlet(); dispatcherServlet.setDispatchOptionsRequest(webMvcProperties.isDispatchOptionsRequest()); dispatcherServlet.setDispatchTraceRequest(webMvcProperties.isDispatchTraceRequest()); dispatcherServlet.setThrowExceptionIfNoHandlerFound(webMvcProperties.isThrowExceptionIfNoHandlerFound()); dispatcherServlet.setPublishEvents(webMvcProperties.isPublishRequestHandledEvents()); dispatcherServlet.setEnableLoggingRequestDetails(webMvcProperties.isLogRequestDetails()); return dispatcherServlet; &#125;&#125; 生效的配置类，会给容器中装配很多不同功能的组件； 这些组件装配到容器中后，项目就具有了该组件所具有的功能； 如果用户自行配置了某一个组件，则以用户配置的优先。 若想实现定制化配置，有两种方法： 方法一：用户自行配置组件，添加 @Bean 注解，用以替换 Spring Boot 底层的默认组件。 方法二：用户查看该组件从配置文件种获取的是什么属性的值，然后按需求自行修改对应的属性值。比如 HttpEncodingAutoConfiguration 对应的就是配置文件中的 server.servlet.encoding 属性。 参考：https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html#application-properties 过程：xxxxxAutoConfiguration.class —&gt; 注册组件 —&gt; 从 xxxxxProperties.class 里面拿值 —-&gt; 绑定 application.properties 文件。 可以看出，一般通过修改 application.properties 文件中相应的配置，就可完成 Spring Boot 功能的修改。 最佳实践 第一步：引入相应的场景依赖。 参考：https://docs.spring.io/spring-boot/docs/current/reference/html/using.html#using.build-systems.starters 第二步：查看 Spring Boot 做了哪些自动配置。 自己查看底层源码，找出对应配置的参数。一般来说，引入一个场景后，该场景对应的自动配置都会生效。 配置文件中添加 debug=true，开启自动配置的报告。启动主程序后，即可在控制台查看所有生效和未生效的配置 — Positive (生效) / Negative (未生效)： 1debug=true 123456@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class, args); &#125;&#125; 第三步：按照需求，确定是否需要修改默写配置。 参照文档修改配置项 参考：https://docs.spring.io/spring-boot/docs/current/reference/html/application-properties.html#application-properties 自己查看底层源码，分析 xxxxxProperties.class 绑定了配置文件的哪些属性。 比如，修改 Spring Boot 启动时的 banner 图： 原图： spring.banner.image.location Banner image file location (jpg or png can also be used). classpath:banner.gif 添加配置到配置文件中，或者将 classpath 路径下的 spring.jpg 重命名为 banner.jpg (Spring Boot 默认查找 classpath 下的 banner 图片)： 1spring.banner.image.location=classpath:spring.jpg 新图： 自定义加入或者替换组件。 @Bean、@Component 等。 自定义器 xxxxxCustomizer； 第四步：实现自己所需功能的业务逻辑。 Spring Boot 的开发工具dev-tools Maven 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 重新启动项目，在后续开发时，如果对项目有改动，使用 ctrl + F9 快捷键，即可刷新项目，实现简单的热更新，其本质上是自动重启项目。 如果项目做了某些改动，ctrl + F9 之后，控制台会打印重启信息。 Spring Initailizr 项目初始化向导，能够快速的创建 Spring Boot 的项目。 New Project 时，选择需要的开发场景，Spring Boot 会自动添加所需要的依赖，并创建好主类： static：静态资源，如 css，js 等；templates：Web 页面。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 自动添加parent --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;cn.xisun.springboot&lt;/groupId&gt; &lt;artifactId&gt;helloworld&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;helloworld&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;!-- 自动添加相关依赖 --&gt; &lt;dependencies&gt; &lt;!-- Web开发 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 单元测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 自动添加打包插件 --&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; Spring Boot 2 核心功能配置文件文件类型 properties：同前面 application.properties 配置文件的写法。 yaml： YAML 是 “YAML Ain’t Markup Language” (YAML 不是一种标记语言 ) 的递归缩写。在开发这种语言时，YAML 的意思其实是：”Yet Another Markup Language” (仍是一种标记语言)。 yarm 非常适合用来做以数据为中心的配置文件。 基本语法： 书写格式：key: value，key 和 value 之间有空格； 大小写敏感； 使用缩进表示层级关系； 缩进不允许使用 tab，只允许使用空格； 缩进的空格数不重要，只要相同层级的元素左对齐即可； # 表示注释； 文件中的字符串无需加引号，如果要加，’ ‘ 内的字符串内容会被转义，” “ 内的字符串内容不会被转义。 单引号： 12person: userName: &#x27;zhangsan \\n 李四&#x27; 1234567891011@SpringBootApplicationpublic class HelloworldApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(HelloworldApplication.class, args); Person person = run.getBean(&quot;person&quot;, Person.class); System.out.println(person.getUserName()); &#125;&#125;输出结果： zhangsan \\n 李四 单引号内的 \\n，没有表现出换行的本意，而是被转义为了 \\n 字符串 — 单引号内的字符串内容会被转义。 双引号： 12person: userName: &quot;zhangsan \\n 李四&quot; 123456789101112@SpringBootApplicationpublic class HelloworldApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(HelloworldApplication.class, args); Person person = run.getBean(&quot;person&quot;, Person.class); System.out.println(person.getUserName()); &#125;&#125;输出结果： zhangsan 李四 双引号内的 \\n，表现出换行的本意，没有被转义为 \\n 字符串 — 双引号内的字符串内容不会被转义。 数据类型： 字面量：单个的、不可再分的值。如 date、boolean、string、number、null。 1key: value 对象：键值对的集合。如 map、hash、set、object。 12345678# 行内写法key: &#123;key1:value1, key2:value2, key3:value3&#125;# 缩进写法key: key1: value1 key2: value2 key3: value3 数组：一组按次序排列的值。如 array、list、queue。 12345678# 行内写法key: &#123;value1, value2, value3&#125;# 缩进写法，一个-代表一个元素key: - value1 - value2 - value3 示例： Person 和 Pet 类： 12345678910111213141516171819202122232425262728@Setter@Getter@NoArgsConstructor@AllArgsConstructor@ToString@Component@ConfigurationProperties(prefix = &quot;person&quot;)public class Person &#123; private String userName; private Boolean boss; private Date birth; private Integer age; private Pet pet; private String[] interests; private List&lt;String&gt; animal; private Map&lt;String, Object&gt; score; private Set&lt;Double&gt; salarys; private Map&lt;String, List&lt;Pet&gt;&gt; allPets;&#125; 123456789101112@Setter@Getter@NoArgsConstructor@AllArgsConstructor@ToString@Component@ConfigurationProperties(prefix = &quot;pet&quot;)public class Pet &#123; private String name; private Double weight;&#125; application.yaml 配置文件 (也可以命名为 application.yml)： 12345678910111213141516171819202122232425person: userName: zhangsan boss: false birth: 2019/12/12 20:12:33 age: 18 pet: name: tomcat weight: 23.4 interests: [篮球, 游泳] animal: - jerry - tom score: english: first: 30 second: 40 third: 50 math: [131, 140, 148] chinese: &#123;first: 128, second: 136&#125; salarys: [3999, 4999.98, 5999.99] allPets: sick: - &#123;name: tom1, weight: 33&#125; - &#123;name: jerry1, weight: 47&#125; healthy: [&#123;name: tom2, weight: 33&#125;, &#123;name: jerry2, weight: 47&#125;] 在实际开发时，配置文件的写法方式，应该统一为行内写法，或者缩进写法，不要混写。 Controller 测试： 1234567891011@Controllerpublic class HelloController &#123; @Autowired private Person person; @RequestMapping(&quot;/person&quot;) @ResponseBody public Person person() &#123; return person; &#125;&#125; 可以看出，容器中的 Person 组件，就是按照 application.yaml 配置文件进行属性配置的。 Spring Boot 项目，可以同时存在 properties 和 yaml 两种配置文件，当二者包含相同属性的配置时，propertire 配置文件会覆盖 yaml 配置文件。 application.properties： 1person.user-name=wangwu application.yaml： 12person: userName: zhangsan 主类： 1234567891011@SpringBootApplicationpublic class HelloworldApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(HelloworldApplication.class, args); Person person = run.getBean(&quot;person&quot;, Person.class); System.out.println(person.getUserName()); &#125;&#125;输出结果： wangwu 配置提示 自定义的类和配置文件绑定一般没有提示，需要添加 spring-boot-configuration-processor 依赖，这样在配置文件书写时，会进行提示： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; user-name 与 userName 效果等同。 因为 spring-boot-configuration-processor 依赖是开发过程中提供帮助，在打包程序时，应将其排除，不打包： 123456789101112131415161718&lt;!-- 打包插件 --&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;!-- 打包时排除依赖 --&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Web 开发 参考：https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.developing-web-applications Spring MVC 自动配置概览 Spring Boot provides auto-configuration for Spring MVC that works well with most applications. 大多场景都无需自定义配置。 The auto-configuration adds the following features on top of Spring’s defaults: Inclusion of ContentNegotiatingViewResolver and BeanNameViewResolver beans. 内容协商视图解析器和 BeanName 视图解析器。 Support for serving static resources, including support for WebJars (covered later in this document)). 静态资源 (包括 WebJars)。 Automatic registration of Converter, GenericConverter, and Formatter beans. 自动注册 Converter， GenericConverter 和 Formatter。 Support for HttpMessageConverters (covered later in this document). 支持 HttpMessageConverters (配合内容协商章节理解原理)。 Automatic registration of MessageCodesResolver (covered later in this document). 自动注册 MessageCodesResolver (国际化用)。 Static index.html support. 静态 index.html 页支持。 Custom Favicon support (covered later in this document). 自定义 Favicon。 Automatic use of a ConfigurableWebBindingInitializer bean (covered later in this document). 自动使用 ConfigurableWebBindingInitializer，(DataBinder 负责将请求数据绑定到 JavaBean 上)。 If you want to keep those Spring Boot MVC customizations and make more MVC customizations (interceptors, formatters, view controllers, and other features), you can add your own @Configuration class of type WebMvcConfigurer but without @EnableWebMvc. 不用 @EnableWebMvc 注解，使用 @Configuration + WebMvcConfigurer 自定义规则。 If you want to provide custom instances of RequestMappingHandlerMapping, RequestMappingHandlerAdapter, or ExceptionHandlerExceptionResolver, and still keep the Spring Boot MVC customizations, you can declare a bean of type WebMvcRegistrations and use it to provide custom instances of those components. 声明 WebMvcRegistrations 改变默认底层组件。 If you want to take complete control of Spring MVC, you can add your own @Configuration annotated with @EnableWebMvc, or alternatively add your own @Configuration-annotated DelegatingWebMvcConfiguration as described in the Javadoc of @EnableWebMvc. 使用 @EnableWebMvc + @Configuration + DelegatingWebMvcConfiguration 全面接管 Spring MVC。 Spring MVC 静态资源访问及原理静态资源访问 静态资源目录 只要静态资源放在类路径下的 /static 或者 /public 或者 /resources 或者 /META-INF/resources，都可以访问。 访问方式：当前项目根路径 / + 静态资源名。例如：http://localhost:8080/spring1.jpg。 原理：Spring Boot 静态资源访问映射 /**，即拦截所有的请求。当一个请求进来时，先去找 Controller 看能不能处理，不能处理的所有请求，都会交给静态资源处理器。如果静态资源也找不到，则响应 404 页面。 改变静态资源默认的存储路径： 12345# 单个路径spring: web: resources: static-locations: classpath:images 12345# 多个路径spring: web: resources: static-locations: [classpath:images, classpath:statics] 静态资源都需要放在 application.yaml 配置文件里标明的路径下 (有时可能不生效，更改一下路径名，刷新几次)。 默认的那几个路径不再生效，默认路径如下： 1private static final String[] CLASSPATH_RESOURCE_LOCATIONS = new String[]&#123;&quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot;&#125;; 静态资源访问前缀 静态资源访问时，默认没有前缀。 改变静态资源的访问前缀： 123spring: mvc: static-path-pattern: /res/** 再次访问静态资源时，都需要添加前缀。比如：http://localhost:8080/res/spring.jpg。 webjar (了解) Spring 把常用的一些 js 打包成 jar 包，添加引用后即可使用。官方地址：https://www.webjars.org/ 例如，使用 jquery，Maven 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.6.0&lt;/version&gt;&lt;/dependency&gt; 访问时，根据添加的 jquery 依赖的资源结构，确定访问地址：http://localhost:8080/webjars/jquery/3.6.0/jquery.js。 不同的 webjars，其访问地址可能不同，需要按照相应依赖里面的资源包路径确定。 欢迎页支持 Spring Boot supports both static and templated welcome pages. It first looks for an index.html file in the configured static content locations. If one is not found, it then looks for an index template. If either is found, it is automatically used as the welcome page of the application. Spring Boot 支持两种方式的欢迎页，一种是存放在静态资源存储路径下的 index.html，另一种是能处理动态请求 /index 的 Controller。 静态欢迎页： 12345678spring: # 配置静态资源路径，会导致welcome page失效 # mvc: # static-path-pattern: /res/** web: resources: static-locations: [classpath:images, classpath:statics] 12345678910&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello, Xisun!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 动态请求：/index，交由相应的 Controller 处理。 静态资源配置原理 Spring Boot 在启动时，默认加载 xxxxxAutoConfiguration.class，即各种自动配置类。 分析 Spring Boot 的某一项功能时，应该先查找其对应的自动配置类，从底层源码开始。 与 Spring MVC 相关的自动配置类，是 org.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration： 12345678@Configuration(proxyBeanMethods = false)@ConditionalOnWebApplication(type = Type.SERVLET)@ConditionalOnClass(&#123; Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class &#125;)@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)@AutoConfigureAfter(&#123; DispatcherServletAutoConfiguration.class, TaskExecutionAutoConfiguration.class, ValidationAutoConfiguration.class &#125;)public class WebMvcAutoConfiguration &#123;&#125; WebMvcAutoConfiguration 配置类中的 WebMvcAutoConfigurationAdapter 组件，对应了静态资源路径和访问前缀有关的规则： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263@Configuration(proxyBeanMethods = false)@Import(EnableWebMvcConfiguration.class)// 开启WebMvcProperties、ResourceProperties和WebProperties类配置绑定功能，并注册到容器中// 1.WebMvcProperties.class: spring.mvc// 2.ResourceProperties.class: spring.resources// 3.WebProperties.class: spring.web@EnableConfigurationProperties(&#123; WebMvcProperties.class, org.springframework.boot.autoconfigure.web.ResourceProperties.class, WebProperties.class &#125;)@Order(0)public static class WebMvcAutoConfigurationAdapter implements WebMvcConfigurer, ServletContextAware &#123; /** * WebMvcAutoConfigurationAdapter配置类只有一个有参构造器 * 有参构造器所有参数的值都会从容器中确定 * resourceProperties: 获取和spring.resources属性的所有值绑定的对象 * webProperties: 获取和spring.web属性的所有值绑定的对象; * mvcProperties: 获取和spring.mvc属性的所有值绑定的对象; * beanFactory: Spring的beanFactory; * messageConvertersProvider: 找到所有的HttpMessageConverters; * resourceHandlerRegistrationCustomizerProvider: 找到资源处理器的自定义器 * dispatcherServletPath: * servletRegistrations: 给应用注册Servlet、Filter.... */ public WebMvcAutoConfigurationAdapter( org.springframework.boot.autoconfigure.web.ResourceProperties resourceProperties, WebProperties webProperties, WebMvcProperties mvcProperties, ListableBeanFactory beanFactory, ObjectProvider&lt;HttpMessageConverters&gt; messageConvertersProvider, ObjectProvider&lt;ResourceHandlerRegistrationCustomizer&gt; resourceHandlerRegistrationCustomizerProvider, ObjectProvider&lt;DispatcherServletPath&gt; dispatcherServletPath, ObjectProvider&lt;ServletRegistrationBean&lt;?&gt;&gt; servletRegistrations) &#123; this.resourceProperties = resourceProperties.hasBeenCustomized() ? resourceProperties : webProperties.getResources(); this.mvcProperties = mvcProperties; this.beanFactory = beanFactory; this.messageConvertersProvider = messageConvertersProvider; this.resourceHandlerRegistrationCustomizer = resourceHandlerRegistrationCustomizerProvider.getIfAvailable(); this.dispatcherServletPath = dispatcherServletPath; this.servletRegistrations = servletRegistrations; this.mvcProperties.checkConfiguration(); &#125; /** * webjars资源处理的默认规则 */ @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; if (!this.resourceProperties.isAddMappings()) &#123; logger.debug(&quot;Default resource handling disabled&quot;); return; &#125; // webjars: 映射规则是/webjars/**,资源路径是各jar包下的classpath:/META-INF/resources/webjars/ addResourceHandler(registry, &quot;/webjars/**&quot;, &quot;classpath:/META-INF/resources/webjars/&quot;); // this.mvcProperties.getStaticPathPattern(): 静态资源默认映射是/** addResourceHandler(registry, this.mvcProperties.getStaticPathPattern(), (registration) -&gt; &#123; registration.addResourceLocations(this.resourceProperties.getStaticLocations()); if (this.servletContext != null) &#123; ServletContextResource resource = new ServletContextResource(this.servletContext, SERVLET_LOCATION); registration.addResourceLocations(resource); &#125; &#125;); &#125;&#125; 12345678910111213141516@ConfigurationProperties(&quot;spring.web&quot;)public class WebProperties &#123; public static class Resources &#123; // 静态资源默认存储路径 private static final String[] CLASSPATH_RESOURCE_LOCATIONS = &#123; &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &#125;; /** * Whether to enable default resource handling. * 通过设置spring.web.add-mappings: false, 能够禁用所有静态资源的规则, * 也就是说无论静态资源存放在哪，都无法访问，默认为true */ private boolean addMappings = true; &#125;&#125; WebMvcAutoConfiguration 配置类中的 EnableWebMvcConfiguration 组件，对应了欢迎页的处理规则： 1234567891011121314151617@Configuration(proxyBeanMethods = false)@EnableConfigurationProperties(WebProperties.class)public static class EnableWebMvcConfiguration extends DelegatingWebMvcConfiguration implements ResourceLoaderAware &#123; // HandlerMapping: 处理器映射,保存了每一个Handler能处理哪些请求. @Bean public WelcomePageHandlerMapping welcomePageHandlerMapping(ApplicationContext applicationContext, FormattingConversionService mvcConversionService, ResourceUrlProvider mvcResourceUrlProvider) &#123; WelcomePageHandlerMapping welcomePageHandlerMapping = new WelcomePageHandlerMapping( new TemplateAvailabilityProviders(applicationContext), applicationContext, getWelcomePage(), this.mvcProperties.getStaticPathPattern()); welcomePageHandlerMapping.setInterceptors(getInterceptors(mvcConversionService, mvcResourceUrlProvider)); welcomePageHandlerMapping.setCorsConfigurations(getCorsConfigurations()); return welcomePageHandlerMapping; &#125;&#125; 12345678910111213141516final class WelcomePageHandlerMapping extends AbstractUrlHandlerMapping &#123; WelcomePageHandlerMapping(TemplateAvailabilityProviders templateAvailabilityProviders, ApplicationContext applicationContext, Resource welcomePage, String staticPathPattern) &#123; // 如果使用欢迎页功能,默认映射是/**,访问index.html if (welcomePage != null &amp;&amp; &quot;/**&quot;.equals(staticPathPattern)) &#123; logger.info(&quot;Adding welcome page: &quot; + welcomePage); setRootViewName(&quot;forward:index.html&quot;); &#125; // 如果上面的条件不满足,转为发送/index请求,查看Controller是否能匹配并处理 else if (welcomeTemplateExists(templateAvailabilityProviders, applicationContext)) &#123; logger.info(&quot;Adding welcome page template: index&quot;); setRootViewName(&quot;index&quot;); &#125; &#125;&#125; Spring MVC 请求参数的处理请求映射 请求映射的方式 常使用 @RequestMapping 注解声明请求映射。比如： 1234567891011121314151617181920212223@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping &#123; String name() default &quot;&quot;; @AliasFor(&quot;path&quot;) String[] value() default &#123;&#125;; @AliasFor(&quot;value&quot;) String[] path() default &#123;&#125;; RequestMethod[] method() default &#123;&#125;; String[] params() default &#123;&#125;; String[] headers() default &#123;&#125;; String[] consumes() default &#123;&#125;; String[] produces() default &#123;&#125;;&#125; 1234567@RestControllerpublic class HelloController &#123; @RequestMapping(&quot;/hello&quot;) public String hello() &#123; return &quot;HellO, Spring Boot!&quot;; &#125;&#125; Rest 风格支持：使用 HTTP 请求方式的动词来表示对资源的操作。 以前：/getUser 表示获取用户请求，/saveUser 表示保存用户请求，/editUser 表示修改用户请求，/deleteUser 表示删除用户请求。 现在： /user 表示所有与 User 相关的请求，GET 请求表示获取用户，POST 请求表示保存用户，PUT 请求表示修改用户，DELETE 请求表示删除用户。 默认情况下，浏览器只发送 GET 和 POST 请求，不支持 PUT 和 DELETE 请求。如果要完成 Rest 风格的请求，需要在容器中配置一个 HiddenHttpMethodFilter 的组件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class HiddenHttpMethodFilter extends OncePerRequestFilter &#123; private static final List&lt;String&gt; ALLOWED_METHODS = Collections.unmodifiableList(Arrays.asList(HttpMethod.PUT.name(), HttpMethod.DELETE.name(), HttpMethod.PATCH.name())); // 表单提交时,添加一个隐藏的_method参数,该参数的值,作为最终的实际请求 /** Default method parameter: &#123;@code _method&#125;. */ public static final String DEFAULT_METHOD_PARAM = &quot;_method&quot;; private String methodParam = DEFAULT_METHOD_PARAM; /** * Set the parameter name to look for HTTP methods. * @see #DEFAULT_METHOD_PARAM */ public void setMethodParam(String methodParam) &#123; Assert.hasText(methodParam, &quot;&#x27;methodParam&#x27; must not be empty&quot;); this.methodParam = methodParam; &#125; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; // 原生Request(POST请求) HttpServletRequest requestToUse = request; // 原理: 发送的请求没有异常,且是POST请求时,会获取请求中的_method参数的值,并根据该值发送实际的请求 if (&quot;POST&quot;.equals(request.getMethod()) &amp;&amp; request.getAttribute(WebUtils.ERROR_EXCEPTION_ATTRIBUTE) == null) &#123; String paramValue = request.getParameter(this.methodParam); if (StringUtils.hasLength(paramValue)) &#123; String method = paramValue.toUpperCase(Locale.ENGLISH); // ALLOWED_METHODS: 兼容PUT、DELETE和PATCH请求 if (ALLOWED_METHODS.contains(method)) &#123; // 创建了一个新的请求,作为最终实际的请求 // 包装Request(根据_method参数的值,作为实际请求) requestToUse = new HttpMethodRequestWrapper(request, method); &#125; &#125; &#125; filterChain.doFilter(requestToUse, response); &#125; /** * Simple &#123;@link HttpServletRequest&#125; wrapper that returns the supplied method for * &#123;@link HttpServletRequest#getMethod()&#125;. */ private static class HttpMethodRequestWrapper extends HttpServletRequestWrapper &#123; private final String method; public HttpMethodRequestWrapper(HttpServletRequest request, String method) &#123; super(request); this.method = method; &#125; @Override public String getMethod() &#123; return this.method; &#125; &#125;&#125; HiddenHttpMethodFilter 会拦截 POST 请求，并根据请求中的 _method 参数的值，发送实际的请求。 HiddenHttpMethodFilter 兼容 PUT、DELETE 和 PATCH 请求，也就是说，除了 GET 和 POST 请求，Rest 风格支持上述的三种请求。 Spring Boot 自动配置的 WebMvcAutoConfiguration 中，默认提供了一个 OrderedHiddenHttpMethodFilter，但 spring.mvc.hiddenmethod.filter 值默认为 false，也就是说，Spring Boot 默认不开启 Rest 风格支持。 123456789101112131415@Configuration(proxyBeanMethods = false)@ConditionalOnWebApplication(type = Type.SERVLET)@ConditionalOnClass(&#123; Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class &#125;)@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)@AutoConfigureAfter(&#123; DispatcherServletAutoConfiguration.class, TaskExecutionAutoConfiguration.class, ValidationAutoConfiguration.class &#125;)public class WebMvcAutoConfiguration &#123; @Bean @ConditionalOnMissingBean(HiddenHttpMethodFilter.class) @ConditionalOnProperty(prefix = &quot;spring.mvc.hiddenmethod.filter&quot;, name = &quot;enabled&quot;) public OrderedHiddenHttpMethodFilter hiddenHttpMethodFilter() &#123; return new OrderedHiddenHttpMethodFilter(); &#125;&#125; 1public class OrderedHiddenHttpMethodFilter extends HiddenHttpMethodFilter implements OrderedFilter &#123;&#125; 配置类中手动开启 Rest 风格支持： 12345spring: mvc: hiddenmethod: filter: enabled: true 用法：表单 method 设置为 POST，添加隐藏域 _method，值按需求设置为 PUT 和 DELETE。如果本意是发送 POST 请求，则不需要 _method 属性。 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello, Xisun!&lt;/h1&gt;&lt;form action=&quot;/user&quot; method=&quot;get&quot;&gt; &lt;input value=&quot;RESET-GET 提交&quot; type=&quot;submit&quot;&gt;&lt;/form&gt;&lt;form action=&quot;/user&quot; method=&quot;post&quot;&gt; &lt;input value=&quot;RESET-POST 提交&quot; type=&quot;submit&quot;&gt;&lt;/form&gt;&lt;form action=&quot;/user&quot; method=&quot;post&quot;&gt; &lt;input name=&quot;_method&quot; value=&quot;PUT&quot; type=&quot;hidden&quot;&gt; &lt;input value=&quot;RESET-PUT 提交&quot; type=&quot;submit&quot;&gt;&lt;/form&gt;&lt;form action=&quot;/user&quot; method=&quot;post&quot;&gt; &lt;input name=&quot;_method&quot; value=&quot;DELETE&quot; type=&quot;hidden&quot;&gt; &lt;input value=&quot;RESET-DELETE 提交&quot; type=&quot;submit&quot;&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 1234567891011121314151617181920212223242526@RestControllerpublic class HelloController &#123; // @RequestMapping(value = &quot;/user&quot;, method = RequestMethod.GET) @GetMapping(&quot;/user&quot;) public String getUser() &#123; return &quot;GET-张三&quot;; &#125; // @RequestMapping(value = &quot;/user&quot;, method = RequestMethod.POST) @PostMapping(&quot;/user&quot;) public String saveUser() &#123; return &quot;POST-张三&quot;; &#125; // @RequestMapping(value = &quot;/user&quot;, method = RequestMethod.PUT) @PutMapping(&quot;/user&quot;) public String putUser() &#123; return &quot;PUT-张三&quot;; &#125; // @RequestMapping(value = &quot;/user&quot;, method = RequestMethod.DELETE) @DeleteMapping(&quot;/user&quot;) public String deleteUser() &#123; return &quot;DELETE-张三&quot;; &#125;&#125; @GetMapping、@PostMapping、@PutMapping 和 @DeleteMapping 四个派生注解，效果等同上面的写法。 原理 (表单提交时的情况)： 表单提交时，只有 GET 请求和 POST 请求两种方式。 表单提交会带上 _method 参数，比如 _method=PUT。 请求过来时，会被 HiddenHttpMethodFilter 拦截： 判断请求是正常的，并且是 POST 请求； 获取到 _method 参数的值。 兼容以下请求：PUT、DELETE 和 PATCH。 将原生 Request (post 请求)，使用包装模式 requesWrapper，重写 getMethod()，返回传入的 _method 的值。 过滤器链放行的时候用 requesWrapper。后续的方法调用 getMethod() 时，调用的是 requesWrapper 重写后的方法。 经过以上过程，实现了表单提交时的 Rest 风格。 如果使用客户端工具，比如 PostMan，会直接发送 PUT、DELETE 等方式的请求，无需使用 HiddenHttpMethodFilter。 扩展：修改默认的 _method 参数名。 123456@Bean@ConditionalOnMissingBean(HiddenHttpMethodFilter.class)@ConditionalOnProperty(prefix = &quot;spring.mvc.hiddenmethod.filter&quot;, name = &quot;enabled&quot;)public OrderedHiddenHttpMethodFilter hiddenHttpMethodFilter() &#123; return new OrderedHiddenHttpMethodFilter();&#125; 根据 OrderedHiddenHttpMethodFilter 的条件性注解，可以看出，当容器内没有 HiddenHttpMethodFilter 组件时，会默认注册一个 OrderedHiddenHttpMethodFilter 组件，而 OrderedHiddenHttpMethodFilter 组件默认使用 _method 参数。因此，如果希望修改 _method 参数，可以自己自定义注册一个 HiddenHttpMethodFilter 组件。 12345678910@Configuration(proxyBeanMethods = false)public class WebMvcConfig &#123; @Bean public HiddenHttpMethodFilter hiddenHttpMethodFilter() &#123; HiddenHttpMethodFilter hiddenHttpMethodFilter = new HiddenHttpMethodFilter(); // 修改默认的_method参数 hiddenHttpMethodFilter.setMethodParam(&quot;_real&quot;); return hiddenHttpMethodFilter; &#125;&#125; 请求映射的原理 处理 Web 请求时，Spring Boot 底层使用的是 Spring MVC，当请求到达时，都会先经过 DispatcherServlet，这是 Web 请求的开始。 DispatcherServlet 的继承树结构 (ctrl + H)： HttpServletBean：没有重写 HttpServlet 的 doGet() 和 doPost()，查看其子类。 FrameworkServlet：重写了 HttpServlet 的 doGet() 和 doPost()，以及其他方法。可以看出，都调用了 processRequest()，最终执行 doService()，这个方法在 FrameworkServlet 类中没有实现，查看其子类。 123456789101112131415161718192021222324252627282930313233343536373839@Overrideprotected final void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; processRequest(request, response);&#125;/** * Delegate POST requests to &#123;@link #processRequest&#125;. * @see #doService */@Overrideprotected final void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; processRequest(request, response);&#125;/** * Delegate PUT requests to &#123;@link #processRequest&#125;. * @see #doService */@Overrideprotected final void doPut(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; processRequest(request, response);&#125;/** * Delegate DELETE requests to &#123;@link #processRequest&#125;. * @see #doService */@Overrideprotected final void doDelete(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; processRequest(request, response);&#125; 123456789101112131415161718192021222324252627282930313233343536373839protected final void processRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; long startTime = System.currentTimeMillis(); Throwable failureCause = null; LocaleContext previousLocaleContext = LocaleContextHolder.getLocaleContext(); LocaleContext localeContext = buildLocaleContext(request); RequestAttributes previousAttributes = RequestContextHolder.getRequestAttributes(); ServletRequestAttributes requestAttributes = buildRequestAttributes(request, response, previousAttributes); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.registerCallableInterceptor(FrameworkServlet.class.getName(), new RequestBindingInterceptor()); initContextHolders(request, localeContext, requestAttributes); try &#123; // 最终执行的方法 doService(request, response); &#125; catch (ServletException | IOException ex) &#123; failureCause = ex; throw ex; &#125; catch (Throwable ex) &#123; failureCause = ex; throw new NestedServletException(&quot;Request processing failed&quot;, ex); &#125; finally &#123; resetContextHolders(request, previousLocaleContext, previousAttributes); if (requestAttributes != null) &#123; requestAttributes.requestCompleted(); &#125; logResult(request, response, failureCause, asyncManager); publishRequestHandledEvent(request, response, startTime, failureCause); &#125;&#125; 123// FrameworkServlet中没有实现doService(),查看其子类的实现protected abstract void doService(HttpServletRequest request, HttpServletResponse response) throws Exception; DispatcherServlet：重写了 doService()，核心方法在于调用 doDispatch()，这个方法是处理 Web 请求的最终方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Overrideprotected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; logRequest(request); // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(DEFAULT_STRATEGIES_PREFIX)) &#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); if (this.flashMapManager != null) &#123; FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); &#125; RequestPath previousRequestPath = null; if (this.parseRequestPath) &#123; previousRequestPath = (RequestPath) request.getAttribute(ServletRequestPathUtils.PATH_ATTRIBUTE); ServletRequestPathUtils.parseAndCache(request); &#125; try &#123; // 核心方法 doDispatch(request, response); &#125; finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; if (this.parseRequestPath) &#123; ServletRequestPathUtils.setParsedRequestPath(previousRequestPath, request); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. // 找出当前请求使用哪个Handler处理,也就是Controller里的哪个方法 mappedHandler = getHandler(processedRequest); if (mappedHandler == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // Determine handler adapter for the current request. HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = HttpMethod.GET.matches(method); if (isGet || HttpMethod.HEAD.matches(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // Actually invoke the handler. mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; applyDefaultViewName(processedRequest, mv); mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; catch (Throwable err) &#123; // As of 4.3, we&#x27;re processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(&quot;Handler dispatch failed&quot;, err); &#125; processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(&quot;Handler processing failed&quot;, err)); &#125; finally &#123; if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125; else &#123; // Clean up any resources used by a multipart request. if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125;&#125; mappedHandler = getHandler(processedRequest);：找出当前请求使用哪个 Handler 处理，也就是 Controller 里的哪个方法。 12345678910111213@Nullableprotected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; if (this.handlerMappings != null) &#123; // 对每一个handlerMapping映射规则循环,以找到符合当前请求的映射 for (HandlerMapping mapping : this.handlerMappings) &#123; HandlerExecutionChain handler = mapping.getHandler(request); if (handler != null) &#123; return handler; &#125; &#125; &#125; return null;&#125; processedRequest：当前的请求，包含了请求的路径。 handlerMappings：处理器映射器，是对请求的处理规则。 不同的 HandlerMapping 会处理不同的请求，上图中的五个 HandlerMapping，对应不同的功能。 此处先着重说明 RequestMappingHandlerMapping 和 WelcomePageHandlerMapping。 请求进来时，会遍历尝试所有的 HandlerMapping，看其是否有符合的请求信息。 如果有，就找到这个请求对应的 handler； 如果没有，就继续遍历下一个 HandlerMapping 查找。 RequestMappingHandlerMapping： 保存了所有 @RequestMapping 注解对应的 handler 的映射规则。 在 Spring Boot 启动时，就会扫描所有包内 Controller 中的 @RequestMapping 注解，然后保存每一个注解中的规则。 每一个映射规则，都有其所在的 Controller 和对应的方法： RequestMappingHandlerMapping 的依赖树： RequestMappingHandlerMapping 的 getHandler()：(Debug 模式下，F7 进入方法内部，查看方法的具体执行方，F8 则跳过当前方法，不查看细节) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public abstract class AbstractHandlerMapping extends WebApplicationObjectSupport implements HandlerMapping, Ordered, BeanNameAware &#123; @Override @Nullable public final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; // 获取当前请求对应的handler!!!F7进入方法内部 Object handler = getHandlerInternal(request); // 找到了handler之后,即可进行之后的业务功能、逻辑处理等操作 if (handler == null) &#123; handler = getDefaultHandler(); &#125; if (handler == null) &#123; return null; &#125; // Bean name or resolved handler? if (handler instanceof String) &#123; String handlerName = (String) handler; handler = obtainApplicationContext().getBean(handlerName); &#125; // Ensure presence of cached lookupPath for interceptors and others if (!ServletRequestPathUtils.hasCachedPath(request)) &#123; initLookupPath(request); &#125; HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Mapped to &quot; + handler); &#125; else if (logger.isDebugEnabled() &amp;&amp; !DispatcherType.ASYNC.equals(request.getDispatcherType())) &#123; logger.debug(&quot;Mapped to &quot; + executionChain.getHandler()); &#125; if (hasCorsConfigurationSource(handler) || CorsUtils.isPreFlightRequest(request)) &#123; CorsConfiguration config = getCorsConfiguration(handler, request); if (getCorsConfigurationSource() != null) &#123; CorsConfiguration globalConfig = getCorsConfigurationSource().getCorsConfiguration(request); config = (globalConfig != null ? globalConfig.combine(config) : config); &#125; if (config != null) &#123; config.validateAllowCredentials(); &#125; executionChain = getCorsHandlerExecutionChain(request, executionChain, config); &#125; return executionChain; &#125;&#125; 1234567891011121314public abstract class RequestMappingInfoHandlerMapping extends AbstractHandlerMethodMapping&lt;RequestMappingInfo&gt; &#123; @Override @Nullable protected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception &#123; request.removeAttribute(PRODUCIBLE_MEDIA_TYPES_ATTRIBUTE); try &#123; // 具体调用!!!F7进入方法内部 return super.getHandlerInternal(request); &#125; finally &#123; ProducesRequestCondition.clearMediaTypesAttribute(request); &#125; &#125;&#125; 1234567891011121314151617181920212223public abstract class AbstractHandlerMethodMapping&lt;T&gt; extends AbstractHandlerMapping implements InitializingBean &#123; // Handler method lookup /** * Look up a handler method for the given request. */ @Override @Nullable protected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception &#123; // 当前请求的路径,例如: /user String lookupPath = initLookupPath(request); // 加锁 this.mappingRegistry.acquireReadLock(); try &#123; // 查找当前请求的lookupPath路径,应该由哪个Handler处理!!!F7进入方法内部 HandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request); return (handlerMethod != null ? handlerMethod.createWithResolvedBean() : null); &#125; finally &#123; this.mappingRegistry.releaseReadLock(); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Nullableprotected HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request) throws Exception &#123; List&lt;Match&gt; matches = new ArrayList&lt;&gt;(); // 查找RequestMappingHandlerMapping中注册的所有能够处理lookupPath请求的Mapping,可能有多个 List&lt;T&gt; directPathMatches = this.mappingRegistry.getMappingsByDirectPath(lookupPath); if (directPathMatches != null) &#123; // 找到的Mapping,经过验证,找到最佳匹配的Mapping,然后添加到matches集合中 addMatchingMappings(directPathMatches, matches, request); &#125; if (matches.isEmpty()) &#123; // 如果没找到,做一些空值处理 addMatchingMappings(this.mappingRegistry.getRegistrations().keySet(), matches, request); &#125; if (!matches.isEmpty()) &#123; // 得到的最佳匹配的Mapping,正常情况下只能有一个 Match bestMatch = matches.get(0); // 同一个请求,如果有多个Mapping,会抛出异常 if (matches.size() &gt; 1) &#123; Comparator&lt;Match&gt; comparator = new MatchComparator(getMappingComparator(request)); matches.sort(comparator); bestMatch = matches.get(0); if (logger.isTraceEnabled()) &#123; logger.trace(matches.size() + &quot; matching mappings: &quot; + matches); &#125; if (CorsUtils.isPreFlightRequest(request)) &#123; for (Match match : matches) &#123; if (match.hasCorsConfig()) &#123; return PREFLIGHT_AMBIGUOUS_MATCH; &#125; &#125; &#125; else &#123; Match secondBestMatch = matches.get(1); if (comparator.compare(bestMatch, secondBestMatch) == 0) &#123; Method m1 = bestMatch.getHandlerMethod().getMethod(); Method m2 = secondBestMatch.getHandlerMethod().getMethod(); String uri = request.getRequestURI(); throw new IllegalStateException( &quot;Ambiguous handler methods mapped for &#x27;&quot; + uri + &quot;&#x27;: &#123;&quot; + m1 + &quot;, &quot; + m2 + &quot;&#125;&quot;); &#125; &#125; &#125; request.setAttribute(BEST_MATCHING_HANDLER_ATTRIBUTE, bestMatch.getHandlerMethod()); handleMatch(bestMatch.mapping, lookupPath, request); // 返回最佳匹配的结果,如下图所示,这个结果就是此次请求所对应的Mapping,以及所在的Controller和方法 return bestMatch.getHandlerMethod(); &#125; else &#123; return handleNoMatch(this.mappingRegistry.getRegistrations().keySet(), lookupPath, request); &#125;&#125; WelcomePageHandlerMapping：处理欢迎页的映射规则，访问 / 能访问到 index.html。 Spring Boot 在启动时，会在 WebMvcAutoConfiguration 配置类中注册 HandlerMapping： 12345678910111213141516171819202122232425262728293031323334353637@Configuration(proxyBeanMethods = false)@ConditionalOnWebApplication(type = Type.SERVLET)@ConditionalOnClass(&#123; Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class &#125;)@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)@AutoConfigureAfter(&#123; DispatcherServletAutoConfiguration.class, TaskExecutionAutoConfiguration.class, ValidationAutoConfiguration.class &#125;)public class WebMvcAutoConfiguration &#123; @Configuration(proxyBeanMethods = false) @EnableConfigurationProperties(WebProperties.class) public static class EnableWebMvcConfiguration extends DelegatingWebMvcConfiguration implements ResourceLoaderAware &#123; // RequestMappingHandlerMapping!!! @Bean @Primary @Override public RequestMappingHandlerMapping requestMappingHandlerMapping( @Qualifier(&quot;mvcContentNegotiationManager&quot;) ContentNegotiationManager contentNegotiationManager, @Qualifier(&quot;mvcConversionService&quot;) FormattingConversionService conversionService, @Qualifier(&quot;mvcResourceUrlProvider&quot;) ResourceUrlProvider resourceUrlProvider) &#123; // Must be @Primary for MvcUriComponentsBuilder to work return super.requestMappingHandlerMapping(contentNegotiationManager, conversionService, resourceUrlProvider); &#125; // WelcomePageHandlerMapping!!! @Bean public WelcomePageHandlerMapping welcomePageHandlerMapping(ApplicationContext applicationContext, FormattingConversionService mvcConversionService, ResourceUrlProvider mvcResourceUrlProvider) &#123; WelcomePageHandlerMapping welcomePageHandlerMapping = new WelcomePageHandlerMapping( new TemplateAvailabilityProviders(applicationContext), applicationContext, getWelcomePage(), this.mvcProperties.getStaticPathPattern()); welcomePageHandlerMapping.setInterceptors(getInterceptors(mvcConversionService, mvcResourceUrlProvider)); welcomePageHandlerMapping.setCorsConfigurations(getCorsConfigurations()); return welcomePageHandlerMapping; &#125; &#125;&#125; 如果需要一些自定义的映射处理，我们也可以自己向容器中注册 HandlerMapping。 自定义 HandlerMapping 的场合，比如：项目内包含一个系统的两个版本，v1 和 v2 版本调用不同的映射。 普通参数与基本注解 注解： @PathVariable，@RequestHeader，@RequestParam，@CookieValue Controller： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@RestControllerpublic class ParameterController &#123; /** * 假设访问路径为: localhost:8080/car/3/owner/lisi?age=18&amp;interests=basketball&amp;interests=football * &lt;p&gt; * 说明: 3这个位置为id,lisi这个位置为userName,?age=18&amp;interests=basketball&amp;interests=football为请求参数 * &lt;p&gt; * 使用@PathVariable注解: * 1.指定变量名时,可以获取访问路径中对应的变量值 * 2.不指定变量名时,可以获取访问路径中所有的变量值,但必须是Map&lt;String, String&gt;格式 * &lt;p&gt; * 使用@RequestHeader注解: * 1.指定变量名时,可以获取请求头中指定的变量值 * 2.不指定变量名时,可以获取请求头中所有的变量值,但必须是Map&lt;String, String&gt;格式 * &lt;p&gt; * 使用@RequestParam注解: * 1.指定变量名时,可以获取请求参数中指定的变量值 * 2.不指定变量名时,可以获取请求参数中所有的变量值,但必须是Map&lt;String, String&gt;格式 * &lt;p&gt; * 使用@CookieValue注解: * 1.指定变量名时,可以获取Cookie中指定的变量值,也可以直接转换为Cookie对象 * 2.注意: 如果请求中没有Cookie,使用@CookieValue注解会报错 */ @GetMapping(&quot;/car/&#123;id&#125;/owner/&#123;userName&#125;&quot;) public Map&lt;String, Object&gt; getCar(@PathVariable(&quot;id&quot;) Integer id, @PathVariable(&quot;userName&quot;) String userName, @PathVariable Map&lt;String, String&gt; paths, @RequestHeader(&quot;User-Agent&quot;) String userAgent, @RequestHeader Map&lt;String, String&gt; headers, @RequestParam(&quot;age&quot;) Integer age, @RequestParam(&quot;interests&quot;) List&lt;String&gt; interests, @RequestParam Map&lt;String, String&gt; params /*@CookieValue(&quot;_ga&quot;) String ga, @CookieValue(&quot;_ga&quot;) Cookie cookie*/) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); // 存放请求中路径变量的值 map.put(&quot;id&quot;, id); map.put(&quot;userName&quot;, userName); map.put(&quot;paths&quot;, paths); // 存放请求头中的值 map.put(&quot;userAgent&quot;, userAgent); map.put(&quot;headers&quot;, headers); // 存放请求参数中的值 map.put(&quot;age&quot;, age); map.put(&quot;interests&quot;, interests); map.put(&quot;params&quot;, params); // 存放Cookie中的值 /*map.put(&quot;_ga&quot;, ga); System.out.println(&quot;打印Cookie对象: &quot; + cookie); System.out.println(cookie.getName() + &quot; ------&gt; &quot; + cookie.getValue());*/ return map; &#125;&#125; 测试： 细节： @RequestBody index.html： 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello, Xisun!&lt;/h1&gt;&lt;form action=&quot;/save&quot; method=&quot;post&quot;&gt; 用户名: &lt;input name=&quot;userName&quot;&gt; 邮箱:&lt;input name=&quot;email&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; Controller： 123456789101112@RestControllerpublic class ParameterController &#123; /** * 对于POST请求,可以使用@RequestBody注解,获取表单提交中的参数 */ @PostMapping(&quot;/save&quot;) public Map&lt;String, Object&gt; postMethod(@RequestBody String content) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;content&quot;, content); return map; &#125;&#125; 测试： @RequestAttribute Controller： 1234567891011121314151617181920212223242526272829303132333435@Controllerpublic class RequestController &#123; @GetMapping(&quot;/goto&quot;) public String goToPage(HttpServletRequest request) &#123; // 收到/goto请求时,在请求中添加一个或多个属性,然后转发到/success请求 request.setAttribute(&quot;msg&quot;, &quot;go to success&quot;); request.setAttribute(&quot;code&quot;, 200); return &quot;forward:/success&quot;;// return &quot;/success&quot;; 这种方式也可以正常转发 &#125; /** * 有两种方式获得Request中的属性值: * 方式一: 使用@RequestAttribute注解,并指定需要获取的属性名 * 方式二: 直接从Request对象中获得属性值 * 注意: 直接访问/success请求会出异常,因为该请求中没有msg和code这两个属性 */ @ResponseBody @GetMapping(&quot;/success&quot;) public Map&lt;String, Object&gt; success(@RequestAttribute(&quot;msg&quot;) String msg, @RequestAttribute(&quot;code&quot;) Integer code, HttpServletRequest request) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); // 方式一: 使用@RequestAttribute注解获取的Request中的属性值 map.put(&quot;annotationMsg&quot;, msg); map.put(&quot;annotationCode&quot;, code); // 方式二: 直接从Request对象中获得属性值 String reqMsg = (String) request.getAttribute(&quot;msg&quot;); map.put(&quot;requestMsg&quot;, reqMsg); Integer reqCode = (Integer) request.getAttribute(&quot;code&quot;); map.put(&quot;requestCode&quot;, reqCode); return map; &#125;&#125; 测试： @MatrixVariable：矩阵变量注解，此处省略不谈。 @ModelAttribute：此处省略不谈。 s 数据访问单元测试指标监控原理解析本文参考https://www.bilibili.com/video/BV19K4y1L7MT https://www.yuque.com/atguigu/springboot 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"}]},{"title":"Docker 入门","slug":"docker-base","date":"2021-05-17T02:30:51.000Z","updated":"2022-01-03T00:58:11.809Z","comments":true,"path":"2021/05/17/docker-base/","link":"","permalink":"http://example.com/2021/05/17/docker-base/","excerpt":"","text":"Docker 简介Docker 出现的背景 一款产品从开发到上线，从操作系统，到运行环境，再到应用配置。作为开发 + 运维之间的协作我们需要关心很多东西，这也是很多互联网公司都不得不面对的问题，特别是各种版本的迭代之后，不同版本环境的兼容，对运维人员都是考验。 环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。 Docker 之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。开发人员利用 Docker 可以消除协作编码时 “在我的机器上可正常工作” 的问题。 之前在服务器配置一个应用的运行环境，要安装各种软件。安装和配置这些东西有多麻烦就不说了，它还不能跨平台。假如我们是在 Windows 上安装的这些环境，到了 Linux 又得重新装。况且就算不跨操作系统，换另一台同样操作系统的服务器，要移植应用也是非常麻烦的。 传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等。而为了让这程序可以顺利执行，开发团队也得准备完整的部署文件，让运维团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件 + 所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker 镜像的设计，使得 Docker 得以打破过去「程序即应用」的观念。透过镜像 (images) 将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运作。 Docker 的理念 Docker 是基于 Go 语言实现的云开源项目。 Docker 的主要目标是 “Build, Ship and Run Any App, Anywhere“，也就是通过对应用组件的封装、分发、部署、运行等生命期的管理，使用户的 APP (可以是一个 WEB 应用或数据库应用等等) 及其运行环境能够做到 “一次封装，到处运行“。 Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。 总之，Docker 是一个解决了运行环境和配置问题的软件容器，是方便做持续集成并有助于整体发布的容器虚拟化技术。 Docker 的基本组成架构图 镜像 (Image) Docker 镜像就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 镜像与容器的关系类似于面向对象编程中的类与对象： Docker 面向对象 镜像 类 容器 对象 容器 (Container) Docker 利用容器独立运行一个或一组应用。容器是用镜像创建的运行实例。 容器可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 容器可以看做是一个简易版的 Linux 环境 (包括 root 用户权限、进程空间、用户空间和网络空间等) 和运行在其中的应用程序。 容器的定义和镜像几乎一模一样，也是一堆层的统一视角， 唯一区别在于容器的最上面那一层是可读可写的。 仓库 (Repository) 仓库是集中存放镜像文件的场所。 仓库和仓库注册服务器 (Registry) 是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多镜像，每个镜像有不同的标签 (tag) 。 仓库分为公开仓库 (Public) 和私有仓库 (Private) 两种形式。 最大的公开仓库是 Docker Hub ( https://hub.docker.com/ )，存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云、网易云等。 总结 Docker本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就是 image 镜像文件。只有通过这个镜像文件才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 image 文件生成的容器实例，本身也是一个文件，称为镜像文件。 一个容器运行一种服务，当我们需要的时候，就可以通过 Docker 客户端创建一个对应的运行实例，也就是我们的容器。 至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。 底层原理Docker 是怎样工作的 Docker 是一个 Client-Server 结构的系统，Docker 守护进程运行在主机上，然后通过 Socket 连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。容器，是一个运行时环境，就是我们前面说到的集装箱。 Docker 为什么比 VM 快 Docker 有着比虚拟机更少的抽象层。由于 Docker 不需要 Hypervisor 实现硬件资源虚拟化，运行在 Docker 容器上的程序直接使用的都是实际物理机的硬件资源。因此在 CPU、内存利用率上，Docker 将会在效率上有明显优势。 Docker 利用的是宿主机的内核，而不需要 Guest OS。因此，当新建一个容器时，Docker 不需要和虚拟机一样重新加载一个操作系统内核，因此可以避免引寻、加载操作系统内核这个比较费时费资源的过程，当新建一个虚拟机时，虚拟机软件需要加载 Guest OS，这个新建过程是分钟级别的。而 Docker 由于直接利用宿主机的操作系统，则省略了返个过程，因此新建一个 Docker 容器只需要几秒钟。 Docker 安装 官网：https://hub.docker.com/ Linux 安装：https://hub.docker.com/search?q=&amp;type=edition&amp;offering=community&amp;operating_system=linux WSL 安装 默认已经安装 WSL。 默认安装的 WSL version 是 1，在 Windows PowerShell 中查看： 123PS C:\\Users\\Xisun\\Desktop&gt; wsl --list -v NAME STATE VERSION* Ubuntu Running 1 在 Windows PowerShell 中，切换 WSL version 为 2： 启用 Hyper-V 功能： 启用 Hyper-V 功能后，需要重启电脑。 再按以下步骤依次执行： 参考：https://docs.microsoft.com/en-us/windows/wsl/install-win10 切换 version： 1wsl --set-version &lt;distribution name&gt; &lt;versionNumber&gt; 1234567PS C:\\Users\\Xisun\\Desktop&gt; wsl --set-version Ubuntu 2正在进行转换，这可能需要几分钟时间...有关与 WSL 2 的主要区别的信息，请访问 https://aka.ms/wsl2转换完成。PS C:\\Users\\Xisun\\Desktop&gt; wsl --list -v NAME STATE VERSION* Ubuntu Running 2 WSL 默认不支持 Docker，需要破解： 破解步骤： 参考：https://github.com/arkane-systems/genie 以管理员身份打开 Windows PowerShell：win + x 快捷键，然后执行命令 wsl，进入 WSL 控制台，并切换到 root 用户： 1234PS C:\\WINDOWS\\system32&gt; wslxisun@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32$ su rootPassword:root@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32# 如果忘记 root 用户密码，可以如下方式重置： 1231.以管理员身份打开Windows PowerShell;2.输入命令: wsl.exe --user root;3.输入命令: passwd root, 修改root用户密码。 安装 dotnet： 查看 Ubuntu 版本： 方式一： 12root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# cat /proc/versionLinux version 5.4.72-microsoft-standard-WSL2 (oe-user@oe-host) (gcc version 8.2.0 (GCC)) #1 SMP Wed Oct 28 23:40:43 UTC 2020 方式二： 123456root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 20.04.2 LTSRelease: 20.04Codename: focal 查看内核版本号： 12root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# uname -r5.4.72-microsoft-standard-WSL2 安装对应版本的 dotnet： 参考：https://docs.microsoft.com/zh-cn/dotnet/core/install/linux-ubuntu 将 Microsoft 包签名密钥添加到受信任密钥列表，并添加包存储库。 12wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.debsudo dpkg -i packages-microsoft-prod.deb 123456789101112131415161718root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.debo dpkg -i packages-microsoft-prod.deb--2021-06-08 21:15:31-- https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.debResolving packages.microsoft.com (packages.microsoft.com)... 65.52.183.205Connecting to packages.microsoft.com (packages.microsoft.com)|65.52.183.205|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 3124 (3.1K) [application/octet-stream]Saving to: ‘packages-microsoft-prod.deb’packages-microsoft-prod.deb 100%[=================================================&gt;] 3.05K --.-KB/s in 0s2021-06-08 21:15:32 (523 MB/s) - ‘packages-microsoft-prod.deb’ saved [3124/3124]root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sudo dpkg -i packages-microsoft-prod.debSelecting previously unselected package packages-microsoft-prod.(Reading database ... 47281 files and directories currently installed.)Preparing to unpack packages-microsoft-prod.deb ...Unpacking packages-microsoft-prod (1.0-ubuntu20.04.1) ...Setting up packages-microsoft-prod (1.0-ubuntu20.04.1) ... 安装 SDK： 1234sudo apt-get update; \\ sudo apt-get install -y apt-transport-https &amp;&amp; \\ sudo apt-get update &amp;&amp; \\ sudo apt-get install -y dotnet-sdk-5.0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sudo apt-get update; \\&gt; sudo apt-get install -y apt-transport-https &amp;&amp; \\&gt; sudo apt-get update &amp;&amp; \\&gt; sudo apt-get install -y dotnet-sdk-5.0Get:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InRelease [10.5 kB]Get:2 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 Packages [74.9 kB]Hit:3 http://archive.ubuntu.com/ubuntu focal InReleaseGet:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]Get:6 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [702 kB]Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [101 kB]Get:8 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [141 kB]Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1026 kB]Get:10 http://security.ubuntu.com/ubuntu focal-security/main amd64 c-n-f Metadata [7780 B]Get:11 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [247 kB]Get:12 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [36.1 kB]Get:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 c-n-f Metadata [456 B]Get:14 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [588 kB]Get:15 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [94.6 kB]Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [11.5 kB]Get:17 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [19.9 kB]Get:18 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [4316 B]Get:19 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [528 B]Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main Translation-en [229 kB]Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 c-n-f Metadata [13.5 kB]Get:22 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [266 kB]Get:23 http://archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [38.9 kB]Get:24 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 c-n-f Metadata [456 B]Get:25 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [781 kB]Get:26 http://archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [170 kB]Get:27 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [17.6 kB]Get:28 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [23.6 kB]Get:29 http://archive.ubuntu.com/ubuntu focal-updates/multiverse Translation-en [6376 B]Get:30 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [648 B]Fetched 4840 kB in 4s (1125 kB/s)Reading package lists... DoneReading package lists... DoneBuilding dependency treeReading state information... DoneThe following NEW packages will be installed: apt-transport-https0 upgraded, 1 newly installed, 0 to remove and 101 not upgraded.Need to get 1704 B of archives.After this operation, 161 kB of additional disk space will be used.Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.5 [1704 B]Fetched 1704 B in 0s (3469 B/s)Selecting previously unselected package apt-transport-https.(Reading database ... 47289 files and directories currently installed.)Preparing to unpack .../apt-transport-https_2.0.5_all.deb ...Unpacking apt-transport-https (2.0.5) ...Setting up apt-transport-https (2.0.5) ...Hit:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InReleaseHit:2 http://security.ubuntu.com/ubuntu focal-security InReleaseHit:3 http://archive.ubuntu.com/ubuntu focal InReleaseHit:4 http://archive.ubuntu.com/ubuntu focal-updates InReleaseHit:5 http://archive.ubuntu.com/ubuntu focal-backports InReleaseReading package lists... DoneReading package lists... DoneBuilding dependency treeReading state information... DoneThe following additional packages will be installed: aspnetcore-runtime-5.0 aspnetcore-targeting-pack-5.0 dotnet-apphost-pack-5.0 dotnet-host dotnet-hostfxr-5.0 dotnet-runtime-5.0 dotnet-runtime-deps-5.0 dotnet-targeting-pack-5.0 netstandard-targeting-pack-2.1The following NEW packages will be installed: aspnetcore-runtime-5.0 aspnetcore-targeting-pack-5.0 dotnet-apphost-pack-5.0 dotnet-host dotnet-hostfxr-5.0 dotnet-runtime-5.0 dotnet-runtime-deps-5.0 dotnet-sdk-5.0 dotnet-targeting-pack-5.0 netstandard-targeting-pack-2.10 upgraded, 10 newly installed, 0 to remove and 101 not upgraded.Need to get 95.1 MB of archives.After this operation, 396 MB of additional disk space will be used.Get:1 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-runtime-deps-5.0 amd64 5.0.6-1 [2642 B]Get:2 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-host amd64 5.0.6-1 [52.5 kB]Get:3 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-hostfxr-5.0 amd64 5.0.6-1 [140 kB]Get:4 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-runtime-5.0 amd64 5.0.6-1 [22.1 MB]Get:5 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 aspnetcore-runtime-5.0 amd64 5.0.6-1 [6086 kB]Get:6 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-targeting-pack-5.0 amd64 5.0.0-1 [2086 kB]Get:7 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 aspnetcore-targeting-pack-5.0 amd64 5.0.0-1 [1316 kB]Get:8 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-apphost-pack-5.0 amd64 5.0.6-1 [3412 kB]Get:9 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 netstandard-targeting-pack-2.1 amd64 2.1.0-1 [1476 kB]Get:10 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-sdk-5.0 amd64 5.0.300-1 [58.4 MB]Fetched 95.1 MB in 1min 11s (1332 kB/s)Selecting previously unselected package dotnet-runtime-deps-5.0.(Reading database ... 47293 files and directories currently installed.)Preparing to unpack .../0-dotnet-runtime-deps-5.0_5.0.6-1_amd64.deb ...Unpacking dotnet-runtime-deps-5.0 (5.0.6-1) ...Selecting previously unselected package dotnet-host.Preparing to unpack .../1-dotnet-host_5.0.6-1_amd64.deb ...Unpacking dotnet-host (5.0.6-1) ...Selecting previously unselected package dotnet-hostfxr-5.0.Preparing to unpack .../2-dotnet-hostfxr-5.0_5.0.6-1_amd64.deb ...Unpacking dotnet-hostfxr-5.0 (5.0.6-1) ...Selecting previously unselected package dotnet-runtime-5.0.Preparing to unpack .../3-dotnet-runtime-5.0_5.0.6-1_amd64.deb ...Unpacking dotnet-runtime-5.0 (5.0.6-1) ...Selecting previously unselected package aspnetcore-runtime-5.0.Preparing to unpack .../4-aspnetcore-runtime-5.0_5.0.6-1_amd64.deb ...Unpacking aspnetcore-runtime-5.0 (5.0.6-1) ...Selecting previously unselected package dotnet-targeting-pack-5.0.Preparing to unpack .../5-dotnet-targeting-pack-5.0_5.0.0-1_amd64.deb ...Unpacking dotnet-targeting-pack-5.0 (5.0.0-1) ...Selecting previously unselected package aspnetcore-targeting-pack-5.0.Preparing to unpack .../6-aspnetcore-targeting-pack-5.0_5.0.0-1_amd64.deb ...Unpacking aspnetcore-targeting-pack-5.0 (5.0.0-1) ...Selecting previously unselected package dotnet-apphost-pack-5.0.Preparing to unpack .../7-dotnet-apphost-pack-5.0_5.0.6-1_amd64.deb ...Unpacking dotnet-apphost-pack-5.0 (5.0.6-1) ...Selecting previously unselected package netstandard-targeting-pack-2.1.Preparing to unpack .../8-netstandard-targeting-pack-2.1_2.1.0-1_amd64.deb ...Unpacking netstandard-targeting-pack-2.1 (2.1.0-1) ...Selecting previously unselected package dotnet-sdk-5.0.Preparing to unpack .../9-dotnet-sdk-5.0_5.0.300-1_amd64.deb ...Unpacking dotnet-sdk-5.0 (5.0.300-1) ...Setting up dotnet-host (5.0.6-1) ...Setting up dotnet-runtime-deps-5.0 (5.0.6-1) ...Setting up netstandard-targeting-pack-2.1 (2.1.0-1) ...Setting up dotnet-hostfxr-5.0 (5.0.6-1) ...Setting up dotnet-apphost-pack-5.0 (5.0.6-1) ...Setting up dotnet-targeting-pack-5.0 (5.0.0-1) ...Setting up aspnetcore-targeting-pack-5.0 (5.0.0-1) ...Setting up dotnet-runtime-5.0 (5.0.6-1) ...Setting up aspnetcore-runtime-5.0 (5.0.6-1) ...Setting up dotnet-sdk-5.0 (5.0.300-1) ...This software may collect information about you and your use of the software, and send that to Microsoft.Please visit http://aka.ms/dotnet-cli-eula for more information.Welcome to .NET!---------------------Learn more about .NET: https://aka.ms/dotnet-docsUse &#x27;dotnet --help&#x27; to see available commands or visit: https://aka.ms/dotnet-cli-docsTelemetry---------The .NET tools collect usage data in order to help us improve your experience. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#x27;1&#x27; or &#x27;true&#x27; using your favorite shell.Read more about .NET CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetryConfiguring...--------------A command is running to populate your local package cache to improve restore speed and enable offline access. This command takes up to one minute to complete and only runs once.Processing triggers for man-db (2.9.1-1) ... 安装运行时： 1234sudo apt-get update; \\ sudo apt-get install -y apt-transport-https &amp;&amp; \\ sudo apt-get update &amp;&amp; \\ sudo apt-get install -y aspnetcore-runtime-5.0 dotnet-sdk-5.0 安装成功后，会一起安装 aspnetcore-runtime-5.0。 检查 dotnet 版本： 123456789101112131415161718root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# dotnetUsage: dotnet [options]Usage: dotnet [path-to-application]Options: -h|--help Display help. --info Display .NET information. --list-sdks Display the installed SDKs. --list-runtimes Display the installed runtimes.path-to-application: The path to an application .dll file to execute.root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# dotnet --list-sdks5.0.300 [/usr/share/dotnet/sdk]root@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32# dotnet --list-runtimesMicrosoft.AspNetCore.App 5.0.6 [/home/xisun/.dotnet/shared/Microsoft.AspNetCore.App]Microsoft.NETCore.App 5.0.6 [/home/xisun/.dotnet/shared/Microsoft.NETCore.App] 安装 wsl-translinux： 参考：https://arkane-systems.github.io/wsl-transdebian/ 123456789101112apt install apt-transport-httpswget -O /etc/apt/trusted.gpg.d/wsl-transdebian.gpg https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpgchmod a+r /etc/apt/trusted.gpg.d/wsl-transdebian.gpgcat &lt;&lt; EOF &gt; /etc/apt/sources.list.d/wsl-transdebian.listdeb https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) maindeb-src https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) mainEOFapt update 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# apt install apt-transport-httpswget -O /etc/apt/trusted.gpg.d/wsl-transdebian.gpg https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpgchmod a+r /etc/apt/trusted.gpg.d/wsl-transdebian.gpgcat &lt;&lt; EOF &gt; /etc/apt/sources.list.d/wsl-transdebian.listdeb https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) maindeb-src https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) mainEOFReading package lists... DoneBuilding dependency treeReading state information... Doneapt-transport-https is already the newest version (2.0.5).0 upgraded, 0 newly installed, 0 to remove and 101 not upgraded.root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# wget -O /etc/apt/trusted.gpg.d/wsl-transdebian.gpg https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpg--2021-06-08 21:23:47-- https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpgResolving arkane-systems.github.io (arkane-systems.github.io)... 185.199.109.153, 185.199.108.153, 185.199.110.153, ...Connecting to arkane-systems.github.io (arkane-systems.github.io)|185.199.109.153|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 2280 (2.2K) [application/octet-stream]Saving to: ‘/etc/apt/trusted.gpg.d/wsl-transdebian.gpg’/etc/apt/trusted.gpg.d/wsl-tr 100%[=================================================&gt;] 2.23K --.-KB/s in 0s2021-06-08 21:23:49 (36.1 MB/s) - ‘/etc/apt/trusted.gpg.d/wsl-transdebian.gpg’ saved [2280/2280]root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# chmod a+r /etc/apt/trusted.gpg.d/wsl-transdebian.gpgroot@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# cat &lt;&lt; EOF &gt; /etc/apt/sources.list.d/wsl-transdebian.list&gt; deb https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) main&gt; deb-src https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) main&gt; EOFroot@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# apt updateHit:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InReleaseHit:2 http://archive.ubuntu.com/ubuntu focal InReleaseHit:3 http://security.ubuntu.com/ubuntu focal-security InReleaseHit:4 http://archive.ubuntu.com/ubuntu focal-updates InReleaseGet:5 https://arkane-systems.github.io/wsl-transdebian/apt focal InRelease [2495 B]Hit:6 http://archive.ubuntu.com/ubuntu focal-backports InReleaseGet:7 https://arkane-systems.github.io/wsl-transdebian/apt focal/main Sources [1338 B]Get:8 https://arkane-systems.github.io/wsl-transdebian/apt focal/main amd64 Packages [1897 B]Fetched 5730 B in 2s (3130 B/s)Reading package lists... DoneBuilding dependency treeReading state information... Done101 packages can be upgraded. Run &#x27;apt list --upgradable&#x27; to see them. 安装 genie： 12sudo apt updatesudo apt install -y systemd-genie 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sudo apt updateHit:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InReleaseHit:2 http://security.ubuntu.com/ubuntu focal-security InReleaseHit:3 http://archive.ubuntu.com/ubuntu focal InReleaseHit:4 http://archive.ubuntu.com/ubuntu focal-updates InReleaseHit:5 https://arkane-systems.github.io/wsl-transdebian/apt focal InReleaseHit:6 http://archive.ubuntu.com/ubuntu focal-backports InReleaseReading package lists... DoneBuilding dependency treeReading state information... Done101 packages can be upgraded. Run &#x27;apt list --upgradable&#x27; to see them.root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sudo apt install -y systemd-genieReading package lists... DoneBuilding dependency treeReading state information... DoneThe following additional packages will be installed: daemonize libnss-mymachines libnss-systemd libpam-systemd libsystemd0 systemd systemd-container systemd-sysv systemd-timesyncdThe following NEW packages will be installed: daemonize libnss-mymachines systemd-container systemd-genieThe following packages will be upgraded: libnss-systemd libpam-systemd libsystemd0 systemd systemd-sysv systemd-timesyncd6 upgraded, 4 newly installed, 0 to remove and 95 not upgraded.Need to get 5359 kB of archives.After this operation, 3892 kB of additional disk space will be used.Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libnss-systemd amd64 245.4-4ubuntu3.6 [95.8 kB]Get:2 https://arkane-systems.github.io/wsl-transdebian/apt focal/main amd64 systemd-genie amd64 1.42 [504 kB]Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 systemd-timesyncd amd64 245.4-4ubuntu3.6 [28.1 kB]Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 systemd-sysv amd64 245.4-4ubuntu3.6 [10.3 kB]Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-systemd amd64 245.4-4ubuntu3.6 [186 kB]Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 systemd amd64 245.4-4ubuntu3.6 [3805 kB]Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsystemd0 amd64 245.4-4ubuntu3.6 [269 kB]Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 daemonize amd64 1.7.8-1 [11.9 kB]Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 systemd-container amd64 245.4-4ubuntu3.6 [317 kB]Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libnss-mymachines amd64 245.4-4ubuntu3.6 [131 kB]Fetched 5359 kB in 5s (1137 kB/s)(Reading database ... 50558 files and directories currently installed.)Preparing to unpack .../0-libnss-systemd_245.4-4ubuntu3.6_amd64.deb ...Unpacking libnss-systemd:amd64 (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../1-systemd-timesyncd_245.4-4ubuntu3.6_amd64.deb ...Unpacking systemd-timesyncd (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../2-systemd-sysv_245.4-4ubuntu3.6_amd64.deb ...Unpacking systemd-sysv (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../3-libpam-systemd_245.4-4ubuntu3.6_amd64.deb ...Unpacking libpam-systemd:amd64 (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../4-systemd_245.4-4ubuntu3.6_amd64.deb ...Unpacking systemd (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../5-libsystemd0_245.4-4ubuntu3.6_amd64.deb ...Unpacking libsystemd0:amd64 (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Setting up libsystemd0:amd64 (245.4-4ubuntu3.6) ...Selecting previously unselected package daemonize.(Reading database ... 50558 files and directories currently installed.)Preparing to unpack .../daemonize_1.7.8-1_amd64.deb ...Unpacking daemonize (1.7.8-1) ...Selecting previously unselected package systemd-container.Preparing to unpack .../systemd-container_245.4-4ubuntu3.6_amd64.deb ...Unpacking systemd-container (245.4-4ubuntu3.6) ...Selecting previously unselected package systemd-genie.Preparing to unpack .../systemd-genie_1.42_amd64.deb ...Unpacking systemd-genie (1.42) ...Selecting previously unselected package libnss-mymachines:amd64.Preparing to unpack .../libnss-mymachines_245.4-4ubuntu3.6_amd64.deb ...Unpacking libnss-mymachines:amd64 (245.4-4ubuntu3.6) ...Setting up daemonize (1.7.8-1) ...Setting up systemd (245.4-4ubuntu3.6) ...Initializing machine ID from random generator.Setting up systemd-timesyncd (245.4-4ubuntu3.6) ...Setting up systemd-container (245.4-4ubuntu3.6) ...Created symlink /etc/systemd/system/multi-user.target.wants/machines.target → /lib/systemd/system/machines.target.Setting up systemd-sysv (245.4-4ubuntu3.6) ...Setting up systemd-genie (1.42) ...Created symlink /etc/systemd/system/sockets.target.wants/wslg-xwayland.socket → /lib/systemd/system/wslg-xwayland.socket.Setting up libnss-systemd:amd64 (245.4-4ubuntu3.6) ...Setting up libnss-mymachines:amd64 (245.4-4ubuntu3.6) ...First installation detected...Checking NSS setup...Setting up libpam-systemd:amd64 (245.4-4ubuntu3.6) ...Processing triggers for libc-bin (2.31-0ubuntu9.2) ...Processing triggers for man-db (2.9.1-1) ...Processing triggers for dbus (1.12.16-2ubuntu2.1) ... 破解完成之后，即可在 WSL 中安装 Docker (利用脚本安装)： 12curl -fsSL https://get.docker.com -o get-docker.shsh get-docker.sh 12345678910111213141516171819202122232425262728293031323334353637383940root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# curl -fsSL https://get.docker.com -o get-docker.shroot@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sh get-docker.sh# Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737WSL DETECTED: We recommend using Docker Desktop for Windows.Please get Docker Desktop from https://www.docker.com/products/docker-desktopYou may press Ctrl+C now to abort this script.+ sleep 20+ sh -c apt-get update -qq &gt;/dev/null+ sh -c DEBIAN_FRONTEND=noninteractive apt-get install -y -qq apt-transport-https ca-certificates curl &gt;/dev/null+ sh -c curl -fsSL &quot;https://download.docker.com/linux/ubuntu/gpg&quot; | apt-key add -qq - &gt;/dev/nullWarning: apt-key output should not be parsed (stdout is not a terminal)+ sh -c echo &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable&quot; &gt; /etc/apt/sources.list.d/docker.list+ sh -c apt-get update -qq &gt;/dev/null+ [ -n ]+ sh -c apt-get install -y -qq --no-install-recommends docker-ce &gt;/dev/null+ [ -n 1 ]+ sh -c DEBIAN_FRONTEND=noninteractive apt-get install -y -qq docker-ce-rootless-extras &gt;/dev/null================================================================================To run Docker as a non-privileged user, consider setting up theDocker daemon in rootless mode for your user: dockerd-rootless-setuptool.sh installVisit https://docs.docker.com/go/rootless/ to learn about rootless mode.To run the Docker daemon as a fully privileged service, but granting non-rootusers access, refer to https://docs.docker.com/go/daemon-access/WARNING: Access to the remote API on a privileged Docker daemon is equivalent to root access on the host. Refer to the &#x27;Docker daemon attack surface&#x27; documentation for details: https://docs.docker.com/go/attack-surface/================================================================================ 参考：https://github.com/docker/docker-install Docker 安装成功后，启动 Docker 服务： 12root@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32# service docker start * Starting Docker: docker [ OK ] Docker 服务如果没有启动，执行 Docker 的命令时，会提示：Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?。 Docker 服务启动后，如果不手动关闭，会一直运行，即使关闭 Windows PowerShell 也不会关闭。 如果关闭电脑，需要重启 Docker 服务。 查看 Docker version： 1234567891011121314151617181920212223242526272829root@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32# docker versionClient: Docker Engine - Community Version: 20.10.6 API version: 1.41 Go version: go1.13.15 Git commit: 370c289 Built: Fri Apr 9 22:47:17 2021 OS/Arch: linux/amd64 Context: default Experimental: trueServer: Docker Engine - Community Engine: Version: 20.10.7 API version: 1.41 (minimum version 1.12) Go version: go1.13.15 Git commit: b0f5bc3 Built: Wed Jun 2 11:54:50 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.6 GitCommit: d71fcd7d8303cbf684402823e425e9dd2e99285d runc: Version: 1.0.0-rc95 GitCommit: b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7 docker-init: Version: 0.19.0 GitCommit: de40ad0 测试 Docker，运行 hello-world： 12345678910111213141516171819202122232425262728293031root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# docker run hello-worldUnable to find image &#x27;hello-world:latest&#x27; locallylatest: Pulling from library/hello-worldb8dfde127a29: Pull completeDigest: sha256:9f6ad537c5132bcce57f7a0a20e317228d382c3cd61edae14650eec68b2b345cStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest d1165f221234 3 months ago 13.3kB 关闭 Docker 服务： 12root@DESKTOP-OJKMETJ:/mnt/c/Users/Ziyoo# service docker stop * Stopping Docker: docker [ OK ] 启动和关闭 Docker 服务时，必须使用 root 用户。 Ubuntu 安装 参考：https://docs.docker.com/engine/install/ubuntu/ 按照官网指示一步步执行，即可安装 Docker。主要涉及如下命令，各命令的含义参考官网： 12345678910111213141516171819202122$ sudo apt-get remove docker docker-engine docker.io containerd runc$ sudo apt-get update$ sudo apt-get install apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \\ sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg$ echo \\ &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null$ sudo apt-get update$ sudo apt-get install docker-ce docker-ce-cli containerd.io 以上命令默认安装的为 Docker 最新版本，若需要安装特定版本，请参考官网。 Docker 常用命令帮助命令12345$ docker version$ docker info$ docker --help 镜像命令 列出本机上的镜像 1$ docker images [OPTIONS] OPTIONS 说明： 1234-a 列出本地所有的镜像(含中间映射层)-q 只显示镜像ID--digests 显示镜像的摘要信息--no-trunc 显示完整的镜像信息 查询某个镜像 1$ docker search [OPTIONS] 镜像名字 OPTIONS 说明： 123--no-trunc 显示完整的镜像描述-s 列出收藏数不小于指定值的镜像--automated 只列出 automated build类型的镜像 官方镜像仓库：https://hub.docker.com/ 下载镜像 1$ docker pull 镜像名字[:TAG] 一般设置从阿里云镜像下载。 docker pull tomcat 等价于 docker pull tomcat:latest，即默认下载最新版本。 删除镜像 删除单个镜像： 1$ docker rmi -f 镜像名[:TAG] 删除多个镜像： 1$ docker rmi -f 镜像名1[:TAG] 镜像名2[:TAG] 镜像名3[:TAG] ... 删除全部镜像： 1$ docker rmi -f $(docker images -qa) 容器命令 有镜像才能创建容器，这是根本前提。先下载一个 CentOS 镜像作为示例： 1$ docker pull centos 新建并启动容器 1$ docker run [OPTIONS] IMAGE [COMMAND][ARG] OPTIONS 说明： 12345678910--name 为容器指定一个名称，若不指定，由系统随机分配-d 后台运行容器，并返回容器ID，即启动守护式容器-i 以交互模式运行容器，通常与-t同时使用-t 为容器重新分配一个伪输入终端，通常与-i同时使用-P 随机端口映射-p 指定端口映射，有以下四种格式： ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 此时启动的是一个交互式的容器。 列出当前所有正在运行的容器 1$ docker ps [OPTIONS] OPTIONS 说明： 12345-a 列出当前所有正在运行的容器+历史上运行过的-| 显示最近创建的容器-n 显示最近n个创建的容器-q 静默模式，只显示容器编号--no-trunc 不截断输出 退出容器 方式一，停止容器并退出：exit。 方式二，不停止容器退出：ctrl + P + Q。 启动容器 1$ docker start 容器ID或容器名 重启容器 1$ docker restart 容器ID或容器名 停止容器 1$ docker stop 容器ID或容器名 强制停止容器 1$ docker kill 容器ID或容器名 删除已停止的容器 普通删除： 1$ docker rm 容器ID或容器名 强制删除： 1$ docker rm -f 容器ID或容器名 -f 可以删除没有停止的容器。 删除多个： 1$ docker rm -f $(docker ps -aq) 1$ docker ps -aq | xargs docker rm 启动守护式容器 1$ docker run -d IMAGE 例如，以后台模式启动一个 CentOS，docker run -d centos，然后 docker ps -a 进行查看，会发现容器已经退出。 Docker 容器若要后台运行，就必须有一个前台进程。 查看容器日志 启动一个一直运行的守护式容器： 1$ docker run -d centos /bin/sh -c &quot;while true; do echo hello xisun; sleep 2; done&quot; 查看该容器的日志： 1$ docker logs [OPTIONS] 容器ID或容器名 OPTION 说明： 12345-t 添加时间戳-f 跟随最新的日志打印--tail number 显示最后number条 查看容器内运行的进程 1$ docker top 容器ID或容器名 查看容器内部的细节 1$ docker inspect 容器ID或容器名 进入正在运行的容器并以命令行交互 方式一： 1$ docker attach 容器ID或容器名 直接进入容器启动命令的终端，不会启动新的进程。然后在该容器的终端内，执行相应的命令。 方式二： 1$ docker exec -t 容器ID或容器名 ls -l /tmp 在容器中打开新的终端，并且可以启动新的进程。然后执行后续的命令，并将结果显示在当前窗口。 1$ docker exec -t 容器ID或容器名 /bin/bash 与方式一等效。 针对执行 ctrl + P + Q 命令退出的容器。 从容器内拷贝文件到主机上 1$ docker cp 容器ID或容器名:容器内路径 目的主机路径 总结 Docker 镜像 镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的有内容，包括代码、运行时、库、环境变量和配置文件。 UnionFS (联合文件系统) UnionFS 是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下 (unite several directories into a single virtual file system)。 UnionFS 是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像 (没有父镜像)，可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。 Docker 镜像加载原理 Docker 的镜像实际上是由一层一层的文件系统组成，这种层级的文件系统即为 UnionFS。主要包含两个部分： bootfs (boot file system)：主要包含 bootloader 和 kernel，bootloader 主要是引导加载 kernel，Linux 刚启动时会加载bootfs文件系统。bootfs 是 Docker 镜像的最底层，这一层与我们典型的 Linux/Unix 系统是一样的，包含 boot 加载器和内核。当 boot 加载完成之后整个内核就都在内存中了，此时内存的使用权由 bootfs 转交给内核，系统也会卸载 bootfs。 rootfs (root file system)：在 bootfs 之上。包含的就是典型 Linux 系统中的 /dev， /proc，/bin，/etc 等标准目录和文件。rootfs 就是各种不同的操作系统发行版，比如 Ubuntu，CentOS 等等。 对于一个精简的 OS，rootfs 可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用 Host (宿主机) 的 kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的 Linux 发行版，bootfs基本是一致的，rootfs会有差别，因此不同的发行版可以公用 bootfs。比如：平时我们安装的虚拟机的 CentOS 都是好几个 G 大小，而 Docker 里才要 200 M 左右。 Docker 镜像是分层的 在执行 pull 命令时，可以看出 docker 的镜像时一层一层的在下载： 以 tomcat 为例，主要分为如下几个层次： Docker 镜像采用分层结构的原因 最大的一个好处就是：共享资源。 比如：有多个镜像都从相同的 base 镜像构建而来，那么宿主机只需在磁盘上保存一份 base 镜像，同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。进一步的，镜像的每一层都可以被共享。 Docker 镜像的特点 Docker 镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部，这一层通常被称为容器层，容器层之下都叫镜像层。 Docker 镜像的最外层是可写的，之下的都是封装好不可写的。 Docker 镜像 commit 操作 docker commit 命令，可以提交容器副本使之称为一个新的镜像。 1$ docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器ID 要创建的目标镜像名:[标签名] 案例演示： 从 Hub 上下载 tomcat 镜像到本地，然后运行。 1$ docker run -it -p 8888:8080 tomcat 12345-p 主机端口:容器端口，主机端口即为暴露的能访问docker的端口，容器端口即为docker内待访问特定容器的端口，如tomcat默认为8080-P 随机分配主机的端口-d 后台运行容器，并返回容器ID，即启动守护式容器-i 以交互模式运行容器，通常与-t同时使用-t 为容器重新分配一个伪输入终端，通常与-i同时使用 故意删除上一步镜像生成的 tomcat 容器的文档，会发现再次进入 tomcat 主页时，点击 Documentation 会返回 404。 也即当前的 tomcat 运行实例是一个没有文档内容的容器，现在，以此为模板 commit 一个没有 doc 文档的 tomcat 新镜像：atguigu/tomcat02:1.2。 启动新镜像并和原来的对比。 启动 atuigu/tomcat02，没有doc 1$ docker run -it -p 7777:8080 atuigu/tomcat02:1.2 启动原来 tomcat，有doc 1$ docker run -it -p 8888:8080 tomcat Docker 容器数据卷 Docker 容器产生的数据，如果不通过 docker commit 生成一个新的镜像，使得数据做为镜像的一部分保存下来，那么当容器删除后，数据自然也就没有了。 在 Docker 中，使用卷来保存数据。有点类似 Redis 里面的 rdb 和 aof 文件。 卷是目录或文件，存在于一个或多个容器中，由 Docker 挂载到容器，但不属于联合文件系统，因此能够绕过 UnionFS 提供一些用于持续存储或共享数据的特性。 卷的设计目的就是数据的持久化，卷完全独立于容器的生存周期，因此 Docker 不会在容器删除时删除其挂载的数据卷。 卷的特点： 数据卷可在容器之间共享或重用数据。 数据卷中的更改可以直接生效。 数据卷中的更改不会包含在镜像的更新中。 数据卷的生命周期一直持续到没有容器使用它为止。 容器内添加数据卷 直接命令添加 1$ docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 案例演示： 查看数据卷是否挂载成功： 1$ docker inspect 容器ID 此时，volume 权限是可读写的。可以在容器或主机内分别对卷进行数据的读写，读写的数据是共享的。 容器运行时，容器和宿主机之间数据能够共享： 容器停止退出后，主机修改后的数据也能同步： 带权限的命令： 1$ docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 此时，volume 权限是不可写的。可以在主机对卷进行数据的读写，读写的数据是共享的。但是，在容器内，只可对卷进行数据的读，不可写。 Dockerfile 添加 在主机根目录下新建 mydocker 文件夹并进入： 12$ mkdir /mydocker$ cd /mydocker 在 Dockerfile 中，使用 VOLUME 指令来给镜像添加一个或多个数据卷： 1VOLUME [&quot;/dataVolumeContainer&quot;,&quot;/dataVolumeContainer2&quot;,&quot;/dataVolumeContainer3&quot;] 出于可移植和分享的考虑，用 -v 主机目录:容器目录 这种方法不能直接在 Dockerfile 中实现。因为宿主机目录是依赖于特定宿主机的，不能保证在所有的宿主机上都存在这样的特定目录。 构建 Dockerfile： 1$ vim Dockerfile2 在 Dockerfile2 中添加如下内容： 12345# volume testFROM centosVOLUME [&quot;/dataVolumeContainer1&quot;,&quot;/dataVolumeContainer2&quot;]CMD echo &quot;finished,--------success1&quot;CMD /bin/bash 大致等同于命令：docker run -it -v /host1:/dataVolumeContainer1 -v /host2:/dataVolumeContainer2 centos /bin/bash。 执行 build 命令生成一个新镜像： 1$ docker builder -f /mydocker/Dockerfile2 -t zzyy/centos . 执行 run 命令启动容器，并查看容器内创建的卷的目录所在： 1$ docker run -it zzyy/centos /bin/bash 执行 inspect 命令查看主机对应的目录： 1$ docker inspect 容器ID 备注： Docker 挂载主机目录 Docker 访问出现 cannot open directory. Permission denied 异常时，在挂载目录后多加一个 --privileged=true 参数即可。 数据卷容器 命名的容器挂载数据卷，其它容器通过挂载这个 (父容器) 实现数据共享，挂载数据卷的容器，称之为数据卷容器。 案例演示： 先启动一个父容器 doc1，启动后在 dataVolumeContainer2 中新增内容 dc01_add.txt： 1$ docker run -it --name dc01 zzyy/centos 启动子容器 dc02 和 dc03，继承 dc01，启动后分别在 dataVolumeContainer2 中新增内容 dc02_add.txt 和 dc03_add.txt： 1$ docker run -it --name dc02 --volume-from dco1 zzyy/centos 1$ docker run -it --name dc03 --volume-from dco1 zzyy/centos 重新进入 dc01 容器，可以看到 dc02 和 dc03 容器内添加的数据，在卷 dataVolumeContainer2 中都可以共享： 1$ docker ps 1$ docker attach dc01 删除 dc01，dc02 和 dc03 仍然能够共享数据： 删除 dc02 后，dc03 仍然能够共享数据： 新建 dc04 继承 dc03，然后删除 dc03，dc04 仍然能够共享数据： 结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止。 Dockerfile 解析 Dockerfile 是用来构建 Docker 镜像的构建文件，由一系列命令和参数构成的脚本。 Dockerfile 构建的三步骤： 手动编写一个 Dockerfile 文件，必须要符合 Dockerfile 的规范； docker build 命令执行编写好的 Dockerfile 文件，获得一个自定义的镜像； doucker run 命令启动容器。 Dockerfile 构建过程解析 Dockerfile 内容基础知识： 每条保留字指令都必须为大写字母，且后面要跟随至少一个参数。 指令按照从上到下，顺序执行。 # 表示注释。 每条指令都会创建一个新的镜像层，并对镜像进行提交。 Docker 执行 Dockerfile 的大致流程： 第一步：Docker 从基础镜像运行一个容器； 第二步：执行一条指令并对容器作出修改； 第三步：执行类似 docker commit 的操作提交一个新的镜像层； 第四步：docker 再基刚提交的镜像运行一个新容器； 第五步：执行 Dockerfile 中的下一条指令，重复第二至第五步，直到所有指令都执行完成。 Dockerfile、 Docker 镜像与 Docker 容器三者的关系： 从应用软件的角度来看，Dockerfile、 Docker 镜像与 Docker 容器分别代表软件的三个不同阶段： Dockerfile 是软件的原材料。 Docker 镜像是软件的交付品。 Docker 容器可以认为是软件的运行态。 Dockerfile 面向开发，Docker 镜像为交付标准，Docker 容器则涉及部署与运维，三者缺一不可，合力充当 Docker 体系的基石。 Dockerfile 定义了进程需要的一切东西。Dockerfile 涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程 (当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计 namespace 的权限控制) 等等。 定义了 Dockerfile 文件后，docker build 命令产生一个 Docker 镜像。 对于生成的 Docker 镜像，docker run 命令生成 Docker 容器，容器是直接提供服务的。 Dockerfile 体系结构 (保留字指令) FROM：基础镜像，即当前新镜像是基于哪个镜像的。 MAINTAINER：镜像维护者的姓名和邮箱地址。 RUN：容器构建时需要运行的命令。 EXPOSE：当前容器对外暴露出的端口。 WORKDIR：指定在创建容器后，终端默认进入的工作目录。如果不指定，则为根目录。 ENV：用来在构建过程中设置环境变量。例如：ENV MY_PATH /usr/mytest，这个环境变量可以在后续的任何 RUN 指令中使用，如同在命令前制定了环境变量前缀；也可以在其他指令中直接使用这个环境变量，如 WORKDIR $MY_PATH。 ADD：将宿主机目录下的文件拷贝进镜像，并且能够自动处理 URL 和解压 tar 压缩包。 COPY：类似 ADD，拷贝文件和目录到镜像中 (只拷贝)。将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层镜像内的 &lt;目标路径&gt; 指向的位置。 COPY src dest COPY [&quot;src&quot;,&quot;dest&quot;] VOLUME：容器数据卷，用于数据保存和持久化工作。 CMD：指定一个容器启动时要运行的命令。 CMD 指令的格式和 RUN 相似： shell 格式：CMD &lt;命令&gt; exec 格式：CMD [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;...] 参数列表格式：CMD [&quot;参数1&quot;, &quot;参数2&quot;...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效。CMD 指令会被 docker run 之后的参数替换。 ENTRYPOINT：指定一个容器启动时要运行的命令。 ENTRYPOINT 的目的和 CMD 一样，不同的是，ENTRYPOINT 指令会被 docker run 之后的参数追加。 ONBUILD：当构建一个被继承的 Dockerfile 时运行命令，父镜像在被子镜像继承时，父镜像的 ONBUILD 指令触发， 案例演示Base 镜像 Docker Hub 中 99% 的镜像都是通过在 base 镜像中，安装和配置需要的软件构建出来的。例如 centos 镜像： 1234FROM scratchADD centos-8-x86_64.tar.xz /LABEL org.label-schema.schema-version=&quot;1.0&quot; org.label-schema.name=&quot;CentOS Base Image&quot; org.label-schema.vendor=&quot;CentOS&quot; org.label-schema.license=&quot;GPLv2&quot; org.label-schema.build-date=&quot;20201204&quot;CMD [&quot;/bin/bash&quot;] 自定义镜像 mycentos 编写 Dockerfile： Docker Hub 默认的 centos 镜像： 需求： 设置登陆后的默认路径； 增加 vim 编辑器； 增加查看网络配置 ifconfig 支持。 在主机 /mydocker 或其他目录下编写 Dockerfile 文件： 1234567891011121314FROM centosMAINTAINER ZZYY&lt;zzyy167@126.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo &quot;success--------------ok&quot;CMD /bin/bash 构建镜像：**docker build -f Dockerfile路径 -t 新镜像名字:TAG .** 1$ docker build -f /mydocker/Dockerfile -t mycentos:1.3 . 运行容器：**docker run -it 新镜像名字:TAG ** 1$ docker run -it mycentos:1.3 列出镜像的变更历史： 1$ docker history 镜像ID CMD/ENTRYPOINT 镜像案例 CMD/ENTRYPOINT 都是指定一个容器启动时要运行的命令。 Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效；另外，CMD 指令会被 docker run 命令之后的参数替换。 1234567891011121314151617181920212223242526FROM openjdk:16-jdk-busterENV CATALINA_HOME /usr/local/tomcatENV PATH $CATALINA_HOME/bin:$PATHRUN mkdir -p &quot;$CATALINA_HOME&quot;WORKDIR $CATALINA_HOME# let &quot;Tomcat Native&quot; live somewhere isolatedENV TOMCAT_NATIVE_LIBDIR $CATALINA_HOME/native-jni-libENV LD_LIBRARY_PATH $&#123;LD_LIBRARY_PATH:+$LD_LIBRARY_PATH:&#125;$TOMCAT_NATIVE_LIBDIR# see https://www.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/KEYS# see also &quot;update.sh&quot; (https://github.com/docker-library/tomcat/blob/master/update.sh)ENV GPG_KEYS A9C5DF4D22E99998D9875A5110C01C5A2F6059E7ENV TOMCAT_MAJOR 10ENV TOMCAT_VERSION 10.0.6ENV TOMCAT_SHA512 3d39b086b6fec86e354aa4837b1b55e6c16bfd5ec985a82a5dd71f928e3fab5370b2964a5a1098cfe05ca63d031f198773b18b1f8c7c6cdee6c90aa0644fb2f2RUN ...# verify Tomcat Native is working properlyRUN ...EXPOSE 8080CMD [&quot;catalina.sh&quot;, &quot;run&quot;] 以 tomcat 的 Dockerfile 为例，可以看到，文件的最后一条指令为 CMD 指令。 运行以下命令，tomcat 能够正常启动： 1$ docker run -it -p 7777:8080 tomcat 运行以下命令，tomcat 不能正常启动： 1$ docker run -it -p 7777:8080 tomcat ls -l 上面的 docker run 命令，末尾的 ls -l 参数会替换 Dockerfile 文件中的 CMD [&quot;catalina.sh&quot;, &quot;run&quot;] 指令，因此，tomcat 不会启动，只会列出 /usr/local/tomcat 路径下的文件。 不同于 CMD 指令，docker run 命令之后的参数，会传递给 ENTRYPOINT 指令，追加形成新的命令组合。 curl 命令解释： curl 命令可以用来执行下载、发送各种 HTTP 请求，指定 HTTP 头部等操作。 如果系统没有 curl，可以使用 yum install -y curl 命令安装。 curl 命令的 URL 如果指向的是 HTML 文档，那么缺省只显示文件头部，即 HTML 文档的 header，要全部显示，则加参数 -i。 CMD 版查询 IP 信息的容器： 123FROM centosRUN yum install -y curlCMD [&quot;curl&quot;, &quot;-s&quot;, &quot;http://ip.cn&quot;] 上面的容器，已经指定了 CMD 指令，如果希望查询结果包含 header，命令 docker run myip -i 会不生效。-i 参数会替换掉 CMD 指令。 ENTRYPOINT 版查询 IP 信息的容器： 123FROM centosRUN yum install -y curlENTRYPOINT [&quot;curl&quot;, &quot;-s&quot;, &quot;http://ip.cn&quot;] 上面的容器，使用的是 ENTRYPOINT 指令，如果希望查询结果包含 header，只需要使用命令 docker run myip -i 即可。-i 参数会追加到 ENTRYPOINT 指令后面。 自定义镜像 tomcat 创建目录： 1$ mkdir -p /zzyy/mydockerfile/tomcat9 再上述目录创建 c.txt： 123$ cd /zzyy/mydockerfile/tomcat9$ touch c.txt 将 JDK 和 tomcat 的安装压缩包拷贝进上一步目录： 123$ cp /opt/jdk-8u171-linux-x64.tar.gz /zzyy/mydockerfile/tomcat9$ cp /opt/apache-tomcat-9.0.8.tar.gz /zzyy/mydockerfile/tomcat9 在 zzyyuse/mydockerfile/tomcat9 目录下新建 Dockerfile 文件： 1$ vim Dockerfie 123456789101112131415161718192021222324FROM centosMAINTAINER zzyy&lt;zzyybs@ 126.com&gt;#把宿主机当前上下文的c.txt拷贝到容器/usr/local/路径下COPY c.txt /usr/local/cincontainer.txt#把java与tomcat添加到容器中ADD jdk-8u171-linux x64.tar.gz /usr/local/ADD apache-tomcat-9.0.8.tar.gz /usr/local/#安装vim编辑器RUN yum -y install vim#设置工作访问时候的WORKDIR路径，登录落脚点ENV MYPATH /usr/localWORKDIR $MYPATH#配置java与tomcat环境变量ENV JAVA_ HOME /usr/local/jdk1.8.0_171ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-9.0.8ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.8ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin#容器运行时监听的端口EXPOSE 8080#启动时运行tomcat# ENTRYPOINT [&quot;/usrl/local/apache-tomcat-9.0.8/bin/startup.sh&quot; ]# CMD [&quot;/usr/local/apache-tomcat-9.0.8/bin/catalina.sh&quot;,&quot;run&quot;]CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.8/in/logs/catalina.out 目录内容： 构建镜像： 1$ docker build -t zzyytomcat9 不添加 -f 参数，默认构建当前路径下的 Dockerfile。 运行容器： 1$ docker run -d -p 9080:8080 -name myt9 -v /zzyyuse/mydockerfile/tomcat9/test:/usrlocal/apache-tomcat9.0.8/webapps/test -v /zzyyuse/mydockerfile/tomcat9/tomcat9logs/:/usrlocal/apache-tomcat-9.0.8/logs -privileged=true zzyytomcat9 -v 参数设置两个数据卷，一个用于存放发布项目，一个用于存放日志记录。 -privileged=true 是 Docker 挂载主机目录 Docker 访问出现 cannot open directory : Permission denied 时的解决办法。 验证： 发布 web 服务 test： 在主机数据卷对应的目录 /zzyyuse/mydockerfile/tomcat9/test 目录下，新建 WEB-INF 目录，并添加 web.xml 文件。然后编写一个 a.jsp 文件作为测试： web.xml： 123456789&lt;?xml version=&quot;1 .0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmIns:xsi=&quot;http://www.w3.org/2001/XML Schema-instance&quot;xmIns=&quot;http://java sun.com/xm/ns/javaee&quot;xsi:schemaL ocation=&quot;http://java. sun.com/xml/ns/javaee htp:/:/java. sun.com/xml/ns/javaee/web-app_ 2_ _5.xsd&quot;id=&quot;WebApp_ ID&quot; version=&quot;2.5&quot;&gt; &lt;display-name&gt;test&lt;/display-name&gt;&lt;/web-app&gt; a.jsp： 123456789101112131415&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC“//W3C//DTD HTML 4.01 Transitional//EN&quot; http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;title&gt;Insert title here &lt;/title&gt; &lt;/head&gt; &lt;body&gt; ---------------welcome--------------- &lt;br&gt; &lt;%=&quot;i am in docker tomcat self &quot;%&gt; &lt;br&gt; &lt;% System.out.printIn(&quot;==========docker tomcat self&quot;);%&gt; &lt;/body&gt;&lt;/htmI&gt; docker restart 命令重新启动 tomcat，然后网页访问 localhost:9080/test/a.jsp，即可查看到 a.jsp 网页的内容。在主机目录下修改 a.jsp 的内容时，会同步到 tomcat 中。 主机上查看日志： 总结 Docker 常用安装总体步骤 搜索镜像 拉取镜像 查看镜像 启动镜像 停止镜像 移除镜像 安装 mysql docker hub 上查找 mysql 镜像： 从 docker hub (阿里云加速器) 拉取 mysql 镜像到本地，标签为 5.6： 使用 mysql:5.6 镜像创建容器 (也叫运行镜像)： 命令说明： 1234567891011121314151617docker run -p 12345:3306 --name mysql -v /zzyyuse/mysql/conf:/etc/mysql/conf.d -v /zzyyuse/mysql/logs:/logs -v /zzyyuse/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6----------------------------------------------命令说明:-p 12345:3306: 将主机的12345端口映射到docker容器的3306端口-name mysql: 运行服务名字-v /zzyyuse/mysql/conf:/etc/mysql/conf.d: 将主机/zzyyuse/mysq|目录下的conf/my.cnf挂载到容器的/etc/mysql/conf.d-v /zzyyuse/mysql/logs:/logs: 将主机/zzyyuse/mysql目录下的logs目录挂载到容器的/logs-v /zzyyuse/mysql/data:/var/lib/mysql: 将主机/zzyyuse/mysql目录下的data目录挂载到容器的/var/lib/mysql-e MYSQL_ROOT_PASSWORD=123456: 初始化root用户的密码-d mysql:5.6: 后台程序运行mysql5.6 ----------------------------------------------docker exec -it mysql运行成功后的容器ID /bin/bash---------------------------------------------- 将 mysql 数据备份测试： 1$ docker exec mysql运行成功后的容器ID sh -c &#x27;exec mysqldump --all-databases -uroot -p&quot;123456&quot;&#x27; &gt;/zzyyuse/all-database.sql 安装 redis 从 docker hub 上 (阿里云加速器) 拉取 redis 镜像到本地，标签为 3.2： 使用 redis:3.2 镜像创建容器 (也叫运行镜像)： 1$ docker run -p 6379:6379 -v /zzyyuse/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -v /zzyyuse/myredis/data:/data -d redis:3.2 redis-server /usr/local/etc/redis/redis.conf --appendonly yes 命令中的 redis.conf 是路径，不是文件。 在主机 /zzyyuse/myredis/conf/redis.conf 目录下新建 redis 配置文件 redis.conf，并添加如下内容： 1$ vim /zzyyuse/myredis/conf/redis.conf/redis.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000100110021003100410051006100710081009101010111012101310141015101610171018101910201021102210231024102510261027102810291030103110321033103410351036103710381039104010411042104310441045104610471048104910501051105210531054105510561057105810591060106110621063106410651066106710681069107010711072107310741075107610771078107910801081108210831084108510861087108810891090109110921093109410951096109710981099110011011102110311041105110611071108110911101111111211131114111511161117111811191120112111221123112411251126112711281129113011311132113311341135113611371138113911401141114211431144114511461147114811491150115111521153115411551156115711581159116011611162116311641165116611671168116911701171117211731174117511761177117811791180118111821183118411851186118711881189119011911192119311941195119611971198119912001201120212031204120512061207120812091210121112121213121412151216121712181219122012211222122312241225122612271228122912301231123212331234123512361237123812391240124112421243124412451246124712481249125012511252125312541255125612571258125912601261126212631264126512661267126812691270127112721273127412751276127712781279128012811282128312841285128612871288128912901291129212931294129512961297129812991300130113021303130413051306130713081309131013111312131313141315131613171318131913201321132213231324132513261327132813291330133113321333133413351336133713381339134013411342134313441345134613471348134913501351135213531354135513561357135813591360136113621363136413651366136713681369137013711372137313741375137613771378137913801381138213831384138513861387138813891390139113921393139413951396139713981399140014011402140314041405140614071408140914101411141214131414141514161417141814191420142114221423142414251426142714281429143014311432143314341435143614371438143914401441144214431444144514461447144814491450145114521453145414551456145714581459146014611462146314641465146614671468146914701471147214731474147514761477147814791480148114821483148414851486148714881489149014911492149314941495149614971498149915001501150215031504150515061507150815091510151115121513151415151516151715181519152015211522152315241525152615271528152915301531153215331534153515361537153815391540154115421543154415451546154715481549155015511552155315541555155615571558155915601561156215631564156515661567156815691570157115721573157415751576157715781579158015811582158315841585158615871588158915901591159215931594159515961597159815991600160116021603160416051606160716081609161016111612161316141615161616171618161916201621162216231624162516261627162816291630163116321633163416351636163716381639164016411642164316441645164616471648164916501651165216531654165516561657165816591660166116621663166416651666166716681669167016711672167316741675167616771678167916801681168216831684168516861687168816891690169116921693169416951696169716981699170017011702170317041705170617071708170917101711171217131714171517161717171817191720172117221723172417251726172717281729173017311732173317341735173617371738173917401741174217431744174517461747174817491750175117521753175417551756175717581759176017611762176317641765176617671768176917701771177217731774177517761777177817791780178117821783178417851786178717881789179017911792179317941795179617971798179918001801180218031804180518061807180818091810181118121813181418151816181718181819182018211822182318241825182618271828182918301831183218331834183518361837183818391840184118421843184418451846184718481849185018511852185318541855185618571858185918601861# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won&#x27;t be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you&#x27;d better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 loopback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.protected-mode yes# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# TLS/SSL ###################################### By default, TLS/SSL is disabled. To enable it, the &quot;tls-port&quot; configuration# directive can be used to define TLS-listening ports. To enable TLS on the# default port, use:## port 0# tls-port 6379# Configure a X.509 certificate and private key to use for authenticating the# server to connected clients, masters or cluster peers. These files should be# PEM formatted.## tls-cert-file redis.crt # tls-key-file redis.key# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange:## tls-dh-params-file redis.dh# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL# clients and peers. Redis requires an explicit configuration of at least one# of these, and will not implicitly use the system wide configuration.## tls-ca-cert-file ca.crt# tls-ca-cert-dir /etc/ssl/certs# By default, clients (including replica servers) on a TLS port are required# to authenticate using valid client side certificates.## If &quot;no&quot; is specified, client certificates are not required and not accepted.# If &quot;optional&quot; is specified, client certificates are accepted and must be# valid if provided, but are not required.## tls-auth-clients no# tls-auth-clients optional# By default, a Redis replica does not attempt to establish a TLS connection# with its master.## Use the following directive to enable TLS on replication links.## tls-replication yes# By default, the Redis Cluster bus uses a plain TCP connection. To enable# TLS for the bus protocol, use the following directive:## tls-cluster yes# Explicitly specify TLS versions to support. Allowed values are case insensitive# and include &quot;TLSv1&quot;, &quot;TLSv1.1&quot;, &quot;TLSv1.2&quot;, &quot;TLSv1.3&quot; (OpenSSL &gt;= 1.1.1) or# any combination. To enable only TLSv1.2 and TLSv1.3, use:## tls-protocols &quot;TLSv1.2 TLSv1.3&quot;# Configure allowed ciphers. See the ciphers(1ssl) manpage for more information# about the syntax of this string.## Note: this configuration applies only to &lt;= TLSv1.2.## tls-ciphers DEFAULT:!MEDIUM# Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more# information about the syntax of this string, and specifically for TLSv1.3# ciphersuites.## tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256# When choosing a cipher, use the server&#x27;s preference instead of the client# preference. By default, the server follows the client&#x27;s preference.## tls-prefer-server-ciphers yes# By default, TLS session caching is enabled to allow faster and less expensive# reconnections by clients that support it. Use the following directive to disable# caching.## tls-session-caching no# Change the default number of TLS sessions cached. A zero value sets the cache# to unlimited size. The default size is 20480.## tls-session-cache-size 5000# Change the default timeout of cached TLS sessions. The default timeout is 300# seconds.## tls-session-cache-timeout 60################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &#x27;yes&#x27; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set &#x27;syslog-enabled&#x27; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &#x27;databases&#x27;-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&#x27;s set to &#x27;yes&#x27; as it&#x27;s almost always a win.# If you want to save some CPU in the saving child set it to &#x27;no&#x27; but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# Remove RDB files used by replication in instances without persistence# enabled. By default this option is disabled, however there are environments# where for regulations or other security concerns, RDB files persisted on# disk by masters in order to feed replicas, or stored on disk by replicas# in order to load them for the initial synchronization, should be deleted# ASAP. Note that this option ONLY WORKS in instances that have both AOF# and RDB persistence disabled, otherwise is completely ignored.## An alternative (and sometimes better) way to obtain the same effect is# to use diskless replication on both master and replicas instances. However# in the case of replicas, diskless is not always an option.rdb-del-sync-files no# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &#x27;dbfilename&#x27; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition replicas automatically try to reconnect to masters# and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.## masterauth &lt;master-password&gt;## However this is not enough if you are using Redis ACLs (for Redis version# 6 or greater), and the default user is not capable of running the PSYNC# command and/or other commands needed for replication. In this case it&#x27;s# better to configure a special user to use with replication, and specify the# masteruser configuration as such:## masteruser &lt;username&gt;## When masteruser is specified, the replica will authenticate against its# master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;.# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to &#x27;yes&#x27; (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to &#x27;no&#x27; the replica will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It&#x27;s just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using &#x27;rename-command&#x27; to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## New replicas and reconnecting replicas that are not able to continue the# replication process just receiving differences, need to do what is called a# &quot;full synchronization&quot;. An RDB file is transmitted from the master to the# replicas.## The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child# producing the RDB file finishes its work. With diskless replication instead# once the transfer starts, new replicas arriving will be queued and a new# transfer will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple# replicas will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the# server waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# -----------------------------------------------------------------------------# WARNING: RDB diskless load is experimental. Since in this setup the replica# does not immediately store an RDB on disk, it may cause data loss during# failovers. RDB diskless load + Redis modules not handling I/O reads may also# cause Redis to abort in case of I/O errors during the initial synchronization# stage with the master. Use only if your do what you are doing.# -----------------------------------------------------------------------------## Replica can load the RDB it reads from the replication link directly from the# socket, or store the RDB to a file and read that file after it was completely# recived from the master.## In many cases the disk is slower than the network, and storing and loading# the RDB file may increase replication time (and even increase the master&#x27;s# Copy on Write memory and salve buffers).# However, parsing the RDB file directly from the socket may mean that we have# to flush the contents of the current database before the full rdb was# received. For this reason we have the following options:## &quot;disabled&quot; - Don&#x27;t use diskless load (store the rdb file to the disk first)# &quot;on-empty-db&quot; - Use diskless load only when it is completely safe.# &quot;swapdb&quot; - Keep a copy of the current db contents in RAM while parsing# the data directly from the socket. note that this requires# sufficient memory, if you don&#x27;t have it, you risk an OOM kill.repl-diskless-load disabled# Replicas send PINGs to server in a predefined interval. It&#x27;s possible to# change this interval with the repl_ping_replica_period option. The default# value is 10 seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a# replica wants to reconnect again, often a full resync is not needed, but a# partial resync is enough, just passing the portion of data the replica# missed while disconnected.## The bigger the replication backlog, the longer the time the replica can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a replica connected.## repl-backlog-size 1mb# After a master has no longer connected replicas for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last replica disconnected, for# the backlog buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO# output. It is used by Redis Sentinel in order to select a replica to promote# into a master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel# will pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a replica is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the replica to connect with the master.## Port: The port is communicated by the replica during the replication# handshake, and is normally the port that the replica is using to# listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may be actually reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234############################### KEYS TRACKING ################################## Redis implements server assisted support for client side caching of values.# This is implemented using an invalidation table that remembers, using# 16 millions of slots, what clients may have certain subsets of keys. In turn# this is used in order to send invalidation messages to clients. Please# to understand more about the feature check this page:## https://redis.io/topics/client-side-caching## When tracking is enabled for a client, all the read only queries are assumed# to be cached: this will force Redis to store information in the invalidation# table. When keys are modified, such information is flushed away, and# invalidation messages are sent to the clients. However if the workload is# heavily dominated by reads, Redis could use more and more memory in order# to track the keys fetched by many clients.## For this reason it is possible to configure a maximum fill value for the# invalidation table. By default it is set to 1M of keys, and once this limit# is reached, Redis will start to evict keys in the invalidation table# even if they were not modified, just to reclaim memory: this will in turn# force the clients to invalidate the cached values. Basically the table# maximum size is a trade off between the memory you want to spend server# side to track information about who cached what, and the ability of clients# to retain cached objects in memory.## If you set the value to 0, it means there are no limits, and Redis will# retain as many keys as needed in the invalidation table.# In the &quot;stats&quot; INFO section, you can find information about the number of# keys in the invalidation table at every given moment.## Note: when key tracking is used in broadcasting mode, no memory is used# in the server side so this setting is useless.## tracking-table-max-keys 1000000################################## SECURITY #################################### Warning: since Redis is pretty fast an outside user can try up to# 1 million passwords per second against a modern box. This means that you# should use very strong passwords, otherwise they will be very easy to break.# Note that because the password is really a shared secret between the client# and the server, and should not be memorized by any human, the password# can be easily a long string from /dev/urandom or whatever, so by using a# long and unguessable password no brute force attack will be possible.# Redis ACL users are defined in the following format:## user &lt;username&gt; ... acl rules ...## For example:## user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99## The special username &quot;default&quot; is used for new connections. If this user# has the &quot;nopass&quot; rule, then new connections will be immediately authenticated# as the &quot;default&quot; user without the need of any password provided via the# AUTH command. Otherwise if the &quot;default&quot; user is not flagged with &quot;nopass&quot;# the connections will start in not authenticated state, and will require# AUTH (or the HELLO command AUTH option) in order to be authenticated and# start to work.## The ACL rules that describe what an user can do are the following:## on Enable the user: it is possible to authenticate as this user.# off Disable the user: it&#x27;s no longer possible to authenticate# with this user, however the already authenticated connections# will still work.# +&lt;command&gt; Allow the execution of that command# -&lt;command&gt; Disallow the execution of that command# +@&lt;category&gt; Allow the execution of all the commands in such category# with valid categories are like @admin, @set, @sortedset, ...# and so forth, see the full list in the server.c file where# the Redis command table is described and defined.# The special category @all means all the commands, but currently# present in the server, and that will be loaded in the future# via modules.# +&lt;command&gt;|subcommand Allow a specific subcommand of an otherwise# disabled command. Note that this form is not# allowed as negative like -DEBUG|SEGFAULT, but# only additive starting with &quot;+&quot;.# allcommands Alias for +@all. Note that it implies the ability to execute# all the future commands loaded via the modules system.# nocommands Alias for -@all.# ~&lt;pattern&gt; Add a pattern of keys that can be mentioned as part of# commands. For instance ~* allows all the keys. The pattern# is a glob-style pattern like the one of KEYS.# It is possible to specify multiple patterns.# allkeys Alias for ~*# resetkeys Flush the list of allowed keys patterns.# &gt;&lt;password&gt; Add this passowrd to the list of valid password for the user.# For example &gt;mypass will add &quot;mypass&quot; to the list.# This directive clears the &quot;nopass&quot; flag (see later).# &lt;&lt;password&gt; Remove this password from the list of valid passwords.# nopass All the set passwords of the user are removed, and the user# is flagged as requiring no password: it means that every# password will work against this user. If this directive is# used for the default user, every new connection will be# immediately authenticated with the default user without# any explicit AUTH command required. Note that the &quot;resetpass&quot;# directive will clear this condition.# resetpass Flush the list of allowed passwords. Moreover removes the# &quot;nopass&quot; status. After &quot;resetpass&quot; the user has no associated# passwords and there is no way to authenticate without adding# some password (or setting it as &quot;nopass&quot; later).# reset Performs the following actions: resetpass, resetkeys, off,# -@all. The user returns to the same state it has immediately# after its creation.## ACL rules can be specified in any order: for instance you can start with# passwords, then flags, or key patterns. However note that the additive# and subtractive rules will CHANGE MEANING depending on the ordering.# For instance see the following example:## user alice on +@all -DEBUG ~* &gt;somepassword## This will allow &quot;alice&quot; to use all the commands with the exception of the# DEBUG command, since +@all added all the commands to the set of the commands# alice can use, and later DEBUG was removed. However if we invert the order# of two ACL rules the result will be different:## user alice on -DEBUG +@all ~* &gt;somepassword## Now DEBUG was removed when alice had yet no commands in the set of allowed# commands, later all the commands are added, so the user will be able to# execute everything.## Basically ACL rules are processed left-to-right.## For more information about ACL configuration please refer to# the Redis web site at https://redis.io/topics/acl# ACL LOG## The ACL Log tracks failed commands and authentication events associated# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in memory. You can reclaim memory with # ACL LOG RESET. Define the maximum entry length of the ACL Log below.acllog-max-len 128# Using an external ACL file## Instead of configuring users here in this file, it is possible to use# a stand-alone file just listing users. The two methods cannot be mixed:# if you configure users here and at the same time you activate the exteranl# ACL file, the server will refuse to start.## The format of the external ACL user file is exactly the same as the# format that is used inside redis.conf to describe users.## aclfile /etc/redis/users.acl# IMPORTANT NOTE: starting with Redis 6 &quot;requirepass&quot; is just a compatiblity# layer on top of the new ACL system. The option effect will be just setting# the password for the default user. Clients will still authenticate using# AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt;# if they follow the new protocol: both will work.## requirepass foobared# Command renaming (DEPRECATED).## ------------------------------------------------------------------------# WARNING: avoid using this option if possible. Instead use ACLs to remove# commands from the default user, and put them only in some admin user you# create for administrative purposes.# ------------------------------------------------------------------------## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &#x27;max number of clients reached&#x27;.## IMPORTANT: When Redis Cluster is used, the max number of connections is also# shared with the cluster bus: every node in the cluster will use two# connections, one incoming and another outgoing. It is important to size the# limit accordingly in case of very large clusters.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&#x27;t remove keys according to the policy, or if the policy is# set to &#x27;noeviction&#x27;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the &#x27;noeviction&#x27; policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is &#x27;noeviction&#x27;).## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select one from the following behaviors:## volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key having an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don&#x27;t evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica# to have a different memory setting, and you are sure all the writes performed# to the replica are idempotent, then you may change this default (but be sure# to understand what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory# and so forth). So make sure you monitor your replicas and make sure they# have enough memory to never hit a real out-of-memory condition before the# master hits the configured maxmemory setting.## replica-ignore-maxmemory yes# Redis reclaims expired keys in two ways: upon access when those keys are# found to be expired, and also in background, in what is called the# &quot;active expire key&quot;. The key space is slowly and interactively scanned# looking for expired keys to reclaim, so that it is possible to free memory# of keys that are expired and will never be accessed again in a short time.## The default effort of the expire cycle will try to avoid having more than# ten percent of expired keys still in memory, and will try to avoid consuming# more than 25% of total memory and to add latency to the system. However# it is possible to increase the expire &quot;effort&quot; that is normally set to# &quot;1&quot;, to a greater value, up to the value &quot;10&quot;. At its maximum value the# system will use more CPU, longer cycles (and technically may introduce# more latency), and will tollerate less already expired keys still present# in the system. It&#x27;s a tradeoff betweeen memory, CPU and latecy.## active-expire-effort 1############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It&#x27;s up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives.lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no# It is also possible, for the case when to replace the user code DEL calls# with UNLINK calls is not easy, to modify the default behavior of the DEL# command to act exactly like UNLINK, using the following configuration# directive:lazyfree-lazy-user-del no################################ THREADED I/O ################################## Redis is mostly single threaded, however there are certain threaded# operations such as UNLINK, slow I/O accesses and other things that are# performed on side threads.## Now it is also possible to handle Redis clients socket reads and writes# in different I/O threads. Since especially writing is so slow, normally# Redis users use pipelining in order to speedup the Redis performances per# core, and spawn multiple instances in order to scale more. Using I/O# threads it is possible to easily speedup two times Redis without resorting# to pipelining nor sharding of the instance.## By default threading is disabled, we suggest enabling it only in machines# that have at least 4 or more cores, leaving at least one spare core.# Using more than 8 threads is unlikely to help much. We also recommend using# threaded I/O only if you actually have performance problems, with Redis# instances being able to use a quite big percentage of CPU time, otherwise# there is no point in using this feature.## So for instance if you have a four cores boxes, try to use 2 or 3 I/O# threads, if you have a 8 cores, try to use 6 threads. In order to# enable I/O threads use the following configuration directive:## io-threads 4## Setting io-threads to 1 will just use the main thread as usually.# When I/O threads are enabled, we only use threads for writes, that is# to thread the write(2) syscall and transfer the client buffers to the# socket. However it is also possible to enable threading of reads and# protocol parsing using the following configuration directive, by setting# it to yes:## io-threads-do-reads no## Usually threading reads doesn&#x27;t help much.## NOTE 1: This configuration directive cannot be changed at runtime via# CONFIG SET. Aso this feature currently does not work when SSL is# enabled.## NOTE 2: If you want to test the Redis speedup using redis-benchmark, make# sure you also run the benchmark itself in threaded mode, using the# --threads option to match the number of Redis theads, otherwise you&#x27;ll not# be able to notice the improvements.############################ KERNEL OOM CONTROL ############################### On Linux, it is possible to hint the kernel OOM killer on what processes# should be killed first when out of memory.## Enabling this feature makes Redis actively control the oom_score_adj value# for all its processes, depending on their role. The default scores will# attempt to have background child processes killed before all others, and# replicas killed before masters.oom-score-adj no# When oom-score-adj is used, this directive controls the specific values used# for master, replica and background child processes. Values range -1000 to# 1000 (higher means more likely to be killed).## Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)# can freely increase their value, but not decrease it below its initial# settings.## Values are used relative to the initial value of oom_score_adj when the server# starts. Because typically the initial value is 0, they will often match the# absolute values.oom-score-adj-values 0 200 800############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don&#x27;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that&#x27;s usually the right compromise between# speed and data safety. It&#x27;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&#x27;s snapshotting),# or on the contrary, use &quot;always&quot; that&#x27;s very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&#x27;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&#x27;t happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&#x27;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################ Normal Redis instances can&#x27;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages# in order to try to give an advantage to the replica with the best# replication offset (more data from the master processed).# Replicas will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the replica will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they&#x27;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&#x27;t be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# This option, when set to yes, allows nodes to serve read traffic while the# the cluster is in a down state, as long as it believes it owns the slots. ## This is useful for two cases. The first case is for when an application # doesn&#x27;t require consistency of data during node failures or network partitions.# One example of this is a cache, where as long as the node has the data it# should be able to serve it. ## The second use case is for configurations that don&#x27;t meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the# entire cluster without this option set, with it set there is only a write outage.# Without a quorum of masters, slot ownership will not change automatically. ## cluster-allow-reads-when-down no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&#x27;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# t Stream commands# m Key-miss events (Note: It is not included in the &#x27;A&#x27; class)# A Alias for g$lshzxet, so that the &quot;AKE&quot; string means all the events# (Except key-miss events which are excluded from &#x27;A&#x27; due to their# unique nature).## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don&#x27;t need# this feature and the feature has some overhead. Note that if you don&#x27;t# specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### GOPHER SERVER ################################## Redis contains an implementation of the Gopher protocol, as specified in# the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt).## The Gopher protocol was very popular in the late &#x27;90s. It is an alternative# to the web, and the implementation both server and client side is so simple# that the Redis server has just 100 lines of code in order to implement this# support.## What do you do with Gopher nowadays? Well Gopher never *really* died, and# lately there is a movement in order for the Gopher more hierarchical content# composed of just plain text documents to be resurrected. Some want a simpler# internet, others believe that the mainstream internet became too much# controlled, and it&#x27;s cool to create an alternative space for people that# want a bit of fresh air.## Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol# as a gift.## --- HOW IT WORKS? ---## The Redis Gopher support uses the inline protocol of Redis, and specifically# two kind of inline requests that were anyway illegal: an empty request# or any request that starts with &quot;/&quot; (there are no Redis commands starting# with such a slash). Normal RESP2/RESP3 requests are completely out of the# path of the Gopher protocol implementation and are served as usually as well.## If you open a connection to Redis when Gopher is enabled and send it# a string like &quot;/foo&quot;, if there is a key named &quot;/foo&quot; it is served via the# Gopher protocol.## In order to create a real Gopher &quot;hole&quot; (the name of a Gopher site in Gopher# talking), you likely need a script like the following:## https://github.com/antirez/gopher2redis## --- SECURITY WARNING ---## If you plan to put Redis on the internet in a publicly accessible address# to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance.# Once a password is set:## 1. The Gopher server (when enabled, not by default) will still serve# content via Gopher.# 2. However other commands cannot be called before the client will# authenticate.## So use the &#x27;requirepass&#x27; option to protect your instance.## To enable Gopher support uncomment the following line and set# the option from no (the default) to yes.## gopher-enabled no############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&#x27;t start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don&#x27;t compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&#x27;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&#x27;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&#x27;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here, but must be 1mb or greater## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it&#x27;s maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don&#x27;t have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag no# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage, to be used when the lower# threshold is reached# active-defrag-cycle-min 1# Maximal effort for defrag in CPU percentage, to be used when the upper# threshold is reached# active-defrag-cycle-max 25# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000# Jemalloc background thread for purging will be enabled by defaultjemalloc-bg-thread yes# It is possible to pin different threads and processes of Redis to specific# CPUs in your system, in order to maximize the performances of the server.# This is useful both in order to pin different Redis threads in different# CPUs, but also in order to make sure that multiple Redis instances running# in the same host will be pinned to different CPUs.## Normally you can do this using the &quot;taskset&quot; command, however it is also# possible to this via Redis configuration directly, both in Linux and FreeBSD.## You can pin the server/IO threads, bio threads, aof rewrite child process, and# the bgsave child process. The syntax to specify the cpu list is the same as# the taskset command:## Set redis server/io threads to cpu affinity 0,2,4,6:# server_cpulist 0-7:2## Set bio threads to cpu affinity 1,3:# bio_cpulist 1,3## Set aof rewrite child process to cpu affinity 8,9,10,11:# aof_rewrite_cpulist 8-11## Set bgsave child process to cpu affinity 1,10,11# bgsave_cpulist 1,10-11 测试 redis-cli 连接： 1$ docker exec -it 运行的redis服务容器的ID redis-cli 查看持久化文件生成： 推送镜像到阿里云本地镜像发布到阿里云的流程 本地镜像推送到阿里云 本地镜像素材原型： 升级为 1.4 (按需)： 1$ docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] -a：提交镜像的作者；-m：提交时的文字说明。 阿里云开发者平台： https://promotion.aliyun.com/ntms/act/kubernetes.html 创建镜像仓库： 将镜像推送到阿里云： 123$ sudo docker login --username= registry.cn-shenzhen.aliyuncs.com$ sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/[命名空间]/[仓库名称]:[镜像版本号]$ sudo docker push registry.cn-shenzhen.aliyuncs.com/[命名空间]/[仓库名称]:[镜像版本号] ImageId 即为要推送的本地镜像。阿里云仓库中的镜像版本号可以与本地镜像 tag 保持一致，也可以不一致。 查看： 下载： 本文参考https://www.bilibili.com/video/BV1Ls411n7mx https://gitee.com/jack-GCQ/brain-map 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}]},{"title":"Flink 入门","slug":"flink","date":"2021-04-25T05:01:45.000Z","updated":"2022-01-10T06:33:45.026Z","comments":true,"path":"2021/04/25/flink/","link":"","permalink":"http://example.com/2021/04/25/flink/","excerpt":"","text":"Flink 流处理简介 Flink 官网：https://flink.apache.org/ Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Apache Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行状态计算。 为什么选择 Flink： 流数据更真实地反映了我们的生活方式。 传统的数据架构是基于有限数据集的。 我们的目标： 低延迟 高吞吐 结果的准确性和良好的容错性 哪些行业需要处理流数据： 电商和市场营销 数据报表、广告投放、业务流程需要。 物联网 (IOT) 传感器实时数据采集和显示、实时报警，交通运输业。 电信业 基站流量调配。 银行和金融业 实时结算和通知推送，实时检测异常行为。 传统数据处理架构 事务处理架构： 特点：实时性好，但数据量大时，难以进行高并发处理。(低延迟、低吞吐) 分析处理架构： 特点：将数据从业务数据库复制到数仓，再进行分析和查询。能够处理大数据，高并发，但实时性差。(高延迟、高吞吐) 有状态的流式处理 (第一代) 数据存储于内存当中，达到 Periodic Checkpoint 条件时，执行持久化存储。能够做到低延迟、高吞吐，但分布式架构下，难以保证数据的顺序。 lambda 架构 (第二代) 采用两套系统，同时保证低延迟和结果准确： 批处理系统处理速度慢，但准确性高。 流处理系统处理速度快，但准确性差。 缺点：实现一个功能，但需要维护两套系统，开发成本高。 流处理系统的演变 Storm 能够做到低延迟，Spark Streaming 能做到高吞吐，而 Flink 不仅综合了它们的优点，同时还能做得更好。 Flink 可以看作第三代流处理架构。 Flink 的特点 事件驱动 (Event-driven) 基于流的世界观 在 Flink 的世界观中，一切都是由流组成的，离线数据是有界的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。 分层 API 越顶层越抽象，表达含义越简明，使用越方便。 越底层越具体，表达能力越丰富，使用越灵活。 支持事件时间 (event-time) 和处理时间 (processing-time) 语义。 精确一次 (exactly-once) 的状态一致性保证。 低延迟，每秒处理数百万个事件，毫秒级延迟。 与众多常用存储系统的连接。 高可用，动态扩展，实现 7 * 24 小时全天候运行。 Flink vs Spark Streaming Flink 是流处理 (stream) 架构，Spark Streaming 是微批处理 (micro-batching) 架构。 数据模型 Spark 采用 RDD 模型，Spark Streaming 的 DStream 实际上也就是一组组小批数据 RDD 的集合。—&gt; 底层实现基于微批 Flink 基本数据模型是数据流，以及事件 (Event) 序列。—&gt; 底层实现就是流，一个一个的处理 运行时架构 Spark 是批计算，将 DAG 划分为不同的 stage，一个完成后才可以计算下一个。—&gt; 需要等待 Flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理。—&gt; 不需要等待 QuickStart 添加依赖： 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.xisun.flink&lt;/groupId&gt; &lt;artifactId&gt;xisun-flink&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;flink.version&gt;1.11.1&lt;/flink.version&gt; &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; flink-streaming-java_2.12：Flink 底层组件依赖 scala，2.12 是 scala 版本。 Flink 1.11 版本之后，需要添加 flink-clients_2.12 依赖，否则会报异常 java.lang.IllegalStateException: No ExecutorFactory found to execute the application.。 批处理实现 WordCount hello.txt 文件内容： 1234567hello javahello worldhello flinkhello sparkhello scalahow are youfine thank you 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package cn.xisun.flink;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.util.Collector;/** * @author XiSun * @Date 2021/4/25 20:39 */public class WordCount &#123; /** * 自定义类，实现FlatMapFunction接口 * 参数说明： * String：传入数据类型 * Tuple2&lt;String, Integer&gt;：传出数据类型 * Tuple2&lt;T0, T1&gt;：Flink自身实现的元组，注意不要用scala的 */ public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; // 按空格分词 String[] words = line.split(&quot; &quot;); // 遍历所有word，包装成二元组输出 for (String str : words) &#123; // 每一个word，都包装成一个二元组对象，并计数为1，然后用out收集 out.collect(new Tuple2&lt;&gt;(str, 1)); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; // 1.创建批处理执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2.从resources路径下的文件中单线程读取数据，是按照一行一行读取的 String inputPath = &quot;src/main/resources/hello.txt&quot;; DataSet&lt;String&gt; inputDataSet = env.readTextFile(inputPath).setParallelism(1); // 3.对数据集进行处理，按空格分词展开，转换成(word, 1)这样的二元组进行统计 DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; resultSet = inputDataSet.flatMap(new MyFlatMapper()) .groupBy(0)// 按照元组第一个位置的word分组 .sum(1);// 按照元组第二个位置上的数据求和 // 4.打印输出 resultSet.print(); &#125;&#125; 输出结果： 12345678910111213141516SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.(scala,1)(you,2)(flink,1)(world,1)(hello,5)(are,1)(java,1)(thank,1)(fine,1)(how,1)(spark,1)Process finished with exit code 0 流处理实现 WordCount 基于文件读取数据，非真正的流式数据。 批处理 —&gt; 几组或所有数据到达后才处理；流处理 —&gt; 有数据来就直接处理，不等数据堆叠到一定数量级。 这里不像批处理有 groupBy —&gt; 所有数据统一处理，而是用流处理的 keyBy —&gt; 每一个数据都对 key 进行 hash 计算，进行类似分区的操作，来一个数据就处理一次，所有中间过程都有输出！ 并行度：本地 IDEA 执行环境的并行度默认就是计算机的 CPU 逻辑核数。 代码实现： 12345678910111213141516171819202122232425262728293031323334package cn.xisun.flink;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;/** * @author XiSun * @Date 2021/4/25 20:45 */public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度，可选操作，默认值=当前计算机的CPU逻辑核数(设置成1即为单线程处理) env.setParallelism(4); // 2.从resources路径下的文件中多线程读取数据，是按照一行一行读取的 String inputPath = &quot;src/main/resources/hello.txt&quot;; DataStream&lt;String&gt; inputDataStream = env.readTextFile(inputPath); // 3.对数据集进行处理，按空格分词展开，转换成(word, 1)这样的二元组进行统计 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); // 4.打印输出 resultStream.print(); // 5.执行任务 env.execute(); &#125;&#125; 不同于批处理，env.execute() 之前的代码，可以理解为是在定义任务，只有执行 env.execute() 后，Flink 才把前面的代码片段当作一个任务整体 (每个线程根据这个任务操作，并行处理流数据)。 输出结果： 123456789101112131415161718192021SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.2&gt; (java,1)7&gt; (flink,1)1&gt; (scala,1)1&gt; (spark,1)4&gt; (are,1)3&gt; (hello,1)3&gt; (hello,2)6&gt; (how,1)3&gt; (thank,1)3&gt; (hello,3)3&gt; (hello,4)5&gt; (fine,1)5&gt; (you,1)5&gt; (you,2)5&gt; (world,1)3&gt; (hello,5)Process finished with exit code 0 因为是流处理，所以所有中间过程都会被输出，前面的序号就是并行执行任务的线程编号。 线程最大编号为 7，是因为本机配置是 4 核 8 处理器，默认并行度为 8。 流式数据源测试 开启适用于 Linux 的 Windows 子系统。 第一次使用时，需要先安装 Ubuntu 系统： 打开 Windows PowerShell，输入 wsl 命令进入系统，然后通过 nc -lk &lt;port&gt; 命令打开一个 Socket 服务，用于模拟实时的流数据。 代码实现： 12345678910111213141516171819202122232425262728293031323334package cn.xisun.flink;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;/** * @author XiSun * @Date 2021/4/26 10:40 */public class StreamWordCount throws Exception &#123; public static void main(String[] args) &#123; // 1.创建流处理执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度，默认值 = 当前计算机的CPU逻辑核数(设置成1即为单线程处理) env.setParallelism(4); // 2.从数据流中读取数据，Socket文本流只能单线程读取 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.对数据集进行处理，按空格分词展开，转换成(word, 1)这样的二元组进行统计 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); // 4.打印输出 resultStream.print(); // 5.执行任务 env.execute(); &#125;&#125; 生产环境时，一般是在程序的启动参数中设置主机和端口号，此时，可以通过 ParameterTool 工具提取参数： 参数设置格式：--host localhost --port 7777。 1234567// 用parameter tool工具从程序启动参数中提取配置项ParameterTool parameterTool = ParameterTool.fromArgs(args);String host = parameterTool.get(&quot;host&quot;);int port = parameterTool.getInt(&quot;port&quot;);// 2.从数据流中读取数据DataStream&lt;String&gt; inputDataStream = env.socketTextStream(host, port); 输出结果：在本地开启的 Socket 中输入数据，并观察 IDEA 的 Console 输出。 1234xisun@DESKTOP-OM8IACS:/mnt/c/Users/Xisun/Desktop$ nc -lk 7777hello worldhello flinkhello scala 123456789SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.3&gt; (world,1)2&gt; (hello,1)4&gt; (flink,1)2&gt; (hello,2)1&gt; (scala,1)2&gt; (hello,3) 对于输出结果，某项数据对应的次数最大值，是该数据统计到当前时间的最终结果。 Flink 的部署 下载地址：https://flink.apache.org/downloads.html 最新版本： 全部稳定版本：https://flink.apache.org/downloads.html#all-stable-releases 对于新版本的 Flink (1.7 之后)，需要额外下载 Hadoop 依赖，否则无法使用 Hadoop 支持的 Yarn 资源： Standalone 模式安装 地址：https://www.apache.org/dyn/closer.lua/flink/flink-1.13.1/flink-1.13.1-bin-scala_2.12.tgz Flink 组件： 其他组件 (Hadoop)： 解压到指定目录，并把 Hadoop 的组件添加到 Flink 解压路径的 lib 目录下： 打开 wsl 控制台，安装 JDK： 12xisun@DESKTOP-OM8IACS:/mnt/d/Program Files/flink-1.13.1$ sudo apt updatexisun@DESKTOP-OM8IACS:/mnt/d/Program Files/flink-1.13.1$ sudo apt install openjdk-8-jdk 1234xisun@DESKTOP-OM8IACS:/mnt/d/Program Files/flink-1.13.1$ java -versionopenjdk version &quot;1.8.0_282&quot;OpenJDK Runtime Environment (build 1.8.0_282-8u282-b08-0ubuntu1~20.04-b08)OpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode) 在 Ubuntu 20.04 系统下安装 OpenJDK 11 和 OpenJDK 8 的方法：https://ywnz.com/linuxjc/6984.html 查看 JDK 安装路径，并在 Flink 安装目录的 conf/flink-conf.yaml 文件中配置 Java 环境： 12# Javaenv.java.home: /usr/lib/jvm/java-8-openjdk-amd64 建议不要使用 Windows 系统下安装的 JDK 路径，可能会有问题。 配置主机和从机 (未验证)： 修改 conf/flink-conf.yaml 文件，配置主机地址： 12345678910# The external address of the host on which the JobManager runs and can be# reached by the TaskManagers and any clients which want to connect. This setting# is only used in Standalone mode and may be overwritten on the JobManager side# by specifying the --host &lt;hostname&gt; parameter of the bin/jobmanager.sh executable.# In high availability mode, if you use the bin/start-cluster.sh script and setup# the conf/masters file, this will be taken care of automatically. Yarn/Mesos# automatically configure the host name based on the hostname of the node where the# JobManager runs.jobmanager.rpc.address: hadoop1 修改 conf/workers 文件，配置从机地址： 12hadoop2hadoop3 将主机 Flink 安装文件，派发给从机： 1$ xsync flink-1.13.1 启动 Flink 集群： 1234xisun@DESKTOP-OM8IACS:/mnt/c/Users/XiSun/Desktop$ bash /mnt/d/Program\\ Files/flink-1.13.1/bin/start-cluster.shStarting cluster.Starting standalonesession daemon on host DESKTOP-OM8IACS.Starting taskexecutor daemon on host DESKTOP-OM8IACS. 1234xisun@DESKTOP-OM8IACS:/mnt/c/Users/XiSun/Desktop$ jps7347 Jps6956 StandaloneSessionClusterEntrypoint7246 TaskManagerRunner 前端页面访问 localhost:8081，可以对 Flink 集群和任务进行监控管理。(8081 是默认端口) 可以查看任务分配，内存使用，Log 日志，标准输出 (控制台) 等。 提交任务 Web 页面提交 上传 Jar 包：Submit New Job —&gt; Add New，将打包好的 Jar 包添加上来。 设置启动参数： Show Plan 查看任务执行计划： Submit 提交任务： 提交失败： 提交任务时，如果 slot 的数量低于设置的线程数量，会提交失败，会一直等待分配更多的资源。—&gt; 增加 slot 数量或者降低任务线程数量。 提交成功：(需要先启动本机的 socket 服务) 1234xisun@DESKTOP-OM8IACS:/mnt/c/Users/XiSun/Desktop$ nc -tl 7777hello worldhello flinkhellojava^[[D^[[D^[[D 命令行提交 准备数据文件，把含数据文件的文件夹，分发到 taskmanage 所在的机器中 (如果需要)： 1$ xsync flink 如果从文件中读取数据，由于是从本地磁盘读取，实际任务会被分发到 taskmanage 的机器中，所以要把数据文件分发到该机器上。 提交命令： 12xisun@DESKTOP-OM8IACS:/mnt/d/Program Files/flink-1.13.1/bin$ ./flink run -p 4 -c cn.xisun.flink.StreamWordCount /mnt/d/JetBrainsWorkSpace/IDEAProjects/xisun-flink/target/xisun-flink-1.0-SNAPSHOT.jar --host localhost --port 7777Job has been submitted with JobID 470883e75e676e9c538d13f509ddc6cc ./flink run：启动命令；-p 参数：指定并行度；-c 参数：指定 Jar 包运行的主程序。 查看结果，如果输出到控制台，应该在 taskmanager 下查看；如果计算结果输出到文件，同样会保存到 taskmanage 的机器下，不会在 jobmanage 下。 如果 Job 要求的 Task Slots 数大于可用的 Task Slots，Job 提交时会一直等待，直到分配到足够的资源。 flink-conf.yaml 配置文件中，taskmanager.numberOfTaskSlots: 8 用于配置可用的最大 slots 数，默认为 1，一般设置与当前主机 CPU 最大逻辑核心数相同。 查看提交的 Job 命令： 123456xisun@DESKTOP-OM8IACS:/mnt/d/Program Files/flink-1.13.1/bin$ ./flink listWaiting for response...------------------ Running/Restarting Jobs -------------------22.07.2021 16:41:11 : 470883e75e676e9c538d13f509ddc6cc : Flink Streaming Job (RUNNING)--------------------------------------------------------------No scheduled jobs. 取消 Job： 123xisun@DESKTOP-OM8IACS:/mnt/d/Program Files/flink-1.13.1/bin$ ./flink cancel 470883e75e676e9c538d13f509ddc6ccCancelling job 470883e75e676e9c538d13f509ddc6cc.Cancelled job 470883e75e676e9c538d13f509ddc6cc. 查看包含已取消的 Job 命令： 1234567xisun@DESKTOP-OM8IACS:/mnt/d/Program Files/flink-1.13.1/bin$ ./flink list -aWaiting for response...No running jobs.No scheduled jobs.---------------------- Terminated Jobs -----------------------22.07.2021 16:41:11 : 470883e75e676e9c538d13f509ddc6cc : Flink Streaming Job (CANCELED)-------------------------------------------------------------- Yarn 模式 以 Yarn 模式部署 Flink 任务时，要求 Flink 是有 Hadoop 支持的版本，Hadoop 环境需要保证版本在 2.2 以上，并且集群中安装有 HDFS 服务。 Flink on Yarn Flink 提供了两种在 Yarn 上运行的模式，分别为 Session-Cluster 模式和 Per-Job-Cluster 模式。 Session-Cluster 模式 Session-Cluster 模式需要先在 Yarn 中初始化一个 Flink 会话集群，开辟指定的资源。之后，所有任务都向这个 Flink 会话集群提交。这个 Flink 会话集群会常驻在 Yarn 集群中，除非手动停止。 Flink 会话集群所占的资源，会一直保持不变。提交任务时，如果资源满了，下一个任务就无法提交，只有等到 Yarn 中的其中一个任务执行完成后，释放了资源，下个任务才会正常提交。 Flink 会话集群中所有任务共享 Dispatcher 和 ResourceManager；共享资源。 Session-Cluster 模式适合规模小执行时间短的任务。 Per-Job-Cluster 模式 Per-Job-Cluster 模式每次提交任务时，都会创建一个新的 Flink 集群，各任务之间互相独立，互不影响，方便管理。任务执行完成之后创建的集群也会消失。 一个 Job 会对应一个集群，每提交一个任务会根据自身的情况，单独向 Yarn 申请资源，直到任务执行完成，一个任务的失败与否并不会影响下一个任务的正常提交和运行。 每个任务独享 Dispatcher 和 ResourceManager，按需接受资源申请；适合规模大长时间运行的作业。 Session-Cluster 启动 Hadoop 集群 (略)。 启动 yarn-session： 1$ ./yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d -n (–container)：TaskManager 的数量，新版本此参数应该无效了。-s (–slots)： 每个 TaskManager 的 slot 数量，默认一个 slot 一个 core，默认每个 taskmanager 的 slot 的个数为 1，有时可以多一些 taskmanager，做冗余。-jm：JobManager 的内存，单位 MB。-tm：每个 taskmanager 的内存，单位 MB。-nm：Yarn 的 appName (出现在 Yarn 的 ui 上的名字)。-d：后台执行。 提交任务： 1$ ./flink run -p 4 -d -c cn.xisun.flink.StreamWordCount /mnt/d/JetBrainsWorkSpace/IDEAProjects/xisun-flink/target/xisun-flink-1.0-SNAPSHOT.jar --host localhost --port 7777 在 Flink 中，如果启动了 yarn-session，提交任务时，默认提交到 yarn-session 中的 Flink 集群；如果没有启动 yarn-session，则提交到 Standalone 中的 Flink 集群。 到 Yarn 控制台查看任务状态： 取消 yarn-session： 1$ yarn application --kill application_1577588252906_0001 Per-Job-Cluster 启动 Hadoop 集群 (略)。 不启动 yarn-session ，直接提交任务： 1$ ./flink run –m yarn-cluster -p 4 -d -c cn.xisun.flink.StreamWordCount /mnt/d/JetBrainsWorkSpace/IDEAProjects/xisun-flink/target/xisun-flink-1.0-SNAPSHOT.jar --host localhost --port 7777 按参数名称提取参数。 1$ ./flink run -m yarn-cluster -p 6 -d reaction-extractor-1.0-SNAPSHOT.jar extractor-patent extractor-reaction extractor-patent-timeout y 按参数位置提取参数。 查看任务和关闭任务，使用 Yarn 命令处理： 1234567891011121314151617# 查看yarn上面的资源使用情况命令，ctrl+c退出$ yarn top# 查看yarn上运行的任务列表命令，如果集群有krb认证的话，需要先kinit，认证后可以看到所有正在运行的任务$ yarn application -list# 查看yarn上运行的指定状态的任务列表命令$ yarn application -list -appStates RUNNING# 查看yarn指定任务的状态信息命令$ yarn application -status &lt;applicationId&gt; # 查看yarn指定application任务日志命令，可以选择输出到本地文件$ yarn logs -applicationId &lt;applicationId&gt; &gt; yarn.log# yarn logs -applicationId application_1606730935892_0095 &gt; yarn.log# yarn logs -applicationId application_1606730935892_0095 &gt; yarn-2001-2005.log# yarn logs -applicationId application_1606730935892_0093 --size 3145728 &gt; yarn-1996-2000.log# kill yarn application命令$ yarn application -kill &lt;applicationId&gt;# kill yarn job命令$ yarn job -kill &lt;jobId&gt; Kubernetes 部署 容器化部署是目前业界很流行的一项技术，基于 Docker 镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是 Kubernetes (k8s)，而 Flink 也在最近的版本中支持了 k8s 部署模式。 搭建 Kubernetes 集群 (略)。 配置各组件的 yaml 文件。 在 k8s 上构建 Flink Session Cluster，需要将 Flink 集群的组件对应的 Docker 镜像分别在 k8s 上启动，包括 JobManager、TaskManager、JobManagerService 三个镜像服务，每个镜像服务都可以从中央镜像仓库中获取。 启动 Flink Session Cluster： 12345678// 启动 jobmanager-service 服务$ kubectl create -f jobmanager-service.yaml// 启动 jobmanager-deployment 服务$ kubectl create -f jobmanager-deployment.yaml// 启动 taskmanager-deployment 服务$ kubectl create -f taskmanager-deployment.yaml 访问 Flink UI 页面。集群启动后，就可以通过 JobManagerServicers 中配置的 WebUI 端口，用浏览器输入以下 url 来访问 Flink UI 页面了：http://&#123;JobManagerHost:Port&#125;/api/v1/namespaces/default/services/flink-jobmanager:ui/proxy Flink 的运行架构Flink 运行时的组件 作业管理器 (JobManager) 控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的 JobManager 所控制执行。 JobManager 会先接收到要执行的应用程序，这个应用程序会包括：作业图 (JobGraph)、逻辑数据流图 (logical dataflow graph) 和打包了所有的类、库和其它资源的 Jar 包。 JobManager 会把 JobGraph 转换成一个物理层面的数据流图，这个图被叫做 “执行图” (ExecutionGraph)，包含了所有可以并发执行的任务。 JobManager 会向资源管理器 (ResourceManager) 请求执行任务必要的资源，也就是任务管理器 (TaskManager) 上的插槽 (slot)。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的 TaskManager 上。而在运行过程中，JobManager 会负责所有需要中央协调的操作，比如说检查点 (checkpoints) 的协调。 任务管理器 (TaskManager) Flink 中的工作进程。通常在 Flink 中会有多个 TaskManager 运行，每一个 TaskManager 都包含了一定数量的插槽 (slots)。插槽的数量限制了 TaskManager 能够执行的任务数量。 启动之后，TaskManager 会向资源管理器 (ResourceManager) 注册它的插槽；收到资源管理器 (ResourceManager) 的指令后，TaskManager 就会将一个或者多个插槽提供给 JobManager 调用，然后 JobManager 就可以向插槽分配任务 (tasks) 来执行了。 在执行过程中，一个 TaskManager 可以跟其它运行同一应用程序的 TaskManager 交换数据。 资源管理器 (ResourceManager) 主要负责管理任务管理器 (TaskManager) 的插槽 (slot)，TaskManger 插槽是 Flink 中定义的处理资源单元。 Flink 为不同的环境和资源管理工具提供了不同资源管理器，比如 YARN、Mesos、K8s，以及 Standalone 部署。 当 JobManager 申请插槽资源时，ResourceManager 会将有空闲插槽的 TaskManager 分配给 JobManager。如果 ResourceManager 没有足够的插槽来满足 JobManager 的请求，它还可以向资源提供平台发起会话，以提供启动 TaskManager 进程的容器。 另外，ResourceManager 还负责终止空闲的 TaskManager，释放计算资源。 分发器 (Dispatcher) 可以跨作业运行，它为应用提交提供了 REST 接口。 当一个应用被提交执行时，分发器就会启动并将应用移交给一个 JobManager。 Dispatcher 也会启动一个 Web UI，用来方便地展示和监控作业执行的信息。 Dispatcher 在架构中可能并不是必需的，这取决于应用提交运行的方式。 任务提交流程 当一个应用提交执行时，Flink 的各个组件交互协作的过程如下： 上图中，步骤 7 指 TaskManager 为 JobManager 提供 slots，步骤 8 表示 JobManager 提交要在 slots 中执行的任务给 TaskManager。 上图是从一个较为高层级的视角来看应用中各组件的交互协作。如果部署的集群环境不同 (例如 Yarn，Mesos，Kubernetes，Standalone等)，其中一些步骤可以被省略，或是有些组件会运行在同一个 JVM 进程中。 具体地，如果我们将 Flink 集群部署到 Yarn 上，那么就会有如下的提交流程： - Flink 任务提交后，Client 向 HDFS 上传 Flink 的 Jar 包和配置。 - 之后，Client 向 Yarn ResourceManager 提交任务，Yarn ResourceManager 分配 Container 资源并通知对应的 NodeManager 启动 ApplicationMaster. - ApplicationMaster 启动后加载 Flink 的 Jar 包和配置构建环境，然后启动 JobManager，之后，**JobManager 向 Flink 自身的 ResourceManager 申请资源，Flink 自身的 ResourceManager 再向 Yarn 的 ResourceManager 申请资源 (因为是 Yarn 模式，所有资源归 Yarn 的 ResourceManager 管理)，**申请到资源后，启动 TaskManager。 - Yarn ResourceManager 分配 Container 资源后 ， 由 ApplicationMaster 通知资源所在节点的 NodeManager 启动 TaskManager。 - NodeManager 加载 Flink 的 Jar 包和配置构建环境并启动 TaskManager，TaskManager 启动后向 JobManager 发送心跳包，并等待 JobManager 向其分配任务。 任务调度原理 客户端不是运行时和程序执行的一部分，但它用于准备并发送 dataflow (JobGraph) 给 Master (JobManager)，然后，客户端断开连接或者维持连接以等待接收计算结果。 当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 Client 为提交 Job 的客户端，可以是运行在任何机器上 (与 JobManager 环境连通即可)。提交 Job 后，Client 可以结束进程 (Streaming 的任务)，也可以不结束并等待结果返回。 JobManager 会产生一个执行图 (Dataflow Graph)，主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 Jar 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 TaskManager 在启动的时候就设置好了槽位数 (Slot)，每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。(如果一个 Slot 中启动多个线程，那么这几个线程类似 CPU 调度一样共用同一个 slot) 并行度 (Parallelism) Flink 程序的执行具有并行、分布式的特性。 在执行过程中，一个流 (Stream) 包含一个或多个分区 (stream partition)，而每一个算子 (operator) 可以包含一个或多个子任务 (operator subtask)，这些子任务在不同的线程、不同的物理机或不同的容器中彼此互不依赖地执行。 一个特定算子的子任务 (subtask) 的个数，称之为该算子的并行度 (parallelism)。一个程序中，不同的算子可能具有不同的并行度。一般情况下，一个流程序的并行度，可以认为就是其所有算子中，设置最大的那个算子的并行度。 并行度优先级：具体算子设置的并行度 &gt; 程序全局设置的并行度 &gt; 提交 Job 时设置的并行度 &gt; flink-conf.yaml 配置文件默认的并行度。 并行度，可以简单理解为：并行执行任务的程度。Flink 程序中，有四种方式设置并行度： 通过 env.setParallelism(1); 设置全局的并行度。 通过 setParallelism() 设置每一个算子的并行度。 提交任务时，通过 Web 页面直接指定，或命令行使用 -p 参数指定并行度。 通过 flink-conf.yaml 配置文件配置。 并行度 parallelism 是动态概念，即 TaskManager 运行程序时实际使用的并发能力。在 flink-conf.yaml 配置文件中，通过 parallelism.default 设置并行度，默认为 1。 123# The parallelism used for programs that did not specify and other parallelism.parallelism.default: 1 TaskManger 与 Slots Flink 中每一个 worker (TaskManager) 都是一个 JVM 进程 (Processes)，它可能会在独立的线程 (Threads) 上执行一个或多个 subtask。为了控制一个 worker 能接收多少个 task，worker 通过 task slot 来进行控制 (一个 worker 至少有一个 task slot)。 每个 task slot 表示 TaskManager 拥有资源的一个固定大小的子集。假如一个 TaskManager 有三个 slot，那么它会将其管理的内存分成三份给各个 slot。资源 slot 化意味着一个 subtask 将不需要跟来自其他 job 的 subtask 竞争被管理的内存，取而代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到 CPU 的隔离，slot 目前仅仅用来隔离 task 的受管理的内存。 slot 实际上就是，执行一个独立任务所需要的计算资源的最小单元 (主要就是 CPU 和内存资源)。 当前 Flink 架构中，每一个 slot 所占的内存是隔离开的，即独享内存资源，互不影响。但 CPU 资源是共享的，如果一个 CPU 被多个 slot 使用，那 CPU 就是时间片上的一个轮转状态，被多个 slot 轮流使用。 通过调整 task slot 的数量，允许用户定义 subtask 之间如何互相隔离。如果一个 TaskManager 只有一个 slot，那将意味着每个 task group 运行在独立的 JVM 中 (该 JVM 可能是通过一个特定的容器启动的)，而一个 TaskManager 多个 slot，则意味着更多的 subtask 可以共享同一个 JVM。而在同一个 JVM 进程中的 task 将共享 TCP 连接 (基于多路复用) 和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个 task 的负载。 默认情况下，Flink 允许子任务共享 slot，即使它们是不同任务的子任务。 这样的结果是，一个 slot 可以保存任务的整个管道 (也就是任务的一个完整流程)。 从上图可以看出，Stream 的并行度为 6，共有 13 个任务，但实际上，只用了 6 个 slot 就完成了全部任务的执行。 共享 slot 的好处：在一个 slot 中可以保存任务的整个管道，即使其他的 slot 挂掉了，也不会影响任务的完整执行，保证了程序的健壮性。另外，如果某个子任务比较占用 CPU，共享 slot 能够充分调用 CPU 的处理能力，防止出现有的 CPU 极其空闲，有的 CPU 极其繁忙。 共享 slot 的前提：必须是一个 Stream 中先后发生的不同的子任务。比如上图中，不同的 source-map 算子任务，就只能放在不同的 slot 中，不能共享一个 slot，因为相同的子任务，如果共享一个 slot，可能会导致这几个相同的子任务间数据的混淆。 Task Slot 是静态的概念，是指 TaskManager 具有的并发执行能力。在 flink-conf.yaml 配置文件中，通过 taskmanager.numberOfTaskSlots 设置 TaskManager 中的 slot 数量，默认为 1，一般应设置为与当前主机 CPU 的逻辑核心数相同。 123# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.taskmanager.numberOfTaskSlots: 1 Flink 中，可以通过 slotSharingGroup() 设置每一个算子所属的 slot 共享组。 如果不同算子的 slot 共享组不同，则运行时一定要占用不同的 slot。 123456789101112131415161718192021public class StreamWordCount throws Exception &#123; public static void main(String[] args) &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); ParameterTool parameterTool = ParameterTool.fromArgs(args); String host = parameterTool.get(&quot;host&quot;); int port = parameterTool.getInt(&quot;port&quot;); DataStream&lt;String&gt; inputDataStream = env.socketTextStream(host, port); // 不配置时，默认slot共享组为default，后面的算子默认与前面的算子同一slot共享组 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; resultStream = inputDataStream .flatMap(new WordCount.MyFlatMapper()).slotSharingGroup(&quot;green&quot;) .keyBy(0) .sum(1).setParallelism(2).slotSharingGroup(&quot;red&quot;); resultStream.print(); env.execute(); &#125;&#125; 上面代码中，source 算子是 default 共享组，flatMap 是 green 共享组，sum 是 red 共享组，print 与 sum 相同。提交上面这个任务时，先考虑分组，再考虑每个分组内最大的并行度，相加之后，即为所需的 slot 数量。因此，上面的任务，至少需要 4 个 slot。 并行子任务的分配 假设一个 JobGraph 如下图左所示，则可以看出，一共有 16 个子任务。如果没有自定义 slot 共享组，则如下图右所示，只需要 4 个 slot 就可以完成任务。 假设一共有 3 个 TaskManager，每一个 TaskManager 中分配了 3 个 TaskSlot，也就是每个 TaskManager 可以接收 3 个 task，一共有 9 个 TaskSlot。在 Example 1 中，如果我们设置 parallelism.default=1，即运行程序默认的并行度为 1，那么 9 个 TaskSlot 只用了 1个，会有 8 个空闲。因此，设置合适的并行度才能提高效率，如 Example 2 ~ 4 所示。 程序与数据流 (DataFlow) 所有的 Flink 程序都是由三部分组成的：Source、Transformation 和 Sink。 Source 负责读取数据源，Transformation 利用各种算子进行处理加工，Sink 负责输出。 在运行时，Flink 上运行的程序会被映射成 “逻辑数据流” (dataflows)，它包含了这三部分。 每一个 dataflow 以一个或多个 source 开始，以一个或多个 sink 结束。dataflow 类似于任意的有向无环图 (DAG) (有方向非环形)。 在大部分情况下，程序中的转换运算 (transformations) 跟 dataflow 中的算子 (operator) 是一一对应的关系，但有时候，一个 transformation 可能对应多个 operator。 执行图 (ExecutionGrap) 由 Flink 程序直接映射成的数据流图是 StreamGraph，也被称为逻辑流图，因为它们表示的是计算逻辑的高级视图。为了执行一个流处理程序，Flink 需要将逻辑流图转换为物理数据流图 (也叫执行图)，详细说明程序的执行方式。 Flink 中的执行图可以分成四层：StreamGraph —&gt; JobGraph —&gt; ExecutionGraph —&gt; 物理执行图。 StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。 JobGraph：StreamGraph 经过优化后生成了 JobGraph，即提交给 JobManager 的数据结构。主要的优化为：将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。 ExecutionGraph：JobManager 根据 JobGraph 生成 ExecutionGraph。ExecutionGraph 是 JobGraph 的并行化版本，是调度层最核心的数据结构。 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个 TaskManager 上部署 Task 后形成的 “图”，并不是一个具体的数据结构。 数据传输形式 一个 Flink 程序中，不同的算子可能具有不同的并行度。 Stream 在算子之间传输数据的形式可以是 one-to-one (forwarding) 的模式，也可以是 redistributing 的模式，具体是哪一种形式，取决于算子的种类 One-to-one：Stream维护着分区以及元素的顺序，比如在 source 和 map 这两个 operator 之间，这意味着 map 算子的子任务看到的元素的个数以及顺序，跟 source 算子的子任务生产的元素的个数、顺序相同。map、fliter、flatMap 等算子都是 one-to-one 的对应关系。 类似于 Spark 中的窄依赖。 Redistributing：Stream 的分区会发生改变，比如 map 跟 keyBy/window 之间，或者 keyBy/window 跟 sink 之间。每一个算子的子任务依据所选择的 transformation 发送数据到不同的目标任务。例如，keyBy 基于 hashCode 重分区、broadcast 和 rebalance会随机重新分区 (rebalance 实际上是一种轮询的随机重新分区操作)，这些算子都会引起 redistribute 过程，而 redistribute 过程就类似于 Spark 中的 shuffle 过程 (Flink 中的 shuffle 算子，是完全随机的重新分区操作)。 类似于 Spark 中的宽依赖。 任务链 (Operator Chain) Flink 采用了一种称为任务链的优化技术，它能减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同的并行度，并通过本地转发 (local forward) 的方式进行连接。 相同并行度的 one-to-one 操作，Flink 将这样相连的算子链接在一起形成一个 task，原来的算子成为里面的 subtask。 算子合并的条件：并行度相同，并且是 one-to-one 操作，两个条件缺一不可。 如上图所示，最终有 5 个任务，如果未自定义共享组，只需要 2 个 slot 即可。 如果不希望 Key Agg 和 Sink 这两个算子合并为一个任务，但也还是能 slot 共享，则有以下几种方式处理： 在 Key Agg 算子后做一个 rebalance (.rebalance()) 或 shuffle (.shuffle()) 操作，改变其传输方式； 使用 .disableChaining()，指定 Key Agg 算子不参与任务链合并操作 (该算子前后都会不参与)； 使用 .startNewChain()，指定 Key Agg 算子后面开始一个新的任务链合并操作，即 Key Agg 算子还可以与它前面的算子合并，但不与后面的算子合并； 如果希望每一个算子都这样处理，可以通过 env.disableOperatorChaining();，对全局进行设置。 Flink 的流处理 API EnvironmentgetExecutionEnvironment() 创建一个执行环境，表示当前执行程序的上下文。 如果程序是独立调用的，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回此集群的执行环境，也就是说，getExecutionEnvironment 会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。 批处理执行环境： 1ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 流处理执行环境： 1StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 在生产环境时，如果没有设置并行度，会以 flink-conf.yaml 中的配置为准，默认是 1： 123# The parallelism used for programs that did not specify and other parallelism.parallelism.default: 1 在本地 IDEA 执行环境时，默认并行度是本地计算机的 CPU 逻辑核数，本地计算机为 4 核 8 处理器，即默认并行度为 8，本文测试代码以此为基准。 createLocalEnvironment() 返回本地执行环境，需要在调用时指定默认的并行度： 1LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1); createRemoteEnvironment() 返回集群执行环境，将 Jar 提交到远程服务器。需要在调用时指定 JobManager 的 IP 和端口号，并指定要在集群中运行的 Jar 包。 12StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(&quot;jobmanage-hostname&quot;, 6123, &quot;YOURPATH//WordCount.jar&quot;); Source从集合读取数据 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 传感器温度读数的数据类型 * * @author XiSun * @Date 2021/4/28 20:59 */public class SensorReading &#123; // 属性：id，时间戳，温度值 private String id; private Long timestamp; private Double temperature; public SensorReading() &#123; &#125; public SensorReading(String id, Long timestamp, Double temperature) &#123; this.id = id; this.timestamp = timestamp; this.temperature = temperature; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public Long getTimestamp() &#123; return timestamp; &#125; public void setTimestamp(Long timestamp) &#123; this.timestamp = timestamp; &#125; public Double getTemperature() &#123; return temperature; &#125; public void setTemperature(Double temperature) &#123; this.temperature = temperature; &#125; @Override public String toString() &#123; return &quot;SensorReading&#123;&quot; + &quot;id=&#x27;&quot; + id + &#x27;\\&#x27;&#x27; + &quot;, timestamp=&quot; + timestamp + &quot;, temperature=&quot; + temperature + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617181920212223242526272829/** * @author XiSun * @Date 2021/4/28 20:59 */public class SourceTest1_Collection &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.Source: 从集合读取数据 DataStream&lt;SensorReading&gt; sensorDataStream = env.fromCollection( Arrays.asList( new SensorReading(&quot;sensor_1&quot;, 1547718199L, 35.8), new SensorReading(&quot;sensor_6&quot;, 1547718201L, 15.4), new SensorReading(&quot;sensor_7&quot;, 1547718202L, 6.7), new SensorReading(&quot;sensor_10&quot;, 1547718205L, 38.1) ) ); DataStream&lt;Integer&gt; intDataStream = env.fromElements(1, 2, 3, 4, 5, 6, 7, 8, 9); // 3. 打印，参数为数据流的名称，可选 sensorDataStream.print(&quot;sensorDataName&quot;); intDataStream.print(&quot;intDataStreamName&quot;); // 4. 执行任务，参数为Job的名称，可选 env.execute(&quot;JobName&quot;); &#125;&#125; 输出结果： 123456789101112131415161718SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.intDataStreamName:1&gt; 8intDataStreamName:5&gt; 4intDataStreamName:4&gt; 3sensorDataName:5&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;sensorDataName:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;intDataStreamName:3&gt; 2sensorDataName:4&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;intDataStreamName:2&gt; 1intDataStreamName:2&gt; 9intDataStreamName:7&gt; 6intDataStreamName:6&gt; 5sensorDataName:6&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;intDataStreamName:8&gt; 7Process finished with exit code 0 从文件读取数据 代码实现： 12345678910111213141516171819/** * @author XiSun * @Date 2021/4/28 22:10 */public class SourceTest2_File &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据：单线程读取文件，按顺序逐行读取，如果不设置，则多线程读取，文件内容会乱序 DataStream&lt;String&gt; dataStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.打印，多线程 dataStream.print(); // 4.执行任务 env.execute(); &#125;&#125; sensor.txt 文件内容： 1234567sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718207,36.3sensor_1,1547718209,32.8sensor_1,1547718212,37.1 输出结果 (多线程打印，每次输出结果都会不同)： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.4&gt; sensor_1,1547718207,36.31&gt; sensor_6,1547718201,15.43&gt; sensor_10,1547718205,38.16&gt; sensor_1,1547718212,37.12&gt; sensor_7,1547718202,6.75&gt; sensor_1,1547718209,32.88&gt; sensor_1,1547718199,35.8Process finished with exit code 0 设置 dataStream.print().setParallelism(1);，单线程打印的输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718207,36.3sensor_1,1547718209,32.8sensor_1,1547718212,37.1Process finished with exit code 0 从 Kafka 消息队列读取数据 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 代码实现： 12345678910111213141516171819202122232425262728/** * @author XiSun * @Date 2021/4/28 22:15 */public class SourceTest3_Kafka &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.创建Kafka消费者 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;); properties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;); properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(&quot;sensor&quot;, new SimpleStringSchema(), properties); // 3.从Kafka读取数据 DataStream&lt;String&gt; dataStream = env.addSource(consumer); // 4.打印 dataStream.print(); // 5.执行任务 env.execute(); &#125;&#125; 自定义 Source 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * @author XiSun * @Date 2021/4/28 22:25 */public class SourceTest4_UDF &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// env.setParallelism(1); // 2.从自定义Source读取数据 DataStream&lt;SensorReading&gt; dataStream = env.addSource(new MySensorSource()); // 3.打印 dataStream.print(); // 4.执行任务 env.execute(); &#125; // 实现自定义的SourceFunction，随机生成传感器数据 public static class MySensorSource implements SourceFunction&lt;SensorReading&gt; &#123; // 定义一个标识位，用来控制数据的产生 private boolean running = true; @Override public void run(SourceContext&lt;SensorReading&gt; ctx) throws Exception &#123; // 定义一个随机数发生器 Random random = new Random(); // 设置10个传感器的初始温度 HashMap&lt;String, Double&gt; sensorTempMap = new HashMap&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; sensorTempMap.put(&quot;sensor_&quot; + (i + 1), 60 + random.nextGaussian() * 20); &#125; while (running) &#123; for (String sensorId : sensorTempMap.keySet()) &#123; // 在当前温度基础上随机波动 Double newtemp = sensorTempMap.get(sensorId) + random.nextGaussian(); sensorTempMap.put(sensorId, newtemp); ctx.collect(new SensorReading(sensorId, System.currentTimeMillis(), newtemp)); &#125; // 控制输出频率 Thread.sleep(1000L); &#125; &#125; @Override public void cancel() &#123; running = false; &#125; &#125;&#125; 输出结果 (程序会一直输出下去)： 12345678910111213141516171819SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.2&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1619670947828, temperature=57.523519020755195&#125;6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1619670947828, temperature=60.16683595604395&#125;6&gt; SensorReading&#123;id=&#x27;sensor_9&#x27;, timestamp=1619670947831, temperature=42.125026316308286&#125;4&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1619670947826, temperature=82.58226594512607&#125;5&gt; SensorReading&#123;id=&#x27;sensor_4&#x27;, timestamp=1619670947828, temperature=78.73909616880852&#125;4&gt; SensorReading&#123;id=&#x27;sensor_5&#x27;, timestamp=1619670947831, temperature=26.71490359887942&#125;5&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1619670947831, temperature=85.09026456845346&#125;1&gt; SensorReading&#123;id=&#x27;sensor_2&#x27;, timestamp=1619670947828, temperature=64.90731492147165&#125;3&gt; SensorReading&#123;id=&#x27;sensor_3&#x27;, timestamp=1619670947822, temperature=31.96909359527708&#125;3&gt; SensorReading&#123;id=&#x27;sensor_8&#x27;, timestamp=1619670947831, temperature=86.34172370619932&#125;4&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1619670948831, temperature=60.71931724451548&#125;6&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1619670948831, temperature=57.36109139383153&#125;3&gt; SensorReading&#123;id=&#x27;sensor_4&#x27;, timestamp=1619670948831, temperature=78.1152626762054&#125;5&gt; SensorReading&#123;id=&#x27;sensor_2&#x27;, timestamp=1619670948831, temperature=63.90599072985537&#125;2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1619670948831, temperature=82.1517010383502&#125;... Transform基本转换算子 map、flatMap、filter 通常被统一称为基本转换算子 (简单转换算子)。 map： flatMap： filter： 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * @author XiSun * @Date 2021/4/28 10:39 */public class TransformTest1_Base &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.map，把String转换成其长度输出 DataStream&lt;Integer&gt; mapStream = inputStream.map(new MapFunction&lt;String, Integer&gt;() &#123; @Override public Integer map(String value) throws Exception &#123; return value.length(); &#125; &#125;); // 4.flatmap，按逗号分割字符串 DataStream&lt;String&gt; flatMapStream = inputStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; String[] fields = value.split(&quot;,&quot;); for (String field : fields) &#123; out.collect(field); &#125; &#125; &#125;); // 5.filter，筛选&quot;sensor_1&quot;开头的id对应的数据 DataStream&lt;String&gt; filterStream = inputStream.filter(new FilterFunction&lt;String&gt;() &#123; @Override public boolean filter(String value) throws Exception &#123; return value.startsWith(&quot;sensor_1&quot;); &#125; &#125;); // 6.打印 mapStream.print(&quot;map&quot;); flatMapStream.print(&quot;flatMap&quot;); filterStream.print(&quot;filter&quot;); // 7.执行任务 env.execute(); &#125;&#125; 输出结果： 1234567891011121314151617181920212223242526272829303132333435363738SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.map:5&gt; 24map:5&gt; 24flatMap:4&gt; sensor_1flatMap:4&gt; 1547718199flatMap:4&gt; 35.8flatMap:4&gt; sensor_1flatMap:4&gt; 1547718212flatMap:4&gt; 37.1flatMap:6&gt; sensor_7flatMap:6&gt; 1547718202flatMap:6&gt; 6.7map:6&gt; 24filter:3&gt; sensor_1,1547718199,35.8filter:3&gt; sensor_1,1547718212,37.1filter:6&gt; sensor_10,1547718205,38.1map:4&gt; 24map:1&gt; 23map:3&gt; 24map:2&gt; 25filter:1&gt; sensor_1,1547718207,36.3filter:2&gt; sensor_1,1547718209,32.8flatMap:5&gt; sensor_6flatMap:5&gt; 1547718201flatMap:5&gt; 15.4flatMap:3&gt; sensor_1flatMap:3&gt; 1547718209flatMap:3&gt; 32.8flatMap:2&gt; sensor_1flatMap:2&gt; 1547718207flatMap:2&gt; 36.3flatMap:1&gt; sensor_10flatMap:1&gt; 1547718205flatMap:1&gt; 38.1Process finished with exit code 0 聚合操作算子 DataStream 里没有 reduce 和 sum 这类聚合操作的方法，因为 Flink 设计中，所有数据必须先分组才能做聚合操作。 先 keyBy 得到 KeyedStream，然后调用其 reduce、sum 等聚合操作方法。(先分组后聚合) 常见的聚合操作算子主要有： keyBy 滚动聚合算子 Rolling Aggregation reduce keyBy： DataStream —&gt; KeyedStream：逻辑地将一个流拆分成不相交的分组，每个分组包含具有相同 key 的元素，在内部以 hash 的形式实现的。 keyBy 会重新分组。相同的 key 一定在同一个分组，而不同的 key 可能会在一个分组，因为是通过 hash 原理实现的，可能存在取模操作。 keyBy 不是计算操作。 keyBy 可以按照元组的位置，或者对象的属性名分组，在 Flink 新版本中，有一些方法被弃用，可以用其他的方法替换。 12345678910111213141516171819202122public &lt;K&gt; KeyedStream&lt;T, K&gt; keyBy(KeySelector&lt;T, K&gt; key) &#123; Preconditions.checkNotNull(key); return new KeyedStream(this, (KeySelector)this.clean(key));&#125;public &lt;K&gt; KeyedStream&lt;T, K&gt; keyBy(KeySelector&lt;T, K&gt; key, TypeInformation&lt;K&gt; keyType) &#123; Preconditions.checkNotNull(key); Preconditions.checkNotNull(keyType); return new KeyedStream(this, (KeySelector)this.clean(key), keyType);&#125;/** @deprecated */@Deprecatedpublic KeyedStream&lt;T, Tuple&gt; keyBy(int... fields) &#123; return !(this.getType() instanceof BasicArrayTypeInfo) &amp;&amp; !(this.getType() instanceof PrimitiveArrayTypeInfo) ? this.keyBy((Keys)(new ExpressionKeys(fields, this.getType()))) : this.keyBy((KeySelector)KeySelectorUtil.getSelectorForArray(fields, this.getType()));&#125;/** @deprecated */@Deprecatedpublic KeyedStream&lt;T, Tuple&gt; keyBy(String... fields) &#123; return this.keyBy((Keys)(new ExpressionKeys(fields, this.getType())));&#125; 滚动聚合算子 (Rolling Aggregation)： 这些算子可以针对 KeyedStream 的每一个支流做聚合，包括： sum() min() max() minBy() maxBy() min()、max() 和 minBy()、maxBy() 的区别在于：前者每次输出时，只有作为参数比较的字段会更新，其他字段不变；而后者除了作为参数比较的字段会更新，其他的字段会一起更新。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @author XiSun * @Date 2021/4/30 9:44 */public class TransformTest2_RollingAggregation &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.按照SensorReading对象的id分组 // 方式一：直接以属性名作为参数，此方法已弃用 /*KeyedStream&lt;SensorReading, Tuple&gt; keyedStream = dataStream.keyBy(&quot;id&quot;);*/ // 方式二：以KeySelector作为参数 /*KeyedStream&lt;SensorReading, String&gt; keyedStream = dataStream.keyBy(new KeySelector&lt;SensorReading, String&gt;() &#123; @Override public String getKey(SensorReading sensorReading) throws Exception &#123; return sensorReading.getId(); &#125; &#125;);*/ // 方式三：方式二的Lambda表达式版 KeyedStream&lt;SensorReading, String&gt; keyedStream = dataStream.keyBy(SensorReading::getId); // 5.滚动聚合，取当前最大的温度值，可以输入对象的属性名，或者元组里面的位置 // DataStream&lt;SensorReading&gt; resultStream = keyedStream.max(&quot;temperature&quot;); DataStream&lt;SensorReading&gt; resultStream = keyedStream.maxBy(&quot;temperature&quot;); // 6.打印 resultStream.print(&quot;result&quot;); // 7.执行任务 env.execute(); &#125;&#125; max() 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.result:4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;result:2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=37.1&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=37.1&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=37.1&#125;Process finished with exit code 0 因为是滚动更新，对于每一个分组，每次来一条数据时，都会输出一次历史最大值，所以有的数据才会出现多次。 sensor_7 和 sensor_10 各属于一个分组 (线程2 和线程 4)，但各只有一条数据。sensor_6 和 sensor_1 在同一个分组 (线程 3)，sensor_6 只有一条数据，对于 sensor_1，有四条数据，temperature 最大值为 37.1，输出了四次，但每次只更新了 temperature 的值，其他字段的值没有更新。 maxBy() 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;result:2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;result:4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;Process finished with exit code 0 对于 sensor_1 的四条数据，每次输出时，除了更新 temperature 的值，其他字段的值也一起更新，但保留时间戳仍是当前 temperature 最大值对应的时间戳，而这个时间戳可能不是实时的值，比如第 9 行，其时间戳应该是 1547718209 (32.8 度的时间戳)，而不是 1547718207。 reduce： reduce，归约，适用于更加一般化的聚合操作场景。比如：在读取文件内容时，可以使用 reduce 算子将前后两行加起来，最终组成完整的文件内容。 KeyedStream —&gt; DataStream：一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。(返回值类型与传入类型一致，不能改变) 在前面 Rolling Aggregation 的前提下，对需求进行修改。获取同组历史温度最高的传感器信息，并要求实时更新其时间戳信息。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author XiSun * @Date 2021/4/30 16:57 */public class TransformTest3_Reduce &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度为1，能更好的体验效果，sensor.txt从上到下时间戳是递增的 env.setParallelism(1); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.按照SensorReading对象的id分组 KeyedStream&lt;SensorReading, String&gt; keyedStream = dataStream.keyBy(SensorReading::getId); // 5.reduce聚合，取每个分组最大的温度值，并更新为当前最新的时间戳 SingleOutputStreamOperator&lt;SensorReading&gt; resultStream = keyedStream.reduce(new ReduceFunction&lt;SensorReading&gt;() &#123; // curSensor：上次聚合的结果，newSensor：当前的元素 // 对同一个分组，id值一直是相同的 @Override public SensorReading reduce(SensorReading curSensor, SensorReading newSensor) throws Exception &#123; return new SensorReading(curSensor.getId(), newSensor.getTimestamp(), Math.max(curSensor.getTemperature(), newSensor.getTemperature())); &#125; &#125;); // 6.打印 resultStream.print(&quot;result&quot;); // 7.执行任务 env.execute(); &#125;&#125; 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.result&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;result&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;result&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;result&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;result&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;result&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=36.3&#125;result&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;Process finished with exit code 0 对于 sensor_1 的四条数据，从第 8 行开始，每次输出时，temperature 都是当前历史温度的最高值，而时间戳也在实时更新。 多流转换算子 多流转换算子一般包括： split 和 select (Filink 1.12.1 版本被移除) connect 和 coMap union split 和 select： split： DataStream —&gt; SplitStream：根据某些特征把一个 DataStream 拆分成两个或者多个 DataStream。 select： SplitStream —&gt; DataStream：从一个 SplitStream 中获取一个或者多个 DataStream。 split 和 select 需要结合使用。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author XiSun * @Date 2021/5/2 11:21 */public class TransformTest4_MultipleStreams &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.分流 SplitStream&lt;SensorReading&gt; splitStream = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading sensorReading) &#123; // 可以获得多个分流，此处按温度是否超过30℃设置了两个分流 return (sensorReading.getTemperature() &gt; 30) ? Collections.singletonList(&quot;high&quot;) : Collections.singletonList(&quot;low&quot;); &#125; &#125;); // 5.获取分流 DataStream&lt;SensorReading&gt; highTempStream = splitStream.select(&quot;high&quot;); DataStream&lt;SensorReading&gt; lowTempStream = splitStream.select(&quot;low&quot;); DataStream&lt;SensorReading&gt; allTempStream = splitStream.select(&quot;high&quot;, &quot;low&quot;); // 6.打印 highTempStream.print(&quot;high&quot;); lowTempStream.print(&quot;low&quot;); allTempStream.print(&quot;all&quot;); // 7.执行任务 env.execute(); &#125;&#125; 以上代码，可使用 Flink 1.11.1 版本测试。 输出结果： 12345678910111213141516171819SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.all:2&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;all:5&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;high:5&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;all:6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;high:3&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;all:3&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;all:4&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;high:4&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;all:1&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;low:1&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;high:6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;all:6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;high:6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;low:2&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;Process finished with exit code 0 connect 和 coMap/coFlatMap： connect： DataStream，DataStream —&gt; ConnectedStreams：连接两个保持他们类型的数据流，两个数据流被 Connect 之后，只是被放在了同一个流中，其内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。 coMap/coFlatMap： ConnectedStreams —&gt; DataStream：作用于 ConnectedStreams 上，功能与 map 和 flatMap 一样，对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。 connect 和 coMap/coFlatMap 需要结合使用。 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @author XiSun * @Date 2021/5/2 11:21 */public class TransformTest4_MultipleStreams2 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.分流 SplitStream&lt;SensorReading&gt; splitStream = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading sensorReading) &#123; // 可以获得多个分流，此处按温度是否超过30℃设置了两个分流 return (sensorReading.getTemperature() &gt; 30) ? Collections.singletonList(&quot;high&quot;) : Collections.singletonList(&quot;low&quot;); &#125; &#125;); // 5.获取分流 DataStream&lt;SensorReading&gt; highTempStream = splitStream.select(&quot;high&quot;); DataStream&lt;SensorReading&gt; lowTempStream = splitStream.select(&quot;low&quot;); DataStream&lt;SensorReading&gt; allTempStream = splitStream.select(&quot;high&quot;, &quot;low&quot;); // 6.合流connect，将高温流转换成二元组类型，再与低温流连接合并之后，输出状态信息 // org.apache.flink.api.java.tuple.Tuple2 // org.apache.flink.api.java.tuple.Tuple3 DataStream&lt;Tuple2&lt;String, Double&gt;&gt; warningStream = highTempStream.map(new MapFunction&lt;SensorReading, Tuple2&lt;String, Double&gt;&gt;() &#123; @Override public Tuple2&lt;String, Double&gt; map(SensorReading sensorReading) throws Exception &#123; return new Tuple2&lt;&gt;(sensorReading.getId(), sensorReading.getTemperature()); &#125; &#125;); ConnectedStreams&lt;Tuple2&lt;String, Double&gt;, SensorReading&gt; connectedStreams = warningStream.connect(lowTempStream); SingleOutputStreamOperator&lt;Object&gt; resultStream = connectedStreams.map(new CoMapFunction&lt;Tuple2&lt;String, Double&gt;, SensorReading, Object&gt;() &#123; @Override public Object map1(Tuple2&lt;String, Double&gt; stringDoubleTuple2) throws Exception &#123; return new Tuple3&lt;&gt;(stringDoubleTuple2.f0, stringDoubleTuple2.f1, &quot;high temperature warning&quot;); &#125; @Override public Object map2(SensorReading sensorReading) throws Exception &#123; return new Tuple3&lt;&gt;(sensorReading.getId(), sensorReading.getTemperature(), &quot;normal temperature&quot;); &#125; &#125;); // 7.打印 resultStream.print(); // 8.执行任务 env.execute(); &#125;&#125; 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.3&gt; (sensor_1,35.8,high temperature warning)2&gt; (sensor_1,32.8,high temperature warning)1&gt; (sensor_1,36.3,high temperature warning)4&gt; (sensor_6,15.4,normal temperature)3&gt; (sensor_1,37.1,high temperature warning)5&gt; (sensor_7,6.7,normal temperature)6&gt; (sensor_10,38.1,high temperature warning)Process finished with exit code 0 union： DataStream —&gt; DataStream：对两个或者两个以上的 DataStream 进行 union 操作，产生一个包含所有 DataStream 元素的新 DataStream。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * @author XiSun * @Date 2021/5/2 11:21 */public class TransformTest4_MultipleStreams3 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.分流 SplitStream&lt;SensorReading&gt; splitStream = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading sensorReading) &#123; // 可以获得多个分流，此处按温度是否超过30℃设置了两个分流 return (sensorReading.getTemperature() &gt; 30) ? Collections.singletonList(&quot;high&quot;) : Collections.singletonList(&quot;low&quot;); &#125; &#125;); // 5.获取分流 DataStream&lt;SensorReading&gt; highTempStream = splitStream.select(&quot;high&quot;); DataStream&lt;SensorReading&gt; lowTempStream = splitStream.select(&quot;low&quot;); DataStream&lt;SensorReading&gt; allTempStream = splitStream.select(&quot;high&quot;, &quot;low&quot;); // 6.联合两个分流 DataStream&lt;SensorReading&gt; unionStream = highTempStream.union(lowTempStream); // 7.打印 unionStream.print(); // 8.执行任务 env.execute(); &#125;&#125; 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;5&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;Process finished with exit code 0 connect 与 union 区别： 执行 union 操作的流的类型必须是一样，connect 可以不一样，可以在之后的 coMap 中再调整为一样的类型。 connect 只能合并两个流，union 可以合并多个流。 算子转换 在 Flink 中，Transformation 算子就是将一个或多个 DataStream 转换为新的 DataStream，可以将多个转换组合成复杂的数据流拓扑。 如上图所示，DataStream 会由不同的 Transformation 操作，转换、过滤、聚合成其他不同的流，从而完成我们的业务要求。 支持的数据类型 Flink 流应用程序处理的是以数据对象表示的事件流。所以在 Flink 内部，我们需要能够处理这些对象。它们需要被序列化和反序列化，以便通过网络传送它们；或者从状态后端、检查点和保存点读取它们。为了有效地做到这一点，Flink 需要明确知道应用程序所处理的数据类型。Flink 使用类型信息的概念来表示数据类型，并为每个数据类型生成特定的序列化器、反序列化器和比较器。 Flink 还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息，从而获得序列化器和反序列化器。但是，在某些情况下，例如 lambda 函数或泛型类型，需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。 Flink 支持 Java 和 Scala 中所有常见数据类型。使用最广泛的类型有以下几种。 基础数据类型 Flink 支持所有的 Java 和 Scala 基础数据类型，Int，Double，Long，String，… 12DataStream&lt;Integer&gt; numberStream = env.fromElements(1, 2, 3, 4);numberStream.map(data -&gt; data * 2); Java 和 和 Scala 元组 (Tuples) Java 不像 Scala 天生支持元组 Tuple 类型，Java 的元组类型由 Flink 的包提供，默认提供 Tuple0 ~ Tuple25。 123DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; personStream = env.fromElements( new Tuple2&lt;&gt;(&quot;Adam&quot;, 17), new Tuple2&lt;&gt;(&quot;Sarah&quot;, 23));personStream.filter(p -&gt; p.f1 &gt; 18); 包位置：import org.apache.flink.api.java.tuple.Tuple2; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293@Publicpublic abstract class Tuple implements Serializable &#123; private static final long serialVersionUID = 1L; public static final int MAX_ARITY = 25; private static final Class&lt;?&gt;[] CLASSES = new Class[]&#123;Tuple0.class, Tuple1.class, Tuple2.class, Tuple3.class, Tuple4.class, Tuple5.class, Tuple6.class, Tuple7.class, Tuple8.class, Tuple9.class, Tuple10.class, Tuple11.class, Tuple12.class, Tuple13.class, Tuple14.class, Tuple15.class, Tuple16.class, Tuple17.class, Tuple18.class, Tuple19.class, Tuple20.class, Tuple21.class, Tuple22.class, Tuple23.class, Tuple24.class, Tuple25.class&#125;; public Tuple() &#123; &#125; public abstract &lt;T&gt; T getField(int var1); public &lt;T&gt; T getFieldNotNull(int pos) &#123; T field = this.getField(pos); if (field != null) &#123; return field; &#125; else &#123; throw new NullFieldException(pos); &#125; &#125; public abstract &lt;T&gt; void setField(T var1, int var2); public abstract int getArity(); public abstract &lt;T extends Tuple&gt; T copy(); public static Class&lt;? extends Tuple&gt; getTupleClass(int arity) &#123; if (arity &gt;= 0 &amp;&amp; arity &lt;= 25) &#123; return CLASSES[arity]; &#125; else &#123; throw new IllegalArgumentException(&quot;The tuple arity must be in [0, 25].&quot;); &#125; &#125; public static Tuple newInstance(int arity) &#123; switch(arity) &#123; case 0: return Tuple0.INSTANCE; case 1: return new Tuple1(); case 2: return new Tuple2(); case 3: return new Tuple3(); case 4: return new Tuple4(); case 5: return new Tuple5(); case 6: return new Tuple6(); case 7: return new Tuple7(); case 8: return new Tuple8(); case 9: return new Tuple9(); case 10: return new Tuple10(); case 11: return new Tuple11(); case 12: return new Tuple12(); case 13: return new Tuple13(); case 14: return new Tuple14(); case 15: return new Tuple15(); case 16: return new Tuple16(); case 17: return new Tuple17(); case 18: return new Tuple18(); case 19: return new Tuple19(); case 20: return new Tuple20(); case 21: return new Tuple21(); case 22: return new Tuple22(); case 23: return new Tuple23(); case 24: return new Tuple24(); case 25: return new Tuple25(); default: throw new IllegalArgumentException(&quot;The tuple arity must be in [0, 25].&quot;); &#125; &#125;&#125; Scala 样例类 (case classes)123case class Person(name: String, age: Int)val persons: DataStream[Person] = env.fromElements(Person(&quot;Adam&quot;, 17), Person(&quot;Sarah&quot;, 23))persons.filter(p =&gt; p.age &gt; 18) Java 简单对象 (POJO) 要求必须提供无参构造函数。 要求成员变量都是 public，或者 private 的但提供 getter、setter 方法。 123456789101112public class Person &#123; public String name; public int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125;&#125; 1234DataStream Person &gt; persons = env.fromElements( new Person(&quot;Alex&quot;, 42), new Person(&quot;Wendy&quot;, 23)); 其它 (Arrays，Lists，Maps，Enums，等等) Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的，比如 Java 的 ArrayList，HashMap，Enum 等等。 实现 UDF 函数——更细粒度的控制流函数类 (Function Classes) Flink 暴露了所有 udf 函数的接口 (实现方式为接口或者抽象类)。例如 MapFunction，FilterFunction，ProcessFunction 等等。 下面的例子，实现了 FilterFunction 接口： 1DataStream&lt;String&gt; flinkTweets = tweets.filter(new FlinkFilter()); 123456public static class FlinkFilter implements FilterFunction&lt;String&gt; &#123; @Override public boolean filter(String value) throws Exception &#123; return value.contains(&quot;flink&quot;); &#125;&#125; 还可以将函数实现成匿名类： 123456DataStream&lt;String&gt; flinkTweets = tweets.filter(new FilterFunction&lt;String&gt;() &#123; @Override public boolean filter(String value) throws Exception &#123; return value.contains(&quot;flink&quot;); &#125;&#125;); 需要 filter 的字符串 “flink” 可以当作参数传进去： 123DataStream&lt;String&gt; tweets = env.readTextFile(&quot;INPUT_FILE &quot;);DataStream&lt;String&gt; flinkTweets = tweets.filter(new KeyWordFilter(&quot;flink&quot;)); 123456789101112public static class KeyWordFilter implements FilterFunction&lt;String&gt; &#123; private String keyWord; KeyWordFilter(String keyWord) &#123; this.keyWord = keyWord; &#125; @Override public boolean filter(String value) throws Exception &#123; return value.contains(this.keyWord); &#125;&#125; 匿名函数123DataStream&lt;String&gt; tweets = env.readTextFile(&quot;INPUT_FILE&quot;);DataStream&lt;String&gt; flinkTweets = tweets.filter(tweet -&gt; tweet.contains(&quot;flink&quot;)); 富函数 (Rich Functions) 富函数是 DataStream API 提供的一个函数类的接口，所有 Flink 函数类都有其 Rich 版本。 它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。 RichMapFunction RichFlatMapFunction RichFilterFunction … Rich Function 有一个生命周期的概念。典型的生命周期方法有： open() 是 RichFunction 的初始化方法，当一个算子例如 map 或者 filter 被调用之前，open() 会被调用。 close() 是生命周期中的最后一个调用的方法，做一些清理工作。 getRuntimeContext() 提供了函数的 RuntimeContext 的一些信息，例如函数执行的并行度，任务的名字，以及 state 状态。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author XiSun * @Date 2021/5/5 14:33 */public class TransformTest5_RichFunction &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; resultStream = dataStream.map(new MyMapper()); // 4.打印 resultStream.print(); // 5.执行任务 env.execute(); &#125; // 传统的Function不能获取上下文信息，只能处理当前数据，不能和其他数据交互 public static class MyMapper0 implements MapFunction&lt;SensorReading, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public Tuple2&lt;String, Integer&gt; map(SensorReading value) throws Exception &#123; return new Tuple2&lt;&gt;(value.getId(), value.getId().length()); &#125; &#125; // 实现自定义富函数类(RichMapFunction是一个abstract类) public static class MyMapper extends RichMapFunction&lt;SensorReading, Tuple2&lt;Integer, String&gt;&gt; &#123; @Override public Tuple2&lt;Integer, String&gt; map(SensorReading value) throws Exception &#123; // getRuntimeContext().getState(); return new Tuple2&lt;&gt;(getRuntimeContext().getIndexOfThisSubtask() + 1, value.getId()); &#125; @Override public void open(Configuration parameters) throws Exception &#123; // 初始化工作，一般是定义状态，或者建立数据库连接 System.out.println(&quot;open&quot;); &#125; @Override public void close() throws Exception &#123; // 一般是关闭连接和清空状态的收尾操作 System.out.println(&quot;close&quot;); &#125; &#125;&#125; 输出结果： 1234567891011121314151617181920SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.openopenopenopen2&gt; (2,sensor_7)2&gt; (2,sensor_1)3&gt; (3,sensor_10)1&gt; (1,sensor_6)1&gt; (1,sensor_1)closeclose4&gt; (4,sensor_1)4&gt; (4,sensor_1)closecloseProcess finished with exit code 0 由于设置了执行环境 env 的并行度为 4，所以有 4 个 slot 执行自定义的 RichFunction，输出 4 次 open 和 close。 数据重分区操作 在多并行度的情况下，Flink 对数据的分配方式有多种： 常用的分配方式有： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are broadcasted * to every parallel instance of the next operation. * * @return The DataStream with broadcast partitioning set. */public DataStream&lt;T&gt; broadcast() &#123; return setConnectionType(new BroadcastPartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are broadcasted * to every parallel instance of the next operation. In addition, it implicitly as many &#123;@link * org.apache.flink.api.common.state.BroadcastState broadcast states&#125; as the specified * descriptors which can be used to store the element of the stream. * * @param broadcastStateDescriptors the descriptors of the broadcast states to create. * @return A &#123;@link BroadcastStream&#125; which can be used in the &#123;@link #connect(BroadcastStream)&#125; * to create a &#123;@link BroadcastConnectedStream&#125; for further processing of the elements. */@PublicEvolvingpublic BroadcastStream&lt;T&gt; broadcast( final MapStateDescriptor&lt;?, ?&gt;... broadcastStateDescriptors) &#123; Preconditions.checkNotNull(broadcastStateDescriptors); final DataStream&lt;T&gt; broadcastStream = setConnectionType(new BroadcastPartitioner&lt;&gt;()); return new BroadcastStream&lt;&gt;(environment, broadcastStream, broadcastStateDescriptors);&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are shuffled * uniformly randomly to the next operation. * * @return The DataStream with shuffle partitioning set. */@PublicEvolvingpublic DataStream&lt;T&gt; shuffle() &#123; return setConnectionType(new ShufflePartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are forwarded to * the local subtask of the next operation. * * @return The DataStream with forward partitioning set. */public DataStream&lt;T&gt; forward() &#123; return setConnectionType(new ForwardPartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are distributed * evenly to instances of the next operation in a round-robin fashion. * * @return The DataStream with rebalance partitioning set. */public DataStream&lt;T&gt; rebalance() &#123; return setConnectionType(new RebalancePartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are distributed * evenly to a subset of instances of the next operation in a round-robin fashion. * * &lt;p&gt;The subset of downstream operations to which the upstream operation sends elements depends * on the degree of parallelism of both the upstream and downstream operation. For example, if * the upstream operation has parallelism 2 and the downstream operation has parallelism 4, then * one upstream operation would distribute elements to two downstream operations while the other * upstream operation would distribute to the other two downstream operations. If, on the other * hand, the downstream operation has parallelism 2 while the upstream operation has parallelism * 4 then two upstream operations will distribute to one downstream operation while the other * two upstream operations will distribute to the other downstream operations. * * &lt;p&gt;In cases where the different parallelisms are not multiples of each other one or several * downstream operations will have a differing number of inputs from upstream operations. * * @return The DataStream with rescale partitioning set. */@PublicEvolvingpublic DataStream&lt;T&gt; rescale() &#123; return setConnectionType(new RescalePartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output values all go to the first * instance of the next processing operator. Use this setting with care since it might cause a * serious performance bottleneck in the application. * * @return The DataStream with shuffle partitioning set. */@PublicEvolvingpublic DataStream&lt;T&gt; global() &#123; return setConnectionType(new GlobalPartitioner&lt;T&gt;());&#125; 默认情况下，使用的分配方式是 rebalance 策略，即轮询。 DataStream 类中，partitionCustom(...) 用于自定义重分区。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637/** * @author XiSun * @Date 2021/5/5 17:39 */public class TransformTest6_Partition &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;);// SingleOutputStreamOperator // 4.SingleOutputStreamOperator多并行度时，默认分配方式是rebalance，即轮询方式分配 dataStream.print(&quot;rebalance&quot;); // 5.shuffle (并非批处理中的获取一批后才打乱，这里每次获取到直接打乱且分区) DataStream&lt;String&gt; shuffleStream = inputStream.shuffle(); shuffleStream.print(&quot;shuffle&quot;); // 6.keyBy (按Hash，然后取模) dataStream.keyBy(SensorReading::getId).print(&quot;keyBy&quot;); // 7.global (直接发送给第一个分区，少数特殊情况才用) dataStream.global().print(&quot;global&quot;); // 8.执行任务 env.execute(); &#125;&#125; 输出结果： 123456789101112131415161718192021222324252627282930313233SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.shuffle:2&gt; sensor_1,1547718199,35.8shuffle:3&gt; sensor_1,1547718207,36.3shuffle:3&gt; sensor_1,1547718212,37.1rebalance:2&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;shuffle:4&gt; sensor_6,1547718201,15.4shuffle:1&gt; sensor_7,1547718202,6.7shuffle:4&gt; sensor_10,1547718205,38.1rebalance:2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;rebalance:3&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;rebalance:4&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;rebalance:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;rebalance:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;shuffle:4&gt; sensor_1,1547718209,32.8rebalance:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;keyBy:2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;keyBy:4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;Process finished with exit code 0 Sink Flink 没有类似于 Spark 中的 foreach 方法，让用户进行迭代的操作。所有对外的输出操作都要利用 Sink 完成，最后通过类似如下的方式，完成整个任务的最终输出操作： 1stream.addSink(new MySink(xxxx)) Flink 官方提供了一部分框架的 Sink。除此以外，需要用户自定义实现 Sink。 地址：https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/datastream/overview/ Kafka 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * @author XiSun * @Date 2021/5/6 12:18 */public class SinkTest1_Kafka &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.创建Kafka消费者 Properties consumerProperties = new Properties(); consumerProperties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); consumerProperties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;); consumerProperties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;); consumerProperties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); consumerProperties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(&quot;sensor&quot;, new SimpleStringSchema(), consumerProperties); // 3.从Kafka读取数据 DataStream&lt;String&gt; inputStream = env.addSource(consumer); // 4.序列化从Kafka中读取的数据 DataStream&lt;String&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])).toString(); &#125;); // 5.创建Kafka生产者 Properties producerProperties = new Properties(); producerProperties.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); producerProperties.put(&quot;group.id&quot;, &quot;producer-group&quot;); producerProperties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); producerProperties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.ByteArraySerializer&quot;); FlinkKafkaProducer&lt;String&gt; producer = new FlinkKafkaProducer&lt;&gt;(&quot;sinkTest&quot;, new SimpleStringSchema(), producerProperties); // 6.将数据写入Kafka dataStream.addSink(producer); // 7.执行任务 env.execute(); &#125; Redis 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * @author XiSun * @Date 2021/5/6 12:35 */public class SinkTest2_Redis &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.定义jedis连接配置(我这里连接的是docker的redis) FlinkJedisPoolConfig redisConfig = new FlinkJedisPoolConfig.Builder() .setHost(&quot;localhost&quot;) .setPort(6379) .setPassword(&quot;123456&quot;) .setDatabase(0) .build(); // 6.将数据写入Redis dataStream.addSink(new RedisSink&lt;&gt;(redisConfig, new MyRedisMapper())); // 7.执行任务 env.execute(); &#125; // 5.自定义RedisMapper public static class MyRedisMapper implements RedisMapper&lt;SensorReading&gt; &#123; // 定义保存数据到Redis的命令，存成哈希表：hset sensor_temp id temperature @Override public RedisCommandDescription getCommandDescription() &#123; return new RedisCommandDescription(RedisCommand.HSET, &quot;sensor_temp&quot;); &#125; @Override public String getKeyFromData(SensorReading sensorReading) &#123; return sensorReading.getId(); &#125; @Override public String getValueFromData(SensorReading sensorReading) &#123; return sensorReading.getTemperature().toString(); &#125; &#125;&#125; 查看 Redis 数据： 123456789localhost:0&gt;hgetall sensor_temp1) &quot;sensor_1&quot;2) &quot;37.1&quot;3) &quot;sensor_6&quot;4) &quot;15.4&quot;5) &quot;sensor_7&quot;6) &quot;6.7&quot;7) &quot;sensor_10&quot;8) &quot;38.1&quot; Elasticsearch 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch7_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @author XiSun * @Date 2021/5/6 12:50 */public class SinkTest3_Es &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.定义es的连接配置 List&lt;HttpHost&gt; httpHosts = new ArrayList&lt;&gt;();// org.apache.http.HttpHost; httpHosts.add(new HttpHost(&quot;localhost&quot;, 9200)); // 6.将数据写入es dataStream.addSink(new ElasticsearchSink.Builder&lt;&gt;(httpHosts, new MyEsSinkFunction()).build()); // 7.执行任务 env.execute(); &#125; // 5.实现自定义的ES写入操作 public static class MyEsSinkFunction implements ElasticsearchSinkFunction&lt;SensorReading&gt; &#123; @Override public void open() throws Exception &#123; &#125; @Override public void close() throws Exception &#123; &#125; @Override public void process(SensorReading sensorReading, RuntimeContext runtimeContext, RequestIndexer requestIndexer) &#123; // 定义写入的数据source HashMap&lt;String, String&gt; dataSource = new HashMap&lt;&gt;(5); dataSource.put(&quot;id&quot;, sensorReading.getId()); dataSource.put(&quot;temp&quot;, sensorReading.getTemperature().toString()); dataSource.put(&quot;ts&quot;, sensorReading.getTimestamp().toString()); // 创建请求，作为向es发起的写入命令(ES7统一type就是_doc，不再允许指定type) IndexRequest indexRequest = Requests.indexRequest() .index(&quot;sensor&quot;) .source(dataSource); // 用index发送请求 requestIndexer.add(indexRequest); &#125; &#125;&#125; 查看 ElasticSearch 数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697$ curl &quot;localhost:9200/sensor/_search?pretty&quot;&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 7, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;jciyWXcBiXrGJa12kSQt&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;35.8&quot;, &quot;id&quot; : &quot;sensor_1&quot;, &quot;ts&quot; : &quot;1547718199&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;jsiyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;15.4&quot;, &quot;id&quot; : &quot;sensor_6&quot;, &quot;ts&quot; : &quot;1547718201&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;j8iyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;6.7&quot;, &quot;id&quot; : &quot;sensor_7&quot;, &quot;ts&quot; : &quot;1547718202&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;kMiyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;38.1&quot;, &quot;id&quot; : &quot;sensor_10&quot;, &quot;ts&quot; : &quot;1547718205&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;kciyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;36.3&quot;, &quot;id&quot; : &quot;sensor_1&quot;, &quot;ts&quot; : &quot;1547718207&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;ksiyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;32.8&quot;, &quot;id&quot; : &quot;sensor_1&quot;, &quot;ts&quot; : &quot;1547718209&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;k8iyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;37.1&quot;, &quot;id&quot; : &quot;sensor_1&quot;, &quot;ts&quot; : &quot;1547718212&quot; &#125; &#125; ] &#125;&#125; JDBC 自定义 Sink 以 MySQL 为例，添加 MySQL 连接依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.19&lt;/version&gt;&lt;/dependency&gt; 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * @author XiSun * @Date 2021/5/6 13:02 */public class SinkTest4_Jdbc &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 使用之前编写的随机变动温度的SourceFunction来生成数据，数据一直生成 /*DataStream&lt;SensorReading&gt; dataStream = env.addSource(new SourceTest4_UDF.MySensorSource());*/ // 4.将数据写入MySQL dataStream.addSink(new MyJdbcSink()); // 6.执行任务 env.execute(); &#125; // 5.实现自定义的SinkFunction public static class MyJdbcSink extends RichSinkFunction&lt;SensorReading&gt; &#123; // 声明连接和预编译语句 Connection connection = null; PreparedStatement insertStmt = null; PreparedStatement updateStmt = null; @Override public void open(Configuration parameters) throws Exception &#123; // 创建连接 connection = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/flink_test?useUnicode=true&amp;&quot; + &quot;serverTimezone=Asia/Shanghai&amp;characterEncoding=UTF-8&amp;useSSL=false&quot;, &quot;root&quot;, &quot;example&quot;); // 创建预编译语句，有占位符，可传入参数 insertStmt = connection.prepareStatement(&quot;insert into sensor_temp (id, temp) values (?, ?)&quot;); updateStmt = connection.prepareStatement(&quot;update sensor_temp set temp = ? where id = ?&quot;); &#125; // 每来一条数据，调用连接，执行sql @Override public void invoke(SensorReading sensorReading, Context context) throws Exception &#123; // 直接执行更新语句，如果没有更新那么就插入 updateStmt.setDouble(1, sensorReading.getTemperature()); updateStmt.setString(2, sensorReading.getId()); updateStmt.execute(); if (updateStmt.getUpdateCount() == 0) &#123; insertStmt.setString(1, sensorReading.getId()); insertStmt.setDouble(2, sensorReading.getTemperature()); insertStmt.execute(); &#125; &#125; @Override public void close() throws Exception &#123; insertStmt.close(); updateStmt.close(); connection.close(); &#125; &#125;&#125; 查看 MySQL 数据： 123456789101112131415161718192021222324252627282930313233mysql&gt; SELECT * FROM sensor_temp;+-----------+--------------------+| id | temp |+-----------+--------------------+| sensor_3 | 20.489172407885917 || sensor_10 | 73.01289164711463 || sensor_4 | 43.402500895809744 || sensor_1 | 6.894772325662007 || sensor_2 | 101.79309911751122 || sensor_7 | 63.070612021580324 || sensor_8 | 63.82606628090501 || sensor_5 | 57.67115738487047 || sensor_6 | 50.84442627975055 || sensor_9 | 52.58400793021675 |+-----------+--------------------+10 rows in set (0.00 sec)mysql&gt; SELECT * FROM sensor_temp;+-----------+--------------------+| id | temp |+-----------+--------------------+| sensor_3 | 19.498209543035923 || sensor_10 | 71.92981963197121 || sensor_4 | 43.566017489470426 || sensor_1 | 6.378208186786803 || sensor_2 | 101.71010087830145 || sensor_7 | 62.11402602179431 || sensor_8 | 64.33196455020062 || sensor_5 | 56.39071692662006 || sensor_6 | 48.952784757264894 || sensor_9 | 52.078086096436685 |+-----------+--------------------+10 rows in set (0.00 sec) Flink 的 WindowWindow 概述 Streaming 流式计算是一种被设计用于处理无限数据集的数据处理引擎，无限数据集是指一种不断增长的本质上无限的数据集，而 Window 是一种切割无限数据为有限块进行处理的手段。 Window 是无限数据流处理的核心，Window 将一个无限的 stream 拆分成有限大小的 “buckets” 桶，我们可以在这些桶上做计算操作。 Window 类型时间窗口 (Time Window) 按照时间生成 Window。 滚动时间窗口 (Tumbling Windows) 滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口中，滚动窗口有一个固定的大小，并且不会出现重叠。 原理：依据固定的窗口长度对数据进行切片。 特点：时间对齐，窗口长度固定，没有重叠。 适用场景：适合做 BI 统计等 (做每个时间段的聚合计算)。 例如，如果指定了一个 5 分钟大小的滚动窗口，窗口的创建如下图所示： 滑动时间窗口 (Sliding Windows) 滑动窗口是固定窗口的更广义的一种形式。滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率。因此，滑动窗口如果滑动参数小于窗口大小的话，窗口是可以重叠的，在这种情况下元素会被分配到多个窗口中。 原理：滑动窗口由固定的窗口长度和滑动间隔组成。 特点：时间对齐，窗口长度固定，可以有重叠。 适用场景：对最近一个时间段内的统计 (比如求某接口最近 5 min 的失败率来决定是否要报警)。 例如，你有 10 分钟的窗口和 5 分钟的滑动，那么每个窗口中 5 分钟的窗口里包含着上个 10 分钟产生的数据，如下图所示： 会话窗口 (Session Windows) session 窗口分配器通过 session 活动来对元素进行分组，session 窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，当它在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关闭。一个 session 窗口通过一个 session 间隔来配置，这个 session 间隔定义了非活跃周期的长度，当这个非活跃周期产生，那么当前的 session 将关闭并且后续的元素将被分配到新的 session 窗口中去。 由一系列事件组合一个指定时间长度的 timeout 间隙组成，类似于 web 应用的 session，也就是一段时间没有接收到新数据就会生成新的窗口。 特点：时间无对齐。 计数窗口 (Count Window) 按照指定的数据条数生成一个 Window，与时间无关。 滚动计数窗口 滑动计数窗口 Window API概述 Flink 使用 window() 来定义一个窗口，然后基于这个 Window 去做一些聚合或者其他处理操作。 window() 是最基础的定义窗口的方法。 window() 必须在 keyBy 之后才能使用。 DataStream 的 windowAll() 类似数据传输分区的 global 操作，这个操作是 non-parallel 的 (并行度强行为 1)，所有的数据都会被传递到同一个算子 operator 上，官方建议如果非必要就不要用这个 API。 window() 之后需要有一个窗口函数。 一个完整的窗口操作参考如下： 123456DataStream&lt;Tuple2&lt;String, Double&gt;&gt; minTempPerWindowStream = datastream ---&gt; 数据流 .map(new MyMapper()) .keyBy(data -&gt; data.f0) ---&gt; 分组 .timeWindow(Time.seconds(15)) ---&gt; 开窗 .minBy(1); ---&gt; 窗口函数 window() 需要接收一个输入参数：WindowAssigner (窗口分配器)。 1234567891011121314151617/** * Windows this data stream to a &#123;@code WindowedStream&#125;, which evaluates windows over a key * grouped stream. Elements are put into windows by a &#123;@link WindowAssigner&#125;. The grouping of * elements is done both by key and by window. * * &lt;p&gt;A &#123;@link org.apache.flink.streaming.api.windowing.triggers.Trigger&#125; can be defined to * specify when windows are evaluated. However, &#123;@code WindowAssigners&#125; have a default &#123;@code * Trigger&#125; that is used if a &#123;@code Trigger&#125; is not specified. * * @param assigner The &#123;@code WindowAssigner&#125; that assigns elements to windows. * @return The trigger windows data stream. */@PublicEvolvingpublic &lt;W extends Window&gt; WindowedStream&lt;T, KEY, W&gt; window( WindowAssigner&lt;? super T, W&gt; assigner) &#123; return new WindowedStream&lt;&gt;(this, assigner);&#125; WindowAssigner 是一个抽象类，负责将每条输入的数据分发到正确的 Window 中。 WindowAssigner 的实现类位于 org.apache.flink.streaming.api.windowing.assigners 包下： 说明：这些实现类的构造方法多是 protected 或 privated 的，需要通过类中的静态方法如 of() 或 withGap() 来获取一个实例。 1234dataStream .keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) .minBy(1); 归纳起来，Flink 提供了四种类型通用的 WindowAssigner： 滚动窗口 (tumbling window) 滑动窗口 (sliding window) 会话窗口 (session window) 全局窗口 (global window) 除了 .window()，Flink 提供了更加简单的 .timeWindow() 和 .countWindow() 方法，用于定义时间窗口和计数窗口。 创建不同类型的窗口 Flink 创建窗口的方法有多种，实际使用时，按需求创建。 滚动时间窗口 (tumbling time window)：当时间达到窗口大小时，就会触发窗口的执行。 .window(TumblingProcessingTimeWindows.of(Time.seconds(10))) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .timeWindow(Time.seconds(15))，Flink 1.12.1 版本已弃用。 123456789101112131415161718192021/** * Windows this &#123;@code KeyedStream&#125; into tumbling time windows. * * &lt;p&gt;This is a shortcut for either &#123;@code .window(TumblingEventTimeWindows.of(size))&#125; or &#123;@code * .window(TumblingProcessingTimeWindows.of(size))&#125; depending on the time characteristic set * using &#123;@link * org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#setStreamTimeCharacteristic(org.apache.flink.streaming.api.TimeCharacteristic)&#125; * * @param size The size of the window. * @deprecated Please use &#123;@link #window(WindowAssigner)&#125; with either &#123;@link * TumblingEventTimeWindows&#125; or &#123;@link TumblingProcessingTimeWindows&#125;. For more information, * see the deprecation notice on &#123;@link TimeCharacteristic&#125; */@Deprecatedpublic WindowedStream&lt;T, KEY, TimeWindow&gt; timeWindow(Time size) &#123; if (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123; return window(TumblingProcessingTimeWindows.of(size)); &#125; else &#123; return window(TumblingEventTimeWindows.of(size)); &#125;&#125; 滑动时间窗口 (sliding time window)：两个参数，前者是 window_size，后者是 sliding_size。每隔 sliding_size 计算输出结果一次，每一次计算的 window 范围是 window_size 内的所有元素。 .window(TumblingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .window(TumblingEventTimeWindows.of(Time.seconds(10), Time.seconds(5s))) .timeWindow(Time.seconds(15), Time.seconds(5))，Flink 1.12.1 版本已弃用。 123456789101112131415161718192021/** * Windows this &#123;@code KeyedStream&#125; into sliding time windows. * * &lt;p&gt;This is a shortcut for either &#123;@code .window(SlidingEventTimeWindows.of(size, slide))&#125; or * &#123;@code .window(SlidingProcessingTimeWindows.of(size, slide))&#125; depending on the time * characteristic set using &#123;@link * org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#setStreamTimeCharacteristic(org.apache.flink.streaming.api.TimeCharacteristic)&#125; * * @param size The size of the window. * @deprecated Please use &#123;@link #window(WindowAssigner)&#125; with either &#123;@link * SlidingEventTimeWindows&#125; or &#123;@link SlidingProcessingTimeWindows&#125;. For more information, * see the deprecation notice on &#123;@link TimeCharacteristic&#125; */@Deprecatedpublic WindowedStream&lt;T, KEY, TimeWindow&gt; timeWindow(Time size, Time slide) &#123; if (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123; return window(SlidingProcessingTimeWindows.of(size, slide)); &#125; else &#123; return window(SlidingEventTimeWindows.of(size, slide)); &#125;&#125; 会话窗口 (session window) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) 滚动计数窗口 (tumbling count window)：当元素数量达到窗口大小时，就会触发窗口的执行。 .countWindow(5) 滑动计数窗口 (sliding count window)：两个参数，前者是 window_size，后者是 sliding_size。每隔 sliding_size 计算输出结果一次，每一次计算的 window 范围是 window_size 内的所有元素。 .countWindow(10, 2) 窗口函数 (window function) window function 定义了要对窗口中收集的数据做的计算操作，主要分为两类。 增量聚合函数 (incremental aggregation functions) 每条数据到来就进行计算，保持一个简单的状态。(来一条处理一条，但是不输出，到窗口临界位置才输出) 典型的增量聚合函数有 ReduceFunction，AggregateFunction。 ReduceFunction： 源码： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Base interface for Reduce functions. Reduce functions combine groups of elements to a single * value, by taking always two elements and combining them into one. Reduce functions may be used on * entire data sets, or on grouped data sets. In the latter case, each group is reduced * individually. * * &lt;p&gt;For a reduce functions that work on an entire group at the same time (such as the * MapReduce/Hadoop-style reduce), see &#123;@link GroupReduceFunction&#125;. In the general case, * ReduceFunctions are considered faster, because they allow the system to use more efficient * execution strategies. * * &lt;p&gt;The basic syntax for using a grouped ReduceFunction is as follows: * * &lt;pre&gt;&#123;@code * DataSet&lt;X&gt; input = ...; * * DataSet&lt;X&gt; result = input.groupBy(&lt;key-definition&gt;).reduce(new MyReduceFunction()); * &#125;&lt;/pre&gt; * * &lt;p&gt;Like all functions, the ReduceFunction needs to be serializable, as defined in &#123;@link * java.io.Serializable&#125;. * * @param &lt;T&gt; Type of the elements that this function processes. */@Public@FunctionalInterfacepublic interface ReduceFunction&lt;T&gt; extends Function, Serializable &#123; /** * The core method of ReduceFunction, combining two values into one value of the same type. The * reduce function is consecutively applied to all values of a group until only a single value * remains. * * @param value1 The first value to combine. * @param value2 The second value to combine. * @return The combined value of both input values. * @throws Exception This method may throw exceptions. Throwing an exception will cause the * operation to fail and may trigger recovery. */ T reduce(T value1, T value2) throws Exception;&#125; 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839/** * @author XiSun * @Date 2021/5/7 12:50 */public class WindowTest1_TimeWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，时间窗口，增量聚合函数 DataStream&lt;SensorReading&gt; resultStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) // 归约 .reduce(new ReduceFunction&lt;SensorReading&gt;() &#123; @Override public SensorReading reduce(SensorReading value1, SensorReading value2) throws Exception &#123; return new SensorReading(value1.getId(), value2.getTimestamp(), Math.max(value1.getTemperature(), value2.getTemperature())); &#125; &#125;); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125;&#125; 输出结果： 12345xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718212,37.1sensor_1,1547718199,35.8sensor_1,1547718209,32.8... 12345log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=37.1&#125;... AggregateFunction： 源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137/** * The &#123;@code AggregateFunction&#125; is a flexible aggregation function, characterized by the following * features: * * &lt;ul&gt; * &lt;li&gt;The aggregates may use different types for input values, intermediate aggregates, and * result type, to support a wide range of aggregation types. * &lt;li&gt;Support for distributive aggregations: Different intermediate aggregates can be merged * together, to allow for pre-aggregation/final-aggregation optimizations. * &lt;/ul&gt; * * &lt;p&gt;The &#123;@code AggregateFunction&#125;&#x27;s intermediate aggregate (in-progress aggregation state) is * called the &lt;i&gt;accumulator&lt;/i&gt;. Values are added to the accumulator, and final aggregates are * obtained by finalizing the accumulator state. This supports aggregation functions where the * intermediate state needs to be different than the aggregated values and the final result type, * such as for example &lt;i&gt;average&lt;/i&gt; (which typically keeps a count and sum). Merging intermediate * aggregates (partial aggregates) means merging the accumulators. * * &lt;p&gt;The AggregationFunction itself is stateless. To allow a single AggregationFunction instance to * maintain multiple aggregates (such as one aggregate per key), the AggregationFunction creates a * new accumulator whenever a new aggregation is started. * * &lt;p&gt;Aggregation functions must be &#123;@link Serializable&#125; because they are sent around between * distributed processes during distributed execution. * * &lt;h1&gt;Example: Average and Weighted Average&lt;/h1&gt; * * &lt;pre&gt;&#123;@code * // the accumulator, which holds the state of the in-flight aggregate * public class AverageAccumulator &#123; * long count; * long sum; * &#125; * * // implementation of an aggregation function for an &#x27;average&#x27; * public class Average implements AggregateFunction&lt;Integer, AverageAccumulator, Double&gt; &#123; * * public AverageAccumulator createAccumulator() &#123; * return new AverageAccumulator(); * &#125; * * public AverageAccumulator merge(AverageAccumulator a, AverageAccumulator b) &#123; * a.count += b.count; * a.sum += b.sum; * return a; * &#125; * * public AverageAccumulator add(Integer value, AverageAccumulator acc) &#123; * acc.sum += value; * acc.count++; * return acc; * &#125; * * public Double getResult(AverageAccumulator acc) &#123; * return acc.sum / (double) acc.count; * &#125; * &#125; * * // implementation of a weighted average * // this reuses the same accumulator type as the aggregate function for &#x27;average&#x27; * public class WeightedAverage implements AggregateFunction&lt;Datum, AverageAccumulator, Double&gt; &#123; * * public AverageAccumulator createAccumulator() &#123; * return new AverageAccumulator(); * &#125; * * public AverageAccumulator merge(AverageAccumulator a, AverageAccumulator b) &#123; * a.count += b.count; * a.sum += b.sum; * return a; * &#125; * * public AverageAccumulator add(Datum value, AverageAccumulator acc) &#123; * acc.count += value.getWeight(); * acc.sum += value.getValue(); * return acc; * &#125; * * public Double getResult(AverageAccumulator acc) &#123; * return acc.sum / (double) acc.count; * &#125; * &#125; * &#125;&lt;/pre&gt; * * @param &lt;IN&gt; The type of the values that are aggregated (input values) ---&gt; 聚合值的类型(输入值) * @param &lt;ACC&gt; The type of the accumulator (intermediate aggregate state). ---&gt; 累加器的类型(中间聚合状态) * @param &lt;OUT&gt; The type of the aggregated result ---&gt; 聚合结果的类型 */@PublicEvolvingpublic interface AggregateFunction&lt;IN, ACC, OUT&gt; extends Function, Serializable &#123; /** * Creates a new accumulator, starting a new aggregate. * * &lt;p&gt;The new accumulator is typically meaningless unless a value is added via &#123;@link * #add(Object, Object)&#125;. * * &lt;p&gt;The accumulator is the state of a running aggregation. When a program has multiple * aggregates in progress (such as per key and window), the state (per key and window) is the * size of the accumulator. * * @return A new accumulator, corresponding to an empty aggregate. */ ACC createAccumulator(); /** * Adds the given input value to the given accumulator, returning the new accumulator value. * * &lt;p&gt;For efficiency, the input accumulator may be modified and returned. * * @param value The value to add * @param accumulator The accumulator to add the value to * @return The accumulator with the updated state */ ACC add(IN value, ACC accumulator); /** * Gets the result of the aggregation from the accumulator. * * @param accumulator The accumulator of the aggregation * @return The final aggregation result. */ OUT getResult(ACC accumulator); /** * Merges two accumulators, returning an accumulator with the merged state. * * &lt;p&gt;This function may reuse any of the given accumulators as the target for the merge and * return that. The assumption is that the given accumulators will not be used any more after * having been passed to this function. * * @param a An accumulator to merge * @param b Another accumulator to merge * @return The accumulator with the merged state */ ACC merge(ACC a, ACC b);&#125; 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @author XiSun * @Date 2021/5/7 12:50 */public class WindowTest1_TimeWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，时间窗口，增量聚合函数 DataStream&lt;Integer&gt; resultStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) // 统计每个分组下数据的个数，中间聚合状态的类型和最终输出的类型是一致的 .aggregate(new AggregateFunction&lt;SensorReading, Integer, Integer&gt;() &#123; // 创建一个累加器 @Override public Integer createAccumulator() &#123; // 初始值，从0开始 return 0; &#125; // 来一条数据后，该怎么累加 @Override public Integer add(SensorReading value, Integer accumulator) &#123; // 累加器基础上+1 return accumulator + 1; &#125; // 返回最终的处理结果 @Override public Integer getResult(Integer accumulator) &#123; // 就是返回累加器 return accumulator; &#125; // merge方法一般在session window中使用，可能会存在一些合并的操作 // 不存在分区合并，因为当前处理的都是keyBy之后的 @Override public Integer merge(Integer a, Integer b) &#123; // 为防止意外，将两个状态a和b相加 return a + b; &#125; &#125;); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125;&#125; 输出结果： 1234567xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718199,35.8sensor_1,1547718207,36.3sensor_1,1547718209,32.8sensor_1,1547718212,37.1sensor_10,1547718205,38.1... 1234567log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; 22&gt; 13&gt; 2... 全窗口函数 (full window functions) 先把窗口所有数据收集起来，等到计算的时候再遍历所有数据。(来一条存放一条，到窗口临界位置才遍历且计算、输出) 典型的全窗口函数有 ProcessWindowFunction，WindowFunction。 ProcessWindowFunction： 源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * Base abstract class for functions that are evaluated over keyed (grouped) windows using a context * for retrieving extra information. * * @param &lt;IN&gt; The type of the input value. * @param &lt;OUT&gt; The type of the output value. * @param &lt;KEY&gt; The type of the key. * @param &lt;W&gt; The type of &#123;@code Window&#125; that this window function can be applied on. */@PublicEvolvingpublic abstract class ProcessWindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends AbstractRichFunction &#123; private static final long serialVersionUID = 1L; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public abstract void process( KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out) throws Exception; /** * Deletes any state in the &#123;@code Context&#125; when the Window expires (the watermark passes its * &#123;@code maxTimestamp&#125; + &#123;@code allowedLateness&#125;). * * @param context The context to which the window is being evaluated * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public void clear(Context context) throws Exception &#123;&#125; /** The context holding window metadata. */ public abstract class Context implements java.io.Serializable &#123; /** Returns the window that is being evaluated. */ public abstract W window(); /** Returns the current processing time. */ public abstract long currentProcessingTime(); /** Returns the current event-time watermark. */ public abstract long currentWatermark(); /** * State accessor for per-key and per-window state. * * &lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt;If you use per-window state you have to ensure that you clean it up by * implementing &#123;@link ProcessWindowFunction#clear(Context)&#125;. */ public abstract KeyedStateStore windowState(); /** State accessor for per-key global state. */ public abstract KeyedStateStore globalState(); /** * Emits a record to the side output identified by the &#123;@link OutputTag&#125;. * * @param outputTag the &#123;@code OutputTag&#125; that identifies the side output to emit to. * @param value The record to emit. */ public abstract &lt;X&gt; void output(OutputTag&lt;X&gt; outputTag, X value); &#125;&#125; 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author XiSun * @Date 2021/5/7 12:50 */public class WindowTest1_TimeWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，时间窗口，全窗口函数 DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; resultStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) // 统计每个分组下数据的个数 .process(new ProcessWindowFunction&lt;SensorReading, Tuple3&lt;String, Long, Integer&gt;, String, TimeWindow&gt;() &#123; @Override public void process(String key, Context context, Iterable&lt;SensorReading&gt; elements, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out) throws Exception &#123; // 把elements转换为List，然后其长度就是当前分组下数据的个数 Integer count = IteratorUtils.toList(elements.iterator()).size(); // 输出一个三元组：key，窗口结束时间，数据个数 out.collect(new Tuple3&lt;&gt;(key, context.window().getEnd(), count)); &#125; &#125;); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125;&#125; 输出结果： 123456xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718199,35.8sensor_7,1547718202,6.7sensor_1,1547718209,32.8sensor_1,1547718212,37.1 ---&gt; 此数据的输入与前面相同的key的时间间隔，超出15s，在下一个窗口处理... 1234567log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.4&gt; (sensor_7,1620451335000,1)3&gt; (sensor_1,1620451335000,2)3&gt; (sensor_1,1620451365000,1)... WindowFunction： 源码： 12345678910111213141516171819202122/** * Base interface for functions that are evaluated over keyed (grouped) windows. * * @param &lt;IN&gt; The type of the input value. * @param &lt;OUT&gt; The type of the output value. * @param &lt;KEY&gt; The type of the key. ---&gt; 分组的key * @param &lt;W&gt; The type of &#123;@code Window&#125; that this window function can be applied on. */@Publicpublic interface WindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends Function, Serializable &#123; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param window The window that is being evaluated. * @param input The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ void apply(KEY key, W window, Iterable&lt;IN&gt; input, Collector&lt;OUT&gt; out) throws Exception;&#125; 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @author XiSun * @Date 2021/5/7 12:50 */public class WindowTest1_TimeWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，时间窗口，全窗口函数 DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; resultStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) // 统计每个分组下数据的个数 .apply(new WindowFunction&lt;SensorReading, Tuple3&lt;String, Long, Integer&gt;, String, TimeWindow&gt;() &#123; /* input：当前输入的所有的数据 out：当前输出的数据 */ @Override public void apply(String key, TimeWindow window, Iterable&lt;SensorReading&gt; input, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out) throws Exception &#123; // 把input转换为List，然后其长度就是当前分组下数据的个数 Integer count = IteratorUtils.toList(input.iterator()).size(); // 输出一个三元组：key，窗口结束时间，数据个数 out.collect(new Tuple3&lt;&gt;(key, window.getEnd(), count)); &#125; &#125;); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125;&#125; 输出结果： 123456xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718199,35.8sensor_7,1547718202,6.7sensor_1,1547718209,32.8sensor_1,1547718212,37.1 ---&gt; 此数据的输入与前面相同的key的时间间隔，超出15s，在下一个窗口处理... 1234567log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; (sensor_1,1620450990000,2)4&gt; (sensor_7,1620450990000,1)3&gt; (sensor_1,1620451095000,1)... 前面的例子是以时间窗口写的，下面以计数窗口为例。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author XiSun * @Date 2021/5/8 13:21 */public class WindowTest2_CountWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，技术窗口，增量聚合函数 DataStream&lt;Double&gt; resultStream = dataStream.keyBy(SensorReading::getId) // 4个数开一个窗口，隔两个数滑动一次 .countWindow(4, 2) // 计算窗口内数据温度的平均值 .aggregate(new MyAvgTemp()); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125; public static class MyAvgTemp implements AggregateFunction&lt;SensorReading, Tuple2&lt;Double, Integer&gt;, Double&gt; &#123; @Override public Tuple2&lt;Double, Integer&gt; createAccumulator() &#123; return new Tuple2&lt;&gt;(0.0, 0); &#125; @Override public Tuple2&lt;Double, Integer&gt; add(SensorReading value, Tuple2&lt;Double, Integer&gt; accumulator) &#123; // 每来一条数据，把温度值加到二元组的第一个元素上，二元组第二个元素自增1 return new Tuple2&lt;&gt;(accumulator.f0 + value.getTemperature(), accumulator.f1 + 1); &#125; @Override public Double getResult(Tuple2&lt;Double, Integer&gt; accumulator) &#123; // 返回所有数据温度的平均值 return accumulator.f0 / accumulator.f1; &#125; @Override public Tuple2&lt;Double, Integer&gt; merge(Tuple2&lt;Double, Integer&gt; a, Tuple2&lt;Double, Integer&gt; b) &#123; return new Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1); &#125; &#125;&#125; 输出结果： 123456789101112xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718199,1sensor_1,1547718199,2sensor_1,1547718199,3sensor_1,1547718199,4sensor_1,1547718199,5sensor_1,1547718199,6sensor_1,1547718199,7sensor_1,1547718199,8sensor_1,1547718199,9sensor_1,1547718199,10... 123456789log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; 1.5 ---&gt; (1+2)/23&gt; 2.5 ---&gt; (1+2+3+4)/43&gt; 4.5 ---&gt; (3+4+5+6)/43&gt; 6.5 ---&gt; (5+6+7+8)/43&gt; 8.5 ---&gt; (7+8+9+10)/4... 滑动的距离是 2，因此前两个数计算一次平均值，后两个数来时，与前面两个数组成一个完整窗口 4 个数，计算一次平均值，后面都是 4 个数计算一次平均值。 其他可选 API .trigger()：触发器，定义 window 什么时候关闭，触发计算并输出结果。一般不使用。 .evictor()：移除器，定义移除某些数据的逻辑。一般不使用。 .allowedLateness()：允许处理迟到的数据。 .sideOutputLateData()：将迟到的数据放入侧输出流。 .getSideOutput()：获取侧输出流。 实例： 12345678910111213OutputTag&lt;SensorReading&gt; outputTag = new OutputTag&lt;SensorReading&gt;(&quot;late&quot;) &#123;&#125;;SingleOutputStreamOperator&lt;SensorReading&gt; sumStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15)))// .trigger()// .evictor() // 允许1分钟内的迟到数据&lt;=比如数据产生时间在窗口范围内，但是要处理的时候已经超过窗口时间了 .allowedLateness(Time.minutes(1)) // 侧输出流，迟到超过1分钟的数据，收集于此 .sideOutputLateData(outputTag) .sum(&quot;temperature&quot;);sumStream.getSideOutput(outputTag).print(&quot;late&quot;); Window API 总览 Flink 的时间语义和 WartermarkFlink 中的时间语义 Event Time：事件创建的时间。 Event Time 是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事件时间戳。 Ingestion Time：数据进入Flink 的时间。 Processing Time：执行操作算子的本地系统时间，与机器相关。 哪种时间语义更重要 不同的时间语义有不同的应用场合。 我们往往更关心事件时间 (Event Time)。 这里假设玩游戏，两分钟内如果过 5 关就有奖励。用户坐地铁玩游戏，进入隧道前已经过 3 关，在隧道中又过了 5 关。但是信号不好，后 5 关通关的信息，等到出隧道的时候 (8:23:20) 才正式到达服务器。 在这个应用场合下，如果为了用户体验，则不应该使用 Processing Time，而是应该按照 Event Time 处理信息，保证用户获得游戏奖励。 Event Time 可以从日志数据的时间戳 (timestamp) 中提取： 2017-11-02 18:37:15.624 INFO Fail over to rm 在代码中设置 Event Time 在 Flink 的流式处理中，绝大部分的业务都会使用 Event Time，一般只在 Event Time 无法使用时，才会被迫使用 Processing Time 或者 Ingestion Time。 如果要使用 Event Time，那么需要引入 Event Time 的时间属性，引入方式如下所示： 123StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 从调用时刻开始给env创建的每一个stream追加时间特征env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); WatermarkWatermark 的基本概念 我们知道，流处理从事件产生，到流经 Source，再到 Operator，中间是有一个过程和时间的，虽然大部分情况下，流到 Operator 的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、分布式等原因，导致乱序的产生，所谓乱序，就是指 Flink 接收到的事件的先后顺序不是严格按照事件的 Event Time 顺序排列的。那么此时出现一个问题，一旦出现乱序，如果只根据 Event Time 决定 Window 的运行，我们不能明确数据是否全部到位，但又不能无限期的等下去，此时必须要有个机制来保证一个特定的时间后，必须触发 Window 去进行计算了，这个特别的机制，就是 Watermark。 当 Flink 以 Event Time 模式处理数据流时，它会根据数据里的时间戳来处理基于时间的算子。 由于网络、分布式等原因，会导致乱序数据的产生。 乱序数据会让窗口计算不准确。 遇到一个时间戳达到了窗口关闭时间，不应该立刻触发窗口计算，而是等待一段时间，等迟到的数据来了再关闭窗口。 Watermark 是一种衡量 Event Time 进展的机制。 Watermark 是用于处理乱序事件的，而正确的处理乱序事件，通常用 Watermark 机制结合 Window 来实现。 数据流中的 Watermark 用于表示 timestamp 小于 Watermark 的数据，都已经到达了，因此，Window 的执行也是由 Watermark 触发的。 Watermark 可以理解成一个延迟触发机制，我们可以设置 Watermark 的延时时长 t，每次系统会校验已经到达的数据中最大的 maxEventTime，然后认定 Event Time 小于 maxEventTime - t 的所有数据都已经到达，如果有窗口的停止时间等于 maxEventTime – t (说明这个窗口的所有数据都已到达)，那么这个窗口被触发执行。 Watermark 用来让程序自己平衡延迟和结果正确性。 有序流的 Watermarker 如下图所示：(Watermark 的延时时长设置为 0s) 乱序流的 Watermarker 如下图所示：(Watermark 的延时时长设置为 2s) 当 Flink 接收到数据时，会按照一定的规则去生成 Watermark，这条 Watermark 就等于当前所有到达数据中的 maxEventTime - 延时时长 t，也就是说，Watermark 是基于数据携带的时间戳生成的，一旦 Watermark 比当前未触发的窗口的停止时间要晚，那么就会触发相应窗口的执行。由于 Event Time 是由数据携带的，因此，如果运行过程中无法获取新的数据，那么没有被触发的窗口将永远都不被触发。上图中，我们设置的允许最大延时时长为 2s，所以时间戳为 7s 的事件对应的 Watermark 是 5s，时间戳为 12s 的事件的 Watermark 是 10s，如果我们的 Window 1 是 1s ~ 5s，Window 2 是 6s ~ 10s，那么时间戳为 7s 的事件到达时的 Watermarker 恰好触发 Window 1，时间戳为 12s 的事件到达时的 Watermark 恰好触发 Window 2。 Watermark 就是触发前一窗口的 “关窗时间”，一旦触发关门那么以当前时刻为准在窗口范围内的所有所有数据都会收入窗中。 只要新来的数据没有达到 Watermark，那么不管现实中的时间推进了多久，都不会触发关窗。 Watermark 的延时时长，应结合实际数据到达的迟到程度来设置。比如下图所示，当前到达数据的最大时间戳为 5s，其后续迟到数据有 2s 和 3s，那 Watermark 延时时长 t 应设置为 3s。 上图中，Watermark 的变化规律：1s 数据为 -2，4s 数据为 1，5s 数据为 2，2s 数据为 2，3s 数据为 2，6s 数据为 3。 Watermark 只单调递增，所以 2s 和 3s 的数据，都为 2。 如果有设置为 2s 的 Window，其会在 5s 数据到达时，触发执行。如果有设置为 5s 的 Window，则会在 8s 数据到达时，才会触发执行。 具体的数据流向，可参考下图： 从图中可以看出，当 8s 数据到达时，Watermark 为 5，此时，触发 0 ~ 5s 的 Window 数据桶关闭，并输出一次结果，如果 8s 数据不到达，0 ~ 5s 的数据桶，会一直开启。 Watermark 的特点 上图中，三角形表示数据自带的时间戳。 Watermark 是一条特殊的数据记录，其本质上就是一个带时间戳的数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package org.apache.flink.api.common.eventtime;public final class Watermark implements Serializable &#123; private static final long serialVersionUID = 1L; /** Thread local formatter for stringifying the timestamps. */ private static final ThreadLocal&lt;SimpleDateFormat&gt; TS_FORMATTER = ThreadLocal.withInitial( () -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;)); // ------------------------------------------------------------------------ /** The watermark that signifies end-of-event-time. */ public static final Watermark MAX_WATERMARK = new Watermark(Long.MAX_VALUE); // ------------------------------------------------------------------------ /** The timestamp of the watermark in milliseconds. */ private final long timestamp; /** * Creates a new watermark with the given timestamp in milliseconds. */ public Watermark(long timestamp) &#123; this.timestamp = timestamp; &#125; /** * Returns the timestamp associated with this Watermark. */ public long getTimestamp() &#123; return timestamp; &#125; /** * Formats the timestamp of this watermark, assuming it is a millisecond timestamp. * The returned format is &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;. */ public String getFormattedTimestamp() &#123; return TS_FORMATTER.get().format(new Date(timestamp)); &#125; // ------------------------------------------------------------------------ @Override public boolean equals(Object o) &#123; return this == o || o != null &amp;&amp; o.getClass() == Watermark.class &amp;&amp; ((Watermark) o).timestamp == this.timestamp; &#125; @Override public int hashCode() &#123; return Long.hashCode(timestamp); &#125; @Override public String toString() &#123; return &quot;Watermark @ &quot; + timestamp + &quot; (&quot; + getFormattedTimestamp() + &#x27;)&#x27;; &#125;&#125; Watermark 必须单调递增，以确保任务的事件时间时钟在向前推进，而不是在后退。 Watermark 与数据的时间戳相关。 Watermark 可以为负值，表示事件还未发生。 Watermark 的传递 Watermark 向下游传递：上游接收到 Watermark 后，会广播到下游的所有任务。 当上游存在多个并行任务时，下游子任务可能会接收到上游广播的多个 Watermark，此时，当前子任务会取时间戳最小的那个 Watermark，因为这样才能保证上游并行任务的每一个，Watermark 之前的数据都到了。 上图 1 中，上游四条并行的数据流，从上到下，当前广播即将到达的 Watermark 分别为 4s，7s 和 6s，而 Task 在之前已经到达并保存的 Watermark 分别为 2s，4s，3s 和 6s，Task 的 Watermark 为 2s。 上图 2 中，Task 接收到第一个分区新的 Watermark 4s，第一个分区的 Watermark 由 2s 更新为 4s，整个 Task 的 Watermark 对比更新为 3s，并广播到下游所有任务。 上图 3 中，Task 接收到第二个分区新的 Watermark 7s，第二个分区的 Watermark 由 4s 更新为 7s，整个 Task 的 Watermark 对比仍为 3s，此时，Watermark 未更新，不广播到下游。 上图 4 中，Task 接收到第三个分区新的 Watermark 6s，第三个分区的 Watermark 由 3s 更新为 6s，整个 Task 的 Watermark 对比更新为 4s，并广播到下游所有任务。 Watermark 的引入 引入 Watermark，需要设置时间语义。 Event Time 的使用一定要指定数据源中的时间戳，否则程序无法知道事件的事件时间是什么 (数据源里的数据没有时间戳的话，就只能使用 Processing Time 了)。 升序数据 (数据流的时间戳是单调递增的，也就是说没有乱序，即理想数据)，不需要延迟触发，可以只指定时间戳，使用 AscendingTimestampExtractor： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class WindowTest3_Watermark1 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.设置Event Time时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 3.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 4.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 5.方式一：理想数据，使用AscendingTimestampExtractor DataStream&lt;SensorReading&gt; watermarkStream = dataStream.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;SensorReading&gt;() &#123; @Override public long extractAscendingTimestamp(SensorReading sensorReading) &#123; // 因为sensorReading的时间戳是秒，要转换为毫秒 return sensorReading.getTimestamp() * 1000; &#125; &#125;); // 侧输出流 OutputTag&lt;SensorReading&gt; outputTag = new OutputTag&lt;SensorReading&gt;(&quot;late&quot;) &#123; &#125;; // 6.基于事件时间的开窗聚合，统计15秒内温度的最小值 SingleOutputStreamOperator&lt;SensorReading&gt; minTempStream = watermarkStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(15)) .allowedLateness(Time.minutes(1)) .sideOutputLateData(outputTag) .minBy(&quot;temperature&quot;); // 打印最小值 minTempStream.print(&quot;minTemp&quot;); // 打印迟到数据 minTempStream.getSideOutput(outputTag).print(&quot;late&quot;); // 7.执行任务 env.execute(); &#125;&#125; 乱序数据，需要设置延迟触发，使用 BoundedOutOfOrdernessTimestampExtractor： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class WindowTest3_Watermark2 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.设置Event Time时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 3.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 4.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 5.方式二：乱序数据，使用BoundedOutOfOrdernessTimestampExtractor // 需要指定延时时长，此处设置为Time.seconds(2)，即2s，实际上生产时，按实际情况设置，可能更多是毫秒级别 // import org.apache.flink.streaming.api.windowing.time.Time; DataStream&lt;SensorReading&gt; watermarkStream = dataStream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(2)) &#123; @Override public long extractTimestamp(SensorReading sensorReading) &#123; // 因为sensorReading的时间戳是秒，要转换为毫秒 return sensorReading.getTimestamp() * 1000L; &#125; &#125;); // 侧输出流 OutputTag&lt;SensorReading&gt; outputTag = new OutputTag&lt;SensorReading&gt;(&quot;late&quot;) &#123; &#125;; // 6.基于事件时间的开窗聚合，统计15秒内温度的最小值 SingleOutputStreamOperator&lt;SensorReading&gt; minTempStream = watermarkStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(15)) .allowedLateness(Time.minutes(1)) .sideOutputLateData(outputTag) .minBy(&quot;temperature&quot;); // 打印最小值 minTempStream.print(&quot;minTemp&quot;); // 打印迟到数据 minTempStream.getSideOutput(outputTag).print(&quot;late&quot;); // 7.执行任务 env.execute(); &#125;&#125; Flink 最新版本中，上面两种方式已经被弃用，建议使用 .assignTimestampsAndWatermarks(WatermarkStrategy) 替代。 AscendingTimestampExtractor 和 BoundedOutOfOrdernessTimestampExtractor，本质上都是 TimestampAssigner 接口的实现类。 Flink 暴露了 TimestampAssigner 接口供我们实现，我们可以自定义如何从事件数据中抽取时间戳以及生成 Watermark。 TimestampAssigner 接口定义了抽取时间戳，以及生成 Watermark 的方法，有两种类型：AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks。 AssignerWithPeriodicWatermarks 周期性的生成 Watermark，系统会周期性的将 Watermark 插入到流中 (Watermark 实际上也是一种特殊的事件)。 在设置时间语义时，Processing Time 语义下默认周期是 0 毫秒，Event Time 和 Ingestion Time 语义下默认周期是 200 毫秒，可以使用 ExecutionConfig.setAutoWatermarkInterval() 进行设置。 123456789@PublicEvolvingpublic void setStreamTimeCharacteristic(TimeCharacteristic characteristic) &#123; this.timeCharacteristic = Preconditions.checkNotNull(characteristic); if (characteristic == TimeCharacteristic.ProcessingTime) &#123; getConfig().setAutoWatermarkInterval(0); &#125; else &#123; getConfig().setAutoWatermarkInterval(200); &#125;&#125; 12// 每隔5秒生成一个Watermarkenv.getConfig().setAutoWatermarkInterval(5000); 产生 Watermark 的逻辑：每隔周期时间，Flink 会调用一次 AssignerWithPeriodicWatermarks 的 getCurrentWatermark()。如果方法返回一个时间戳大于之前 Watermark 的时间戳，新的 Watermark 会被插入到流中。如果方法返回的时间戳小于等于之前 Watermark 的时间戳，则不会产生新的 Watermark。这种检查模式，保证了 Watermark 是单调递增的。 前面升序和乱序使用的 AscendingTimestampExtractor 和 BoundedOutOfOrdernessTimestampExtractor，都是基于周期性 Watermark 的。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class WindowTest3_Watermark3 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.设置Event Time时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 3.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 4.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 5.周期性的生成Watermark env.getConfig().setAutoWatermarkInterval(5000);// 每隔5秒生成一个Watermark DataStream&lt;SensorReading&gt; watermarkStream = dataStream.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks&lt;SensorReading&gt;() &#123; private Long bound = 60 * 1000L;// 延迟一分钟 private Long maxTs = Long.MIN_VALUE;// 当前最大时间戳 // 返回Watermark @Nullable @Override public Watermark getCurrentWatermark() &#123; return new Watermark(maxTs - bound); &#125; // 当前到达数据的时间戳，与之前保存的最大时间戳对比，拿到当前数据到达后，最大的时间戳 @Override public long extractTimestamp(SensorReading sensorReading, long recordTimestamp) &#123; maxTs = Math.max(maxTs, sensorReading.getTimestamp() * 1000L); return sensorReading.getTimestamp(); &#125; &#125;); // 侧输出流 OutputTag&lt;SensorReading&gt; outputTag = new OutputTag&lt;SensorReading&gt;(&quot;late&quot;) &#123; &#125;; // 6.基于事件时间的开窗聚合，统计15秒内温度的最小值 SingleOutputStreamOperator&lt;SensorReading&gt; minTempStream = watermarkStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(15)) .allowedLateness(Time.minutes(1)) .sideOutputLateData(outputTag) .minBy(&quot;temperature&quot;); // 打印最小值 minTempStream.print(&quot;minTemp&quot;); // 打印迟到数据 minTempStream.getSideOutput(outputTag).print(&quot;late&quot;); // 7.执行任务 env.execute(); &#125;&#125; AssignerWithPunctuatedWatermarks 没有时间周期规律，间断式地生成 Watermark。 和周期性生成的方式不同，这种方式不是固定时间的，而是可以根据需要对每条数据进行筛选和处理。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class WindowTest3_Watermark4 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.设置Event Time时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 3.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 4.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 5.间断性的生成Watermark，只给sensor_1的传感器的数据流插入Watermark DataStream&lt;SensorReading&gt; watermarkStream = dataStream.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks&lt;SensorReading&gt;() &#123; private Long bound = 60 * 1000L;// 延迟一分钟 @Override public long extractTimestamp(SensorReading sensorReading, long recordTimestamp) &#123; return sensorReading.getTimestamp() * 1000L; &#125; @Nullable @Override public Watermark checkAndGetNextWatermark(SensorReading sensorReading, long extractedTimestamp) &#123; if (&quot;sensor_1&quot;.equals(sensorReading.getId())) &#123; return new Watermark(extractedTimestamp - bound); &#125; else &#123; return null; &#125; &#125; &#125;); // 侧输出流 OutputTag&lt;SensorReading&gt; outputTag = new OutputTag&lt;SensorReading&gt;(&quot;late&quot;) &#123; &#125;; // 6.基于事件时间的开窗聚合，统计15秒内温度的最小值 SingleOutputStreamOperator&lt;SensorReading&gt; minTempStream = watermarkStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(15)) .allowedLateness(Time.minutes(1)) .sideOutputLateData(outputTag) .minBy(&quot;temperature&quot;); // 打印最小值 minTempStream.print(&quot;minTemp&quot;); // 打印迟到数据 minTempStream.getSideOutput(outputTag).print(&quot;late&quot;); // 7.执行任务 env.execute(); &#125;&#125; Watermark 的设定 在 Flink 中，Watermark 由应用程序开发人员生成，这通常需要对相应的领域有一定的了解。 如果 Watermark 设置的延迟太长，收到结果的速度可能就会很慢，解决办法是在 Watermark 到达之前输出一个近似结果。 如果 Watermark 设置的延迟太短，则可能收到错误结果，不过 Flink 处理迟到数据的机制 (侧输出流等) 可以解决这个问题。 Watermark 设置的位置离 Source 越近越好。 Evnet Time 在 Window 中的测试说明流的并行度为 1 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class WindowTest3_Watermark2 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.设置Event Time时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 3.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 4.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); dataStream.print(); // 5.乱序数据设置Watermark DataStream&lt;SensorReading&gt; watermarkStream = dataStream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(2)) &#123; @Override public long extractTimestamp(SensorReading sensorReading) &#123; // 因为sensorReading的时间戳是秒，要转换为毫秒 return sensorReading.getTimestamp() * 1000L; &#125; &#125;); // 侧输出流 OutputTag&lt;SensorReading&gt; outputTag = new OutputTag&lt;SensorReading&gt;(&quot;late&quot;) &#123; &#125;; // 6.基于事件时间的开窗聚合，统计15秒内温度的最小值，并设置窗口延迟1分钟关闭，同时，窗口关闭后的迟到数据都输出到侧输出流 // 这个延迟1分钟，是基于Event Time的，不是现实中的1分钟 SingleOutputStreamOperator&lt;SensorReading&gt; minTempStream = watermarkStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(15)) .allowedLateness(Time.minutes(1)) .sideOutputLateData(outputTag) .minBy(&quot;temperature&quot;); // 打印最小值 minTempStream.print(&quot;minTemp&quot;); // 打印迟到数据 minTempStream.getSideOutput(outputTag).print(&quot;late&quot;); // 7.执行任务 env.execute(); &#125;&#125; 输入参数： 12345678xisun@DESKTOP-OM8IACS:/mnt/c/Users/XiSun$ nc -lk 7777sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,37.1 输出结果： 1234567891011121314log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718206, temperature=36.3&#125;SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718210, temperature=34.7&#125;SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;minTemp&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;minTemp&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;minTemp&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;minTemp&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125; 源码分析：窗口起始点的确定。 12345678public WindowedStream&lt;T, KEY, TimeWindow&gt; timeWindow(Time size) &#123; if (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123; return window(TumblingProcessingTimeWindows.of(size)); &#125; else &#123; // Event Time语义 return window(TumblingEventTimeWindows.of(size)); &#125;&#125; 123456789101112131415161718192021222324252627282930313233@PublicEvolvingpublic class TumblingEventTimeWindows extends WindowAssigner&lt;Object, TimeWindow&gt; &#123; private static final long serialVersionUID = 1L; private final long size; private final long offset; protected TumblingEventTimeWindows(long size, long offset) &#123; if (Math.abs(offset) &gt;= size) &#123; throw new IllegalArgumentException(&quot;TumblingEventTimeWindows parameters must satisfy abs(offset) &lt; size&quot;); &#125; this.size = size; this.offset = offset; &#125; // 此方法确定当前数据数据哪个窗口，即如何开窗 // element：当前数据；timestamp：当前数据的时间戳 @Override public Collection&lt;TimeWindow&gt; assignWindows(Object element, long timestamp, WindowAssignerContext context) &#123; if (timestamp &gt; Long.MIN_VALUE) &#123; // Long.MIN_VALUE is currently assigned when no timestamp is present // 确定起始点 long start = TimeWindow.getWindowStartWithOffset(timestamp, offset, size); return Collections.singletonList(new TimeWindow(start, start + size)); &#125; else &#123; throw new RuntimeException(&quot;Record has Long.MIN_VALUE timestamp (= no timestamp marker). &quot; + &quot;Is the time characteristic set to &#x27;ProcessingTime&#x27;, or did you forget to call &quot; + &quot;&#x27;DataStream.assignTimestampsAndWatermarks(...)&#x27;?&quot;); &#125; &#125;&#125; 123456789101112131415161718@PublicEvolvingpublic class TimeWindow extends Window &#123; /** * Method to get the window start for a timestamp. * * @param timestamp epoch millisecond to get the window start. * @param offset The offset which window start would be shifted by. * @param windowSize The size of the generated windows. * @return window start */ // timestamp：当前数据的时间戳；offset：偏移量，未设置时默认为0；windowSize：开窗大小 // offset一般用于处理不同时区的偏移时间。标准时间是按伦敦所在时区，如果在北京时间东八区，获取的时间戳比标准时间早8个小时， // 如果想统计每天0点到0点的窗口，应该设置偏移量offset为-8h public static long getWindowStartWithOffset(long timestamp, long offset, long windowSize：开窗大小) &#123; // offset为0，此式化简为timestamp减去timestamp对windowSize取余，结果是windowSize的整数倍 return timestamp - (timestamp - offset + windowSize) % windowSize; &#125;&#125; 由以上分析，第一个数据时间戳为 1547718199，开窗尺寸为 15s，则窗口起始点为：1547718199 - 1547718199 % 15 = 1547718195。而窗口尺寸为 15s，则后续窗口为：[195, 210)，[210, 225)，[225, 240)，以此类推。后续到达的每个数据，也会进入对应的数据桶中。 从图中可以看出，因为 Watermark 延时时长设置为 2s，所以当 sensor_1,1547718212,37.1 数据到达时，会触发 [195, 210) 窗口关闭 (因为设置了窗口延迟 1 分钟关闭，212 数据到达时，会触发窗口返回一个结果，之后 1 分钟之内，来一个新数据会返回一个结果，直到 272 数据到达时，窗口关闭，关闭之后的迟到数据，都会输出到侧输出流)，这也是第一个被触发关闭的窗口。然后根据 id 分组，输出四个结果。其中，sensor_1 对应的最小温度值为 sensor_1,1547718199,35.8。 流的并行度不为 1 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class WindowTest3_Watermark2 &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.设置Event Time时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 3.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 4.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); dataStream.print(); // 5.乱序数据设置Watermark DataStream&lt;SensorReading&gt; watermarkStream = dataStream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(2)) &#123; @Override public long extractTimestamp(SensorReading sensorReading) &#123; // 因为sensorReading的时间戳是秒，要转换为毫秒 return sensorReading.getTimestamp() * 1000L; &#125; &#125;); // 侧输出流 OutputTag&lt;SensorReading&gt; outputTag = new OutputTag&lt;SensorReading&gt;(&quot;late&quot;) &#123; &#125;; // 6.基于事件时间的开窗聚合，统计15秒内温度的最小值，并设置窗口延迟1分钟关闭，同时，窗口关闭后的迟到数据都输出到侧输出流 // 这个延迟1分钟，是基于Event Time的，不是现实中的1分钟 SingleOutputStreamOperator&lt;SensorReading&gt; minTempStream = watermarkStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(15)) .allowedLateness(Time.minutes(1)) .sideOutputLateData(outputTag) .minBy(&quot;temperature&quot;); // 打印最小值 minTempStream.print(&quot;minTemp&quot;); // 打印迟到数据 minTempStream.getSideOutput(outputTag).print(&quot;late&quot;); // 7.执行任务 env.execute(); &#125;&#125; 输入参数： 1234567891011xisun@DESKTOP-OM8IACS:/mnt/c/Users/Ziyoo$ nc -lk 7777sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_1,1547718212,31.9sensor_1,1547718212,30.8sensor_1,1547718212,36.7 输出结果： 1234567891011121314151617log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;1&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718206, temperature=36.3&#125;3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718210, temperature=34.7&#125;4&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=33.1&#125;1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=31.9&#125;2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=30.8&#125;3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=36.7&#125;minTemp:4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;minTemp:2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;minTemp:3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;minTemp:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125; 此时，第一条数据是 199，如前面分析，开窗仍为 [195, 210)，[210, 225)，[225, 240)，依次类推。Scoket 文本流的并行度为 1，map 算子的并行度为 4。因此，每条数据会轮询的方式进入 map 算子。数据桶的分布情况如下： 由 Watermark 的传递可知，对于四个分区，只有每个分区的 Watermark 都更新为 210s 时，才会触发 [195, 210) 窗口的关闭，也就是最有一条数据 sensor_1,1547718212,36.7 到达时，触发输出计算结果。其中，sensor_1 对应的最小温度值为 sensor_1,1547718199,35.8。 第一条数据 sensor_1,1547718199,35.8 到达时，第一个分区的 Watermark 是 197s，其他三个分区的 Watermark 是初始值，是一个很大的负值。此时，下游任务的 Watermark 取四个分区的最小值。 1this.currentMaxTimestamp = Long.MIN_VALUE + this.maxOutOfOrderness; 如果继续输入数据，可得相应结果。从结果中可以看出，数据通过轮询方式进入四个并行的分区中，当四个分区的 Watermark 都更新为 225s 和 240s 时，才会触发 [210, 225) 和 [225, 240) 窗口输出结果。 12345678sensor_10,1547718227,3.1sensor_10,1547718227,3.2sensor_10,1547718227,3.3sensor_10,1547718227,3.4sensor_10,1547718242,4.1sensor_10,1547718242,4.2sensor_10,1547718242,4.3sensor_10,1547718242,4.4 123456789104&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718227, temperature=3.1&#125;1&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718227, temperature=3.2&#125;2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718227, temperature=3.3&#125;3&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718227, temperature=3.4&#125;minTemp:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=30.8&#125;4&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718242, temperature=4.1&#125;1&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718242, temperature=4.2&#125;2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718242, temperature=4.3&#125;3&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718242, temperature=4.4&#125;minTemp:2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718227, temperature=3.1&#125; Flink 的状态管理 流式计算分为无状态和有状态两种情况。无状态的计算观察每个独立事件，并根据最后一个事件输出结果。例如，流处理应用程序从传感器接收温度读数，并在温度超过 90 度时发出警告。有状态的计算则会基于多个事件输出结果。以下是一些例子： 所有类型的窗口。例如，计算过去一小时的平均温度，就是有状态的计算。 所有用于复杂事件处理的状态机。例如，若在一分钟内收到两个相差 20 度以上的温度读数，则发出警告，这是有状态的计算。 流与流之间的所有关联操作，以及流与静态表或动态表之间的关联操作，都是有状态的计算。 下图展示了无状态流处理和有状态流处理的主要区别。无状态流处理分别接收每条数据记录 (图中的黑条)，然后根据最新输入的数据生成输出数据 (白条)。有状态流处理会维护状态 (根据每条输入记录进行更新)，并基于最新输入的记录和当前的状态值生成输出记录 (灰条)。 上图中输入数据由黑条表示。无状态流处理每次只转换一条输入记录，并且仅根据最新的输入记录输出结果 (白条)。有状态流处理维护所有已处理记录的状态值，并根据每条新输入的记录更新状态，因此输出记录 (灰条) 反映的是综合考虑多个事件之后的结果。 尽管无状态的计算很重要，但是流处理对有状态的计算更感兴趣。事实上，正确地实现有状态的计算比实现无状态的计算难得多。旧的流处理系统并不支持有状态的计算，而新一代的流处理系统则将状态及其正确性视为重中之重。 Flink 中的状态 Flink 内置的很多算子，数据源 Source，数据存储 Sink 都是有状态的，流中的数据都是 buffer records，会保存一定的元素或者元数据。例如：ProcessWindowFunction 会缓存输入流的数据，ProcessFunction 会保存设置的定时器信息等等。 由一个任务维护，并且用来计算某个结果的所有数据，都属于这个任务的状态。 可以认为状态就是一个本地变量 (保存在内存中)，可以被任务的业务逻辑访问。 Flink 会进行状态管理，包括状态一致性、故障处理以及高效存储和访问，以便开发人员可以专注于应用程序的逻辑。 在 Flink 中，状态始终与特定算子相关联。 为了使运行时的 Flink 了解算子的状态，算子需要预先注册其状态。 总的说来，有两种类型的状态： 算子状态 (Operator State) 算子状态的作用范围限定为算子任务。 后面的算子任务，无法访问前面的算子任务的状态。 键控状态 (Keyed State) 根据输入数据流中定义的键 (key) 来维护和访问。 访问原则：只有当前 key 对应的数据，才能访问当前 key 对应的状态。 算子状态 (Operator State) 算子状态的作用范围限定为算子任务，由同一并行任务所处理的所有数据都可以访问到相同的状态。 到达当前算子的所有任务，共享算子状态，不论这些任务的 key 是否相同。注意：需要是同一个分区的。如果一个算子有多个并行分区，每一个分区的子任务，享有自己所在分区的算子状态。 状态对于同一子任务而言是共享的。 算子状态不能由相同或不同算子的另一个子任务访问。 算子状态数据结构 列表状态 (List state) 将状态表示为一组数据的列表。 列表状态方便于后续算子任务可能存在的并行度的调整。 联合列表状态 (Union list state) 也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保存点 (savepoint) 启动应用程序时如何恢复。 广播状态 (Broadcast state) 如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应用广播状态。比如：算子的状态是某个配置项，则其对每项任务都是相同的状态。 算子状态的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class StateTest1_OperatorState &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.定义一个有状态的map操作，统计当前分区数据个数 SingleOutputStreamOperator&lt;Integer&gt; resultStream = dataStream.map(new MyCountMapper()); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125; // 自定义MapFunction // ListCheckpointed&lt;Integer&gt;：列表状态，存放当前要保存的算子状态 public static class MyCountMapper implements MapFunction&lt;SensorReading, Integer&gt;, ListCheckpointed&lt;Integer&gt; &#123; // 定义一个本地变量，作为算子状态 // 算子状态，从使用上来看，就相当于一个本地变量 private Integer count = 0; @Override public Integer map(SensorReading value) throws Exception &#123; count++; return count; &#125; // 保存状态使用的方法 @Override public List&lt;Integer&gt; snapshotState(long checkpointId, long timestamp) throws Exception &#123; return Collections.singletonList(count); &#125; // 恢复状态使用的方法 @Override public void restoreState(List&lt;Integer&gt; state) throws Exception &#123; // 可能有多个分区，每个分区都有自己的算子状态 for (Integer num : state) &#123; count += num; &#125; &#125; &#125;&#125; 键控状态 (Keyed State) 键控状态是根据输入数据流中定义的键 (key) 来维护和访问的。 Flink 为每个 key 维护一个状态实例，并将具有相同键的所有数据，都分区到同一个算子任务中，这个任务会维护和处理这个 key 对应的状态。 当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的 key。因此，具有相同 key 的所有数据都会访问相同的状态。 不同 key 的数据，即使分配在同一个 Task 内，也会按 key 保存不同的状态，访问时，不同的 key 之间不共享状态。 键控状态很类似于一个分布式的 key-value map 数据结构，只能用于 KeyedStream (keyBy 算子处理之后)。 键控状态数据结构 值状态 (Value state) ValueState&lt;T&gt;：将状态表示为单个的值，值的类型为 T。 ValueState.value()：get 操作。 ValueState.update(T value)：set 操作。 ValueState.clear()：清空。 列表状态 (List state) ListState&lt;T&gt;：将状态表示为一组数据的列表，列表里的元素的数据类型为 T。 ListState.add(T value) ListState.addAll(List&lt;T&gt; values) ListState.get()：返回 Iterable&lt;T&gt;。 ListState.update(List&lt;T&gt; values) ListState.clear()：清空。 映射状态 (Map state) MapState&lt;K, V&gt;：将状态表示为一组 Key-Value 对。 MapState.get(UK key) MapState.put(UK key, UV value) MapState.contains(UK key) MapState.remove(UK key) MapState.clear()：清空。 聚合状态 (Reducing state &amp; Aggregating state) ReducingState&lt;T&gt; 或 AggregatingState&lt;I, O&gt;：将状态表示为一个用于聚合操作的列表。 State.clear()：清空。 键控状态的使用 声明一个键控状态： 1keyCountState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Integer&gt;(&quot;key-count&quot;, Integer.class, 0)); 需要使用运行时上下文，意味着键控状态的使用不同于算子状态，必须要在富函数中实现。 在 open() 中赋值 state 变量。 通过 RuntimeContext 注册 StateDescriptor。StateDescriptor 以状态 state 的名字和存储的数据类型为参数。state 的名字不能重复。 读取状态： 1Integer count = keyCountState.value(); 对状态赋值： 1keyCountState.update(count); 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class StateTest2_KeyedState &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.定义一个有状态的map操作，统计当前分区数据个数 SingleOutputStreamOperator&lt;Integer&gt; resultStream = dataStream .keyBy(&quot;id&quot;) .map(new MyKeyCountMapper()); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125; // 自定义RichMapFunction public static class MyKeyCountMapper extends RichMapFunction&lt;SensorReading, Integer&gt; &#123; // 值状态 private ValueState&lt;Integer&gt; keyCountState; // 其它类型状态的声明 private ListState&lt;String&gt; myListState;// 列表状态 private MapState&lt;String, Double&gt; myMapState;// 映射状态 private ReducingState&lt;SensorReading&gt; myReducingState;// 聚合状态 // 必须要在open方法里赋值，不同状态的名称不能相同 @Override public void open(Configuration parameters) throws Exception &#123; // 值状态的赋值，此处初始值为0，如果不设置，则是null，在使用时需要先判断是否为null // 但此方法已被弃用，推荐不设初始值，在使用时手动判断并赋值 keyCountState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Integer&gt;(&quot;key-count&quot;, Integer.class, 0)); // 其它类型状态的赋值 myListState = getRuntimeContext().getListState(new ListStateDescriptor&lt;String&gt;(&quot;my-list&quot;, String.class)); myMapState = getRuntimeContext().getMapState(new MapStateDescriptor&lt;String, Double&gt;(&quot;my-map&quot;, String.class, Double.class)); myReducingState = getRuntimeContext().getReducingState(new ReducingStateDescriptor&lt;SensorReading&gt;(&quot;my-reduce&quot;, new ReduceFunction&lt;SensorReading&gt;() &#123; @Override public SensorReading reduce(SensorReading value1, SensorReading value2) throws Exception &#123; // 按照具体需求，返回对应的对象 return null; &#125; &#125;, SensorReading.class));// 聚合状态需要传入一个聚合函数 &#125; @Override public Integer map(SensorReading value) throws Exception &#123; // 值状态的使用，先取值，再赋值 Integer count = keyCountState.value(); count++; keyCountState.update(count); // 其它状态API调用 // list state --- List的常规操作 Iterable&lt;String&gt; lists = myListState.get();// 取 for (String str : lists) &#123; System.out.println(str); &#125; myListState.add(&quot;hello&quot;);// 一个一个追加，也可以addAll()添加一个List myListState.clear();// 清空 // map state --- Map的常规操作 myMapState.get(&quot;1&quot;);// 取 myMapState.put(&quot;2&quot;, 12.3);// 存 myMapState.remove(&quot;2&quot;);// 移除一个 myMapState.clear();// 清空 // reducing state SensorReading sensorReading = myReducingState.get();// 取 myReducingState.add(value);// 会调用聚合状态声明的聚合函数来处理传入的值 myReducingState.clear();// 清空 return count; &#125; &#125;&#125; 实例：检测传感器的温度值，如果连续的两个温度差值超过 10 度，就输出报警。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class StateTest3_KeyedStateApplicationCase &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.定义一个flatmap操作，检测温度跳变，输出报警 // map是一对一，实际情况时，可能有null数据，输出结果可能不同于输入数据，因此选择flatmap SingleOutputStreamOperator&lt;Tuple3&lt;String, Double, Double&gt;&gt; resultStream = dataStream .keyBy(&quot;id&quot;) .flatMap(new TempChangeWarning(10.0)); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125; // 实现自定义函数类 public static class TempChangeWarning extends RichFlatMapFunction&lt;SensorReading, Tuple3&lt;String, Double, Double&gt;&gt; &#123; // 温度跳变阈值 private final double threshold; public TempChangeWarning(double threshold) &#123; this.threshold = threshold; &#125; // 定义值状态，保存上一次的温度值 private ValueState&lt;Double&gt; lastTempState; @Override public void open(Configuration parameters) throws Exception &#123; // 赋值，不定义初始值，使用时判断 lastTempState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Double&gt;(&quot;last-temp-state&quot;, Double.class)); &#125; @Override public void flatMap(SensorReading sensorReading, Collector&lt;Tuple3&lt;String, Double, Double&gt;&gt; out) throws Exception &#123; Double lastTempValue = lastTempState.value(); // 如果状态不为null，那么就判断两次温度差值 if (lastTempValue != null) &#123; // 当前温度与上一次温度的差值 double diff = Math.abs(sensorReading.getTemperature() - lastTempValue); if (diff &gt;= threshold) &#123; // 输出报警信息 out.collect(new Tuple3&lt;&gt;(sensorReading.getId(), lastTempValue, sensorReading.getTemperature())); &#125; &#125; // 更新状态为当前温度 lastTempState.update(sensorReading.getTemperature()); &#125; @Override public void close() throws Exception &#123; lastTempState.clear(); &#125; &#125;&#125; 参数输入： 1234567xisun@DESKTOP-OM8IACS:/mnt/c/Users/Ziyoo$ nc -lk 7777sensor_1,1547718199,36.3sensor_1,1547718199,37.9sensor_1,1547718199,48sensor_6,1547718201,15.4sensor_6,1547718201,35sensor_1,1547718199,36.9 输出结果： 123456log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; (sensor_1,37.9,48.0)3&gt; (sensor_6,15.4,35.0)3&gt; (sensor_1,48.0,36.9) 输入 sensor_1,1547718199,48 时触发 sensor_1 报警，输入 sensor_6,1547718201,35 时触发 sensor_6 报警，输入 sensor_1,1547718199,36.9 时再次触发 sensor_1 报警。 状态后端 (State Backends) 每传入一条数据，有状态的算子任务都会读取和更新状态。 由于有效的状态访问对于处理数据的低延迟至关重要，因此每个并行任务都会在本地维护其状态，以确保快速的状态访问。 状态的存储、访问以及维护，由一个可插入的组件决定，这个组件就叫做状态后端 (state backend)。 状态后端主要负责两件事：本地的状态管理，以及将检查点 (Checkpoint) 状态写入远程存储。 选择一个状态后端 MemoryStateBackend 内存级的状态后端，会将键控状态作为内存中的对象进行管理，将它们存储在 TaskManager 的 JVM 堆上，而将 Checkpoint 存储在 JobManager 的内存中。 特点：快速、低延迟，但不稳定。 FsStateBackend 将 Checkpoint 存到远程的持久化文件系统 (FileSystem) 上，而对于本地状态，跟 MemoryStateBackend 一样，也会存储在 TaskManager 的 JVM 堆上。 同时拥有内存级的本地访问速度，和更好的容错保证。 RocksDBStateBackend 将所有状态序列化后，存入本地的 RocksDB 中存储。 RocksDB 的支持并不直接包含在 Flink 中，需要引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-statebackend-rocksdb_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 状态后端的使用配置 在 conf/flink-conf.yaml 配置文件中： 1234567891011121314151617181920212223242526272829303132333435#==============================================================================# Fault tolerance and checkpointing#==============================================================================# The backend that will be used to store operator state checkpoints if# checkpointing is enabled.## Supported backends are &#x27;jobmanager&#x27;, &#x27;filesystem&#x27;, &#x27;rocksdb&#x27;, or the# &lt;class-name-of-factory&gt;.## state.backend: filesystem# 默认使用FsStateBackend# Directory for checkpoints filesystem, when using any of the default bundled# state backends.#state.checkpoints.dir: hdfs://mghadoop:8020/flink-checkpoints# checkpoints保存路径# Default target directory for savepoints, optional.## state.savepoints.dir: hdfs://namenode-host:port/flink-checkpoints# Flag to enable/disable incremental checkpoints for backends that# support incremental checkpoints (like the RocksDB state backend). ## state.backend.incremental: false# 是否增量化的进行checkpoints保存，默认false，比如FsStateBackend不容易支持此操作，但RocksDB可以# The failover strategy, i.e., how the job computation recovers from task failures.# Only restart tasks that may have been affected by the task failure, which typically includes# downstream tasks and potentially upstream tasks if their produced data is no longer available for consumption.jobmanager.execution.failover-strategy: region# 区域重启的策略，即需要重启时，不是把所有的任务全部停掉再重启，而是只对受影响的区域执行重启，其他区域正常执行 在代码中对每个任务配置： MemoryStateBackend 1env.setStateBackend(new MemoryStateBackend()); MemoryStateBackend 的构造函数： 123456789// 无参public MemoryStateBackend() &#123; this(null, null, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);&#125;// asynchronousSnapshots：是否开启异步快照，即执行快照时，不影响任务的继续操作public MemoryStateBackend(boolean asynchronousSnapshots) &#123; this(null, null, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.fromBoolean(asynchronousSnapshots));&#125; FsStateBackend 12// 需要传入一个checkpoints保存路径对应的urienv.setStateBackend(new FsStateBackend(&quot;&quot;)); RocksDBStateBackend 123456try &#123; // 需要传入一个checkpoints保存路径对应的uri env.setStateBackend(new RocksDBStateBackend(&quot;&quot;));&#125; catch (IOException exception) &#123; exception.printStackTrace();&#125; RocksDBStateBackend 的构造函数： 12345678public RocksDBStateBackend(String checkpointDataUri) throws IOException &#123; this((new Path(checkpointDataUri)).toUri());&#125;// enableIncrementalCheckpointing：是否开启增量化的进行checkpoints保存public RocksDBStateBackend(String checkpointDataUri, boolean enableIncrementalCheckpointing) throws IOException &#123; this((new Path(checkpointDataUri)).toUri(), enableIncrementalCheckpointing);&#125; Flink 的容错机制一致性检查点 (Checkpoint) Flink 故障恢复机制的核心，就是应用状态的一致性检查点。 有状态流应用的一致检查点，其实就是所有任务的状态，在某个时间点的一份拷贝 (一份快照)；这个时间点，应该是所有任务都恰好处理完一个相同的输入数据的时候。 从检查点恢复状态 在执行流应用程序期间，Flink 会定期保存状态的一致检查点。 如果发生故障， Flink 将会使用最近的检查点来一致恢复应用程序的状态，并重新启动处理流程。 遇到故障之后，第一步就是重启应用： 第二步是从 Checkpoint 中读取状态，将状态重置： 从检查点重新启动应用程序后，其内部状态与检查点完成时的状态完全相同。 第三步，开始消费并处理检查点到发生故障之间的所有数据： 这种检查点的保存和恢复机制可以为应用程序状态提供 “精确一次” (exactly-once) 的一致性，因为所有算子都会保存检查点并恢复其所有状态，这样一来所有的输入流就都会被重置到检查点完成时的位置。 检查点的实现算法 一种简单的想法： 暂停应用，保存状态到检查点，再重新恢复应用。 Flink 的改进实现： 基于 Chandy-Lamport 算法的分布式快照。 将检查点的保存和数据处理分离开，不暂停整个应用。 Flink 检查点算法 Flink 检查点算法的正式名称是异步分界线快照 (asynchronous barrier snapshotting)。 检查点分界线 (Checkpoint Barrier) Flink 的检查点算法用到了一种称为分界线 (barrier) 的特殊数据形式，用来把一条流上数据按照不同的检查点分开。 分界线之前到来的数据导致的状态更改，都会被包含在当前分界线所属的检查点中；而基于分界线之后的数据导致的所有更改，就会被包含在之后的检查点中。 假设现在是一个有两个输入流的应用程序，用并行的两个 Source 任务来读取。 首先，JobManager 会向每个 Source 任务同时发送一条带有新检查点 ID 的消息，通过这种方式来启动检查点。 然后，每个数据源将它们的状态写入检查点，并向后续任务广播发出一个检查点 barrier。状态后端在状态存入检查点之后，会返回通知给 Source 任务，Source 任务就会向 JobManager 确认检查点完成。 之后，每个 Source 发送的 barrier 向下游传递，sum 任务 (下游任务) 会等待所有输入分区的 barrier 到达，这叫分界线对齐。 对于 barrier 已经到达的分区，继续到达的数据会被缓存。 而 barrier 尚未到达的分区，数据会被正常处理。 当收到所有输入分区的 barrier 时，任务就将其状态保存到状态后端的检查点中，然后将 barrier 继续向下游转发。 向下游转发检查点 barrier 后，任务继续正常的数据处理。 Sink 任务收到 barrier 后，也向 JobManager 确认状态保存到 checkpoint 完毕。 至此，当所有任务都确认已成功将状态保存到检查点时，当前这个检查点就真正完成了。 如果检查点操作失败，Flink 可以丢弃该检查点并继续正常执行，因为之后的某一个检查点可能会成功。虽然恢复时间可能更长，但是对于状态的保证依旧很有力。只有在一系列连续的检查点操作失败之后，Flink 才会抛出错误，因为这通常预示着发生了严重且持久的错误。 保存点 (Savepoints) Flink 还提供了可以自定义的镜像保存功能，就是保存点 (savepoints)。 原则上，创建保存点使用的算法与检查点完全相同，因此保存点可以认为就是具有一些额外元数据的检查点。 检查点可以看作是自动存盘，保存点可以看作是手动存盘。 Flink不会自动创建保存点，因此用户 (或者外部调度程序) 必须明确地触发创建操作。 保存点是一个强大的功能。除了故障恢复外，保存点可以用于：有计划的手动备份，更新应用程序，版本迁移，暂停和重启应用，等等。 检查点的使用配置 检查点默认情况下关闭，需要手动设置才能开启使用。 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class StateTest4_FaultTolerance &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.检查点的配置 // 开启检查点，参数表示每隔300毫秒触发一次Checkpoint，默认为500毫秒 env.enableCheckpointing(300L); // 状态一致性的选择 env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // 执行Checkpoint的超时时间，60s内完成，否则丢掉此次的Checkpoint env.getCheckpointConfig().setCheckpointTimeout(60000L); // 最大同时执行Checkpoint的数量，是指前一个Checkpoint还未执行结束，下一个Checkpoint已经开始执行， // 默认为1，即只能前一个Checkpoint执行完成后，下一个Checkpoint才能开始 env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // 两个Checkpoint之间最小的时间间隔，即前一个Checkpoint执行完成时间，与下一个Checkpoint执行开始时间之间的最小间隔 // 可以防止Checkpoint处理过慢，整个集群都在处理Checkpoint，而没有时间处理真正的数据 // 设置最小时间间隔后，则最大同时执行Checkpoint的数量就只能为1 env.getCheckpointConfig().setMinPauseBetweenCheckpoints(100L); // 倾向于使用Checkpoint进行故障回复，即使存在一个更近的Savepoint，默认false env.getCheckpointConfig().setPreferCheckpointForRecovery(false); // 允许Checkpoint失败的次数，默认为0，即Checkpoint执行时如果失败了，会认为是整个任务失败，需要重启 env.getCheckpointConfig().setTolerableCheckpointFailureNumber(0); // 3.重启策略的配置 // env.setRestartStrategy(RestartStrategies.noRestart());// 不重启，若工作失败则直接宣告失败，不启用Checkpoint时的策略 // env.setRestartStrategy(RestartStrategies.fallBackRestart());// 回滚重启，将重启的策略交给上级的资源管理平台 // 固定延迟重启，每隔30s重启一次，尝试3次，超过之后，工作宣告失败。 // 启用Checkpoint后，这是默认的重启策略，尝试重启次数为Integer.MAX_VALUE // env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 30000L)); // 失败率重启，常用，任务失败后重启工作，如果10min内失败次数超过3次，则工作宣告失败，连续两次重启尝试中间隔1min env.setRestartStrategy(RestartStrategies.failureRateRestart(3, Time.minutes(10), Time.minutes(1))); // 4.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 5.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 6.打印 dataStream.print(); // 7.执行任务 env.execute(); &#125;&#125; Flink 的状态一致性状态一致性的概念 当在分布式系统中引入状态时，自然也引入了一致性问题。一致性实际上是 “正确性级别” 的另一种说法，也就是说在成功处理故障并恢复之后得到的结果，与没有发生任何故障时得到的结果相比，前者到底有多正确？举例来说，假设要对最近一小时登录的用户计数。在系统经历故障之后，计数结果是多少？如果有偏差，是有漏掉的计数还是重复计数？ 有状态的流处理，内部每个算子任务都可以有自己的状态 对于流处理器内部来说，所谓的状态一致性，其实就是我们所说的计算结果要保证准确。 一条数据不应该丢失，也不应该重复计算。 在遇到故障时可以恢复状态，恢复以后的重新计算，结果应该也是完全正确的。 状态一致性的级别 在流处理中，一致性可以分为 3 个级别。 AT-MOST-ONCE：最多一次。 当任务故障时，最简单的做法是什么都不干，既不恢复丢失的状态，也不处理丢失的数据。at-most-once 语义的含义是最多处理一次事件。 AT-LEAST-ONCE：至少一次。 在大多数的真实应用场景，我们希望不丢失事件。这种类型的保障称为 at-least-once，意思是所有的事件都得到了处理，但一些事件可能被处理多次。这表示计算结果可能大于正确值，但绝不会小于正确值。 EXACTLY-ONCE：精确一次。 恰好处理一次是最严格的保证，也是最难实现的。恰好处理一次语义不仅仅意味着没有事件丢失，还意味着针对每一个数据，内部状态仅仅更新一次。 曾经，at-least-once 非常流行。第一代流处理器 (如 Storm 和 Samza) 刚问世时，只保证 at-least-once，原因有二： 保证 exactly-once 的系统实现起来更复杂。这在基础架构层 (决定什么代表正确，以及 exactly-once 的范围是什么) 和实现层都很有挑战性。 流处理系统的早期用户愿意接受框架的局限性，并在应用层想办法弥补 (例如使应用程序具有幂等性，或者用批量计算层再做一遍计算)。 最先保证 exactly-once 的系统 (Storm Trident 和 Spark Streaming)，在性能和表现力这两个方面付出了很大的代价。为了保证 exactly-once，这些系统无法单独地对每条记录运用应用逻辑，而是同时处理多条 (一批) 记录，保证对每一批的处理要么全部成功，要么全部失败。这就导致在得到结果前，必须等待一批记录处理结束。因此，用户经常不得不使用两个流处理框架 (一个用来保证 exactly-once，另一个用来对每个元素做低延迟处理)，结果使基础设施更加复杂。曾经，用户不得不在保证 exactly-once 与获得低延迟和效率之间权衡利弊。而 Flink 避免了这种权衡。 Flink 的一个重大价值在于， 它既保证了 exactly-once ，也具有低延迟和高吞吐力的处理能力。从根本上说，Flink 通过使自身满足所有需求来避免权衡，它是业界的一次意义重大的技术飞跃。尽管这在外行看来很神奇，但是一旦了解，就会恍然大悟。 exactly-once 的实现 Flink 使用了一种轻量级快照机制 —— 检查点 (Checkpoint) 来保证 exactly-once 语义。 有状态流应用的一致检查点，其实就是：所有任务的状态，在某个时间点的一份拷贝 (一份快照)。而这个时间点，应该是所有任务都恰好处理完一个相同的输入数据的时候。 应用状态的一致检查点，是 Flink 故障恢复机制的核心。 端到端 (end-to-end) 状态一致性 目前我们看到的一致性保证都是由流处理器实现的，也就是说都是在 Flink 流处理器内部保证的；而在真实应用中，流处理应用除了流处理器以外还包含了数据源 (例如 Kafka) 和输出到持久化系统。 端到端的一致性保证，意味着结果的正确性贯穿了整个流处理应用的始终；每一个组件都需要保证它自己的一致性。 整个端到端的一致性级别取决于所有组件中一致性最弱的组件。 端到端 exactly-once 内部保证 —— Checkpoint 机制。 Source 端 —— 可重设数据的读取位置。 Sink 端 —— 从故障恢复时，数据不会重复写入外部系统。有两种实现方式： 幂等写入 事务写入 幂等写入 (Idempotent Writes) 所谓幂等操作，是说一个操作，可以重复执行很多次，但只导致一次结果更改，也就是说，后面再重复执行就不起作用了。 能够保证最终的结果是正确的，但不能保证中间过程全部正确。当前一个 Checkpoint 结束，后一个 Checkpoint 还未开始时，中间的数据会被处理，也会输出结果，此时如果出现故障，回滚到前一个 Checkpoint 保存的状态，中间的数据又会被处理一次并输出结果。虽然最终的结果只会保留一次，但中间过程会对外界造成一种反复执行的错觉。 事务写入 (Transactional Writes) 事务（Transaction） 应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤消。 具有原子性：一个事务中的一系列的操作要么全部成功，要么一个都不做 实现思想：构建的事务对应着 Flink 的 Checkpoint 机制，等到 Checkpoint 真正完成的时候，才把所有对应的结果写入 Sink 系统中。 有两种实现方式： 预写日志 两阶段提交 预写日志 (Write-Ahead-Log，WAL) 把结果数据先当成状态保存，然后在收到 Checkpoint 完成的通知时，一次性写入 Sink 系统。 简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么 Sink 系统，都能用这种方式一批搞定。 预写日志在特殊情况下不能真正实现 exactly-once，比如数据提交时，不能保证一定是全部提交，可能是提交一半另一半还未提交时，任务出现故障。 DataStream API 提供了一个模板类：GenericWriteAheadSink，来实现这种事务性 Sink。 两阶段提交 (Two-Phase-Commit，2PC) 对于每个 Checkpoint，Sink 任务会启动一个事务，并将接下来所有接收的数据添加到事务里。 然后将这些数据写入外部 Sink 系统，但不提交它们 —— 这时只是 “预提交”。 当它收到 Checkpoint 完成的通知时，它才正式提交事务，实现结果的真正写入。 这种方式真正实现了 exactly-once，它需要一个提供事务支持的外部 Sink 系统。Flink 提供了 TwoPhaseCommitSinkFunction 接口，来实现两阶段提交的 Sink。比如，FlinkKakfaProducer 就继承了这个接口： 12public class FlinkKafkaProducer&lt;IN&gt; extends TwoPhaseCommitSinkFunction&lt;IN, FlinkKafkaProducer.KafkaTransactionState, FlinkKafkaProducer.KafkaTransactionContext&gt; &#123;&#125; 123public abstract class TwoPhaseCommitSinkFunction&lt;IN, TXN, CONTEXT&gt; extends RichSinkFunction&lt;IN&gt; implements CheckpointedFunction, CheckpointListener &#123;&#125; 2PC 对外部 Sink 系统的要求 外部 Sink 系统必须提供事务支持，或者 Sink 任务必须能够模拟外部系统上的事务 在 Checkpoint 的间隔期间里，必须能够开启一个事务并接受数据写入 在收到 Checkpoint 完成的通知之前，事务必须是 “等待提交” 的状态。在故障恢复的情况下，这可能需要一些时间。如果这个时候 Sink 系统关闭事务 (例如超时了)，那么未提交的数据就会丢失。 Sink 任务必须能够在进程失败后恢复事务。 提交事务必须是幂等操作。 不同 Source 和 Sink 的一致性保证 2PC 是最精准，也是最难实现得一种方式。 Flink + Kafka 端到端状态一致性的保证 内部 —— 利用 Checkpoint 机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性。 Source —— Kafka Consumer 作为 Source，可以自动将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性。 Sink —— Kafka Producer 作为 Sink，采用两阶段提交 sink，需要实现一个 TwoPhaseCommitSinkFunction。 Exactly-once 两阶段提交图解 定义一个任务，JobManager 协调各个 TaskManager 进行 Checkpoint 存储。Checkpoint 保存在 StateBackend中，默认 StateBackend 是内存级的，也可以改为文件级的进行持久化保存。 第一步：当 Checkpoint 启动时，JobManager 会将检查点分界线 (barrier) 注入数据流。barrier 会在算子间传递下去。 第二步：每个算子会对当前的状态做个快照，并保存到状态后端。Checkpoint 机制可以保证内部的状态一致性。 第三步：与第二步类似，后续每个内部的 transform 任务遇到 barrier 时，都会先把状态保存到 Checkpoint 里，然后通知 JobManager，barrier 继续向下游传递。最后，数据先到达 Sink 任务，Sink 会把数据写入外部 Kafka，这些数据都属于预提交的事务 TX1；然后，barrier 后到达 Sink 任务，Sink 会先把自己的状态保存到状态后端，并开启一个新的预提交事务 TX2，barrier 后的数据属于这个新的预提交事务。(因为流的并行度可能不为 1，当前的 Sink 任务收到了 barrier，不代表所有的 Sink 任务都收到了，此时，不能正式提交事务) 第四步：当所有算子任务的快照完成，也就是当前的 Checkpoint 完成时，JobManager 会向所有任务发通知，确认当前 Checkpoint 完成。Sink 任务收到确认通知后，会正式提交之前的事务，Kafka 中未确认数据改为 “已确认”。 Exactly-once 两阶段提交步骤 第一条数据来了之后，开启一个 Kafka 的事务 (transaction)，记作 TX1，数据正常写入 Kafka 分区日志，但标记为未提交，这就是 “预提交”； Jobmanager 触发 Checkpoint 操作，barrier 从 Source 开始向下传递，遇到 barrier 的算子将状态存入状态后端，并通知 Jobmanager； Sink 连接器收到 barrier，保存当前状态，存入 Checkpoint，通知 Jobmanager，并开启下一阶段的事务 TX2，用于提交下个检查点前的数据； Jobmanager 收到所有任务的通知，发出确认信息，表示 Checkpoint 完成； Sink 任务收到 Jobmanager 的确认信息，正式提交这段时间的数据，即 TX1 的数据； 外部 Kafka 关闭事务 TX1，提交的数据可以正常消费了。 注意：Kafka 事务默认超时时间 transaction.timeout.ms 设置为 60s，Checkpoint 设置的超时时间不能大于 Kafka 事务的超时时间，否则，可能当 Checkpoint 仍在尝试执行时，Kafka 事务已经到达超时时间关闭了，当 Checkpoint 正常执行完成时，Kafka 之前预提交的数据就会丢失。同时，Kafka 下游的消费者，需要设置隔离级别 isolation.level 为 read_committed (默认为 read_uncommitted），保证 Kafka 未提交的数据不能被消费。 ProcessFunction API (底层 API) 之前学习的转换算子是无法访问事件的时间戳信息和水位线信息的。而这在一些应用场景下，极为重要。例如 MapFunction 这样的 map 转换算子就无法访问时间戳或者当前事件的事件时间。 基于此，DataStream API 提供了一系列的 Low-Level 转换算子。可以访问时间戳、Watermark 以及注册定时事件。还可以输出特定的一些事件，例如超时事件等。Process Function 用来构建事件驱动的应用以及实现自定义的业务逻辑 (使用之前的 Window 函数和转换算子无法实现)。例如，Flink SQL 就是使用 Process Function 实现的。 Flink 提供了 8 个 Process Function： ProcessFunction 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public abstract class ProcessFunction&lt;I, O&gt; extends AbstractRichFunction &#123; private static final long serialVersionUID = 1L; /** * Process one element from the input stream. * * &lt;p&gt;This function can output zero or more elements using the &#123;@link Collector&#125; parameter * and also update internal state or set timers using the &#123;@link Context&#125; parameter. * * @param value The input value. * @param ctx A &#123;@link Context&#125; that allows querying the timestamp of the element and getting * a &#123;@link TimerService&#125; for registering timers and querying the time. The * context is only valid during the invocation of this method, do not store it. * @param out The collector for returning result values. * * @throws Exception This method may throw exceptions. Throwing an exception will cause the operation * to fail and may trigger recovery. */ // I：输入参数 ctx：上下文 O：输出类型 public abstract void processElement(I value, Context ctx, Collector&lt;O&gt; out) throws Exception; /** * Called when a timer set using &#123;@link TimerService&#125; fires. * * @param timestamp The timestamp of the firing timer. * @param ctx An &#123;@link OnTimerContext&#125; that allows querying the timestamp of the firing timer, * querying the &#123;@link TimeDomain&#125; of the firing timer and getting a * &#123;@link TimerService&#125; for registering timers and querying the time. * The context is only valid during the invocation of this method, do not store it. * @param out The collector for returning result values. * * @throws Exception This method may throw exceptions. Throwing an exception will cause the operation * to fail and may trigger recovery. */ public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out) throws Exception &#123;&#125; /** * Information available in an invocation of &#123;@link #processElement(Object, Context, Collector)&#125; * or &#123;@link #onTimer(long, OnTimerContext, Collector)&#125;. */ public abstract class Context &#123; /** * Timestamp of the element currently being processed or timestamp of a firing timer. * * &lt;p&gt;This might be &#123;@code null&#125;, for example if the time characteristic of your program * is set to &#123;@link org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime&#125;. */ public abstract Long timestamp(); /** * A &#123;@link TimerService&#125; for querying time and registering timers. */ public abstract TimerService timerService(); /** * Emits a record to the side output identified by the &#123;@link OutputTag&#125;. * * @param outputTag the &#123;@code OutputTag&#125; that identifies the side output to emit to. * @param value The record to emit. */ public abstract &lt;X&gt; void output(OutputTag&lt;X&gt; outputTag, X value); &#125; /** * Information available in an invocation of &#123;@link #onTimer(long, OnTimerContext, Collector)&#125;. */ public abstract class OnTimerContext extends Context &#123; /** * The &#123;@link TimeDomain&#125; of the firing timer. */ public abstract TimeDomain timeDomain(); &#125;&#125; KeyedProcessFunction CoProcessFunction ProcessJoinFunction BroadcastProcessFunction KeyedBroadcastProcessFunction ProcessWindowFunction ProcessAllWindowFunction KeyedProcessFunction 这里重点介绍 KeyedProcessFunction。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110/** * A keyed function that processes elements of a stream. * * &lt;p&gt;For every element in the input stream &#123;@link #processElement(Object, Context, Collector)&#125; * is invoked. This can produce zero or more elements as output. Implementations can also * query the time and set timers through the provided &#123;@link Context&#125;. For firing timers * &#123;@link #onTimer(long, OnTimerContext, Collector)&#125; will be invoked. This can again produce * zero or more elements as output and register further timers. * * &lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt; Access to keyed state and timers (which are also scoped to a key) is only * available if the &#123;@code KeyedProcessFunction&#125; is applied on a &#123;@code KeyedStream&#125;. * * &lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt; A &#123;@code KeyedProcessFunction&#125; is always a * &#123;@link org.apache.flink.api.common.functions.RichFunction&#125;. Therefore, access to the * &#123;@link org.apache.flink.api.common.functions.RuntimeContext&#125; is always available and setup and * teardown methods can be implemented. See * &#123;@link org.apache.flink.api.common.functions.RichFunction#open(org.apache.flink.configuration.Configuration)&#125; * and &#123;@link org.apache.flink.api.common.functions.RichFunction#close()&#125;. * * @param &lt;K&gt; Type of the key. * @param &lt;I&gt; Type of the input elements. * @param &lt;O&gt; Type of the output elements. */@PublicEvolvingpublic abstract class KeyedProcessFunction&lt;K, I, O&gt; extends AbstractRichFunction &#123; private static final long serialVersionUID = 1L; /** * Process one element from the input stream. * * &lt;p&gt;This function can output zero or more elements using the &#123;@link Collector&#125; parameter * and also update internal state or set timers using the &#123;@link Context&#125; parameter. * * @param value The input value. * @param ctx A &#123;@link Context&#125; that allows querying the timestamp of the element and getting * a &#123;@link TimerService&#125; for registering timers and querying the time. The * context is only valid during the invocation of this method, do not store it. * @param out The collector for returning result values. * * @throws Exception This method may throw exceptions. Throwing an exception will cause the operation * to fail and may trigger recovery. */ public abstract void processElement(I value, Context ctx, Collector&lt;O&gt; out) throws Exception; /** * Called when a timer set using &#123;@link TimerService&#125; fires. * * @param timestamp The timestamp of the firing timer. * @param ctx An &#123;@link OnTimerContext&#125; that allows querying the timestamp, the &#123;@link TimeDomain&#125;, and the key * of the firing timer and getting a &#123;@link TimerService&#125; for registering timers and querying the time. * The context is only valid during the invocation of this method, do not store it. * @param out The collector for returning result values. * * @throws Exception This method may throw exceptions. Throwing an exception will cause the operation * to fail and may trigger recovery. */ // 定时操作 public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out) throws Exception &#123;&#125; /** * Information available in an invocation of &#123;@link #processElement(Object, Context, Collector)&#125; * or &#123;@link #onTimer(long, OnTimerContext, Collector)&#125;. */ public abstract class Context &#123; /** * Timestamp of the element currently being processed or timestamp of a firing timer. * * &lt;p&gt;This might be &#123;@code null&#125;, for example if the time characteristic of your program * is set to &#123;@link org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime&#125;. */ public abstract Long timestamp(); /** * A &#123;@link TimerService&#125; for querying time and registering timers. */ public abstract TimerService timerService(); /** * Emits a record to the side output identified by the &#123;@link OutputTag&#125;. * * @param outputTag the &#123;@code OutputTag&#125; that identifies the side output to emit to. * @param value The record to emit. */ public abstract &lt;X&gt; void output(OutputTag&lt;X&gt; outputTag, X value); /** * Get key of the element being processed. */ public abstract K getCurrentKey(); &#125; /** * Information available in an invocation of &#123;@link #onTimer(long, OnTimerContext, Collector)&#125;. */ public abstract class OnTimerContext extends Context &#123; /** * The &#123;@link TimeDomain&#125; of the firing timer. */ public abstract TimeDomain timeDomain(); /** * Get key of the firing timer. */ @Override public abstract K getCurrentKey(); &#125;&#125; KeyedProcessFunction 用来操作 KeyedStream。KeyedProcessFunction 会处理流的每一个元素，输出为 0 个、1 个或者多个元素。 所有的 Process Function 都继承自RichFunction 接口，所以都有 open()、close() 和 getRuntimeContext() 等方法。而 KeyedProcessFunction&lt;K, I, O&gt; 还额外提供了两个方法: processElement(I value, Context ctx, Collector&lt;O&gt; out)：流中的每一个元素都会调用这个方法，调用结果将会放在 Collector 数据类型中输出。Context 可以访问元素的时间戳，元素的 key，以及 TimerService 时间服务。Context 还可以将结果输出到别的流 (side outputs)。 onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)：是一个回调函数。当之前注册的定时器触发时调用。参数 timestamp 为定时器所设定的触发的时间戳。Collector 为输出结果的集合。OnTimerContext 和 processElement() 的 Context 参数一样，提供了上下文的一些信息，例如定时器触发的时间信息 (事件时间或者处理时间)。 Context 和 OnTimerContext 所持有的 TimerService 对象拥有以下方法： long currentProcessingTime()：返回当前处理时间。 long currentWatermark()：返回当前 Watermark 的时间戳。 void registerProcessingTimeTimer(long timestamp)：会注册当前 key 的 Processing Time 的定时器。当 Processing Time 到达定时时间时，触发 timer。 当定时器 timer 触发时，会执行回调函数 onTimer()。注意定时器 timer 只能在 keyed streams 上面使用。 void registerEventTimeTimer(long timestamp)：会注册当前 key 的 Event Time 定时器。当 Watetmark 大于等于定时器注册的时间时，触发定时器执行回调函数 onTimer()。 void deleteProcessingTimeTimer(long timestamp)：删除之前注册处理时间定时器。如果没有这个时间戳的定时器，则不执行。 void deleteEventTimeTimer(long timestamp)：删除之前注册的事件时间定时器。如果没有此时间戳的定时器，则不执行。 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class ProcessTest1_KeyedProcessFunction &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.测试KeyedProcessFunction，先分组然后自定义处理 dataStream.keyBy(&quot;id&quot;) .process(new MyProcess()) .print(); // 5.执行任务 env.execute(); &#125; // 实现自定义的处理函数 public static class MyProcess extends KeyedProcessFunction&lt;Tuple, SensorReading, Integer&gt; &#123; private ValueState&lt;Long&gt; tsTimerState; @Override public void open(Configuration parameters) throws Exception &#123; tsTimerState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Long&gt;(&quot;ts-timer&quot;, Long.class)); &#125; @Override public void processElement(SensorReading sensorReading, Context ctx, Collector&lt;Integer&gt; out) throws Exception &#123; out.collect(sensorReading.getId().length()); // Context的使用 // 1.获取时间戳 ctx.timestamp(); // 2.获取当前key ctx.getCurrentKey(); // 3.按需将满足某条件的数据输出到侧输出流 OutputTag&lt;Double&gt; outputTag = new OutputTag&lt;Double&gt;(&quot;temp&quot;) &#123; &#125;; ctx.output(outputTag, sensorReading.getTemperature()); // 4.定时服务 ctx.timerService().currentProcessingTime();// 获取当前Processing Time ctx.timerService().currentWatermark();// 获取当前Watermark // 注册Processing Time定时器，参数是要触发时的时间。从1970年1月1日开始的毫秒数，即当前时间基础上延迟1s ctx.timerService().registerProcessingTimeTimer(ctx.timerService().currentProcessingTime() + 1000L); // 保存定时器指定的触发时间状态 tsTimerState.update(ctx.timerService().currentProcessingTime() + 1000L); // 注册Event Time定时器，参数是要触发时的时间。在当前参数时间戳基础上延迟10s ctx.timerService().registerEventTimeTimer((sensorReading.getTimestamp() + 10) * 1000L); // 取消注册的Processing Time定时器，以定时器设定的时间区分 ctx.timerService().deleteProcessingTimeTimer(ctx.timerService().currentProcessingTime() + 1000L); // 删除定时器的操作，可能在定义定时器时间一段时间之后才执行，因此，保存定时器定义时的状态，然后在后续删除时取出这个状态 ctx.timerService().deleteProcessingTimeTimer(tsTimerState.value()); // 取消注册的Event Time定时器，以定时器设定的时间区分 ctx.timerService().deleteEventTimeTimer((sensorReading.getTimestamp() + 10) * 1000L); &#125; // 定时器触发时执行的方法 @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;Integer&gt; out) throws Exception &#123; // timestamp即为触发的定时器的时间 System.out.println(timestamp + &quot;定时器触发&quot;); // ctx和out用法与processElement()类似，ctx多了下面这个方法 // 获取当前的时间语义，PROCESSING_TIME或EVENT_TIME ctx.timeDomain(); &#125; @Override public void close() throws Exception &#123; tsTimerState.clear(); &#125; &#125;&#125; 实例：监控温度传感器的温度值，如果温度值在 10 秒钟之内 (Processing Time) 连续上升，则报警。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class ProcessTest2_ApplicationCase &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.测试KeyedProcessFunction，先分组然后自定义处理 dataStream.keyBy(&quot;id&quot;) .process(new TempConsIncreWarning(10)) .print(); // 5.执行任务 env.execute(); &#125; // 实现自定义处理函数，检测一段时间内的温度连续上升，输出报警 public static class TempConsIncreWarning extends KeyedProcessFunction&lt;Tuple, SensorReading, String&gt; &#123; // 定义私有属性，当前统计的时间间隔 private final Integer interval; public TempConsIncreWarning(Integer interval) &#123; this.interval = interval; &#125; // 定义状态，保存上一次的温度值和定时器时间戳 private ValueState&lt;Double&gt; lastTempState; private ValueState&lt;Long&gt; timerTsState; @Override public void open(Configuration parameters) throws Exception &#123; lastTempState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Double&gt;(&quot;last-temp&quot;, Double.class)); timerTsState = getRuntimeContext().getState(new ValueStateDescriptor&lt;Long&gt;(&quot;timer-ts&quot;, Long.class)); &#125; @Override public void processElement(SensorReading value, Context ctx, Collector&lt;String&gt; out) throws Exception &#123; // 取出状态 Double lastTemp = lastTempState.value(); if (lastTemp != null) &#123; Long timerTs = timerTsState.value(); // 如果温度出现上升并且没有定时器，注册10秒后触发的定时器，开始等待 // 之后的温度如果仍在上升，则不需要额外处理，等待定时器触发即可 if (value.getTemperature() &gt; lastTemp &amp;&amp; timerTs == null) &#123; // 计算定时器触发的时间戳，注意是执行操作算子的本地系统时间，10s时间很快 long ts = ctx.timerService().currentProcessingTime() + interval * 1000L; // 注册定时器 ctx.timerService().registerProcessingTimeTimer(ts); // 保存定时器触发的时间戳，用于删除 timerTsState.update(ts); &#125; else if (value.getTemperature() &lt;= lastTemp &amp;&amp; timerTs != null) &#123; // 如果温度下降，那么删除定时器 ctx.timerService().deleteProcessingTimeTimer(timerTs); timerTsState.clear(); &#125; &#125; // 更新温度状态 lastTempState.update(value.getTemperature()); &#125; @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception &#123; // 如果触发了定时器，说明10s内温度连续上升，输出报警信息 out.collect(&quot;传感器&quot; + ctx.getCurrentKey().getField(0) + &quot;的温度值&quot; + interval + &quot;s内连续秒上升&quot;); // 触发定时器后，清空 timerTsState.clear(); &#125; @Override public void close() throws Exception &#123; lastTempState.clear(); &#125; &#125;&#125; 输入参数： 12345678xisun@DESKTOP-OM8IACS:/mnt/c/Users/Ziyoo$ nc -lk 7777sensor_1,1547718199,35.8sensor_1,1547718199,35sensor_1,1547718199,36.7sensor_1,1547718199,37.2sensor_6,1547718199,36.7sensor_6,1547718199,35.7sensor_6,1547718199,34.7 sensor_1 报警信息是从 35 度开始计算的，sensor_6 不会报警。 输出结果： 1234log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.5&gt; 传感器sensor_1温度值10s内连续秒上升 侧输出流 (SideOutput) 大部分的 DataStream API 的算子的输出是单一输出，也就是某种数据类型的流。除了 split 算子，可以将一条流分成多条流，但这些流的数据类型也都相同。ProcessFunction 的 side outputs 功能可以产生多条流，并且这些流的数据类型可以不一样。一个 side output 可以定义为 OutputTag[X] 对象，X 是输出流的数据类型。ProcessFunction 可以通过 Context 对象发射一个事件到一个或者多个 side outputs。 实例：监控传感器温度值，将温度值低于 30 度的数据输出到 side output。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839public class ProcessTest3_SideOuptCase &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从Scoket文本流读取数据 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 定义一个OutputTag，用来表示侧输出流低温流 OutputTag&lt;SensorReading&gt; lowTempTag = new OutputTag&lt;SensorReading&gt;(&quot;lowTemp&quot;) &#123; &#125;; // 4.测试ProcessFunction，自定义侧输出流实现分流操作 SingleOutputStreamOperator&lt;SensorReading&gt; highTempStream = dataStream.process(new ProcessFunction&lt;SensorReading, SensorReading&gt;() &#123; @Override public void processElement(SensorReading sensorReading, Context ctx, Collector&lt;SensorReading&gt; out) throws Exception &#123; // 判断温度，大于30度，高温流输出到主流；小于低温流输出到侧输出流 if (sensorReading.getTemperature() &gt; 30) &#123; out.collect(sensorReading); &#125; else &#123; ctx.output(lowTempTag, sensorReading); &#125; &#125; &#125;); // 5.打印 highTempStream.print(&quot;high-temp&quot;); highTempStream.getSideOutput(lowTempTag).print(&quot;low-temp&quot;); // 6.执行任务 env.execute(); &#125;&#125; 输入参数： 12345xisun@DESKTOP-OM8IACS:/mnt/c/Users/Ziyoo$ nc -lk 7777sensor_1,1547718199,35.8sensor_1,1547718199,28.7sensor_6,1547718213,15.3sensor_10,1547718213,38.3 输出结果： 1234567log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.high-temp:2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;low-temp:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=28.7&#125;low-temp:4&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718213, temperature=15.3&#125;high-temp:5&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718213, temperature=38.3&#125; 与 id 无关，来一条数据判断一次，然后输出结果。 CoProcessFunction 对于两条输入流，DataStream API 提供了 CoProcessFunction 这样的 low-level 操作。CoProcessFunction 提供了操作每一个输入流的方法：processElement1() 和 processElement2()。 类似于 ProcessFunction，这两种方法都通过 Context 对象来调用。这个 Context 对象可以访问事件数据，定时器时间戳，TimerService，以及 side outputs。CoProcessFunction 也提供了 onTimer() 回调函数。 Table API 与 SQL 说明：本章节示例的很多方法已过时，此处只做学习用，具体使用时再做更改。 Table API 和 Flink SQL 是什么 Flink 对批处理和流处理，提供了统一的上层 API。 Table API 是一套内嵌在 Java 和 Scala 语言中的查询 API，它允许以非常直观的方式组合来自一些关系运算符的查询。 Flink 的 SQL 支持基于实现了 SQL 标准的 Apache Calcite。 Table API 是流处理和批处理通用的关系型 API，Table API 可以基于流输入或者批输入来运行而不需要进行任何修改。Table API 是 SQL 语言的超集并专门为 Apache Flink 设计的，Table API 是 Scala 和 Java 语言集成式的 API。与常规 SQL 语言中将查询指定为字符串不同，Table API 查询是以 Java 或 Scala 中的语言嵌入样式来定义的，具有 IDE 支持如自动完成和语法检测。 Maven 引入依赖： 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 实例： 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940public class TableTest1_Example &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.读取数据 DataStreamSource&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;); // 3.转换成POJO DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 5.基于流创建一张表，表中的字段就是SensorReading的属性 Table dataTable = tableEnv.fromDataStream(dataStream); // 6-1.调用Table API进行转换操作，选取表中指定数据 Table resultTable = dataTable.select(&quot;id, temperature&quot;) .where(&quot;id = &#x27;sensor_1&#x27;&quot;); // 6-2.执行SQL进行查询操作，这种方式与用Table API是等价的 tableEnv.createTemporaryView(&quot;sensor&quot;, dataTable);// 需要先将表注册，sensor就是表明 String sql = &quot;select id, temperature from sensor where id = &#x27;sensor_1&#x27;&quot;;// 从注册的表中查询 Table resultSqlTable = tableEnv.sqlQuery(sql);// 执行sql // 7.将6-1和6-2的结果转换成流，然后打印 // Row.class，org.apache.flink.types.Row，指定查询的结果按行输出 // 也可以使用自定义的类，如SensorReading.class，但要求该类是public的，且查询的结果和类的属性要能对应 tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;); tableEnv.toAppendStream(resultSqlTable, Row.class).print(&quot;sql&quot;); // 8.执行任务 env.execute(); &#125;&#125; 输入参数： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 输出结果： 1234567891011log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.result&gt; sensor_1,35.8sql&gt; sensor_1,35.8result&gt; sensor_1,36.3sql&gt; sensor_1,36.3result&gt; sensor_1,34.7sql&gt; sensor_1,34.7result&gt; sensor_1,33.1sql&gt; sensor_1,33.1 程序实现的基本结构 Table API 和 SQL 的程序结构，与流式处理的程序结构十分类似： 不同类型 TableEnvironment 的创建 TableEnvironment 是 Flink 中集成 Table API 和 SQL 的核心概念，所有对表的操作都基于 TableEnvironment： 注册 Catalog。 在 Catalog 中注册表。 执行 SQL 查询。 注册用户自定义函数 (UDF)。 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738public class TableTest2_CreateTableEnv &#123; public static void main(String[] args) &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.创建表处理环境，Flink 1.11版本之前，默认使用老版本planner，1.11版本及之后，默认使用Blink版本 // 因此，可以自行添加配置，创建所需要的环境版本 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 3.不同版本环境的创建 // 3-1.基于老版本planner的流处理 EnvironmentSettings oldStreamSettings = EnvironmentSettings.newInstance() .useOldPlanner()// 老版本 .inStreamingMode()// 流式处理 .build(); StreamTableEnvironment oldStreamTableEnv = StreamTableEnvironment.create(env, oldStreamSettings); // 3-2.基于老版本planner的批处理 ExecutionEnvironment batchEnv = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment oldBatchTableEnv = BatchTableEnvironment.create(batchEnv); // 3-3.基于Blink的流处理(即新版本) EnvironmentSettings blinkStreamSettings = EnvironmentSettings.newInstance() .useBlinkPlanner()// 新版本 .inStreamingMode()// 流式处理 .build(); StreamTableEnvironment blinkStreamTableEnv = StreamTableEnvironment.create(env, blinkStreamSettings); // 3-4.基于Blink的批处理(即新版本) EnvironmentSettings blinkBatchSettings = EnvironmentSettings.newInstance() .useBlinkPlanner()// 新版本 .inBatchMode()// 批处理 .build(); TableEnvironment blinkBatchTableEnv = TableEnvironment.create(blinkBatchSettings); &#125;&#125; 表 (Table) TableEnvironment 可以注册目录 Catalog，并可以基于 Catalog 注册表。 表 (Table) 是由一个 “标识符” (identifier) 来指定的，由 3 部分组成：Catalog 名、数据库 (database) 名和对象名。 Catalog 名、数据库 (database) 名如果不指定，默认为 defaultCatalog 和 defaultDatabase。 表可以是常规的，也可以是虚拟的 (视图，View)。 连接到外部系统 (比如 Kafka) 的叫常规表，如果是 Flink 程序转换过程中临时创建的，是虚拟表。 常规表 (Table) 一般可以用来描述外部数据，比如文件、数据库表或消息队列的数据，也可以直接从 DataStream 转换而来。 视图 (View) 可以从现有的表中创建，通常是 Table API 或者 SQL 查询的一个结果集。 更新模式 对于流式查询，需要声明如何在表和外部连接器之间执行转换。 与外部系统交换的消息类型，由更新模式 (Update Mode) 指定： 追加 (Append) 模式 表只做插入操作，和外部连接器只交换插入 (Insert) 消息。 撤回 (Retract) 模式 表和外部连接器交换添加 (Add) 和撤回 (Retract) 消息。 插入操作 (Insert) 编码为 Add 消息；删除操作 (Delete) 编码为 Retract 消息；更新操作 (Update) 编码为上一条的 Retract 消息和下一条的 Add 消息。 更新插入 (Upsert) 模式 更新和插入操作都被编码为 Upsert 消息 (需要指定 key，key 存在为更新操作，不存在为插入操作)；删除操作编码为 Delete 消息。 从文件创建表 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class TableTest3_CommonApi &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 3.表的创建：连接外部系统，读取数据 // 3-1.读取外部文件创建表 String inputPath = &quot;src/main/resources/sensor.txt&quot;; tableEnv.connect(new FileSystem().path(inputPath))// 连接器的描述器 .withFormat(new Csv())// 格式化，也可以是json .withSchema(new Schema() .field(&quot;id&quot;, DataTypes.STRING()) .field(&quot;timestamp&quot;, DataTypes.BIGINT()) .field(&quot;temp&quot;, DataTypes.DOUBLE()) )// 定义表结构，参考数据库表结构的定义，注意：表结构的顺序，应和文件内字段顺序一一对应，字段名可以不相同 .createTemporaryTable(&quot;inputTable&quot;);// 注册表 // 3-2.获取注册的表 Table inputTable = tableEnv.from(&quot;inputTable&quot;); inputTable.printSchema();// 打印表结构 tableEnv.toAppendStream(inputTable, Row.class).print();// 打印表数据 // 4.查询转换 // 4-1.Table API的操作 // 简单转换 Table resultTable = inputTable.select(&quot;id, temp&quot;)// 查询id和temp字段 .filter(&quot;id === &#x27;sensor_6&#x27;&quot;);// 查询id为sensor_6的数据 // 聚合统计 Table aggTable = inputTable.groupBy(&quot;id&quot;) .select(&quot;id, id.count as count, temp.avg as avgTemp&quot;); // 4-2.SQL的操作 tableEnv.sqlQuery(&quot;select id, temp from inputTable where id = &#x27;senosr_6&#x27;&quot;); Table sqlAggTable = tableEnv.sqlQuery(&quot;select id, count(id) as cnt, avg(temp) as avgTemp from inputTable group by id&quot;); // 5.打印输出 tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;);// 来一条数据，resultTable追加一条数据 tableEnv.toRetractStream(aggTable, Row.class).print(&quot;agg&quot;);// 来一条数据，aggTable更新一次结果 tableEnv.toRetractStream(sqlAggTable, Row.class).print(&quot;sqlagg&quot;); // 6.执行任务 env.execute(); &#125;&#125; Csv (org.apache.flink.table.descriptors.Csv) 需要添加依赖： 123456&lt;!-- 新版本CSV支持 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-csv&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; Table API 是集成在 Scala 和 Java 语言内的查询 API。 Table API 基于代表 “表” 的 Table 类，并提供一整套操作处理的方法 API；这些方法会返回一个新的 Table 对象，表示对输入表应用转换操作的结果。 有些关系型转换操作，可以由多个方法调用组成，构成链式调用结构。 Flink 的 SQL 集成，基于实现 了SQL 标准的 Apache Calcite。 在 Flink 中，用常规字符串来定义 SQL 查询语句。 SQL 查询的结果，也是一个新的 Table。 输入文件： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 src/main/resources/sensor.txt 输出结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849root |-- id: STRING |-- timestamp: BIGINT |-- temp: DOUBLElog4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1result&gt; sensor_6,15.4sensor_1,1547718206,36.3sensor_1,1547718210,34.7result&gt; sensor_6,15.3sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3sqlagg&gt; (true,sensor_1,1,35.8)agg&gt; (true,sensor_1,1,35.8)sqlagg&gt; (true,sensor_6,1,15.4)agg&gt; (true,sensor_6,1,15.4)sqlagg&gt; (true,sensor_7,1,6.7)agg&gt; (true,sensor_7,1,6.7)sqlagg&gt; (true,sensor_10,1,38.1)agg&gt; (true,sensor_10,1,38.1)sqlagg&gt; (false,sensor_1,1,35.8)sqlagg&gt; (true,sensor_1,2,36.05)agg&gt; (false,sensor_1,1,35.8)agg&gt; (true,sensor_1,2,36.05)sqlagg&gt; (false,sensor_1,2,36.05)sqlagg&gt; (true,sensor_1,3,35.6)agg&gt; (false,sensor_1,2,36.05)sqlagg&gt; (false,sensor_1,3,35.6)sqlagg&gt; (true,sensor_1,4,34.975)agg&gt; (true,sensor_1,3,35.6)sqlagg&gt; (false,sensor_6,1,15.4)agg&gt; (false,sensor_1,3,35.6)sqlagg&gt; (true,sensor_6,2,15.350000000000001)agg&gt; (true,sensor_1,4,34.975)sqlagg&gt; (false,sensor_7,1,6.7)agg&gt; (false,sensor_6,1,15.4)sqlagg&gt; (true,sensor_7,2,6.5)agg&gt; (true,sensor_6,2,15.350000000000001)agg&gt; (false,sensor_7,1,6.7)agg&gt; (true,sensor_7,2,6.5)Process finished with exit code 0 表结构： 1234root |-- id: STRING |-- timestamp: BIGINT |-- temp: DOUBLE 表数据： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 resultTable： 12result&gt; sensor_6,15.4result&gt; sensor_6,15.3 aggTable： 1234567891011121314agg&gt; (true,sensor_1,1,35.8)agg&gt; (true,sensor_6,1,15.4)agg&gt; (true,sensor_7,1,6.7)agg&gt; (true,sensor_10,1,38.1)agg&gt; (false,sensor_1,1,35.8)agg&gt; (true,sensor_1,2,36.05) // 2表示的是当前id数据的个数agg&gt; (false,sensor_1,2,36.05)agg&gt; (true,sensor_1,3,35.6)agg&gt; (false,sensor_1,3,35.6)agg&gt; (true,sensor_1,4,34.975)agg&gt; (false,sensor_6,1,15.4)agg&gt; (true,sensor_6,2,15.350000000000001)agg&gt; (false,sensor_7,1,6.7)agg&gt; (true,sensor_7,2,6.5) 可以看出：对于同一个 id，来一条新数据时，会做一次聚合操作，并撤销前一次的聚合结果 (如 agg&gt; (false,sensor_1,1,35.8))，再添加聚合后的新结果 (如 agg&gt; (true,sensor_1,2,36.05))。聚合操作是把新数据的温度值，与之前的聚合结果重新聚合 (此处是取平均值)。 sqlAggTable： 1234567891011121314sqlagg&gt; (true,sensor_1,1,35.8)sqlagg&gt; (true,sensor_6,1,15.4)sqlagg&gt; (true,sensor_7,1,6.7)sqlagg&gt; (true,sensor_10,1,38.1)sqlagg&gt; (false,sensor_1,1,35.8)sqlagg&gt; (true,sensor_1,2,36.05)sqlagg&gt; (false,sensor_1,2,36.05)sqlagg&gt; (true,sensor_1,3,35.6)sqlagg&gt; (false,sensor_1,3,35.6)sqlagg&gt; (true,sensor_1,4,34.975)sqlagg&gt; (false,sensor_6,1,15.4)sqlagg&gt; (true,sensor_6,2,15.350000000000001)sqlagg&gt; (false,sensor_7,1,6.7)sqlagg&gt; (true,sensor_7,2,6.5) sqlAggTable 与 aggTable 效果一样。 输出表到文件 表的输出，是通过将数据写入 TableSink 来实现的。 TableSink 是一个通用接口，可以支持不同的文件格式、存储数据库和消息队列。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class TableTest4_FileOutput &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 3.表的创建：连接外部系统，读取数据 // 3-1.读取外部文件创建表 String inputPath = &quot;src/main/resources/sensor.txt&quot;; tableEnv.connect(new FileSystem().path(inputPath))// 连接器的描述器 .withFormat(new Csv())// 格式化，也可以是json .withSchema(new Schema() .field(&quot;id&quot;, DataTypes.STRING()) .field(&quot;timestamp&quot;, DataTypes.BIGINT()) .field(&quot;temp&quot;, DataTypes.DOUBLE()) )// 定义表结构，参考数据库表结构的定义，注意：表结构的顺序，应和文件内字段顺序一一对应 .createTemporaryTable(&quot;inputTable&quot;);// 注册表 // 3-2.获取注册的表 Table inputTable = tableEnv.from(&quot;inputTable&quot;); // 4.查询转换 // 4-1.Table API的操作 // 简单转换 Table resultTable = inputTable.select(&quot;id, temp&quot;)// 查询id和temp字段 .filter(&quot;id === &#x27;sensor_6&#x27;&quot;);// 查询id为sensor_6的数据 // 聚合统计 Table aggTable = inputTable.groupBy(&quot;id&quot;) .select(&quot;id, id.count as count, temp.avg as avgTemp&quot;); // 4-2.SQL的操作 tableEnv.sqlQuery(&quot;select id, temp from inputTable where id = &#x27;senosr_6&#x27;&quot;); Table sqlAggTable = tableEnv.sqlQuery(&quot;select id, count(id) as cnt, avg(temp) as avgTemp from inputTable group by id&quot;); // 5.输出到文件 // 5-1.连接外部文件，注册输出表 String outputPath = &quot;src/main/resources/out.txt&quot;; tableEnv.connect(new FileSystem().path(outputPath))// 连接器的描述器 .withFormat(new Csv())// 格式化 .withSchema(new Schema() .field(&quot;id&quot;, DataTypes.STRING()) .field(&quot;temperature&quot;, DataTypes.DOUBLE()) )// 定义表结构，注意：表结构的内容，应和前面创建的表的字段的顺序一一对应，字段名可以不相同 .createTemporaryTable(&quot;outputTable&quot;);// 注册表 // 5-2.写出到外部文件 // 注意：aggTable或sqlAggTable不支持往外部文件写入，因为外部文件写入只能追加数据， // 而aggTable或sqlAggTable存在数据更新，无法操控文件把旧数据删除再更新为新数据 resultTable.executeInsert(&quot;outputTable&quot;); // 7.执行任务，使用executeInsert()后，不需要再次执行execute() // env.execute(); &#125;&#125; 输入文件： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 src/main/resources/sensor.txt 输出文件： 12sensor_6,15.4sensor_6,15.3 src/main/resources/out.txt 对接 Kafka 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class TableTest4_KafkaPipeLine &#123; public static void main(String[] args) &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 3.连接Kafka，读取数据 // 3-1.创建表 tableEnv .connect(new Kafka() .version(&quot;2.3&quot;) .topic(&quot;sensor&quot;) .property(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;) .property(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;) ) .withFormat(new Csv()) .withSchema(new Schema() .field(&quot;id&quot;, DataTypes.STRING()) .field(&quot;timestamp&quot;, DataTypes.BIGINT()) .field(&quot;temp&quot;, DataTypes.DOUBLE()) ) .createTemporaryTable(&quot;inputTable&quot;); // 3-2.获取注册的表 Table sensorTable = tableEnv.from(&quot;inputTable&quot;); // 4.查询转换 // 4-1.简单转换 Table resultTable = sensorTable.select(&quot;id, temp&quot;) .filter(&quot;id === &#x27;sensor_6&#x27;&quot;); // 4-2.聚合统计 Table aggTable = sensorTable.groupBy(&quot;id&quot;) .select(&quot;id, id.count as count, temp.avg as avgTemp&quot;); // 5.建立Kafka连接，输出到不同的topic下 // 5-1.创建表 tableEnv .connect(new Kafka() .version(&quot;2.3&quot;) .topic(&quot;sinkTest&quot;) .property(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;) .property(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;) ) .withFormat(new Csv()) .withSchema(new Schema() .field(&quot;id&quot;, DataTypes.STRING()) .field(&quot;temp&quot;, DataTypes.DOUBLE()) ) .createTemporaryTable(&quot;outputTable&quot;); // 5-2.输出到Kafka // 注意：aggTable或sqlAggTable也不支持往Kafka写入 resultTable.executeInsert(&quot;outputTable&quot;); &#125;&#125; 对接 ES 代码实现： 1234567891011121314151617tableEnv .connect( new Elasticsearch() .version(&quot;6&quot;) .host(&quot;localhost&quot;, 9200, &quot;http&quot;) .index(&quot;sensor&quot;) .documentType(&quot;temp&quot;) ) .inUpsertMode() .withFormat(new Json()) .withSchema(new Schema() .field(&quot;id&quot;, DataTypes.STRING()) .field(&quot;count&quot;, DataTypes.BIGINT()) ) .createTemporaryTable(&quot;esOutputTable&quot;);aggTable.executeInsert(&quot;esOutputTable&quot;); Json (org.apache.flink.table.descriptors.Json) 需要添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-json&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 对接 MySQL 引入 MySQL 连接器依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-jdbc_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;1.10.3&lt;/version&gt;&lt;/dependency&gt; 代码实现： 123456789101112131415String sinkDDL = &quot;create table jdbcOutputTable (&quot; + &quot; id varchar(20) not null, &quot; + &quot; cnt bigint not null &quot; + &quot;) with (&quot; + &quot; &#x27;connector.type&#x27; = &#x27;jdbc&#x27;, &quot; + &quot; &#x27;connector.url&#x27; = &#x27;jdbc:mysql://localhost:3306/test&#x27;, &quot; + &quot; &#x27;connector.table&#x27; = &#x27;sensor_count&#x27;, &quot; + &quot; &#x27;connector.driver&#x27; = &#x27;com.mysql.jdbc.Driver&#x27;, &quot; + &quot; &#x27;connector.username&#x27; = &#x27;root&#x27;, &quot; + &quot; &#x27;connector.password&#x27; = &#x27;123456&#x27; )&quot;;tableEnv.sqlUpdate(sinkDDL);// 执行DDL创建表aggResultSqlTable.insertInto(&quot;jdbcOutputTable&quot;); 将表转换成 DataStream 表可以转换为 DataStream 或 DataSet ，这样自定义流处理或批处理程序就可以继续在 Table API 或 SQL 查询的结果上运行了。 将表转换为 DataStream 或 DataSet 时，需要指定生成的数据类型，即要将表的每一行转换成的数据类型。 表作为流式查询的结果，是动态更新的。 转换有两种转换模式：追加 (Append) 模式和撤回 (Retract) 模式。 追加模式 (Append Mode) 用于表只会被插入 (Insert) 操作更改的场景。比如： 1DataStream&lt;Row&gt; resultStream = tableEnv.toAppendStream(resultTable, Row.class); 撤回模式 (Retract Mode) 用于任何场景。类似于更新模式中的 Retract 模式，它只有 Insert 和 Delete 两类操作。 得到的数据会增加一个 Boolean 类型的标识位 (返回的第一个字段)，用它来表示到底是新增的数据 (Insert)，还是被删除的数据 (Delete)。比如： 1DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; aggResultStream = tableEnv.toRetractStream(aggResultTable, Row.class); 将 DataStream 转换成表 对于一个 DataStream，可以直接转换成 Table，进而方便地调用 Table API 做转换操作。 12DataStream&lt;SensorReading&gt; dataStream = ...Table sensorTable = tableEnv.fromDataStream(dataStream); 默认转换后的 Table schema 和 DataStream 中的字段定义一一对应，也可以单独指定出来。 12DataStream&lt;SensorReading&gt; dataStream = ...Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp as ts, temperature&quot;); 创建临时视图 (Temporary View) 基于 DataStream 创建临时视图： 12tableEnv.createTemporaryView(&quot;sensorView&quot;, dataStream);tableEnv.createTemporaryView(&quot;sensorView&quot;, dataStream, &quot;id, temperature, timestamp as ts&quot;); 基于 Table 创建临时视图： 1tableEnv.createTemporaryView(&quot;sensorView&quot;, sensorTable); 查看执行计划 Table API 提供了一种机制来解释计算表的逻辑和优化查询计划。 查看执行计划，可以通过 TableEnvironment.explain(table) 方法或 TableEnvironment.explain() 方法完成： 12String explaination = tableEnv.explain(resultTable);System.out.println(explaination); 方法返回一个字符串，描述三个计划： 优化的逻辑查询计划 优化后的逻辑查询计划 实际执行计划 流处理和关系代数的区别 关系代数 (表)/SQL 流处理 处理的数据对象 字段元组的有界集合 字段元组的无限序列 查询 (Query) 对数据的访问 可以访问到完整的数据输入 无法访问所有数据，必须持续 “等待” 流式输入 查询终止条件 生成固定大小的结果集后终止 永不停止，根据持续收到的数据不断更新查询结果 动态表 (Dynamic Tables) 动态表是 Flink 对流数据的 Table API 和 SQL 支持的核心概念。 与表示批处理数据的静态表不同，动态表是随时间变化的。 动态表可以像静态的批处理表一样进行查询，查询一个动态表会产生持续查询 (Continuous Query)。 持续查询永远不会终止，并会生成另一个动态结果表。 持续查询会不断更新其动态结果表，以反映其动态输入表上的更改。 流式表查询 (持续查询) 的处理过程： 第一步：流被转换为动态表。 第二步：对动态表计算连续查询，生成新的动态表。 第三步：生成的动态表被转换回流。 将流转换成动态表 为了处理带有关系查询的流，必须先将其转换为表。 从概念上讲，流的每个数据记录，都被解释为对结果表的插入 (Insert) 修改操作： 持续查询 持续查询会在动态表上做计算处理，并作为结果生成新的动态表： 将动态表转换成 DataStream 与常规的数据库表一样，动态表可以通过插入 (Insert)、更新 (Update) 和删除 (Delete) 操作，进行持续的修改。 将动态表转换为流或将其写入外部系统时，需要对这些操作进行编码。 仅追加 (Append-only) 流 仅通过插入 (Insert) 更改来修改的动态表，可以直接转换为仅追加流。 撤回 (Retract) 流 撤回流是包含两类消息的流：添加 (Add) 消息和撤回 (Retract) 消息。 更新插入 (Upsert) 流 Upsert 流也包含两种类型的消息：Upsert 消息和删除 (Delete) 消息。 时间特性 (Time Attributes) 基于时间的操作 (比如 Table API 和 SQL 中窗口操作)，需要定义相关的时间语义和时间数据来源的信息。 Table 可以提供一个逻辑上的时间字段，用于在表处理程序中，指示时间和访问相应的时间戳。 时间属性，可以是每个表 Schema 的一部分。一旦定义了时间属性，它就可以作为一个字段引用，并且可以在基于时间的操作中使用。 时间属性的行为类似于常规时间戳，可以访问，并且进行计算。 定义处理时间 (Processing Time) 处理时间语义下，允许表处理程序根据机器的本地时间生成结果。它是时间的最简单概念。它既不需要提取时间戳，也不需要生成 Watermark。 定义处理时间，有三种方法。 方式一：由 DataStream 转换成表时指定。 1Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, temperature, timestamp, pt.proctime&quot;); 在定义 Schema 时，可以使用 .proctime，指定字段名 (如示例中的 pt 字段) 定义处理时间字段。 这个 proctime 属性只能通过附加逻辑字段，来扩展物理 Schema。因此，只能在 Schema 定义的末尾定义它。 代码实现： 1234567891011121314151617181920212223242526272829public class TableTest6_TimeAndWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2. 读入文件数据，得到DataStream DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;); // 3. 转换成POJO DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 5. 将流转换成表，并定义时间特性，字段名pt自定义，不可与sql关键字冲突，pt是时间戳类型，精确到毫秒 Table dataTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp as ts, temperature as temp, pt.proctime&quot;); // 6.转换成流输出 dataTable.printSchema(); tableEnv.toAppendStream(dataTable, Row.class).print(); // 7.执行任务 env.execute(); &#125;&#125; 输入参数： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 输出结果： 1234567891011121314151617181920log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.root |-- id: STRING |-- ts: BIGINT |-- temp: DOUBLE |-- pt: TIMESTAMP(3) *PROCTIME*sensor_1,1547718199,35.8,2021-08-20T02:40:18.998sensor_6,1547718201,15.4,2021-08-20T02:40:19.002sensor_7,1547718202,6.7,2021-08-20T02:40:19.002sensor_10,1547718205,38.1,2021-08-20T02:40:19.002sensor_1,1547718206,36.3,2021-08-20T02:40:19.002sensor_1,1547718210,34.7,2021-08-20T02:40:19.002sensor_1,1547718212,33.1,2021-08-20T02:40:19.002sensor_6,1547718212,15.3,2021-08-20T02:40:19.002sensor_7,1547718212,6.3,2021-08-20T02:40:19.003Process finished with exit code 0 方式二：定义 Table Schema 时指定。 1234567.withSchema(new Schema() .field(&quot;id&quot;, DataTypes.STRING()) .field(&quot;timestamp&quot;, DataTypes.BIGINT()) .field(&quot;temperature&quot;, DataTypes.DOUBLE()) .field(&quot;pt&quot;, DataTypes.TIMESTAMP(3)) .proctime()) 方式三：在创建表的 DDL 中定义。 1234567891011String sinkDDL = &quot;create table dataTable (&quot; + &quot; id varchar(20) not null, &quot; + &quot; ts bigint, &quot; + &quot; temperature double, &quot; + &quot; pt AS PROCTIME() &quot; + &quot;) with (&quot; + &quot; &#x27;connector.type&#x27; = &#x27;filesystem&#x27;, &quot; + &quot; &#x27;connector.path&#x27; = &#x27;/sensor.txt&#x27;, &quot; + &quot; &#x27;format.type&#x27; = &#x27;csv&#x27;)&quot;;tableEnv.sqlUpdate(sinkDDL); 定义事件时间 (Event Time) 事件时间语义下，允许表处理程序根据每个记录中包含的时间生成结果。这样即使在有乱序事件或者延迟事件时，也可以获得正确的结果。 为了处理无序事件，并区分流中的准时和迟到事件；Flink 需要从事件数据中，提取时间戳，并用来推进事件时间的进展。 定义事件时间，同样有三种方法。 方式一：由 DataStream 转换成表时指定。 12345// 将DataStream转换为Table，并指定时间字段Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime, temperature&quot;);// 或者，直接追加时间字段Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, temperature, timestamp, rt.rowtime&quot;); 在 DataStream 转换成 Table，使用 .rowtime 可以定义事件时间属性。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class TableTest6_TimeAndWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 指定事件时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 2. 读入文件数据，得到DataStream DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;); // 3. 转换成POJO，并设置Watermark为2s延迟 DataStream&lt;SensorReading&gt; dataStream = inputStream .map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;) .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(2)) &#123; @Override public long extractTimestamp(SensorReading sensorReading) &#123; return sensorReading.getTimestamp() * 1000L; &#125; &#125;); // 4.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 5. 将流转换成表 // 方式一：指定数据的timestamp属性为事件时间，数据的timestamp属性会被覆盖重写 Table dataTable1 = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime as ts, temperature as temp&quot;); // 方式二：追加一个新的字段，数据的原有属性不做变化 Table dataTable2 = tableEnv.fromDataStream(dataStream, &quot;id, temperature, timestamp, rt.rowtime&quot;); // 6.转换成流输出 dataTable1.printSchema(); tableEnv.toAppendStream(dataTable1, Row.class).print(); dataTable2.printSchema(); tableEnv.toAppendStream(dataTable2, Row.class).print(); // 7.执行任务 env.execute(); &#125;&#125; 输入参数： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 输出结果： 覆盖数据的属性： 12345678910111213141516171819log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.root |-- id: STRING |-- ts: TIMESTAMP(3) *ROWTIME* |-- temp: DOUBLEsensor_1,2019-01-17T09:43:19,35.8sensor_6,2019-01-17T09:43:21,15.4sensor_7,2019-01-17T09:43:22,6.7sensor_10,2019-01-17T09:43:25,38.1sensor_1,2019-01-17T09:43:26,36.3sensor_1,2019-01-17T09:43:30,34.7sensor_1,2019-01-17T09:43:32,33.1sensor_6,2019-01-17T09:43:32,15.3sensor_7,2019-01-17T09:43:32,6.3Process finished with exit code 0 追加新的属性： 1234567891011121314151617181920log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.root |-- id: STRING |-- temperature: DOUBLE |-- timestamp: BIGINT |-- rt: TIMESTAMP(3) *ROWTIME*sensor_1,35.8,1547718199,2019-01-17T09:43:19sensor_6,15.4,1547718201,2019-01-17T09:43:21sensor_7,6.7,1547718202,2019-01-17T09:43:22sensor_10,38.1,1547718205,2019-01-17T09:43:25sensor_1,36.3,1547718206,2019-01-17T09:43:26sensor_1,34.7,1547718210,2019-01-17T09:43:30sensor_1,33.1,1547718212,2019-01-17T09:43:32sensor_6,15.3,1547718212,2019-01-17T09:43:32sensor_7,6.3,1547718212,2019-01-17T09:43:32Process finished with exit code 0 方式二：定义 Table Schema 时指定。 12345678910.withSchema(new Schema() .field(&quot;id&quot;, DataTypes.STRING()) .field(&quot;timestamp&quot;, DataTypes.BIGINT()) .rowtime( new Rowtime() .timestampsFromField(&quot;timestamp&quot;)// 从字段中提取时间戳 .watermarksPeriodicBounded(1000)// Watermark延迟1秒 ) .field(&quot;temperature&quot;, DataTypes.DOUBLE())) 方式三：在创建表的 DDL 中定义。 123456789101112String sinkDDL = &quot;create table dataTable (&quot; + &quot; id varchar(20) not null, &quot; + &quot; ts bigint, &quot; + &quot; temperature double, &quot; + &quot; rt AS TO_TIMESTAMP( FROM_UNIXTIME(ts) ), &quot; + &quot; watermark for rt as rt - interval &#x27;1&#x27; second&quot; + &quot;) with (&quot; + &quot; &#x27;connector.type&#x27; = &#x27;filesystem&#x27;, &quot; + &quot; &#x27;connector.path&#x27; = &#x27;/sensor.txt&#x27;, &quot; + &quot; &#x27;format.type&#x27; = &#x27;csv&#x27;)&quot;;tableEnv.sqlUpdate(sinkDDL); 窗口 时间语义，要配合窗口操作才能发挥作用。 在 Table API 和 SQL 中，主要有两种窗口： Group Windows (分组窗口) 根据时间或行计数间隔，将行聚合到有限的组 (Group) 中，并对每个组的数据执行一次聚合函数。 Over Windows 针对每个输入行，计算相邻行范围内的聚合。 Group Windows Group Windows 是使用 .window(w:GroupWindow)子句定义的，并且必须由 as 为子句指定一个别名。 为了按窗口对表进行分组，窗口的别名必须在 group by 子句中，像常规的分组字段一样引用。 1234Table table = input .window([w: GroupWindow] as &quot;w&quot;)// 定义窗口，别名为w .groupBy(&quot;w, a&quot;)// 按照字段a和窗口w分组 .select(&quot;a, b.sum&quot;);// 聚合 Table API 提供了一组具有特定语义的预定义 Window 类，这些类会被转换为底层 DataStream 或 DataSet 的窗口操作。 滚动窗口 (Tumbling windows) 滚动窗口要用 Tumble 类来定义： 12345678// Tumbling Event-time Window.window(Tumble.over(&quot;10.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))// 10min的滚动窗口，事件时间 // Tumbling Processing-time Window.window(Tumble.over(&quot;10.minutes&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))// 10min的滚动窗口，处理时间 // Tumbling Row-count Window.window(Tumble.over(&quot;10.rows&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))// 计数窗口，10个数据一行，处理时间 示例： 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class TableTest6_TimeAndWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 指定事件时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 2. 读入文件数据，得到DataStream DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;); // 3. 转换成POJO，并设置Watermark为2s延迟 DataStream&lt;SensorReading&gt; dataStream = inputStream .map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;) .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(2)) &#123; @Override public long extractTimestamp(SensorReading sensorReading) &#123; return sensorReading.getTimestamp() * 1000L; &#125; &#125;); // 4.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 5. 将流转换成表 // 方式一：指定数据的timestamp属性为事件时间，数据的timestamp属性会被覆盖重写// Table dataTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime as ts, temperature as temp&quot;); // 方式二：追加一个新的字段 Table dataTable = tableEnv.fromDataStream(dataStream, &quot;id, temperature as temp, timestamp as ts, rt.rowtime&quot;); tableEnv.createTemporaryView(&quot;sensor&quot;, dataTable);// 注册表，在SQL中用 // 6.窗口操作 // 6-1.Group Window // Table API写法 Table resultTable = dataTable.window(Tumble.over(&quot;10.seconds&quot;).on(&quot;rt&quot;).as(&quot;tw&quot;)) .groupBy(&quot;id, tw&quot;) .select(&quot;id, id.count, temp.avg, tw.end&quot;);// id, 当前id数据个数，温度平均值，开窗结束时间 // SQL写法 Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, count(id) as cnt, avg(temp) as avgTemp, tumble_end(rt, interval &#x27;10&#x27; second) &quot; + &quot;from sensor group by id, tumble(rt, interval &#x27;10&#x27; second)&quot;); // 7.转换成流输出 tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;); tableEnv.toAppendStream(resultSqlTable, Row.class).print(&quot;sql&quot;); // 8.执行任务 env.execute(); &#125;&#125; 输入参数： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 输出结果： 123456789101112131415161718192021log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.result&gt; sensor_1,1,35.8,2019-01-17T09:43:20result&gt; sensor_6,1,15.4,2019-01-17T09:43:30sql&gt; sensor_1,1,35.8,2019-01-17T09:43:20result&gt; sensor_10,1,38.1,2019-01-17T09:43:30sql&gt; sensor_6,1,15.4,2019-01-17T09:43:30result&gt; sensor_1,1,36.3,2019-01-17T09:43:30result&gt; sensor_7,1,6.7,2019-01-17T09:43:30result&gt; sensor_7,1,6.3,2019-01-17T09:43:40result&gt; sensor_6,1,15.3,2019-01-17T09:43:40result&gt; sensor_1,2,33.900000000000006,2019-01-17T09:43:40sql&gt; sensor_10,1,38.1,2019-01-17T09:43:30sql&gt; sensor_1,1,36.3,2019-01-17T09:43:30sql&gt; sensor_7,1,6.7,2019-01-17T09:43:30sql&gt; sensor_7,1,6.3,2019-01-17T09:43:40sql&gt; sensor_6,1,15.3,2019-01-17T09:43:40sql&gt; sensor_1,2,33.900000000000006,2019-01-17T09:43:40Process finished with exit code 0 滑动窗口 (Sliding windows) 滑动窗口要用 Slide 类来定义： 12345678// Sliding Event-time Window.window(Slide.over(&quot;10.minutes&quot;).every(&quot;5.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))// 10min的滑动窗口，滑动步长5min，事件时间 // Sliding Processing-time window.window(Slide.over(&quot;10.minutes&quot;).every(&quot;5.minutes&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))// 10min的滑动窗口，滑动步长5min，处理时间 // Sliding Row-count window.window(Slide.over(&quot;10.rows&quot;).every(&quot;5.rows&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))// 计数窗口，10个数据一行，5个数据一滑动，处理时间 会话窗口（Session windows） 会话窗口要用 Session 类来定义： 12345// Session Event-time Window.window(Session.withGap(&quot;10.minutes&quot;).on(&quot;rowtime&quot;).as(&quot;w&quot;))// Session时长10min，事件时间 // Session Processing-time Window.window(Session.withGap(&quot;10.minutes&quot;).on(&quot;proctime&quot;).as(&quot;w&quot;))// Session时长10min，处理时间 SQL 中的 Group Windows Group Windows 定义在 SQL 查询的 Group By 子句中。 TUMBLE(time_attr, interval) 定义一个滚动窗口，第一个参数是时间字段，第二个参数是窗口长度。 HOP(time_attr, interval, interval) 定义一个滑动窗口，第一个参数是时间字段，第二4 个参数是窗口滑动步长，第三个是窗口长度。 SESSION(time_attr, interval) 定义一个会话窗口，第一个参数是时间字段，第二个参数是窗口间隔。 Over Windows Over Windows 聚合是标准 SQL 中已有的 (over 子句)，可以在查询的 SELECT 子句中定义。 Over Windows 聚合，会针对每个输入行，计算相邻行范围内的聚合。 Over Windows 使用 .window(w:overwindows) 子句定义，并在 select() 中通过别名来引用： 123Table table = input .window([w: OverWindow] as &quot;w&quot;)// 定义窗口，别名为w .select(&quot;a, b.sum over w, c.min over w&quot;);// 聚合 Table API 提供了 Over 类，来配置 Over 窗口的属性。 无界 Over Windows 可以在事件时间或处理时间，以及指定为时间间隔、或行计数的范围内，定义 Over Windows。 无界的 Over Windows 是使用常量指定的： 1234567891011// 无界的事件时间Over Window.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding(UNBOUNDED_RANGE).as(&quot;w&quot;))// 无界的处理时间Over Window.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding(UNBOUNDED_RANGE).as(&quot;w&quot;))// 无界的事件时间Row-count Over Window.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding(UNBOUNDED_ROW).as(&quot;w&quot;))// 无界的处理时间Row-count Over Window.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding(UNBOUNDED_ROW).as(&quot;w&quot;)) 有界 Over Windows 有界的 Over Windows 是用间隔的大小指定的： 1234567891011// 有界的事件时间Over Window.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding(&quot;1.minutes&quot;).as(&quot;w&quot;))// 有界的处理时间Over Window.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;proctime&quot;).preceding(&quot;1.minutes&quot;).as(&quot;w&quot;))// 有界的事件时间Row-count Over Window.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;rowtime&quot;).preceding(&quot;10.rows&quot;).as(&quot;w&quot;))// 有界的处理时间Row-count Over Window.window(Over.partitionBy(&quot;a&quot;).orderBy(&quot;procime&quot;).preceding(&quot;10.rows&quot;).as(&quot;w&quot;)) 示例： 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class TableTest6_TimeAndWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 指定事件时间语义 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); // 2. 读入文件数据，得到DataStream DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;); // 3. 转换成POJO，并设置Watermark为2s延迟 DataStream&lt;SensorReading&gt; dataStream = inputStream .map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;) .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(2)) &#123; @Override public long extractTimestamp(SensorReading sensorReading) &#123; return sensorReading.getTimestamp() * 1000L; &#125; &#125;); // 4.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 5. 将流转换成表 // 方式一：指定数据的timestamp属性为事件时间，数据的timestamp属性会被覆盖重写// Table dataTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp.rowtime as ts, temperature as temp&quot;); // 方式二：追加一个新的字段 Table dataTable = tableEnv.fromDataStream(dataStream, &quot;id, temperature as temp, timestamp as ts, rt.rowtime&quot;); tableEnv.createTemporaryView(&quot;sensor&quot;, dataTable);// 注册表，在SQL中用 // 6.窗口操作 // 6-2.Over Window // Table API写法 Table overResultTable = dataTable.window(Over.partitionBy(&quot;id&quot;).orderBy(&quot;rt&quot;).preceding(&quot;2.rows&quot;).as(&quot;ow&quot;)) .select(&quot;id, rt, id.count over ow, temp.avg over ow&quot;); // SQL写法 Table overSqlResultTable = tableEnv.sqlQuery(&quot;select id, rt, count(id) over ow, avg(temp) over ow &quot; + &quot; from sensor &quot; + &quot; window ow as (partition by id order by rt rows between 2 preceding and current row)&quot;); // 7.转换成流输出 tableEnv.toAppendStream(overResultTable, Row.class).print(&quot;result&quot;); tableEnv.toAppendStream(overSqlResultTable, Row.class).print(&quot;sql&quot;); // 8.执行任务 env.execute(); &#125;&#125; 输入参数： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 输出结果： 1234567891011121314151617181920212223log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.result&gt; sensor_1,2019-01-17T09:43:19,1,35.8sql&gt; sensor_1,2019-01-17T09:43:19,1,35.8sql&gt; sensor_6,2019-01-17T09:43:21,1,15.4result&gt; sensor_6,2019-01-17T09:43:21,1,15.4sql&gt; sensor_7,2019-01-17T09:43:22,1,6.7result&gt; sensor_7,2019-01-17T09:43:22,1,6.7sql&gt; sensor_10,2019-01-17T09:43:25,1,38.1result&gt; sensor_10,2019-01-17T09:43:25,1,38.1sql&gt; sensor_1,2019-01-17T09:43:26,2,36.05result&gt; sensor_1,2019-01-17T09:43:26,2,36.05sql&gt; sensor_1,2019-01-17T09:43:30,3,35.6result&gt; sensor_1,2019-01-17T09:43:30,3,35.6result&gt; sensor_7,2019-01-17T09:43:32,2,6.5sql&gt; sensor_7,2019-01-17T09:43:32,2,6.5result&gt; sensor_1,2019-01-17T09:43:32,3,34.699999999999996sql&gt; sensor_1,2019-01-17T09:43:32,3,34.699999999999996result&gt; sensor_6,2019-01-17T09:43:32,2,15.350000000000001sql&gt; sensor_6,2019-01-17T09:43:32,2,15.350000000000001Process finished with exit code SQL 中的 Over Windows12345SELECT COUNT(amount) OVER ( PARTITION BY user ORDER BY proctime ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)FROM Orders 用 Over 做窗口聚合时，所有聚合必须在同一窗口上定义，也就是说必须是相同的分区、排序和范围。 目前仅支持在当前行范围之前的窗口。 ORDER BY 必须在单一的时间属性上指定。 函数 (Functions) Flink Table API 和 SQL 为用户提供了一组用于数据转换的内置函数。 SQL 中支持的很多函数，Table API 和 SQL 都已经做了实现： Table API SQL 比较函数 ANY1 === ANY2ANY1 &gt; ANY2 value1 = value2value1 &gt; value2 逻辑函数 `BOOLEAN1 算数函数 NUMERIC1 + NUMERIC2NUMERIC1.power(NUMERIC2) numeric1 + numeric2POWER(numeric1, numeric2) 字符串函数 STRING1 + STRING2STRING.upperCase()STRING.charLength() `string1 时间函数 STRING.toDateSTRING.toTimestampcurrentTime()NUMERIC.daysNUMERIC.minutes DATE stringTIMESTAMP stringCURRENT_TIMEINTERVAL string range 聚合函数 FIELD.countFIELD.sum0 COUNT(*)SUM(expression)RANK()ROW_NUMBER() 用户自定义函数 (UDF) 用户定义函数 (User-defined Functions，UDF) 是一个重要的特性，它们显著地扩展了查询的表达能力。 在大多数情况下，用户定义的函数必须先注册，然后才能在查询中使用。 函数通过调用 registerFunction() 在 TableEnvironment 中注册。当用户定义的函数被注册时，它被插入到 TableEnvironment 的函数目录中，这样 Table API 或 SQL 解析器就可以识别并正确地解释它。 标量函数 (Scalar Functions) 用户定义的标量函数，可以将 0、1 或多个标量值，映射到新的标量值。 为了定义标量函数，必须在 org.apache.flink.table.functions 中扩展基类 Scalar Function，并实现 (一个或多个) 求值 (eval) 方法。 标量函数的行为由求值方法决定，求值方法必须公开声明并命名为 eval()。 1234567891011public static class HashCode extends ScalarFunction &#123; private int factor = 13; public HashCode(int factor) &#123; this.factor = factor; &#125; public int eval(String str) &#123; return str.hashCode() * factor; &#125;&#125; 实例： 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class UdfTest1_ScalarFunction &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.读取数据 DataStreamSource&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;); // 3.转换成POJO DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 5.将流转换成表 Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp as ts, temperature as temp&quot;); // 6.自定义标量函数，实现求id的hash值 // 6-1.需要在环境中注册UDF HashCode hashCode = new HashCode(23);// 创建实例 tableEnv.registerFunction(&quot;hashCode&quot;, hashCode); // 6-2.Table API写法 Table resultTable = sensorTable.select(&quot;id, ts, hashCode(id)&quot;); // 6-3.SQL写法 tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable); Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, ts, hashCode(id) from sensor&quot;); // 7.打印输出 tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;); tableEnv.toAppendStream(resultSqlTable, Row.class).print(&quot;sql&quot;); // 8.执行任务 env.execute(); &#125; // 实现自定义的ScalarFunction public static class HashCode extends ScalarFunction &#123; private int factor = 13; public HashCode(int factor) &#123; this.factor = factor; &#125; // 必须是public，方法名必须叫eval，其他按需自定义 public int eval(String str) &#123; return str.hashCode() * factor; &#125; &#125;&#125; 输入参数： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 输出结果： 1234567891011121314151617181920212223log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.result&gt; sensor_1,1547718199,-1036124876sql&gt; sensor_1,1547718199,-1036124876result&gt; sensor_6,1547718201,-1036124761sql&gt; sensor_6,1547718201,-1036124761result&gt; sensor_7,1547718202,-1036124738sql&gt; sensor_7,1547718202,-1036124738result&gt; sensor_10,1547718205,-2055098980sql&gt; sensor_10,1547718205,-2055098980result&gt; sensor_1,1547718206,-1036124876sql&gt; sensor_1,1547718206,-1036124876result&gt; sensor_1,1547718210,-1036124876sql&gt; sensor_1,1547718210,-1036124876result&gt; sensor_1,1547718212,-1036124876sql&gt; sensor_1,1547718212,-1036124876result&gt; sensor_6,1547718212,-1036124761sql&gt; sensor_6,1547718212,-1036124761result&gt; sensor_7,1547718212,-1036124738sql&gt; sensor_7,1547718212,-1036124738Process finished with exit code 0 表函数 (Table Functions) 用户定义的表函数，也可以将 0、1 或多个标量值作为输入参数；与标量函数不同的是，它可以返回任意数量的行作为输出，而不是单个值。 为了定义一个表函数，必须扩展 org.apache.flink.table.functions 中的基类 TableFunction 并实现 (一个或多个) 求值方法。 表函数的行为由其求值方法决定，求值方法必须是 public 的，并命名为 eval()。 12345678910111213public static class Split extends TableFunction&lt;Tuple2&lt;String, Integer&gt;&gt; &#123; private String separator = &quot;,&quot;; public Split(String separator) &#123; this.separator = separator; &#125; public void eval(String str) &#123; for (String s : str.split(separator)) &#123; collect(new Tuple2&lt;&gt;(s, s.length())); &#125; &#125;&#125; 实例： 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class UdfTest2_TableFunction &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.读取数据 DataStreamSource&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;); // 3.转换成POJO DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 5.将流转换成表 Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp as ts, temperature as temp&quot;); // 6.自定义表函数，实现将id拆分，并输出(word, length) // 6-1.需要在环境中注册UDF Split split = new Split(&quot;_&quot;); tableEnv.registerFunction(&quot;split&quot;, split); // 6-2.Table API写法 Table resultTable = sensorTable .joinLateral(&quot;split(id) as (word, length)&quot;) .select(&quot;id, ts, word, length&quot;); // 6-3.SQL写法 tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable); Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, ts, word, length &quot; + &quot; from sensor, lateral table(split(id)) as splitid(word, length)&quot;); // 7.打印输出 tableEnv.toAppendStream(resultTable, Row.class).print(&quot;result&quot;); tableEnv.toAppendStream(resultSqlTable, Row.class).print(&quot;sql&quot;); // 8.执行任务 env.execute(); &#125; // 实现自定义TableFunction public static class Split extends TableFunction&lt;Tuple2&lt;String, Integer&gt;&gt; &#123; // 定义属性，分隔符 private String separator = &quot;,&quot;; public Split(String separator) &#123; this.separator = separator; &#125; // 必须实现一个eval方法，没有返回值 public void eval(String str) &#123; for (String s : str.split(separator)) &#123; collect(new Tuple2&lt;&gt;(s, s.length())); &#125; &#125; &#125;&#125;java 输入参数： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 输出结果： 1234567891011121314151617181920212223242526272829303132333435363738394041log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.result&gt; sensor_1,1547718199,sensor,6result&gt; sensor_1,1547718199,1,1sql&gt; sensor_1,1547718199,sensor,6sql&gt; sensor_1,1547718199,1,1result&gt; sensor_6,1547718201,sensor,6result&gt; sensor_6,1547718201,6,1sql&gt; sensor_6,1547718201,sensor,6sql&gt; sensor_6,1547718201,6,1result&gt; sensor_7,1547718202,sensor,6result&gt; sensor_7,1547718202,7,1sql&gt; sensor_7,1547718202,sensor,6sql&gt; sensor_7,1547718202,7,1result&gt; sensor_10,1547718205,sensor,6result&gt; sensor_10,1547718205,10,2sql&gt; sensor_10,1547718205,sensor,6sql&gt; sensor_10,1547718205,10,2result&gt; sensor_1,1547718206,sensor,6result&gt; sensor_1,1547718206,1,1sql&gt; sensor_1,1547718206,sensor,6sql&gt; sensor_1,1547718206,1,1result&gt; sensor_1,1547718210,sensor,6result&gt; sensor_1,1547718210,1,1sql&gt; sensor_1,1547718210,sensor,6sql&gt; sensor_1,1547718210,1,1result&gt; sensor_1,1547718212,sensor,6result&gt; sensor_1,1547718212,1,1sql&gt; sensor_1,1547718212,sensor,6sql&gt; sensor_1,1547718212,1,1result&gt; sensor_6,1547718212,sensor,6result&gt; sensor_6,1547718212,6,1sql&gt; sensor_6,1547718212,sensor,6sql&gt; sensor_6,1547718212,6,1result&gt; sensor_7,1547718212,sensor,6result&gt; sensor_7,1547718212,7,1sql&gt; sensor_7,1547718212,sensor,6sql&gt; sensor_7,1547718212,7,1Process finished with exit code 0 聚合函数 (Aggregate Functions) 用户自定义的聚合函数 (User-Defined Aggregate Functions，UDAGGs)，可以把一个表中的数据，聚合成一个标量值。 用户自定义的聚合函数，是通过继承 AggregateFunction 抽象类实现的。 AggregationFunction 要求必须实现的方法： createAccumulator()：创建一个空累加器。 accumulate()：更新累加器。 getValue()：获取最终结果。 AggregateFunction 的工作原理如下： 首先，它需要一个累加器 (Accumulator)，用来保存聚合中间结果的数据结构。通过调用 createAccumulator() 创建空累加器。 随后，对每个输入行调用函数的 accumulate() 来更新累加器。 处理完所有行后，将调用函数的 getValue() 来计算并返回最终结果。 实例： 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class UdfTest3_AggregateFunction &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.读取数据 DataStreamSource&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;); // 3.转换成POJO DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.创建表处理环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 5.将流转换成表 Table sensorTable = tableEnv.fromDataStream(dataStream, &quot;id, timestamp as ts, temperature as temp&quot;); // 6.自定义聚合函数，求当前传感器的平均温度值 // 6-1.需要在环境中注册UDF AvgTemp avgTemp = new AvgTemp(); tableEnv.registerFunction(&quot;avgTemp&quot;, avgTemp); // 6-2.Table API写法 Table resultTable = sensorTable .groupBy(&quot;id&quot;) .aggregate(&quot;avgTemp(temp) as avgtemp&quot;) .select(&quot;id, avgtemp&quot;); // 6-3.SQL写法 tableEnv.createTemporaryView(&quot;sensor&quot;, sensorTable); Table resultSqlTable = tableEnv.sqlQuery(&quot;select id, avgTemp(temp) &quot; + &quot; from sensor group by id&quot;); // 7.打印输出 tableEnv.toRetractStream(resultTable, Row.class).print(&quot;result&quot;); tableEnv.toRetractStream(resultSqlTable, Row.class).print(&quot;sql&quot;); // 8.执行任务 env.execute(); &#125; // 实现自定义的AggregateFunction public static class AvgTemp extends AggregateFunction&lt;Double, Tuple2&lt;Double, Integer&gt;&gt; &#123; @Override public Double getValue(Tuple2&lt;Double, Integer&gt; accumulator) &#123; return accumulator.f0 / accumulator.f1; &#125; @Override public Tuple2&lt;Double, Integer&gt; createAccumulator() &#123; return new Tuple2&lt;&gt;(0.0, 0); &#125; // 必须实现一个accumulate方法，来数据之后更新状态 public void accumulate(Tuple2&lt;Double, Integer&gt; accumulator, Double temp) &#123; accumulator.f0 += temp; accumulator.f1 += 1; &#125; &#125;&#125; 输入参数： 123456789sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718206,36.3sensor_1,1547718210,34.7sensor_1,1547718212,33.1sensor_6,1547718212,15.3sensor_7,1547718212,6.3 输出结果： 123456789101112131415161718192021222324252627282930313233log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.typeutils.TypeExtractor).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.sql&gt; (true,sensor_1,35.8)result&gt; (true,sensor_1,35.8)sql&gt; (true,sensor_6,15.4)result&gt; (true,sensor_6,15.4)sql&gt; (true,sensor_7,6.7)result&gt; (true,sensor_7,6.7)sql&gt; (true,sensor_10,38.1)result&gt; (true,sensor_10,38.1)sql&gt; (false,sensor_1,35.8)result&gt; (false,sensor_1,35.8)sql&gt; (true,sensor_1,36.05)result&gt; (true,sensor_1,36.05)sql&gt; (false,sensor_1,36.05)sql&gt; (true,sensor_1,35.6)result&gt; (false,sensor_1,36.05)sql&gt; (false,sensor_1,35.6)sql&gt; (true,sensor_1,34.975)result&gt; (true,sensor_1,35.6)sql&gt; (false,sensor_6,15.4)result&gt; (false,sensor_1,35.6)sql&gt; (true,sensor_6,15.350000000000001)result&gt; (true,sensor_1,34.975)sql&gt; (false,sensor_7,6.7)sql&gt; (true,sensor_7,6.5)result&gt; (false,sensor_6,15.4)result&gt; (true,sensor_6,15.350000000000001)result&gt; (false,sensor_7,6.7)result&gt; (true,sensor_7,6.5)Process finished with exit code 0 表聚合函数 (Table Aggregate Functions) 用户自定义的表聚合函数 (User-Defined Table Aggregate Functions，UDTAGGs)，可以把一个表中数据，聚合为具有多行和多列的结果表。 用户自定义的表聚合函数，是通过继承 TableAggregateFunction 抽象类实现的。 TableAggregateFunction 要求必须实现的方法： createAccumulator()：创建一个空累加器。 accumulate()：更新累加器。 emitValue()：获取最终结果。 TableAggregateFunction 的工作原理如下: 首先，它同样需要一个累加器 (Accumulator)，用来保存聚合中间结果的数据结构。通过调用 createAccumulator() 创建空累加器。 随后，对每个输入行调用函数的 accumulate() 来更新累加器。 处理完所有行后，将调用函数的 emitValue() 来计算并返回最终结果。 本文参考https://www.bilibili.com/video/BV1qy4y1q728 https://ashiamd.github.io/docsify-notes/#/study/BigData/Flink/%E5%B0%9A%E7%A1%85%E8%B0%B7Flink%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0?id=_1-flink%e7%9a%84%e7%89%b9%e7%82%b9 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}]},{"title":"Spring 之 WebFlux","slug":"spring-webflux","date":"2021-04-23T07:35:11.000Z","updated":"2021-07-07T08:55:01.084Z","comments":true,"path":"2021/04/23/spring-webflux/","link":"","permalink":"http://example.com/2021/04/23/spring-webflux/","excerpt":"","text":"Spring WebFlux 介绍 官方文档：https://docs.spring.io/spring-framework/docs/5.2.7.RELEASE/spring-framework-reference/web-reactive.html#spring-webflux Spring WebFlux 是 Spring5 添加的新模块，用于 Web 开发，功能和 Spring MVC 类似的，底层实现不同。 Spring WebFlux 是契合使用响应式编程而出现的框架。 传统的 Web 框架，比如 Spring MVC、Struts2 等，是基于 Servlet 容器运行的。Spring WebFlux 是一种异步非阻塞的框架，异步非阻塞的框架在 Servlet3.1 以后才支持，其核心是基于 Reactor 的相关 API 实现的。 异步非阻塞： 异步和同步针对调用者：调用者发送请求，如果等着对方回应之后才去做其他事情就是同步，如果发送请求之后不等着对方回应就去做其他事情就是异步。 阻塞和非阻塞针对被调用者：被调用者收到请求时，如果做完请求任务之后才给出反馈就是阻塞，如果收到请求之后马上给出反馈，然后再去做任务就是非阻塞。 阻塞需要等待，非阻塞不需要等待。 Spring WebFlux 的特点： 非阻塞式：能够在有限的资源下，提高系统的吞吐量和伸缩性，从而处理更多的请求。Spring WebFlux 是以 Reactor 为基础来实现的响应式编程框架。 函数式编程：Spring5 框架基于 Java8，Spring Webflux 能够使用 Java8 的函数式编程方式来实现路由请求。 Spring WebFlux 和 Spring MVC 的对比： 两个框架都可以使用注解方式操作，也都可以运行在 Tomcat 等容器中。 Spring MVC 采用命令式编程，Spring WebFlux 采用异步响应式编程。 响应式编程概述 响应式编程是一种面向数据流和变化传播的编程范式。这意味着可以在编程语言中很方便地表达静态或动态的数据流，而相关的计算模型会自动将变化的值通过数据流进行传播。 例如，对于 a = b + c 这个表达式的处理，在命令式编程中，会先计算 b + c 的结果，再把此结果赋值给变量 a，因此 b，c 两值的变化不会对变量 a 产生影响。但在响应式编程中，变量 a 的值会随时跟随 b，c 的变化而变化。 电子表格程序就是响应式编程的一个例子。单元格可以包含字面值或类似 “= B1 + C1” 的公式，而包含公式的单元格的值会依据其他单元格的值的变化而变化。 Java8 及其之前版本的实现方式： 本质上使用的是观察者设计模式。 Java8 提供的观察者模式的两个类 Observer 和 Observable： 12345678910111213141516171819202122public class ObserverDemo extends Observable &#123; public static void main(String[] args) &#123; ObserverDemo observer = new ObserverDemo(); // 添加观察者 observer.addObserver(new Observer() &#123; @Override public void update(Observable o, Object arg) &#123; System.out.println(&quot;发生了变化&quot;); &#125; &#125;); observer.addObserver(new Observer() &#123; @Override public void update(Observable o, Object arg) &#123; System.out.println(&quot;收到被观察者通知，准备改变&quot;); &#125; &#125;); observer.setChanged();// 监控数据是否发生变化 observer.notifyObservers();// 通知 &#125;&#125; Java9 及之后的版本，使用 Flow 类替换了 Observer 和 Observable。 1234567891011121314151617181920212223242526272829303132public class Test &#123; public static void main(String[] args) &#123; Flow.Publisher&lt;String&gt; publisher = subscriber -&gt; &#123; subscriber.onNext(&quot;1&quot;);// 1 subscriber.onNext(&quot;2&quot;); subscriber.onError(new RuntimeException(&quot;出错&quot;));// 2 // subscriber.onComplete(); &#125;; publisher.subscribe(new Flow.Subscriber&lt;&gt;() &#123; @Override public void onSubscribe(Flow.Subscription subscription) &#123; subscription.cancel(); &#125; @Override public void onNext(String item) &#123; System.out.println(item); &#125; @Override public void onError(Throwable throwable) &#123; System.out.println(&quot;出错了&quot;); &#125; @Override public void onComplete() &#123; System.out.println(&quot;publish complete&quot;); &#125; &#125;); &#125;&#125; Reactor 实现。 响应式编程操作中，都需要满足 Reactive 规范，Reactor 即为这样的一个框架，WebFlux 的核心即是使用 Reactor 实现的。 Reactor 有两个核心类，Mono 和 Flux ，这两个类都实现了 Publisher 接口，提供了丰富的操作符。 Flux 对象实现发布者时，返回 N 个元素；Mono 实现发布者时，返回 0 或者 1 个元素。 Flux 和 Mono 都是数据流的发布者，使用 Flux 和 Mono 都可以发出三种数据信号：元素值、错误信号、完成信号。 错误信号和完成信号都代表终止信号，终止信号用于告诉订阅者数据流结束了。 错误信号在终止数据流的同时，会把错误信息传递给订阅者。 错误信号和完成信号不能共存。 如果没有发送任何元素值，而是直接发送错误信号或者完成信号，表示是空数据流。 如果既没有错误信号，也没有完成信号，表示是无限数据流。 代码演示 Flux 和 和 Mono： 第一步：引入依赖。 12345&lt;dependency&gt; &lt;groupId&gt;io.projectreactor&lt;/groupId&gt; &lt;artifactId&gt;reactor-core&lt;/artifactId&gt; &lt;version&gt;3.3.9.RELEASE&lt;/version&gt;&lt;/dependency&gt; 第二步：声明数据流，有以下几种方式。 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; // just方法直接声明数据流，此时没有订阅，数据是不会输出的 Flux.just(1, 2, 3, 4, 5); Mono.just(1); // 其他方法声明数据流 Integer[] arr = &#123;1, 2, 3, 4, 5&#125;; Flux.fromArray(arr);// 来自数组 List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(5); Flux.fromIterable(list);// 来自集合 Stream&lt;Integer&gt; stream = list.stream(); Flux.fromStream(stream);// 来自流 &#125;&#125; 第三步：订阅。调用 just() 或者其他方法只是声明数据流，数据流并没有发出，只有进行订阅之后才会触发数据流，不订阅什么都不会发生。 1234567891011public class Test &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(5); list.add(1); list.add(2); list.add(3); list.add(4); list.add(5); Flux.fromIterable(list).subscribe(System.out::print); &#125;&#125; 常用操作符： 对数据流进行一道道操作，称为操作符，比如工厂流水线。 **map()**：将数据流中的每一个元素，按一定的规则映射为新元素。 **flatmap()**：将数据流中的每一个元素，按一定的规则转换成流，然后再把所有的流合并为一个整体的流。 **filter()**：将数据流中的元素，按一定的规则进行筛选。 **zip()**：将数据流中的元素，按一定的规则进行压缩。 Spring WebFlux 的执行流程和核心 API Spring WebFlux 基于 Reactor，默认使用的容器是 Netty，Netty 是一个高性能的异步非阻塞的 NIO 框架。 BIO：阻塞方式。 NIO：非阻塞方式。 Channel：通道；Register：注册；Selector：选择器。 Spring WebFlux 执行过程和 Spring MVC 相似。 Spring MVC 的核心控制器是 DispatcherServlet，Spring WebFlux 的核心控制器是 DispatcherHandler，DispatcherHandler 实现了 WebHandler 接口，重写了 handle()： 12345678910public interface WebHandler &#123; /** * Handle the web server exchange. * @param exchange the current server exchange * @return &#123;@code Mono&lt;Void&gt;&#125; to indicate when request handling is complete */ Mono&lt;Void&gt; handle(ServerWebExchange exchange);&#125; 123456789101112@Overridepublic Mono&lt;Void&gt; handle(ServerWebExchange exchange) &#123; if (this.handlerMappings == null) &#123; return createNotFoundError(); &#125; return Flux.fromIterable(this.handlerMappings) .concatMap(mapping -&gt; mapping.getHandler(exchange)) .next() .switchIfEmpty(createNotFoundError()) .flatMap(handler -&gt; invokeHandler(exchange, handler)) .flatMap(result -&gt; handleResult(exchange, result));&#125; exchange：放 http 请求响应的信息。 mapping.getHandler(exchange)：根据 http 请求地址获得其对应的 handlerMapping。 invokeHandler(exchange, handler)：调用具体的业务方法处理 http 请求。 handleResult(exchange, result))：返回处理的结果。 Spring WebFlux 除了 DispatcherHandler 组件外，还有其他几个重要的组件： DispatcherHandler：负责请求的处理。 HandlerMapping：负责查询请求对应的处理的方法。 HandlerAdapter：负责请求处理的实际的业务。 HandlerResultHandler：负责响应结果的处理。 Spring WebFlux 实现函数式编程，依赖于两个接口：RouterFunction (负责路由处理) 和 HandlerFunction (负责处理函数)。 Spring WebFlux 实现 Spring MVC 方式实现，是同步阻塞的方式，基于 Spring MVC + Servlet + Tomcat。 Spring WebFlux 方式实现，是异步非阻塞的方式，基于 Spring WebFlux + Reactor + Netty。 基于注解编程模型 第一步：创建 Spring Boot 工程，引入 Spring WebFlux 依赖。 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;cn.xisun.spring.webflux&lt;/groupId&gt; &lt;artifactId&gt;xisun-webflux&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;xisun-webflux&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二步：打开 application.properties 配置文件，配置启动端口号。 1server.port=8081 第三步：创建包和相关类。 entity 层： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * @Author XiSun * @Date 2021/4/24 10:58 */public class User &#123; private String name; private String gender; private Integer age; public User() &#123; &#125; public User(String name, String gender, Integer age) &#123; this.name = name; this.gender = gender; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (!Objects.equals(name, user.name)) &#123; return false; &#125; if (!Objects.equals(gender, user.gender)) &#123; return false; &#125; return Objects.equals(age, user.age); &#125; @Override public int hashCode() &#123; int result = name != null ? name.hashCode() : 0; result = 31 * result + (gender != null ? gender.hashCode() : 0); result = 31 * result + (age != null ? age.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, gender=&#x27;&quot; + gender + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; dao 层： 12345678910public interface UserDao &#123; // 根据id查询用户 User getUserById(int id); // 查询所有用户 List&lt;User&gt; getAllUser(); // 添加用户 String saveUser(User user);&#125; 123456789101112131415161718192021222324252627282930313233@Repositorypublic class UserDaoImpl implements UserDao &#123; // 创建map集合存储数据，代替从数据库查询 private final Map&lt;Integer, User&gt; users = new HashMap&lt;&gt;(); &#123; this.users.put(1, new User(&quot;Lucy&quot;, &quot;male&quot;, 20)); this.users.put(2, new User(&quot;Mary&quot;, &quot;female&quot;, 30)); this.users.put(3, new User(&quot;Jack&quot;, &quot;male&quot;, 50)); &#125; @Override public User getUserById(int id) &#123; System.out.println(&quot;dao: &quot; + id); return this.users.get(id); &#125; @Override public List&lt;User&gt; getAllUser() &#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(5); Collection&lt;User&gt; values = this.users.values(); userList.addAll(values); return userList; &#125; @Override public String saveUser(User user) &#123; int id = this.users.size() + 1; this.users.put(id, user); System.out.println(this.users); return &quot;success&quot;; &#125;&#125; service 层： 12345678910public interface UserService &#123; // 根据id查询用户 Mono&lt;User&gt; getUserById(int id); // 查询所有用户 Flux&lt;User&gt; getAllUser(); // 添加用户 Mono&lt;String&gt; saveUser(Mono&lt;User&gt; user);&#125; 123456789101112131415161718192021222324@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private UserDao userDao; @Override public Mono&lt;User&gt; getUserById(int id) &#123; System.out.println(&quot;service: &quot; + id); User user = userDao.getUserById(id); return Mono.just(user); &#125; @Override public Flux&lt;User&gt; getAllUser() &#123; List&lt;User&gt; allUser = userDao.getAllUser(); return Flux.fromIterable(allUser); &#125; @Override public Mono&lt;String&gt; saveUser(Mono&lt;User&gt; userMono) &#123; // return userMono.doOnNext(person -&gt; userDao.saveUser(person)).thenEmpty(Mono.empty());// 返回 Mono&lt;Void&gt; return userMono.map(user -&gt; userDao.saveUser(user)); &#125;&#125; controller 层： 12345678910111213141516171819202122232425@RestControllerpublic class UserController &#123; @Autowired private UserService userService; // 根据id查询用户 @GetMapping(&quot;/getUserById/&#123;id&#125;&quot;) public Mono&lt;User&gt; getUserById(@PathVariable int id) &#123; System.out.println(&quot;controller: &quot; + id); return userService.getUserById(id); &#125; // 查询所有用户 @GetMapping(&quot;/getAllUser&quot;) public Flux&lt;User&gt; getAllUser() &#123; return userService.getAllUser(); &#125; // 添加用户 @PostMapping(&quot;/saveUserMessage&quot;) public Mono&lt;String&gt; saveUser(@RequestBody User user) &#123; System.out.println(&quot;save user: &quot; + user); return userService.saveUser(Mono.just(user)); &#125;&#125; main 方法： 123456@SpringBootApplicationpublic class XisunWebfluxApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(XisunWebfluxApplication.class, args); &#125;&#125; 整体结构： 测试： 12345678910111213 . ____ _ __ _ _ /\\\\ / ___&#x27;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) &#x27; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.4.5)2021-04-24 18:54:21.418 INFO 4836 --- [ main] c.x.s.w.x.XisunWebfluxApplication : Starting XisunWebfluxApplication using Java 1.8.0_222 on DESKTOP-OM8IACS with PID 4836 (D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-webflux\\target\\classes started by Ziyoo in D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-webflux)2021-04-24 18:54:21.423 INFO 4836 --- [ main] c.x.s.w.x.XisunWebfluxApplication : No active profile set, falling back to default profiles: default2021-04-24 18:54:22.719 INFO 4836 --- [ main] o.s.b.web.embedded.netty.NettyWebServer : Netty started on port 80812021-04-24 18:54:22.730 INFO 4836 --- [ main] c.x.s.w.x.XisunWebfluxApplication : Started XisunWebfluxApplication in 2.007 seconds (JVM running for 3.003) 基于函数式编程模型 在使用函数式编程模型操作的时候，需要自己初始化服务器。 基于函数式编程模型操作的时候，有两个核心接口：RouterFunction (实现路由功能，将请求转发给对应的 handler) 和 HandlerFunction (处理请求并生成响应的函数)。基于函数式编程模型的核心任务就是定义这两个函数式接口的实现，并且启动需要的服务器。 Spring WebFlux 请求和响应不再是 ServletRequest 和 ServletResponse，而是 ServerRequest 和 ServerResponse。 第一步：创建 Spring Boot 工程，引入 Spring WebFlux 依赖。 第二步：打开 application.properties 配置文件，配置启动端口号。 第三步：创建包和相关类。 entity 层： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class User &#123; private String name; private String gender; private Integer age; public User() &#123; &#125; public User(String name, String gender, Integer age) &#123; this.name = name; this.gender = gender; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (!Objects.equals(name, user.name)) &#123; return false; &#125; if (!Objects.equals(gender, user.gender)) &#123; return false; &#125; return Objects.equals(age, user.age); &#125; @Override public int hashCode() &#123; int result = name != null ? name.hashCode() : 0; result = 31 * result + (gender != null ? gender.hashCode() : 0); result = 31 * result + (age != null ? age.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, gender=&#x27;&quot; + gender + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; dao 层： 1234567public interface UserDao &#123; User getUserById(int id); List&lt;User&gt; getAllUser(); String saveUser(User user);&#125; 1234567891011121314151617181920212223242526272829303132public class UserDaoImpl implements UserDao &#123; // 创建map集合存储数据，代替从数据库查询 private final Map&lt;Integer, User&gt; users = new HashMap&lt;&gt;(); &#123; this.users.put(1, new User(&quot;Lucy&quot;, &quot;male&quot;, 20)); this.users.put(2, new User(&quot;Mary&quot;, &quot;female&quot;, 30)); this.users.put(3, new User(&quot;Jack&quot;, &quot;male&quot;, 50)); &#125; @Override public User getUserById(int id) &#123; System.out.println(&quot;dao: &quot; + id); return this.users.get(id); &#125; @Override public List&lt;User&gt; getAllUser() &#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(5); Collection&lt;User&gt; values = this.users.values(); userList.addAll(values); return userList; &#125; @Override public String saveUser(User user) &#123; int id = this.users.size() + 1; this.users.put(id, user); System.out.println(this.users); return &quot;success&quot;; &#125;&#125; service 层： 12345678910public interface UserService &#123; // 根据id查询用户 Mono&lt;User&gt; getUserById(int id); // 查询所有用户 Flux&lt;User&gt; getAllUser(); // 添加用户 Mono&lt;Void&gt; saveUser(Mono&lt;User&gt; user);&#125; 1234567891011121314151617181920212223242526272829public class UserServiceImpl implements UserService &#123; private UserDao userDao; public UserServiceImpl() &#123; &#125; public UserServiceImpl(UserDao userDao) &#123; this.userDao = userDao; &#125; @Override public Mono&lt;User&gt; getUserById(int id) &#123; System.out.println(&quot;service: &quot; + id); User user = userDao.getUserById(id); return Mono.just(user); &#125; @Override public Flux&lt;User&gt; getAllUser() &#123; List&lt;User&gt; allUser = userDao.getAllUser(); return Flux.fromIterable(allUser); &#125; @Override public Mono&lt;Void&gt; saveUser(Mono&lt;User&gt; userMono) &#123; // return userMono.map(user -&gt; userDao.saveUser(user)); return userMono.doOnNext(person -&gt; userDao.saveUser(person)).thenEmpty(Mono.empty());// 返回 Mono&lt;Void&gt; &#125;&#125; 创建 Handler (具体实现方法)： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class UserHandler &#123; private final UserService userService; public UserHandle(UserService userService) &#123; this.userService = userService; &#125; // 根据id查询用户 public Mono&lt;ServerResponse&gt; getUserById(ServerRequest request) &#123; // 获取路径中的id值，返回的是String int userId = Integer.parseInt(request.pathVariable(&quot;id&quot;)); // 可能查询不到用户，进行空值处理 Mono&lt;ServerResponse&gt; notFound = ServerResponse.notFound().build(); // 调用userService的方法查询用户 Mono&lt;User&gt; userMono = userService.getUserById(userId); // 把userMono进行转换，返回Mono&lt;ServerResponse&gt; return userMono.flatMap(user -&gt; ServerResponse .ok() .contentType(MediaType.APPLICATION_JSON) .body(BodyInserters.fromObject(user)) .switchIfEmpty(notFound)); &#125; // 查询所有用户，ServerRequest参数即使不用，也要添加，否则在Server中会找不到这个方法 public Mono&lt;ServerResponse&gt; getAllUsers(ServerRequest request) &#123; // 调用userService的方法查询所有用户 Flux&lt;User&gt; userFlux = userService.getAllUser(); return ServerResponse .ok() .contentType(MediaType.APPLICATION_JSON) .body(userFlux, User.class); &#125; // 添加用户 public Mono&lt;ServerResponse&gt; saveUser(ServerRequest request) &#123; // 从请求中拿到user对象 Mono&lt;User&gt; userMono = request.bodyToMono(User.class); return ServerResponse .ok() .build(userService.saveUser(userMono)); &#125;&#125; 第四步：初始化服务器，编写 Router。 创建路由，创建服务器完成适配。 12345678910111213141516171819202122232425262728293031323334353637public class Server &#123; // 1.创建Router路由 public RouterFunction&lt;ServerResponse&gt; routingFunction() &#123; // 创建hanler对象(@Repository这些注解无效，需手动注入dao和service，是否有其他方法？) UserDaoImpl userDao = new UserDaoImpl(); UserService userService = new UserServiceImpl(userDao); UserHandler handler = new UserHandler(userService); // 设置路由 return RouterFunctions .route(RequestPredicates.GET(&quot;/getUserById/&#123;id&#125;&quot;) .and(RequestPredicates.accept(MediaType.APPLICATION_JSON)), handler::getUserById) .andRoute(RequestPredicates.GET(&quot;/getAllUser&quot;) .and(RequestPredicates.accept(MediaType.APPLICATION_JSON)), handler::getAllUser) .andRoute(RequestPredicates.POST(&quot;/saveUserMessage&quot;) .and(RequestPredicates.accept(MediaType.APPLICATION_JSON)), handler::saveUser); &#125; // 2.创建服务器完成适配 public void createReactorServer() &#123; // 路由和handler适配 RouterFunction&lt;ServerResponse&gt; route = routingFunction(); HttpHandler httpHandler = RouterFunctions.toHttpHandler(route); ReactorHttpHandlerAdapter adapter = new ReactorHttpHandlerAdapter(httpHandler); // 创建服务器 HttpServer httpServer = HttpServer.create(); httpServer.handle(adapter).bindNow(); &#125; // 3.最终调用 public static void main(String[] args) throws Exception &#123; Server server = new Server(); server.createReactorServer(); System.out.println(&quot;enter to exit&quot;); System.in.read(); &#125; 最终调用：启动 main 方法，并在网页上输入地址进行测试。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555609:45:43.306 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework09:45:43.726 [main] DEBUG org.springframework.web.server.adapter.HttpWebHandlerAdapter - enableLoggingRequestDetails=&#x27;false&#x27;: form data and headers will be masked to prevent unsafe logging of potentially sensitive data09:45:43.785 [main] DEBUG io.netty.util.internal.logging.InternalLoggerFactory - Using SLF4J as the default logging framework09:45:43.786 [main] DEBUG io.netty.util.internal.PlatformDependent - Platform: Windows09:45:43.792 [main] DEBUG io.netty.util.internal.PlatformDependent0 - -Dio.netty.noUnsafe: false09:45:43.792 [main] DEBUG io.netty.util.internal.PlatformDependent0 - Java version: 809:45:43.794 [main] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.theUnsafe: available09:45:43.796 [main] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.copyMemory: available09:45:43.799 [main] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Buffer.address: available09:45:43.800 [main] DEBUG io.netty.util.internal.PlatformDependent0 - direct buffer constructor: available09:45:43.802 [main] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Bits.unaligned: available, true09:45:43.802 [main] DEBUG io.netty.util.internal.PlatformDependent0 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java909:45:43.802 [main] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.DirectByteBuffer.&lt;init&gt;(long, int): available09:45:43.802 [main] DEBUG io.netty.util.internal.PlatformDependent - sun.misc.Unsafe: available09:45:43.804 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.tmpdir: C:\\Users\\Ziyoo\\AppData\\Local\\Temp (java.io.tmpdir)09:45:43.805 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.bitMode: 64 (sun.arch.data.model)09:45:43.808 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.maxDirectMemory: 1653604352 bytes09:45:43.809 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.uninitializedArrayAllocationThreshold: -109:45:43.810 [main] DEBUG io.netty.util.internal.CleanerJava6 - java.nio.ByteBuffer.cleaner(): available09:45:43.811 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.noPreferDirect: false09:45:43.873 [main] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.level: simple09:45:43.873 [main] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.targetRecords: 409:45:43.911 [main] DEBUG reactor.netty.tcp.TcpResources - [http] resources will use the default LoopResources: DefaultLoopResources &#123;prefix=reactor-http, daemon=true, selectCount=8, workerCount=8&#125;09:45:43.911 [main] DEBUG reactor.netty.tcp.TcpResources - [http] resources will use the default ConnectionProvider: reactor.netty.resources.DefaultPooledConnectionProvider@5552768b09:45:43.913 [main] DEBUG reactor.netty.resources.DefaultLoopIOUring - Default io_uring support : false09:45:44.117 [main] DEBUG reactor.netty.resources.DefaultLoopEpoll - Default Epoll support : false09:45:44.118 [main] DEBUG reactor.netty.resources.DefaultLoopKQueue - Default KQueue support : false09:45:44.125 [main] DEBUG io.netty.channel.MultithreadEventLoopGroup - -Dio.netty.eventLoopThreads: 1609:45:44.154 [main] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 102409:45:44.154 [main] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 409609:45:44.161 [main] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.noKeySetOptimization: false09:45:44.161 [main] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.selectorAutoRebuildThreshold: 51209:45:44.170 [main] DEBUG io.netty.util.internal.PlatformDependent - org.jctools-core.MpscChunkedArrayQueue: available09:45:44.218 [main] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.processId: 7052 (auto-detected)09:45:44.221 [main] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false09:45:44.221 [main] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false09:45:44.317 [main] DEBUG io.netty.util.NetUtilInitializations - Loopback interface: lo (Software Loopback Interface 1, 127.0.0.1)09:45:44.318 [main] DEBUG io.netty.util.NetUtil - Failed to get SOMAXCONN from sysctl and file \\proc\\sys\\net\\core\\somaxconn. Default: 20009:45:44.432 [main] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)09:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numHeapArenas: 1609:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numDirectArenas: 1609:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.pageSize: 819209:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxOrder: 1109:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.chunkSize: 1677721609:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.smallCacheSize: 25609:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.normalCacheSize: 6409:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedBufferCapacity: 3276809:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimInterval: 819209:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimIntervalMillis: 009:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.useCacheForAllThreads: true09:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 102309:45:44.466 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled09:45:44.466 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 009:45:44.466 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 1638409:45:44.589 [reactor-http-nio-1] DEBUG reactor.netty.transport.ServerTransport - [id:ae5a7227, L:/0:0:0:0:0:0:0:0:11779] Bound new serverenter to exit 除了上面的调用方式，也可以使用 WebClient 调用，这个不需要在浏览器中输入地址，可以直接在本地进行模拟测试： 123456789101112131415161718public class Client &#123; public static void main(String[] args) &#123; // 先启动Server，查看端口，然后调用服务器的地址 WebClient webClient = WebClient.create(&quot;http://127.0.0.1:12009&quot;); // 根据id查询 String id = &quot;1&quot;; User user = webClient.get().uri(&quot;/getUserById/&#123;id&#125;&quot;, id) .accept(MediaType.APPLICATION_JSON).retrieve().bodyToMono(User.class).block(); System.out.println(user); // 查询所有 Flux&lt;User&gt; users = webClient.get().uri(&quot;/getAllUser&quot;) .accept(MediaType.APPLICATION_JSON).retrieve().bodyToFlux(User.class); // 打印每一个User的名字 users.map(User::getName).buffer().doOnNext(System.out::println).blockFirst(); &#125;&#125; 说明：需要先启动 Server，然后查询端口号，设置 WebClient 的地址，然后启动 Client，即可在控制台查询相应操作的输出结果。 本文参考https://www.bilibili.com/video/BV1Vf4y127N5 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"}]},{"title":"IDEA 快捷键","slug":"tool-idea","date":"2021-04-13T12:38:59.000Z","updated":"2021-07-05T07:56:25.650Z","comments":true,"path":"2021/04/13/tool-idea/","link":"","permalink":"http://example.com/2021/04/13/tool-idea/","excerpt":"","text":"ctrl + H：查看类的继承层级关系 ctrl + alt + B：查找接口的实现类 ctrl + alt + S：打开 settings ctrl + alt + T：对一段代码添加包围语句，如 try/catch。 ctrl + Y：删除当前行 ctrl + D：复制当前行 shift + F6：重命名 ctrl + F：查找 ctrl + R：替换","categories":[],"tags":[{"name":"tool","slug":"tool","permalink":"http://example.com/tags/tool/"}]},{"title":"Spring5 入门","slug":"spring-base","date":"2021-04-13T05:09:15.000Z","updated":"2021-07-09T06:22:20.798Z","comments":true,"path":"2021/04/13/spring-base/","link":"","permalink":"http://example.com/2021/04/13/spring-base/","excerpt":"","text":"Spring 框架概述 Spring 官网：https://spring.io/ Spring 各版本源码下载地址：https://repo.spring.io/release/org/springframework/spring/ Spring 官方文档： 全部版本：https://docs.spring.io/spring-framework/docs/ 5.2.7.RELEASE：https://docs.spring.io/spring-framework/docs/5.2.7.RELEASE/spring-framework-reference/ Spring Framework 5 中文文档：https://cntofu.com/book/95/index.html Spring 是轻量级的开源的 JavaEE 框架。 Spring 可以解决企业应用开发的复杂性。 Spring 两个核心部分：IOC 和 AOP。 IOC：Inversion of Control，即控制反转。是面向对象编程中的一种设计原则，可以用来降低计算机代码之间的耦合度，其中最常见的方式叫做依赖注入 (Dependency Injection，简称 DI)。Spring 就是采用依赖注入的方式，来管理容器中的 Bean 实例对象。 AOP：Aspect Oriented Programming，即面向切面。可以在不修改源代码的前提下，通过预编译方式和运行期间动态代理方式实现对原有代码的增强 (添加新功能)。 Spring 特点： 方便解耦，简化开发。 AOP 编程支持。 方便程序测试。 方便和其他框架进行整合。 方便进行事务操作。 降低 API 开发难度。 Spring 模块： Spring 入门案例 创建一个 Maven 工程。 引入依赖：spring-beans、spring-context、spring-core、spring-expression，另外，Spring 还需依赖 commons-logging 实现日志功能。 123456789101112131415&lt;!-- Spring核心依赖 --&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 这个依赖好像不需要 --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 引入 spring-context 依赖时，会一并将其他几个依赖引入： 创建 Bean 类： 12345678910111213141516171819202122232425262728293031323334353637public class Student &#123; private Integer studentId; private String studentName; public Student() &#123; &#125; public Student(Integer studentId, String studentName) &#123; this.studentId = studentId; this.studentName = studentName; &#125; public Integer getStudentId() &#123; return studentId; &#125; public void setStudentId(Integer studentId) &#123; this.studentId = studentId; &#125; public String getStudentName() &#123; return studentName; &#125; public void setStudentName(String studentName) &#123; this.studentName = studentName; &#125; @Override public String toString() &#123; return &quot;Student&#123;&quot; + &quot;studentId=&quot; + studentId + &quot;, studentName=&#x27;&quot; + studentName + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 编写 Spring 配置文件：Spring 配置文件使用 xml 格式。 在 resources 包下点击鼠标右键，选择【New】–&gt;【XML Configuration File】–&gt;【Spring Config】，输入配置文件名 (自定义) 创建。注：resource 包下的配置文件在执行时会被拷贝至类路径的根目录。 在配置文件中添加如下配置：使用 &lt;bean&gt; 标签创建 Student 对象的实例，并注入属性的默认值。 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 使用bean元素定义一个由IOC容器创建的对象 --&gt; &lt;!-- id属性指定用于引用bean实例的标识 --&gt; &lt;!-- class属性指定用于创建bean的全类名 --&gt; &lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.bean.Student&quot;&gt; &lt;!-- 使用property子元素为bean的属性赋值 --&gt; &lt;property name=&quot;studentId&quot; value=&quot;007&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 编写测试代码： 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象 Student student = iocContainer.getBean(&quot;student&quot;, Student.class); // 3.打印bean System.out.println(student); &#125;&#125; 输出结果： 测试说明：Spring 在创建 IOC 容器时，就已经完成了 Bean 的创建和属性的赋值。 Spring 基本语法SqEL 表达式语言 SpEL 的全称是 Spring Expression Language，即 Spring 表达式语言，简称 SpEL，支持运行时查询并可以操作对象图，和 JSP 页面上的 EL 表达式、Struts2 中用到的 OGNL 表达式一样，SpEL 根据 JavaBean 风格的 getXxx()、setXxx() 方法定义的属性访问对象图，完全符合我们熟悉的操作习惯。 基本语法： SpEL 使用 #&#123;…&#125;作为定界符，所有在大框号中的字符都将被认为是 SpEL 表达式。 字面量： 整数：&lt;property name=&quot;count&quot; value=&quot;#&#123;5&#125;&quot;/&gt; 小数：&lt;property name=&quot;frequency&quot; value=&quot;#&#123;89.7&#125;&quot;/&gt; 科学计数法：&lt;property name=&quot;capacity&quot; value=&quot;#&#123;1e4&#125;&quot;/&gt; String 类型的字面量可以使用单引号或者双引号作为字符串的定界符号： &lt;property name=&quot;name&quot; value=&quot;#&#123;&#39;xisun&#39;&#125;&quot;/&gt; &lt;property name=&#39;name&#39; value=&#39;#&#123;&quot;xisun&quot;&#125;&#39;/&gt; Boolean：&lt;property name=&quot;enabled&quot; value=&quot;#&#123;false&#125;&quot;/&gt; 引用其他 Bean： 在 &lt;bean&gt; 标签的 value 属性中通过 #&#123;对象名&#125; 引用其他 Bean，注意：不能使用 ref 属性。 1234567891011&lt;!-- 引用其他Bean --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.bean.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;233&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;computer&quot; value=&quot;#&#123;computer&#125;&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;computer&quot; class=&quot;cn.xisun.spring.bean.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;666&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;HP&quot;/&gt;&lt;/bean&gt; 引用其他 Bean 的属性: 在 &lt;property&gt; 标签中通过 #&#123;对象名.属性名&#125; 引用其他 Bean 的属性。 12345678910111213141516&lt;!-- 引用其他Bean的属性 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.bean.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;233&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;computer&quot; &gt; &lt;bean class=&quot;cn.xisun.spring.bean.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;#&#123;computer.computerId&#125;&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;#&#123;computer.computerName&#125;&quot;/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;computer&quot; class=&quot;cn.xisun.spring.bean.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;666&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;HP&quot;/&gt;&lt;/bean&gt; 调用非静态方法： 通过 #&#123;对象名.方法名&#125; 调用对象的非静态方法。 12345678910111213141516&lt;!-- 调用非静态方法 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.bean.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;233&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Oneby&quot;/&gt; &lt;property name=&quot;computer&quot;&gt; &lt;bean class=&quot;cn.xisun.spring.bean.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;#&#123;computer.getComputerId()&#125;&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;#&#123;computer.getComputerName()&#125;&quot;/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;computer&quot; class=&quot;cn.xisun.spring.bean.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;666&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;HP&quot;/&gt;&lt;/bean&gt; 调用静态方法： 通过 T(静态类路径).方法名 调用静态方法。举例：定义获取随机整数的方法，随机整数的范围为 [start, end]。 12345public class MathUtil &#123; public static int getRandomInt(int start, int end) &#123; return (int) (Math.random() * (end - start + 1) + start); &#125;&#125; 12345&lt;!-- 调用静态方法 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.entity.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;#&#123;T(cn.xisun.spring.util.MathUtil).getRandomInt(0, 255)&#125;&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt;&lt;/bean&gt; Spring 中多个配置文件的整合 Spring 允许通过 &lt;import&gt; 标签将多个配置文件引入到一个文件中，进行配置文件的集成。这样在启动 Spring 容器时，仅需要指定这个合并好的配置文件就可以。 &lt;import&gt; 标签的 resource 属性支持 Spring 的标准的路径资源： Application context not configured for this file IDEA 中，对于 Spring 的配置类或配置文件，可能会提示 Application context not configured for this file，大概意思就是没有将该配置类或配置文件配置到项目中。 解决办法： Spring 中的 BeanSpring 中 Bean 的类型 Spring 内置了两种类型的 Bean ，一种是普通 Bean ，另外一种是工厂 Bean (FactoryBean)。 普通 Bean：在配置文件中定义的 Bean 类型与返回类型一致。这种最常见。 123&lt;bean id=&quot;myBook&quot; class=&quot;cn.xisun.spring.bean.Book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;三体&quot;/&gt;&lt;/bean&gt; 123456789101112public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Book book = iocContainer.getBean(&quot;myBook&quot;, Book.class); // 3.打印bean System.out.println(book); &#125;&#125; 配置文件中定义的 Bean 类型是 Book，实际返回的类型也是 Book。 工厂 Bean：在配置文件中定义的 Bean 类型可以和返回类型不一样。 第一步：创建类，实现 FactoryBean 接口，让这个类作为工厂 Bean。 FactoryBean 接口中有如下三个方法：getObject() 负责将创建好的 Bean 实例返回给 IOC 容器；getObjectType() 负责返回工厂生产的 Bean 类型；isSingleton() 用于指示该 Bean 实例是否为单例，默认是单例 Bean。 12345678910111213public interface FactoryBean&lt;T&gt; &#123; String OBJECT_TYPE_ATTRIBUTE = &quot;factoryBeanObjectType&quot;; @Nullable T getObject() throws Exception; @Nullable Class&lt;?&gt; getObjectType(); default boolean isSingleton() &#123; return true; &#125;&#125; 第二步：实现接口里面的方法，在实现的方法中定义返回的 Bean 类型。 123456789101112131415161718192021public class Book &#123; private String name; private String author; public void setName(String name) &#123; this.name = name; &#125; public void setAuthor(String author) &#123; this.author = author; &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, author=&#x27;&quot; + author + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 12345678910111213141516171819public class MyFactoryBean implements FactoryBean&lt;Book&gt; &#123; // 在getObject()方法中定义返回的Bean @Override public Book getObject() throws Exception &#123; Book book = new Book(); book.setName(&quot;三体&quot;); return book; &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Book.class; &#125; @Override public boolean isSingleton() &#123; return false; &#125;&#125; 第三步：在 Spring 配置文件中进行配置并测试，注意获取 Bean 的时候要使用工厂 Bean 返回的那个 Bean 的类型。 1&lt;bean id=&quot;myBean&quot; class=&quot;cn.xisun.spring.factory.MyFactoryBean&quot;&gt;&lt;/bean&gt; 123456789101112public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Book book = iocContainer.getBean(&quot;myBean&quot;, Book.class); // 3.打印bean System.out.println(book); &#125;&#125; 配置文件中定义的 Bean 类型是 MyFactoryBean，但实际返回的类型是 Book。 Spring 中 Bean 的作用域 默认情况下，Spring 只为每个在 IOC 容器里声明的 Bean 创建唯一一个实例 (单例对象)，整个 IOC 容器范围内都能共享该实例：所有后续的 getBean() 调用和 Bean 引用都将返回这个唯一的 Bean 实例。该作用域被称为 singleton，它是所有 Bean 的默认作用域。 在 Spring 中，可以在 &lt;bean&gt; 标签的 scope 属性里设置 Bean 的作用域，以决定这个 Bean 是单实例的还是多实例的。scope 属性值有四个： singleton：在 Spring IOC 容器中仅存在一个 Bean 实例，Bean 以单实例的方式存在。默认值。 1234&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.bean.Book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;平凡的世界&quot;/&gt; &lt;property name=&quot;author&quot; value=&quot;路遥&quot;/&gt;&lt;/bean&gt; 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Book book = iocContainer.getBean(&quot;book&quot;, Book.class); Book book1 = iocContainer.getBean(&quot;book&quot;, Book.class); // 3.打印bean System.out.println(book == book1); &#125;&#125; 输出结果是 true，说明 book 和 book1 的地址一样，二者指向同一个对象。 prototype：每次调用 getBean() 时都会返回一个新的实例，Bean 以多实例的方式存在。 1234&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.bean.Book&quot; scope=&quot;prototype&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;平凡的世界&quot;/&gt; &lt;property name=&quot;author&quot; value=&quot;路遥&quot;/&gt;&lt;/bean&gt; 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的Bean实例对象，要求使用返回的Bean的类型 Book book = iocContainer.getBean(&quot;book&quot;, Book.class); Book book1 = iocContainer.getBean(&quot;book&quot;, Book.class); // 3.打印bean System.out.println(book == book1); &#125;&#125; 输出结果是 false，说明 book 和 book1 的地址不一样，二者指向不同的对象。 设置 scope 值是 singleton 时候，加载 Spring 配置文件时候就会创建单实例对象；设置 scope 值是 prototype 时候，不是在加载 Spring 配置文件的时候创建对象，而是在调用 getBean() 时创建多实例对象。 request 和 session 不常用。 Spring 中 Bean 的生命周期 生命周期：一个对象从创建到销毁的过程，是这个对象的生命周期。 Spring IOC 容器可以管理 Bean 的生命周期，Spring 允许在 Bean 生命周期内特定的时间点执行指定的任务。Spring IOC 容器对 Bean 的生命周期进行管理的过程： 1. 通过构造器或工厂方法创建 Bean 实例。 2. 为 Bean 的属性设置值和对其他 Bean 的引用。 3. 调用 Bean 的初始化方法 (需要创建和配置初始化的方法)。 4. 获取 Bean 实例并使用。 5. 当容器关闭时，调用 Bean 的销毁方法 (需要创建和配置销毁的方法)。 代码演示： 1234567891011121314151617181920212223242526272829public class Book &#123; private String name; public Book() &#123; System.out.println(&quot;第一步：执行无参数构造方法创建bean实例&quot;); &#125; public void setName(String name) &#123; System.out.println(&quot;第二步：调用setter方法设置属性值&quot;); this.name = name; &#125; // 创建执行的初始化的方法 public void initMethod()&#123; System.out.println(&quot;第三步：执行初始化的方法&quot;); &#125; // 创建执行的销毁的方法 public void destroyMethod()&#123; System.out.println(&quot;第五步：执行销毁的方法&quot;); &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 1234&lt;!-- 在&lt;bean&gt;标签中指定book实例的init-method属性(初始化方法)和destroy-method属性(销毁方法) --&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.bean.Book&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;平凡的世界&quot;/&gt;&lt;/bean&gt; ​```java public class SpringTest { public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 System.out.println(&quot;第四步：获取创建的bean实例对象&quot;); Book book = iocContainer.getBean(&quot;book&quot;, Book.class); // 3.打印bean System.out.println(book); // 手动销毁bean的实例，会调用Book中定义的destroyMethod()，前提：在Spring配置文件中bean标签配置了destroy-method // ApplicationContext接口没有close()，需要它的子接口或实现类才能调用 ((ClassPathXmlApplicationContext)iocContainer).close(); &#125; } 输出结果： 第一步：执行无参数构造方法创建bean实例 第二步：调用setter方法设置属性值 第三步：执行初始化的方法 第四步：获取创建的bean实例对象 Book{name=’平凡的世界’} 第五步：执行销毁的方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 &gt;注意：要手动关闭 IOC 容器才会执行 destroy-method 方法。- Spring 中可以设置 Bean **后置处理器**： - Bean 后置处理器允许在调用初始化方法前后对 Bean 进行额外的处理。 - Bean 后置处理器对 IOC 容器里的所有 Bean 实例逐一处理，而非单一实例。其典型应用是：检查 Bean 属性的正确性或根据特定的标准更改 Bean 的属性。 - 定义 Bean 后置处理器时需要实现接口：&#96;org.springframework.beans.factory.config.BeanPostProcessor&#96;。在 Bean 的初始化方法被调用前后，Spring 将把每个 Bean 实例分别传递给上述接口的以下两个方法： - &#96;postProcessBeforeInitialization(Object, String)&#96; - &#96;postProcessAfterInitialization(Object, String)&#96;- Bean 添加后置处理器后的生命周期： - **1. 通过构造器或工厂方法创建 Bean 实例。** - **2. 为 Bean 的属性设置值和对其他 Bean 的引用。** - **3. 将 Bean 实例传递给 Bean 后置处理器的 &#96;postProcessBeforeInitialization()&#96;。** - **4. 调用 Bean 的初始化方法 (需要创建和配置初始化的方法)。** - **5. 将 Bean 实例传递给 Bean 后置处理器的 &#96;postProcessAfterInitialization()&#96;。** - **6. 获取 Bean 实例并使用。** - **7. 当容器关闭时，调用 Bean 的销毁方法 (需要创建和配置销毁的方法)。** - 代码演示： &#96;&#96;&#96;java &#x2F;** * 自定义bean后置处理器 *&#x2F; public class MyBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;第三步：执行初始化方法之前，执行postProcessBeforeInitialization方法&quot;); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;第五步：执行初始化方法之后，执行postProcessAfterInitialization方法&quot;); return bean; &#125; &#125; 1234567891011121314151617181920212223242526272829public class Book &#123; private String name; public Book() &#123; System.out.println(&quot;第一步：执行无参数构造方法创建bean实例&quot;); &#125; public void setName(String name) &#123; System.out.println(&quot;第二步：调用setter方法设置属性值&quot;); this.name = name; &#125; // 创建执行的初始化的方法 public void initMethod()&#123; System.out.println(&quot;第四步：执行初始化的方法&quot;); &#125; // 创建执行的销毁的方法 public void destroyMethod()&#123; System.out.println(&quot;第七步：执行销毁的方法&quot;); &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 123456&lt;!-- 配置后置处理器，适用于配置的所有的bean --&gt;&lt;bean id=&quot;myBeanPostProcessor&quot; class=&quot;cn.xisun.spring.processor.MyBeanPostProcessor&quot;/&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.bean.Book&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;平凡的世界&quot;/&gt;&lt;/bean&gt; 1234567891011121314151617181920212223242526public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 System.out.println(&quot;第六步：获取创建的bean实例对象&quot;); Book book = iocContainer.getBean(&quot;book&quot;, Book.class); // 3.打印bean System.out.println(book); // 手动销毁bean的实例，会调用Book中定义的destroyMethod()，前提：在Spring配置文件中bean标签配置了destroy-method // ApplicationContext接口没有close()，需要它的子接口或实现类才能调用 ((ClassPathXmlApplicationContext)iocContainer).close(); &#125;&#125;输出结果：第一步：执行无参数构造方法创建bean实例第二步：调用setter方法设置属性值第三步：执行初始化方法之前，执行postProcessBeforeInitialization方法第四步：执行初始化的方法第五步：执行初始化方法之后，执行postProcessAfterInitialization方法第六步：获取创建的bean实例对象Book&#123;name=&#x27;平凡的世界&#x27;&#125;第七步：执行销毁的方法 Spring 中 Bean 的自动装配 手动装配：在配置文件中，使用 &lt;bean&gt; 标签，以 value 或 ref 的方式明确指定属性值的方式，都是手动装配。 自动装配：根据指定的装配规则 (属性名称或者属性类型)，不需要明确指定，Spring 自动将匹配的属性值注入 Bean 中。 自动装配的装配模式： 根据类型自动装配 (byType)：将类型匹配的 Bean 作为属性注入到另一个 Bean 中。若 IOC 容器中有多个与目标 Bean 类型一致的 Bean，Spring 将无法判定哪个 Bean 最合适该属性，继而不能执行自动装配。 123456789101112131415&lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.bean.Department&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt;&lt;!-- 不能出现两个Department类型的bean --&gt;&lt;!--&lt;bean id=&quot;department1&quot; class=&quot;cn.xisun.spring.bean.Department&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt;--&gt;&lt;!-- 通过bean标签属性autowire，实现自动装配。 autowire 属性常用两个值： byName：根据属性名称注入，要求注入值bean的id值和类对应的属性名称一样。 byType：根据属性类型注入，要求配置文件中只能有一个与目标bean类型一致的bean。--&gt;&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.bean.Employee&quot; autowire=&quot;byType&quot;/&gt; 根据名称自动装配 (byName)：必须将目标 Bean 的名称和属性名设置的完全相同。 1234567891011&lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.bean.Department&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt;&lt;!-- 通过bean标签属性autowire，实现自动装配。 autowire 属性常用两个值： byName：根据属性名称注入，要求注入值bean的id值和类对应的属性名称一样。 byType：根据属性类型注入，要求配置文件中只能有一个与目标bean类型一致的bean。--&gt;&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.bean.Employee&quot; autowire=&quot;byName&quot;/&gt; 根据构造器自动装配 (constructor)：当 Bean 中存在多个构造器时，此种自动装配方式将会很复杂。不推荐使用。 相对于使用注解的方式实现的自动装配，在 xml 配置文件中进行的自动装配略显笨拙，在项目中更多的是使用注解的方式实现。 代码演示： 1234567891011121314public class Department &#123; private String name; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Department&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 123456789101112131415161718192021public class Employee &#123; private String name; private Department department; public void setName(String name) &#123; this.name = name; &#125; public void setDepartment(Department department) &#123; this.department = department; &#125; @Override public String toString() &#123; return &quot;Employee&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, department=&quot; + department + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Employee employee = iocContainer.getBean(&quot;employee&quot;, Employee.class); // 3.打印bean System.out.println(employee); &#125;&#125;输出结果：Employee&#123;name=&#x27;null&#x27;, department=Department&#123;name=&#x27;IT&#x27;&#125;&#125; Spring 中 Bean 的配置信息的继承 Spring 允许继承 Bean 的配置，被继承的 Bean 称为父 Bean，继承这个父 Bean 的 Bean 称为子 Bean。子 Bean 可以从父 Bean 中继承配置，包括 Bean 的属性配置，子 Bean 也可以覆盖从父 Bean 继承过来的配置。 父 Bean 可以作为配置模板，也可以作为 Bean 实例。若只想把父 Bean 作为模板，可以设置 &lt;bean&gt; 标签的 abstract 属性为 true，这样 Spring 将不会实例化这个 Bean。 创建实体类： 12345678910111213141516171819202122232425262728public class Book &#123; private String name; private String author; private String era; public void setName(String name) &#123; this.name = name; &#125; public void setAuthor(String author) &#123; this.author = author; &#125; public void setEra(String era) &#123; this.era = era; &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, author=&#x27;&quot; + author + &#x27;\\&#x27;&#x27; + &quot;, era=&#x27;&quot; + era + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 不使用继承配置 Bean： 12345678910111213&lt;bean id=&quot;book1&quot; class=&quot;cn.xisun.spring.bean.Book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;论语&quot;/&gt; &lt;!-- 以下都是重复的属性 --&gt; &lt;property name=&quot;author&quot; value=&quot;孔子&quot;/&gt; &lt;property name=&quot;era&quot; value=&quot;春秋末期&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;book2&quot; class=&quot;cn.xisun.spring.bean.Book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;春秋&quot;/&gt; &lt;!-- 以下都是重复的属性 --&gt; &lt;property name=&quot;author&quot; value=&quot;孔子&quot;/&gt; &lt;property name=&quot;era&quot; value=&quot;春秋末期&quot;/&gt;&lt;/bean&gt; book1 和 book2 两个 Bean 的 author 和 era 两个属性的值相同，像上面的配置会有点冗余。 使用配置信息的继承配置 Bean： 1234567891011&lt;bean id=&quot;book1&quot; class=&quot;cn.xisun.spring.bean.Book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;论语&quot;/&gt; &lt;!-- 以下都是重复的属性 --&gt; &lt;property name=&quot;author&quot; value=&quot;孔子&quot;/&gt; &lt;property name=&quot;era&quot; value=&quot;春秋末期&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;book2&quot; parent=&quot;book1&quot;&gt; &lt;!-- 重写不同值的属性即可 --&gt; &lt;property name=&quot;name&quot; value=&quot;春秋&quot;/&gt;&lt;/bean&gt; 代码演示： 1234567891011121314151617public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Book book1 = iocContainer.getBean(&quot;book1&quot;, Book.class); Book book2 = iocContainer.getBean(&quot;book2&quot;, Book.class); // 3.打印bean System.out.println(book1); System.out.println(book2); &#125;&#125;输出结果：Book&#123;name=&#x27;论语&#x27;, author=&#x27;孔子&#x27;, era=&#x27;春秋末期&#x27;&#125;Book&#123;name=&#x27;春秋&#x27;, author=&#x27;孔子&#x27;, era=&#x27;春秋末期&#x27;&#125; Spring 中 Bean 之间的依赖 有的时候创建一个 Bean 的时候，需要保证另外一个 Bean 也被创建，这时我们称前面的 Bean 对后面的 Bean 有依赖。例如：要求创建 Student 对象的时候必须创建 Book。这里需要注意的是依赖关系不等于引用关系，Student 即使依赖 Book 也可以不引用它。 12345678910&lt;!-- 一定要创建一个book对象，否则student对象无法创建 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.pojo.Student&quot; depends-on=&quot;book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;论语&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;论语&quot;/&gt; &lt;property name=&quot;author&quot; value=&quot;孔子&quot;/&gt; &lt;property name=&quot;era&quot; value=&quot;春秋末期&quot;/&gt;&lt;/bean&gt; Spring 引入外部 Properties 文件 当 Bean 的配置信息逐渐增多时，查找和修改一些 Bean 的配置信息就变得愈加困难。这时可以将一部分信息提取到 Bean 配置文件的外部，以 properties 格式的属性文件保存起来，同时在 Bean 的配置文件中引用 properties 属性文件中的内容，从而实现一部分属性值在发生变化时仅修改 properties 属性文件即可。这种技术多用于连接数据库的基本信息的配置。 引入 druid 依赖和 mysql-connector-java 驱动依赖： 12345678910111213&lt;!-- druid连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.20&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.19&lt;/version&gt;&lt;/dependency&gt; 在 Spring 配置文件中直接配置数据库连接信息： 1234567&lt;!-- 直接配置数据库连接池 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/userDb&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;&lt;/bean&gt; 在 Spring 配置文件中引入外部 properties 文件中单独存放的数据库连接信息： 在类路径下创建 jdbc.properties 数据库配置文件： 1234prop.driverClass=com.mysql.cj.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/userDbprop.userName=rootprop.password=root 在 Spring 配置文件中引入 context 名称空间： 1234567&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; 通过 &lt;context:property-placeholder&gt; 标签中的 location 属性来制定配置文件的路径，classpath: 表示该配置文件位于类路径下，并通过 SpEL 表达式语言如 $&#123;prop.userName&#125; 的方式来取出配置文件中的属性值。 12345678910&lt;!-- 引用外部属性文件来配置数据库连接池 --&gt;&lt;!-- 指定properties属性文件的位置，classpath:xxx表示属性文件位于类路径下 --&gt;&lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt;&lt;!-- 从properties属性文件中引入属性值 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;prop.driverClass&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;prop.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;prop.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;prop.password&#125;&quot;/&gt;&lt;/bean&gt; 代码演示： 123456789101112131415161718192021222324public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 DataSource dataSource = iocContainer.getBean(&quot;dataSource&quot;, DataSource.class); // 3.打印bean System.out.println(dataSource); &#125;&#125;输出结果：&#123; CreateTime:&quot;2021-04-15 15:36:05&quot;, ActiveCount:0, PoolingCount:0, CreateCount:0, DestroyCount:0, CloseCount:0, ConnectCount:0, Connections:[ ]&#125; IOCIOC 思想的底层原理 IOC 控制反转的思想： 在应用程序中的组件需要获取资源时，传统的方式是组件主动的从容器中获取所需要的资源，在这样的模式下，开发人员往往需要知道在具体容器中特定资源的获取方式。比如 ClassA 中需要用到 ClassB 的对象，一般情况下，需要在 ClassA 的代码中显式的 new 一个 ClassB 的对象。 控制反转的思想完全颠覆了应用程序组件获取资源的传统方式：反转了资源的获取方向 — 改由容器主动的将资源推送给需要的组件，开发人员不需要知道容器是如何创建资源对象的，只需要提供接收资源的方式即可。采用依赖注入技术之后，ClassA 的代码只需要定义一个私有的 ClassB 对象属性，不需要直接 new 来获得这个对象，而是通过相关的容器控制程序来将 ClassB 对象在外部 new 出来并注入到 ClassA 类里的引用中。而具体获取的方法、对象被获取时的状态由配置文件 (如 XML) 来指定。 DI 依赖注入：可以将 DI 看作是 IOC 的一种实现方式 — 即组件以一些预先定义好的方式 (例如 setter 方法) 接受来自于容器的资源注入。相对于 IOC 而言，这种表述更直接：IOC 容器在 Spring 中的实现。 IOC 底层原理：xml 解析，工厂模式，反射。 图解： 代码演示： 原始方式：自己 new 对象，再通过 setter 方法注入器属性值。—&gt; 代码耦合度极高。 123Student student = new Student();student.setStudentId(7);student.setStudentName(&quot;Tom&quot;); 进阶方式：通过工厂创建对象。—&gt; 可以降低代码的耦合度，不需要自己 new 对象，但仍需要手动去获取和管理 Bean。 12345&lt;!-- 1.先通过xml配置文件配置bean的属性 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.xisun.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;007&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt;&lt;/bean&gt; 12345678910111213// 2.再通过工厂模式 + 反射的方法创建该对象的实例，并注入属性值public class StudentFactory &#123; public static Student getStudent()&#123; String className = ...;// 通过xml解析获取全类名 String[] fieldNames = ..;// 通过xml解析获取字段名 String[] fieldValues = ...;// 通过xml解析获取字段值 Class clazz = Class.forName(className);// 通过反射创建对象实例 for (int i = 0; i &lt; fieldNames.length; i++) &#123; // 依次为字段赋值 &#125; return clazz;// 返回创建的实例对象 &#125;&#125; 最终方式：通过 Spring IOC 管理 Bean。—&gt; Bean 的创建与它们之间的依赖关系完全交给 Spring IOC 容器去管理，代码耦合程度极大降低。 12345&lt;!-- 1.先通过xml配置文件配置bean的属性 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.bean.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;007&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt;&lt;/bean&gt; 12// 2.再通过iocContainer.getbean(&quot;beanId&quot;, 类.class)方法或者@Autowire方式获取beanStudent student = iocContainer.getBean(&quot;student&quot;, Student.class); IOC 思想基于 IOC 容器完成，IOC 容器底层就是对象工厂。 IOC 容器的实现方式 Spring 为 IOC 容器提供的两种实现方式 (即两个接口 BeanFactory 和 ApplicationContext)： 在通过 IOC 容器读取 Bean 的实例之前，需要先将 IOC 容器本身实例化。 BeanFactory 接口： IOC 容器的基本实现，是 Spring 内部的使用接口。面向 Spring 本身，不提供给开发人员使用。 BeanFactory 在加载配置文件的时候，不会创建对象，而是在使用对象的时候才去创建。 BeanFactory 接口的实现类： ApplicationContext 接口： BeanFactory 的子接口 ，面向 Spring 的使用者，提供了更多功能，一般由开发人员进行使用。几乎所有场合都使用 ApplicationContext 而不是底层的 BeanFactory。 ApplicationContext 在加载配置文件的时候，就会把配置文件中配置的对象进行创建。(在服务启动的时候，就把加载对象等耗时的工作全部完成，而不是在用到的时候才创建，这对于 web 项目等的使用者，会有比较好的效果，因为一般项目部署到服务器启动后，都尽量不再关闭。) ApplicationContext 接口的重要子接口和实现类： ConfigurableApplicationContext 子接口：扩展了一些方法，如 refresh() 和 close()，这些方法能够让 ApplicationContext 具有启动、关闭和刷新上下文的能力。 1234567891011121314151617181920212223242526public interface ConfigurableApplicationContext extends ApplicationContext, Lifecycle, Closeable &#123; /** * Load or refresh the persistent representation of the configuration, * which might an XML file, properties file, or relational database schema. * &lt;p&gt;As this is a startup method, it should destroy already created singletons * if it fails, to avoid dangling resources. In other words, after invocation * of that method, either all or no singletons at all should be instantiated. * @throws BeansException if the bean factory could not be initialized * @throws IllegalStateException if already initialized and multiple refresh * attempts are not supported */ void refresh() throws BeansException, IllegalStateException; /** * Close this application context, releasing all resources and locks that the * implementation might hold. This includes destroying all cached singleton beans. * &lt;p&gt;Note: Does &lt;i&gt;not&lt;/i&gt; invoke &#123;@code close&#125; on a parent context; * parent contexts have their own, independent lifecycle. * &lt;p&gt;This method can be called multiple times without side effects: Subsequent * &#123;@code close&#125; calls on an already closed context will be ignored. */ @Override void close(); ...&#125; FileSystemXmlApplicationContext：对应文件系统中的 xml 格式的配置文件。(xml 配置文件的绝对路径) 12ApplicationContext iocContainer = new FileSystemXmlApplicationContext( &quot;D:\\\\JetBrainsWorkSpace\\\\IDEAProjects\\\\xisun-projects\\\\xisun-spring\\\\src\\\\main\\\\resources\\\\spring.xml&quot;); ClassPathXmlApplicationContext：对应类路径下的 xml 格式的配置文件。(xml 配置文件的相对路径，常用) 1ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); WebApplicationContext 子接口：扩展了 ApplicationContext，是专门为 Web 应用准备的，它允许从相对于 Web 根目录的路径中装载配置文件完成初始化。 需要额外引入 spring-web 依赖： 123456&lt;!-- Spring Web依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; IOC 管理 Bean 的方式 IOC 操作 Bean 管理： Bean 管理指的是两个操作： Spring 创建对象。—&gt; 实例化 Spirng 注入属性。—&gt; 初始化 Bean 管理操作有两种方式： 基于 xml 配置文件方式实现 (基础)。 基于注解方式实现。 Bean 对象的三种获取方式 (定义在 beanFactory 接口中)： Object getbean(String name) throws beansException;：通过 Bean 的 name 获取 Bean 实例。 1Student student = (Student) iocContainer.getBean(&quot;student&quot;); &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException;：通过 Bean 的 class 获取 Bean 实例。 1Student student1 = iocContainer.getBean(Student.class); &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException;：通过 Bean 的 name 和 Bean 的 class 获取 Bean 实例。 1Student student = iocContainer.getBean(&quot;student&quot;, Student.class); 基于 xml 配置文件方式实现 第一步：基于 xml 方式创建对象。 12&lt;!-- 配置Student对象 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.bean.Student&quot;&gt;&lt;/bean&gt; 在 Spring 配置文件中，使用 &lt;bean&gt; 标签，标签里面添加对应属性，就可以实现对象创建。 &lt;bean&gt; 标签中有很多属性，常用的属性： id 属性：bean 实例的唯一标识。 class 属性：bean 的全类名。 创建对象时候，默认执行无参数构造方法完成对象创建。 第二步：基于 xml 方式注入对象的属性。 DI：依赖注入，就是注入属性。 第一种注入方式：通过 Bean 的 setter 方法注入属性值。 创建类，定义属性，创建属性对应的 setter 方法。 12345678910111213public class Book &#123; private String bookName; private String bookAuthor; public void setBookName(String bookName) &#123; this.bookName = bookName; &#125; public void setBookAuthor(String bookAuthor) &#123; this.bookAuthor = bookAuthor; &#125;&#125; 在 Spring 配置文件配置对象创建，配置属性注入。 123456789&lt;!-- 配置Book对象 --&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.bean.Book&quot;&gt; &lt;!-- 使用property完成属性注入： name：类里面属性名称 value：向属性注入的值 --&gt; &lt;property name=&quot;bookName&quot; value=&quot;论语&quot;/&gt; &lt;property name=&quot;bookAuthor&quot; value=&quot;孔子&quot;/&gt;&lt;/bean&gt; 通过 &lt;property&gt; 标签指定属性名，Spring 会帮我们找到该属性对应的 setter 方法，并注入其属性值。 第二种注入方式：通过 Bean 的有参数构造方法注入属性值。 创建类，定义属性，创建属性对应的有参数构造方法。 12345678910public class Orders &#123; private String orderName; private String address; public Orders(String orderName, String address) &#123; this.orderName = orderName; this.address = address; &#125;&#125; 在 Spring 配置文件配置对象创建，配置属性注入。 12345&lt;!-- 配置Orders对象 --&gt;&lt;bean id=&quot;orders&quot; class=&quot;cn.xisun.spring.bean.Orders&quot;&gt; &lt;constructor-arg name=&quot;orderName&quot; value=&quot;computer&quot;/&gt; &lt;constructor-arg name=&quot;address&quot; value=&quot;China&quot;/&gt;&lt;/bean&gt; 通过 &lt;constructor-arg&gt; 标签为对象的属性赋值，name 指定属性名，value 指定属性值。 第三种注入方式：通过 p 名称空间注入属性值。 为了简化 xml 文件的配置，越来越多的 xml 文件采用属性而非子元素配置信息。Spring 从 2.5 版本开始引入了一个新的 p 命名空间，可以通过 &lt;bean&gt; 标签属性的方式配置 Bean 的属性。使用 p 命名空间后，基于 xml 的配置方式将进一步简化。 添加 p 名称空间在配置文件中。 12345&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; 通过 p 名称空间注入属性值，也是调用 Bean 的 setter 方法设置属性值的。 12&lt;!-- 配置Book对象 --&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.bean.Book&quot; p:bookName=&quot;论语&quot; p:bookAuthor=&quot;孔子&quot;/&gt; 基于 xml 方式注入其他类型的属性。 第一种：字面量 null 值。 1234567&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.bean.Book&quot;&gt; &lt;property name=&quot;bookName&quot; value=&quot;无名&quot;/&gt; &lt;!-- null值--&gt; &lt;property name=&quot;bookAuthor&quot;&gt; &lt;null/&gt; &lt;/property&gt;&lt;/bean&gt; 效果：Book{bookName=’无名’, bookAuthor=’null’} 属性值包含特殊符号。 12345678910&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.bean.Book&quot;&gt; &lt;property name=&quot;bookName&quot; value=&quot;春秋&quot;/&gt; &lt;property name=&quot;bookAuthor&quot;&gt; &lt;!-- 方式一：将特殊字符进行转义，比如：&lt;&gt;转义为&amp;lt; &amp;gt; --&gt; &lt;!--&lt;value&gt;&amp;lt;相传是孔子&amp;gt;&lt;/value&gt;--&gt; &lt;!-- 方式二：把带特殊符号内容写到CDATA中 --&gt; &lt;value&gt;&lt;![CDATA[&lt;相传是孔子&gt;]]&gt;&lt;/value&gt; &lt;/property&gt;&lt;/bean&gt; 效果：Book{bookName=’春秋’, bookAuthor=’&lt;相传是孔子&gt;’} 第二种：外部 Bean。 创建两个类。 12345public class UserDao &#123; public void update()&#123; &#125;&#125; 123456789101112public class UserService &#123; private UserDao userDao; public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125; public void add() &#123; System.out.println(&quot;service add...............&quot;); userDao.update(); &#125;&#125; 在 Spring 配置文件中进行配置。 12345678910&lt;bean id=&quot;userService&quot; class=&quot;cn.xisun.spring.service.UserService&quot;&gt; &lt;!-- 注入userDao对象： name属性：类里面属性名称 ref属性：配置userDao对象的bean标签的id值 --&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt;&lt;/bean&gt;&lt;!-- 外部Bean --&gt;&lt;bean id=&quot;userDao&quot; class=&quot;cn.xisun.spring.bean.UserDao&quot;/&gt; 第三种：内部 Bean。 当 Bean 实例仅仅给一个特定的属性使用时，可以将其声明为内部 Bean。内部 Bean 声明直接包含在 &lt;property&gt; 或 &lt;constructor-arg&gt; 标签里，不需要设置任何 id 或 name 属性，内部 Bean 不能使用在任何其他地方。 一对多关系：部门和员工，一个部门有多个员工，一个员工属于一个部门，部门是一，员工是多。 1234567891011121314public class Department &#123; private String depName; public void setDepName(String depName) &#123; this.depName = depName; &#125; @Override public String toString() &#123; return &quot;Department&#123;&quot; + &quot;depName=&#x27;&quot; + depName + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 12345678910111213141516171819202122232425262728public class Employee &#123; private String name; private String gender; private Department dep; public void setName(String name) &#123; this.name = name; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public void setDep(Department dep) &#123; this.dep = dep; &#125; @Override public String toString() &#123; return &quot;Employee&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, gender=&#x27;&quot; + gender + &#x27;\\&#x27;&#x27; + &quot;, dep=&quot; + dep + &#x27;&#125;&#x27;; &#125;&#125; 在 spring 配置文件中进行配置。 12345678910&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.pojo.Employee&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;gender&quot; value=&quot;male&quot;/&gt; &lt;property name=&quot;dep&quot;&gt; &lt;!-- 内部Bean --&gt; &lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.pojo.Department&quot;&gt; &lt;property name=&quot;depName&quot; value=&quot;IT&quot;/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 第四种：级联赋值。 写法一： 12345678910&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.bean.Employee&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;gender&quot; value=&quot;male&quot;/&gt; &lt;!-- 级联赋值写法一 --&gt; &lt;property name=&quot;dep&quot; ref=&quot;department&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.bean.Department&quot;&gt; &lt;property name=&quot;depName&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt; 写法二：注意，必须要在 Employee 类中添加 dep 属性的 getter 方法，否则会报错。 1234567891011&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.bean.Employee&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;gender&quot; value=&quot;male&quot;/&gt; &lt;!-- 级联赋值写法二 --&gt; &lt;property name=&quot;dep&quot; ref=&quot;department&quot;/&gt; &lt;property name=&quot;dep.depName&quot; value=&quot;editorial&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.pojo.Department&quot;&gt; &lt;property name=&quot;depName&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt; 基于 xml 方式注入集合属性：数组类型、List 类型、Map 类型、Set 类型。 在 Spring 中可以通过一组内置的 xml 标签来配置集合属性，比如：&lt;array&gt;、&lt;list&gt;、&lt;map&gt;、&lt;set&gt;、&lt;props&gt;，并且可以用过引入 util 名称空间来提取集合类型的 Bean。 第一种：集合中元素是基本数据类型。 创建类，定义数组、List、Map、Set 类型属性，并生成对应的 setter 方法。 12345678910111213141516171819202122232425262728293031public class CollectionExample &#123; private String[] array; private List&lt;String&gt; list; private Map&lt;String, String&gt; map; private Set&lt;String&gt; set; private Properties properties; public void setArray(String[] array) &#123; this.array = array; &#125; public void setList(List&lt;String&gt; list) &#123; this.list = list; &#125; public void setMap(Map&lt;String, String&gt; map) &#123; this.map = map; &#125; public void setSet(Set&lt;String&gt; set) &#123; this.set = set; &#125; public void setProperties(Properties properties) &#123; this.properties = properties; &#125;&#125; 在 Spring 配置文件进行配置。 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;bean id=&quot;collectionExample&quot; class=&quot;cn.xisun.spring.bean.CollectionExample&quot;&gt; &lt;!-- 数组类型属性注入 --&gt; &lt;property name=&quot;array&quot;&gt; &lt;array value-type=&quot;java.lang.String&quot;&gt; &lt;value&gt;Java&lt;/value&gt; &lt;value&gt;数据库&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!-- List类型属性注入 --&gt; &lt;property name=&quot;list&quot;&gt; &lt;list value-type=&quot;java.lang.String&quot;&gt; &lt;value&gt;张三&lt;/value&gt; &lt;value&gt;李四&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- Map类型属性注入 --&gt; &lt;property name=&quot;map&quot;&gt; &lt;map key-type=&quot;java.lang.String&quot; value-type=&quot;java.lang.String&quot;&gt; &lt;entry key=&quot;JAVA&quot; value=&quot;java&quot;/&gt; &lt;entry key=&quot;PYTHON&quot; value=&quot;python&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- Set类型属性注入 --&gt; &lt;property name=&quot;set&quot;&gt; &lt;list value-type=&quot;java.lang.String&quot;&gt; &lt;value&gt;MySQL&lt;/value&gt; &lt;value&gt;Redis&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- Properties类型属性注入 --&gt; &lt;property name=&quot;properties&quot;&gt; &lt;props value-type=&quot;java.lang.String&quot;&gt; &lt;prop key=&quot;SPRING&quot;&gt;spring&lt;/prop&gt; &lt;prop key=&quot;JVM&quot;&gt;jvm&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 第二种：集合中元素是对象类型值。 创建两个类。 1234567public class Course &#123; private String name; public void setName(String name) &#123; this.name = name; &#125;&#125; 1234567public class Student &#123; private List&lt;Course&gt; coursesist; public void setCoursesist(List&lt;Course&gt; coursesist) &#123; this.coursesist = coursesist; &#125;&#125; 在 Spring 配置文件进行配置。 1234567891011121314151617&lt;!-- 1.创建多个Course对象 --&gt;&lt;bean id=&quot;course1&quot; class=&quot;cn.xisun.spring.bean.Course&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Spring&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;course2&quot; class=&quot;cn.xisun.spring.bean.Course&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;SpringMVC&quot;/&gt;&lt;/bean&gt;&lt;!-- 2.注入list集合类型，值是Course对象 --&gt;&lt;bean id=&quot;stu&quot; class=&quot;cn.xisun.spring.bean.Student&quot;&gt; &lt;property name=&quot;coursesist&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;course1&quot;/&gt; &lt;ref bean=&quot;course2&quot;/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 把集合注入部分提取出来作为公共部分。 创建一个类： 1234567891011121314public class Book &#123; private List&lt;String&gt; bookList; public void setBookList(List&lt;String&gt; bookList) &#123; this.bookList = bookList; &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;bookList=&quot; + bookList + &#x27;&#125;&#x27;; &#125;&#125; 在 Spring 配置文件中引入名称空间 util。 1234567&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd&quot;&gt; 使用 util 标签完成 list 集合注入提取。 1234567891011&lt;!-- 1.提取list集合类型属性注入 --&gt;&lt;util:list id=&quot;bookList&quot;&gt; &lt;value&gt;论语&lt;/value&gt; &lt;value&gt;孟子&lt;/value&gt; &lt;value&gt;大学&lt;/value&gt;&lt;/util:list&gt;&lt;!-- 2.注入list集合类型，值是对象 --&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name=&quot;bookList&quot; ref=&quot;bookList&quot;/&gt;&lt;/bean&gt; Map 和 Set 参考 List 的写法。 基于注解方式实现 什么是注解： 注解是代码特殊标记，格式：@注解名称(属性名称=属性值, 属性名称=属性值...)。 使用注解的时候，注解作用在类上面、方法上面、属性上面。 相对于 xml 方式而言，通过注解的方式配置 bean 更加简洁和优雅，而且和 MVC 组件化开发的理念十分契合，是开发中常用的使用方式。 Spring 中用于标识 Bean 的四个注解： @Component：普通组件，用于标识一个受 Spring IOC 容器管理的组件。 @Respository：持久化层组件，用于标识一个受 Spring IOC 容器管理的持久化层组件。 @Service：业务逻辑层组件，用于标识一个受 Spring IOC 容器管理的业务逻辑层组件。 @Controller：表述层控制器组件，用于标识一个受 Spring IOC 容器管理的表述层控制器组件。 事实上 Spring 并没有能力识别一个组件到底是不是它所标记的类型，即使将 @Respository 注解用在一个非持久化层组件上面，也不会产生任何错误，所以 @Respository、@Service、@Controller 这几个注解仅仅是为了让开发人员自己明确当前的组件扮演的角色。 组件命名规则： 默认情况：使用组件的简单类名首字母小写后得到的字符串作为 Bean 的 id。 也可以使用四个组件注解的 value 属性指定 Bean 的 id。 第一步：开启 Spring 注解方式的整体流程。 第一步：引入 spring-aop 依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 第二步：在配置文件中引入 context 名称空间。 1234567&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; 第三步：在配置文件中开启组件扫描。 123456&lt;!-- 开启组件扫描： 1.如果扫描多个包，多个包间使用逗号隔开。 2.扫描包的上层目录。--&gt;&lt;context:component-scan base-package=&quot;cn.xisun.spring&quot;/&gt; 第四步：创建类，在类上面添加创建对象注解。 12345678910package cn.xisun.spring.service;import org.springframework.stereotype.Service;@Servicepublic class UserService &#123; public void add() &#123; System.out.println(&quot;user service add ......&quot;); &#125;&#125; 第五步：获取和使用 Bean。 12345678910111213141516public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 UserService userService = iocContainer.getBean(&quot;userService&quot;, UserService.class); // 3.打印bean System.out.println(userService); userService.add(); &#125;&#125;输出结果：cn.xisun.spring.service.UserService@8e0379duser service add ...... 开启组件扫描的注意事项： base-package 属性指定一个需要扫描的基类包，Spring 容器将会扫描这个基类包及其子包中的所有类。 当需要扫描多个包时可以使用逗号分隔，或者指定这多个包的上层包。 如果仅希望扫描特定的类而非基包下的所有类，可使用 resource-pattern 属性过滤特定的类，示例： 12&lt;!-- resource-pattern：只扫描cn.xisun.spring包下的dao子包下的所有类。 --&gt;&lt;context:component-scan base-package=&quot;cn.xisun.spring&quot; resource-pattern=&quot;dao/*.class&quot;/&gt; 使用 resource-pattern 属性并不能提供完善的功能，所有我们得使用过滤子元素的方法。 &lt;context:include-filter&gt;：表示要包含的目标类。 123456789&lt;!-- 示例1： use-default-filters=&quot;false&quot;：表示现在不使用默认filter，而是使用自己配置filter。 context:include-filter：用于设置需要扫描哪些内容(这里配置扫描Repository、Service和Controller注解)--&gt; &lt;context:component-scan base-package=&quot;cn.xisun.spring&quot; use-default-filters=&quot;false&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Repository&quot;/&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Service&quot;/&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; 通常需要与 use-default-filters 属性配合使用才能够达到 “仅包含某些组件” 这样的效果。即：通过将 use-default-filters 属性设置为 false，禁用默认过滤器，然后扫描的就只是 &lt;context:include-filter&gt; 标签中的规则指定的组件了。 &lt;context:exclude-filter&gt;：表示要排除在外的目标类。 1234&lt;!-- 示例2：下面配置扫描包所有内容context:exclude-filter，设置哪些内容不进行扫描(这里排除Controller注解) --&gt;&lt;context:component-scan base-package=&quot;cn.xisun.spring&quot;&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt;&lt;/context:component-scan&gt; 一个 &lt;context:component-scan&gt; 标签下可以有多个 &lt;context:include-filter&gt; 和 &lt;context:exclude-filter&gt;。 &lt;context:include-filter&gt; 和 &lt;context:exclude-filter&gt; 的 type 属性所支持的类型如下表： 在这些类型当中，除了 custom 外，aspectj 的过滤功能最强大，它能轻易的实现其他类别的过滤规则。 第二步：基于注解方式实现属性注入。 项目中组件装配时，Controller 组件中往往需要用到 Service 组件的实例，Service 组件中往往需要用到 Repository 组件的实例。Spring 可以通过注解的方式帮我们实现属性的装配。 在指定要扫描的包时，&lt;context:component-scan&gt; 标签会自动注册一个 Bean 的后置处理器 AutowiredAnnotationBeanPostProcessor 的实例。该后置处理器可以自动装配标记了 @Autowired、@Resource 或 @Inject 注解的属性。这就是组件扫描的原理。 @Autowired 根据属性类型实现自动装配。 构造器、普通字段 (即使是非 public)、一切具有参数的方法都可以应用 @Autowired 注解。 默认情况下，所有使用 @Autowired 注解的属性都需要被设置。当 Spring 找不到匹配的 Bean 装配属性时，会抛出异常。 若某一属性允许不被设置，可以设置 @Autowired 注解的 required 属性为 false。 默认情况下，当 IOC 容器里存在多个类型兼容的 Bean 时，Spring 会尝试匹配 Bean 的 id 值是否与变量名相同，如果相同则进行装配。如果 Bean 的 id 值不相同，通过类型的自动装配将无法工作。此时可以在 @Qualifier 注解里提供 Bean 的名称。Spring 甚至允许在方法的形参上标注 @Qualifiter 注解以指定注入 Bean 的名称。 @Autowired 注解也可以应用在数组类型的属性上，此时 Spring 将会把所有匹配的 Bean 进行自动装配。 @Autowired 注解也可以应用在集合属性上，此时 Spring 读取该集合的类型信息，然后自动装配所有与之兼容的 Bean。 @Autowired 注解用在 java.util.Map上时，若该 Map 的键值为 String，那么 Spring 将自动装配与值类型兼容的 Bean 作为值，并以 Bean 的 id 值作为键。 @Autowired 注解使用过程： 第一步：创建 service 和 dao 对象，在 service 和 dao 类添加创建对象注解。 第二步：在 service 中注入 dao 对象，在 service 类添加 dao 类型属性，在属性上面使用注解。 123public interface UserDao &#123; public void add();&#125; 1234567@Repositorypublic class UserDaoImpl implements UserDao &#123; @Override public void add() &#123; System.out.println(&quot;dao add ......&quot;); &#125;&#125; 1234567891011@Servicepublic class UserService &#123; // 定义dao类型属性，添加注入属性注解，不需要添加setter方法 @Autowired private UserDao userDao; public void add() &#123; System.out.println(&quot;user service add ......&quot;); userDao.add(); &#125;&#125; 1234567891011121314151617public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 UserService userService = iocContainer.getBean(&quot;userService&quot;, UserService.class); // 3.打印bean System.out.println(userService); userService.add(); &#125;&#125;输出结果：cn.xisun.spring.service.UserService@161b062auser service add ......dao add ...... @Qualifier 根据属性名称实现自动装配。 @Qualifier 注解需要和上面 @Autowired 注解一起使用。 如果存在多个类型相同的 Bean，可以为每个 Bean 单独命名，然后根据名称使用 @Qualifier 注解指定需要注入的 Bean。 @Qualifier 注解使用过程： 123public interface UserDao &#123; public void add();&#125; 1234567@Repository(value = &quot;userDaoImpl1&quot;)public class UserDaoImpl implements UserDao &#123; @Override public void add() &#123; System.out.println(&quot;dao add ......&quot;); &#125;&#125; 1234567891011@Servicepublic class UserService &#123; @Autowired @Qualifier(value = &quot;userDaoImpl1&quot;)// 需要与指定的bean的value相同，否则会找不到 private UserDao userDao; public void add() &#123; System.out.println(&quot;user service add ......&quot;); userDao.add(); &#125;&#125; 1234567891011121314151617public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 UserService userService = iocContainer.getBean(&quot;userService&quot;, UserService.class); // 3.打印bean System.out.println(userService); userService.add(); &#125;&#125;输出结果：cn.xisun.spring.service.UserService@3ee0fea4user service add ......dao add ...... @Resource 可以根据类型注入，也可以根据名称注入。@Resource 注解要求提供一个 Bean 名称的属性，若该属性为空，则自动采用标注处的变量或方法名作为 Bean 的名称。 @Resource 是 JDK 提供的注解，不建议使用，开发中应该尽量使用 Spring 提供的注解。 @Resource 注解使用说明： 123// @Resource // 根据类型进行注入@Resource(name = &quot;userDaoImpl1&quot;) // 根据Bean名称进行注入 private UserDao userDao; @Value 注入普通属性的值。 @Value 注解使用说明： 123456789101112131415@Servicepublic class UserService &#123; @Autowired @Qualifier(value = &quot;userDaoImpl1&quot;) private UserDao userDao; @Value(value = &quot;Tom&quot;) private String name;// @Value注解为name属性注入了一个值Tom public void add() &#123; System.out.println(&quot;name is: &quot; + this.name);// name is: Tom System.out.println(&quot;user service add ......&quot;); userDao.add(); &#125;&#125; 进阶：完全注解开发 第一步：创建 SpringConfig 配置类，代替之前的 xml 配置文件。 12345678910111213141516171819/** * 1.配置类本身也是一个组件 * 2.配置类里使用@Bean注解，标注在方法上给容器注册组件，默认是单实例的 */@Configuration@ComponentScan(&quot;cn.xisun.spring&quot;)public class SpringConfig &#123; // 给容器中添加组件。以方法名作为组件的id，返回类型就是组件的类型，返回的值，就是组件在容器中的实例 @Bean public Student student01() &#123; return new Student(1000, &quot;Jerry&quot;); &#125; // 可以重新指定组件的id @Bean(value = &quot;Tom&quot;) public Student student02() &#123; return new Student(1001, &quot;Tom&quot;); &#125;&#125; @Configuration：标识这是一个配置类。 @ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;)：配置组件扫描路径。 在 Spring 配置文件中，以 &lt;bean&gt; 标签注册的对象，均可在此配置类中实现。 如果需要注册一些特殊的对象，比如 Student 类的特定实例，需要在此配置类中以 @Bean 注解配置。而诸如以 @Repository 等注解标注的类，已经在 IOC 容器中注册，不需要在此配置。如： 123@Repositorypublic class UserDao &#123;&#125; 第二步：编写测试类，通过 new 一个 AnnotationConfigApplicationContext 对象创建 IOC 容器对象。其他与前面的相同。 12345678910111213141516public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置类，创建IOC容器对象 ApplicationContext iocContainer = new AnnotationConfigApplicationContext(SpringConfig.class); // 2.根据id值获取配置类中的Bean实例对象和容器中注册的组件，要求使用返回的Bean的类型 Student student01 = context.getBean(&quot;student01&quot;, Student.class);// 指向SpringConfig类中的第一个Bean Student student = context.getBean(&quot;Tom&quot;, Student.class);// 指向SpringConfig类中的第二个Bean UserDao userDao = context.getBean(&quot;userDao&quot;, UserDao.class);// 指向@Repository注解标注的UserDao // 3.打印Bean System.out.println(student01); System.out.println(student); System.out.println(userDao); &#125;&#125; 效果： Student{studentId=1000, studentName=’Jerry’}Student{studentId=1001, studentName=’Tom’}cn.xisun.spring.dao.UserDao@55a1c291 AOP AOP (Aspect-Oriented Programming，面向切面编程)：是一种新的方法论，是对传统 OOP (Object-Oriented Programming，面向对象编程) 的补充。 AOP 编程操作的主要对象是切面 (aspect)，而切面模块化横切关注点。 在应用 AOP 编程时，仍然需要定义公共功能，但可以明确的定义这个功能应用在哪里，以什么方式应用，并且不必修改受影响的类。这样一来横切关注点就被模块化到特殊的类里 — 这样的类我们通常称之为 “切面”。 AOP 的好处：每个事物逻辑位于一个位置，代码不分散，便于维护和升级；业务模块更简洁，只包含核心业务代码。以上面的计算器案例说明： 通俗的说：AOP 是面向切面 (方面) 编程，利用 AOP 可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。即：可在不通过修改源代码方式，在主干功能里面添加新功能。 AOP 底层原理 AOP 底层使用动态代理。 第一种：有接口的情况 使用 JDK 动态代理。 创建接口实现类代理对象，增强类的方法。 数学计算器要求：① 执行加减乘除运算；② 日志增强：在程序执行期间追踪正在发生的活动；③ 验证增强：希望计算器只能处理正数的运算。 数学计算器的常规实现代码 (这里为了简便形参类型设置为 int)： 123456789101112/** * 计算器接口 */public interface ArithmeticCalculator &#123; Integer add(int i, int j); Integer subtract(int i, int j); Integer multiply(int i, int j); Integer div(int i, int j);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 常规方法实现类 */public class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public Integer add(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method add() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i + j; System.out.println(&quot;The method add() ends with [&quot; + result + &quot;]&quot;); return result; &#125; @Override public Integer subtract(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method subtract() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i - j; System.out.println(&quot;The method subtract() ends with [&quot; + result + &quot;]&quot;); return result; &#125; @Override public Integer multiply(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method multiply() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i * j; System.out.println(&quot;The method multiply() ends with [&quot; + result + &quot;]&quot;); return result; &#125; @Override public Integer div(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method div() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i / j; System.out.println(&quot;The method div() ends with [&quot; + result + &quot;]&quot;); return result; &#125;&#125; 存在的问题一：代码混乱。越来越多的非业务需求 (日志和验证等) 加入后，原有的业务方法急剧膨胀。每个方法在处理核心逻辑的同时还必须兼顾其他多个关注点。 存在的问题二：代码分散。以日志需求为例，只是为了满足这个单一需求，就不得不在多个模块 (方法) 里多次重复相同的日志代码。如果日志需求发生变化，必须修改所有模块。 使用 JDK 动态代理改进： 123456789101112/** * 计算器接口 */public interface ArithmeticCalculator &#123; Integer add(int i, int j); Integer subtract(int i, int j); Integer multiply(int i, int j); Integer div(int i, int j);&#125; 12345678910111213141516171819202122232425262728/** * ArithmeticCalculator实现类，只做计算的核心功能 */public class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public Integer add(int i, int j) &#123; System.out.println(&quot;add 核心方法&quot;); return i + j; &#125; @Override public Integer subtract(int i, int j) &#123; System.out.println(&quot;subtract 核心方法&quot;); return i - j; &#125; @Override public Integer multiply(int i, int j) &#123; System.out.println(&quot;multiply 核心方法&quot;); return i * j; &#125; @Override public Integer div(int i, int j) &#123; System.out.println(&quot;div 核心方法&quot;); return i / j; &#125;&#125; 12345678910111213141516171819202122232425/** * 日志处理器：在计算的过程中添加日志记录 */public class ArithmeticCalculatorLoggingHandler implements InvocationHandler &#123; private Object obj; public ArithmeticCalculatorLoggingHandler(Object obj) &#123; this.obj = obj; &#125; // 重写invoke()，增加日志处理 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;The method &quot; + method.getName() + &quot;() begins with &quot; + Arrays.toString(args)); Object result = method.invoke(obj, args); System.out.println(&quot;The method &quot; + method.getName() + &quot;() ends with [&quot; + result + &quot;]&quot;); return result; &#125; // 创建当前代理的代理对象 public static Object createProxy(Object obj) &#123; ArithmeticCalculatorLoggingHandler handler = new ArithmeticCalculatorLoggingHandler(obj); return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), handler); &#125;&#125; 12345678910111213141516171819202122232425262728293031/** * 验证处理器：在计算之前对参数进行验证 */public class ArithmeticCalculatorValidationHandler implements InvocationHandler &#123; private Object obj; public ArithmeticCalculatorValidationHandler(Object obj) &#123; this.obj = obj; &#125; // 重写invoke()，增加验证处理 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; for (Object arg : args) &#123; validate((int) arg); &#125; return method.invoke(obj, args); &#125; private void validate(int number) &#123; if (number &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + number); &#125; &#125; // 创建当前代理的代理对象 public static Object createProxy(Object obj) &#123; ArithmeticCalculatorValidationHandler handler = new ArithmeticCalculatorValidationHandler(obj); return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), handler); &#125;&#125; 12345678910// 测试方法public class SpringTest &#123; public static void main(String[] args) &#123; // 两级增强：普通计算 ---&gt; 日志增强 ---&gt; 验证增强 ArithmeticCalculator calculator = (ArithmeticCalculator) ArithmeticCalculatorValidationHandler.createProxy( ArithmeticCalculatorLoggingHandler.createProxy(new ArithmeticCalculatorImpl())); int addResult = calculator.add(-1, 2); System.out.println(&quot;result: &quot; + addResult); &#125;&#125; 第二种：没有接口的情况 使用 CGLIB 动态代理。 创建子类的代理对象，增强类的方法。 数学计算器要求：① 执行加减乘除运算；② 日志增强：在程序执行期间追踪正在发生的活动；③ 验证增强：希望计算器只能处理正数的运算。 数学计算器的常规实现代码 (这里为了简便形参类型设置为 int)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 常规方法实现类 */public class ArithmeticCalculator &#123; public Integer add(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method add() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i + j; System.out.println(&quot;The method add() ends with [&quot; + result + &quot;]&quot;); return result; &#125; public Integer subtract(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method subtract() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i - j; System.out.println(&quot;The method subtract() ends with [&quot; + result + &quot;]&quot;); return result; &#125; public Integer multiply(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method multiply() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i * j; System.out.println(&quot;The method multiply() ends with [&quot; + result + &quot;]&quot;); return result; &#125; public Integer div(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method div() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i / j; System.out.println(&quot;The method div() ends with [&quot; + result + &quot;]&quot;); return result; &#125;&#125; 使用 CGLIB 动态代理改进： 123456789101112131415161718192021public class ArithmeticCalculator &#123; public Integer add(int i, int j) &#123; System.out.println(&quot;add 核心方法&quot;); return i + j; &#125; public Integer subtract(int i, int j) &#123; System.out.println(&quot;subtract 核心方法&quot;); return i - j; &#125; public Integer multiply(int i, int j) &#123; System.out.println(&quot;multiply 核心方法&quot;); return i * j; &#125; public Integer div(int i, int j) &#123; System.out.println(&quot;div 核心方法&quot;); return i / j; &#125;&#125; 1234567891011121314151617181920/** * 日志拦截器：在计算的过程中添加日志记录 */public class ArithmeticCalculatorLoggingInterceptor implements MethodInterceptor &#123; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; System.out.println(&quot;The method &quot; + method.getName() + &quot;() begins with &quot; + Arrays.toString(args)); Object result = methodProxy.invokeSuper(obj, args); System.out.println(&quot;The method &quot; + method.getName() + &quot;() ends with [&quot; + result + &quot;]&quot;); return result; &#125; public static Object createProxy(Object obj) &#123; Enhancer enhancer = new Enhancer(); enhancer.setClassLoader(obj.getClass().getClassLoader()); enhancer.setSuperclass(obj.getClass()); enhancer.setCallback(new ArithmeticCalculatorLoggingInterceptor()); return enhancer.create(); &#125;&#125; 1234567891011121314151617181920212223242526/** * 验证处理器：在计算之前对参数进行验证 */public class ArithmeticCalculatorValidationInterceptor implements MethodInterceptor &#123; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; for (Object arg : args) &#123; validate((int) arg); &#125; return methodProxy.invokeSuper(obj, args); &#125; private void validate(int number) &#123; if (number &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + number); &#125; &#125; public static Object createProxy(Object obj) &#123; Enhancer enhancer = new Enhancer(); enhancer.setClassLoader(obj.getClass().getClassLoader()); enhancer.setSuperclass(obj.getClass()); enhancer.setCallback(new ArithmeticCalculatorValidationInterceptor()); return enhancer.create(); &#125;&#125; 12345678910// 测试方法public class SpringTest &#123; public static void main(String[] args) &#123; // 日志增强 ArithmeticCalculator arithmeticCalculator = (ArithmeticCalculator) ArithmeticCalculatorLoggingInterceptor .createProxy(new ArithmeticCalculator()); Integer addResult = arithmeticCalculator.add(-1, 2); System.out.println(addResult); &#125;&#125; CGLIB 不支持类嵌套增强，如果需要多个多个嵌套增强，需要其他方法实现，此处不涉及。 切入点表达式 AOP 相关术语： 连接点 (JoinPoint)**：类里面可以被增强的方法被称为连接点。**就是 Spring 允许使用通知的地方，基本每个方法的前、后 (两者都有也行)，或抛出异常时都可以是连接点，Spring 只支持方法连接点。 切入点 (Pointcut)**：实际被真正增强的方法，称为切入点。**在上面说的连接点的基础上，来定义切入点，假设一个类里，有 15 个方法，那就可能有几十个连接点，但不一定需要在所有方法附近都使用通知，而是只想让其中的几个方法使用通知。则在调用这几个方法之前，之后或者抛出异常时，利用切入点来定义这几个方法，让切入点来筛选连接点，选中那几个需要使用通知的方法。 通知 (Advice)**：实际增强的逻辑部分，也就是想要的功能，比如上面说的日志处理、验证处理等。**事先定义好，然后在想用的地方用一下。通知的类型：前置通知、最终通知、后置通知、异常通知、环绕通知。 前置通知 (Before Advice)：在切入点选择的连接点处的方法之前执行的通知，该通知不影响正常程序执行流程 (除非该通知抛出异常，该异常将中断当前方法链的执行而返回)。 最终通知 (After Advice)：在切入点选择的连接点处的方法之后执行的通知 (无论方法执行是否成功都会被调用)。 后置通知 (After returning Advice)：在切入点选择的连接点处的方法正常执行完毕时执行的通知，必须是连接点处的方法没抛出任何异常正常返回时才调用。 异常通知 (After throwing Advice)：在切入点选择的连接点处的方法抛出异常返回时执行的通知，必须是连接点处的方法抛出任何异常返回时才调用异常通知。 环绕通知 (Around Advices)：环绕着在切入点选择的连接点处的方法所执行的通知，环绕通知可以在方法调用之前和之后自定义任何行为，并且可以决定是否执行连接点处的方法、替换返回值、抛出异常等等。 切面 (Aspect)**：把通知应用到切入点的过程 (是动作)。**切面是通知和切入点的结合，也就是说，没连接点什么事情，连接点是为了好理解切入点而提出来的概念。 **引入 (introduction)**：允许我们向现有的类添加新方法属性，也就是把切面 (即新方法属性：通知定义的) 用到目标类中。 **目标 (target)**：引入中所提到的目标类，也就是要被通知的对象，即真正的业务逻辑，他可以在毫不知情的情况下，被织入切面。而自己专注于业务本身的逻辑。 **代理 (proxy)**：怎么实现整套 AOP 机制的，都是通过代理。 **织入 (weaving)**：把切面应用到目标对象来创建新的代理对象的过程。有 3 种方式，Spring 采用的是运行时。 切入点表达式： 切入点表达式作用：表明对哪个类里面的哪个方法进行增强。 语法结构： execution([权限修饰符] [返回类型] [类全类名] [方法名称]([参数列表]) )。 权限修饰符一般使用 * 替代；返回类型可以省略；参数列表使用 .. 代替。 举例 1：对 cn.xisun.spring.dao.UserDao 类里面的 add() 进行增强。 execution(* cn.xisun.spring.dao.UserDao.add(..)) 举例 2：对 cn.xisun.spring.dao.UserDao 类里面的所有的方法进行增强。 execution(* cn.xisun.spring.dao.UserDao.*(..)) 举例 3：对 cn.xisun.spring.dao 包里面所有类，类里面所有方法进行增强。 execution(* cn.xisun.spring.dao.*.*(..)) 举例 4：对 cn.xisun.spring.dao.UserDao 类里面返回 double 类型的方法进行增强。 execution(* double cn.xisun.spring.dao.UserDao.*(..)) 举例 5：对 cn.xisun.spring.dao.UserDao 类里面第一个参数为 double 类型的方法进行增强。 execution(* cn.xisun.spring.dao.UserDao.*(double, ..)) 举例 6：对 cn.xisun.spring.dao.UserDao 类里面里面的 add() 或 div() 进行增强。 execution(* cn.xisun.spring.dao.UserDao.add(..)) || execution(* cn.xisun.spring.dao.UserDap.div(..)) 在 AspectJ 中，切入点表达式可以通过 &amp;&amp;、||、! 等操作符结合起来。 实现 AOP 操作的方式 实现 AOP 操作的准备工作： Spring 框架一般都是基于 AspectJ 实现 AOP 操作： AspectJ 不是 Spring 组成部分，它是 Java 社区里最完整最流行的 AOP 框架。在 Spring 2.0 以上版本中，可以使用基于 AspectJ 注解或基于 xml 配置的 AOP。 基于 AspectJ 实现 AOP 操作： 基于注解方式实现 (常用)。 基于 xml 配置文件实现。 引入 AOP 和 AspectJ 的相关依赖： 123456789101112131415161718192021222324252627282930&lt;!-- Spring AOP和AspectJ相关依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;5.1.10.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.9.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;aopalliance&lt;/groupId&gt; &lt;artifactId&gt;aopalliance&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;net.sourceforge.cglib&lt;/groupId&gt; &lt;artifactId&gt;com.springsource.net.sf.cglib&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 基于注解方式实现 第一步：编写 Spring 配置文件，引入 context 和 aop 名称空间，并开启组件扫描，指明包路径，以及开启自动代理功能。 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=&quot;cn.xisun.spring.aop&quot;/&gt; &lt;!-- 开启Aspect生成代理对象--&gt; &lt;!-- 被增强类有接口，需指定proxy-target-class为true，如果没有接口，不需要指定这个参数 --&gt; &lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt;&lt;/beans&gt; 第二步：被增强类 (目标类) 的定义。添加 @Component 注解。 123456789public interface ArithmeticCalculator &#123; int add(int i, int j); int subtract(int i, int j); int multiply(int i, int j); int div(int i, int j);&#125; 1234567891011121314151617181920212223242526272829/** * 需要被增强的类 */@Componentpublic class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public Integer add(int i, int j) &#123; System.out.println(&quot;add 核心方法&quot;); return i + j; &#125; @Override public Integer subtract(int i, int j) &#123; System.out.println(&quot;subtract 核心方法&quot;); return i - j; &#125; @Override public Integer multiply(int i, int j) &#123; System.out.println(&quot;multiply 核心方法&quot;); return i * j; &#125; @Override public Integer div(int i, int j) &#123; System.out.println(&quot;div 核心方法&quot;); return i / j; &#125;&#125; 第三步：增强类 (切面类) 的定义。在增强类上添加 @Component 和 @Aspect 注解；在增强类里面，在作为通知的方法上面添加对应的通知类型注解，并使用切入点表达式配置需要增强的方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 日志增强 */@Component@Aspect@Order(1)public class ArithmeticCalculatorLoggingAspect &#123; // 相同的切入点抽取 @Pointcut(value = &quot;execution(* cn.xisun.spring.aop.ArithmeticCalculatorImpl.*(..))&quot;) public void pointSame() &#123; &#125; @Before(value = &quot;pointSame()&quot;) public void before() &#123; System.out.println(&quot;@Before 前置通知&quot;); &#125; @AfterReturning(value = &quot;pointSame()&quot;) public void afterReturning() &#123; System.out.println(&quot;@AfterReturning 后置通知&quot;); &#125; @After(value = &quot;pointSame()&quot;) public void after() &#123; System.out.println(&quot;@After 最终通知&quot;); &#125; @AfterThrowing(value = &quot;pointSame()&quot;) public void afterThrowing() &#123; System.out.println(&quot;@AfterThrowing 异常通知&quot;); &#125; @Around(value = &quot;execution(* cn.xisun.spring.aop.ArithmeticCalculatorImpl.add(..))&quot;) public Object around(ProceedingJoinPoint proceedingJoinPoint) &#123; System.out.println(&quot;@Around 环绕通知之前&quot;); // 被增强的方法执行，proceed是该方法的返回结果，如果原方法为void，则proceed为null Object proceed = null; try &#123; proceed = proceedingJoinPoint.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; System.out.println(&quot;@Around 环绕通知之后&quot;); return proceed; &#125;&#125; 前置通知、后置通知、异常通知和最终通知，可以额外接受一个 JoinPoint 参数，用来获取目标对象和目标方法相关信息，但是一定要保证这个参数是第一个参数。在环绕通知中必须显式的通过调用 ProceedingJoinPoint 来执行目标方法，否则目标方法不会执行。 123456789101112131415161718192021222324/** * 验证增强 */@Component@Aspect@Order(0)public class ArithmeticCalculatorValidationAspect &#123; @Before(value = &quot;execution(* cn.xisun.spring.aop.ArithmeticCalculatorImpl.*(..))&quot;) public void before(JoinPoint joinPoint) &#123; System.out.println(&quot;验证方法开始执行&quot;); Class&lt;?&gt; clazz = joinPoint.getTarget().getClass();// 当前执行的方法所属的类 String name = joinPoint.getSignature().getName();// 当前执行的方法名 Object[] args = joinPoint.getArgs();// 当前执行的方法的参数 for (Object arg : args) &#123; validate((int) arg); &#125; &#125; private void validate(int number) &#123; if (number &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + number); &#125; &#125;&#125; 第四步：测试方法。 12345678910111213141516171819public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); ArithmeticCalculator arithmeticCalculatorImpl = context.getBean(&quot;arithmeticCalculatorImpl&quot;, ArithmeticCalculatorImpl.class); Integer addResult = arithmeticCalculatorImpl.add(1, 2); System.out.println(&quot;计算结果：&quot; + addResult); &#125;&#125;输出结果：Spring 测试版本：5.2.7.RELEASE验证方法开始执行@Around 环绕通知之前@Before 前置通知add 核心方法@AfterReturning 后置通知@After 最终通知@Around 环绕通知之后计算结果：3 进阶操作： 1. 相同的切入点抽取： 在编写 AspectJ 切面时，可以直接在通知注解中书写切入点表达式。但同一个切点表达式可能会在多个通知中重复出现。此时，在 AspectJ 切面中，可以通过 @Pointcut 注解将一个重复的切入点声明成简单的方法，该切入点的方法体通常是空的。 切入点方法的访问权限控制符同时也控制着这个切入点的可见性。如果切入点要在多个切面中共用，最好将它们集中在一个公共的类中。在这种情况下，它们必须被声明为 public。在引入这个切入点时，必须将类名也包括在内。如果类没有与这个切面放在同一个包中，还必须包含包名。 比如，前面的日志增强类，各个通知的切入点表达式主要是 execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.*(..))，可以把它单独抽取出来： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 日志增强 */@Component@Aspectpublic class LoggingAspect implements CutAspect &#123; // 相同的切入点抽取 @Pointcut(value = &quot;execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.*(..))&quot;) public void pointSame() &#123; &#125; @Override @Before(value = &quot;pointSame()&quot;) public void before() &#123; System.out.println(&quot;@Before 前置通知&quot;); &#125; @Override @AfterReturning(value = &quot;pointSame()&quot;) public void afterReturning() &#123; System.out.println(&quot;@AfterReturning 后置通知&quot;); &#125; @Override @After(value = &quot;pointSame()&quot;) public void after() &#123; System.out.println(&quot;@After 最终通知&quot;); &#125; @Override @AfterThrowing(value = &quot;pointSame()&quot;) public void afterThrowing() &#123; System.out.println(&quot;@AfterThrowing 异常通知&quot;); &#125; @Override @Around(value = &quot;execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.add(..))&quot;) public Object around(ProceedingJoinPoint proceedingJoinPoint) &#123; System.out.println(&quot;proceedingJoinPoint: &quot; + proceedingJoinPoint); System.out.println(&quot;@Around 环绕通知之前&quot;); // 被增强的方法执行，proceed是该方法的返回结果，如果原方法为void，则proceed为null Object proceed = null; try &#123; proceed = proceedingJoinPoint.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; System.out.println(&quot;@Around 环绕通知之后&quot;); return proceed; &#125;&#125; 2. 指定切面的优先级： 在同一个连接点上应用不止一个切面时，除非明确指定，否则它们的优先级是不确定的。切面的优先级可以通过实现 Ordered 接口或利用 @Order(数值类型值) 注解指定。 若是实现 Ordered 接口，getOrder() 方法的返回值越小，优先级越高。 若是使用 @Order(数值类型值) 注解，数字类型值越小，优先级越高。 123456789@Component@Aspect@Order(1)public class ArithmeticCalculatorLoggingAspect implements CutAspect &#123;&#125; @Component@Aspect@Order(0)public class ArithmeticCalculatorValidationAspect implements CutAspect &#123;&#125; 完全使用注解方式实现 第一步：创建配置类，替代 xml 配置文件。其他操作，与基于注解方式实现 AOP 操作相同。 12345@Configuration@ComponentScan(basePackages = &#123;&quot;cn.xisun.spring.aop&quot;&#125;)@EnableAspectJAutoProxy(proxyTargetClass = true)public class SpringAopConfig &#123;&#125; @Configuration：表示这是一个配置类。 @ComponentScan(basePackages = &#123;&quot;cn.xisun.spring.aop&quot;&#125;)：配置包扫描路径为 cn.xisun.spring.aop。 @EnableAspectJAutoProxy(proxyTargetClass = true)：表示开启 AOP 自动代理。如果被增强类有接口，需指定 proxy-target-class 为 true，如果被增强类没有接口，不需要指定这个参数。 第二步：编写测试代码。 12345678910111213141516171819public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new AnnotationConfigApplicationContext(SpringAopConfig.class); ArithmeticCalculator arithmeticCalculatorImpl = context.getBean(&quot;arithmeticCalculatorImpl&quot;, ArithmeticCalculatorImpl.class); Integer addResult = arithmeticCalculatorImpl.add(1, 2); System.out.println(&quot;计算结果：&quot; + addResult); &#125;&#125;输出结果：Spring 测试版本：5.2.7.RELEASE验证方法开始执行@Around 环绕通知之前@Before 前置通知add 核心方法@AfterReturning 后置通知@After 最终通知@Around 环绕通知之后计算结果：3 基于 xml 配置文件实现 了解，不建议深究。 除了使用 AspectJ 注解声明切面，Spring 也支持在 bean 配置文件中声明切面。这种声明是通过 AOP 名称空间中的 xml 元素完成的。 正常情况下，基于注解的声明要优先于基于 xml 的声明，尽可能不使用基于 xml 的声明。通过 AspectJ 注解，切面可以与 AspectJ 兼容，而基于 xml 的配置则是 Spring 专有的。由于 AspectJ 得到越来越多的 AOP 框架支持，因此以注解风格编写的切面将会有更多重用的机会。 具体步骤： 第一步：编写 Spring 配置文件，引入 aop 名称空间，并开启自动代理功能。 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 开启Aspect生成代理对象--&gt; &lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt;&lt;/beans&gt; 第二步：定义增强类和被增强类。 123456789public interface ArithmeticCalculator &#123; Integer add(int i, int j); Integer subtract(int i, int j); Integer multiply(int i, int j); Integer div(int i, int j);&#125; 12345678910111213141516171819202122232425262728/** * 需要被增强的类 */public class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public Integer add(int i, int j) &#123; System.out.println(&quot;add 核心方法&quot;); return i + j; &#125; @Override public Integer subtract(int i, int j) &#123; System.out.println(&quot;subtract 核心方法&quot;); return i - j; &#125; @Override public Integer multiply(int i, int j) &#123; System.out.println(&quot;multiply 核心方法&quot;); return i * j; &#125; @Override public Integer div(int i, int j) &#123; System.out.println(&quot;div 核心方法&quot;); return i / j; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344public interface CutAspect &#123; /** * 前置通知：在方法执行前执行 */ default void before() &#123; &#125; default void before(JoinPoint joinPoint) &#123; &#125; /** * 后置通知 */ default void afterReturning() &#123; &#125; default void afterReturning(JoinPoint joinPoint) &#123; &#125; /** * 异常通知 */ default void afterThrowing() &#123; &#125; default void afterThrowing(JoinPoint joinPoint) &#123; &#125; /** * 环绕通知 */ default Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123; return proceedingJoinPoint.proceed(); &#125; /** * 最终通知 */ default void after() &#123; &#125; default void after(JoinPoint joinPoint) &#123; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738/** * 日志增强 */public class LoggingAspect implements CutAspect &#123; @Override public void before() &#123; System.out.println(&quot;@Before 前置通知&quot;); &#125; @Override public void afterReturning() &#123; System.out.println(&quot;@AfterReturning 后置通知&quot;); &#125; @Override public void after() &#123; System.out.println(&quot;@After 最终通知&quot;); &#125; @Override public void afterThrowing() &#123; System.out.println(&quot;@AfterThrowing 异常通知&quot;); &#125; @Override public Object around(ProceedingJoinPoint proceedingJoinPoint) &#123; System.out.println(&quot;@Around 环绕通知之前&quot;); // 被增强的方法执行，proceed是该方法的返回结果，如果原方法为void，则proceed为null Object proceed = null; try &#123; proceed = proceedingJoinPoint.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; System.out.println(&quot;@Around 环绕通知之后&quot;); return proceed; &#125;&#125; 123456789101112131415161718192021/** * 验证增强 */public class ArithmeticCalculatorValidationAspect implements CutAspect &#123; @Override public void before(JoinPoint joinPoint) &#123; System.out.println(&quot;验证方法开始执行&quot;); Class&lt;?&gt; clazz = joinPoint.getTarget().getClass();// 当前执行的方法所属的类 String name = joinPoint.getSignature().getName();// 当前执行的方法名 Object[] args = joinPoint.getArgs();// 当前执行的方法的参数 for (Object arg : args) &#123; validate((int) arg); &#125; &#125; private void validate(int number) &#123; if (number &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + number); &#125; &#125;&#125; 第三步：在 Spring 配置文件中配置两个类的对象。 123&lt;!-- 配置增强类LoggingAspect和被增强类ArithmeticCalculatorImpl的对象 --&gt;&lt;bean id=&quot;arithmeticCalculatorImpl&quot; class=&quot;cn.xisun.spring.dao.ArithmeticCalculatorImpl&quot;/&gt;&lt;bean id=&quot;loggingAspect&quot; class=&quot;cn.xisun.spring.dao.LoggingAspect&quot;/&gt; 第四步：配置切入点和切面。 12345678910111213141516&lt;!-- 配置aop切入点 --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点表达式 --&gt; &lt;aop:pointcut id=&quot;add&quot; expression=&quot;execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.add(..))&quot;/&gt; &lt;aop:pointcut id=&quot;all&quot; expression=&quot;execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.*(..))&quot;/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:aspect ref=&quot;loggingAspect&quot;&gt; &lt;!-- 配置通知的类型，以及对应的切入点 --&gt; &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:after-returning method=&quot;afterReturning&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;add&quot;/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 在 bean 配置文件中，所有的 Spring AOP 配置都必须定义在 &lt;aop:config&gt; 元素内部。对于每个切面而言，都要创建一个 &lt;aop:aspect&gt; 元素来为具体的切面实现引用后端 bean 实例。切面 bean 必须有一个标识符，供 &lt;aop:aspect&gt; 元素引用。 切入点： 切入点使用 &lt;aop:pointcut&gt; 元素声明。 切入点必须定义在 &lt;aop:aspect&gt; 元素下，或者直接定义在 &lt;aop:config&gt; 元素下。 切入点定义在 &lt;aop:aspect&gt; 元素下时：只对当前切面有效。 切入点定义在 &lt;aop:config&gt; 元素下：对所有切面都有效。 基于 xml 的 AOP 配置不允许在切入点表达式中用名称引用其他切入点。 通知： 在 aop 名称空间中，每种通知类型都对应一个特定的 xml 元素。 通知元素需要使用 &lt;pointcut-ref&gt; 来引用切入点，或用 &lt;pointcut&gt; 直接嵌入切入点表达式。 method 属性指定切面类中通知方法的名称。 xml 在配置带参数的通知时，有部分细节未搞清楚，ArithmeticCalculatorValidationAspect 配置不成功，不做探讨了。 第五步：测试方法。 123456789public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); ArithmeticCalculator arithmeticCalculatorImpl = context.getBean(&quot;arithmeticCalculatorImpl&quot;, ArithmeticCalculatorImpl.class); Integer add = arithmeticCalculatorImpl.add(7, 2); System.out.println(&quot;计算结果：&quot; + add); &#125;&#125; JdbcTemplate 为了使 JDBC 更加易于使用，Spring 在 JDBC API 上定义了一个抽象层，以此建立一个 JDBC 存取框架。 作为 Spring JDBC 框架的核心，JDBC 模板的设计目的是为不同类型的 JDBC 操作提供模板方法，通过这种方式，可以在尽可能保留灵活性的情况下，将数据库存取的工作量降到最低。 可以将 Spring 的 JdbcTemplate 看作是一个小型的轻量级持久化层框架，和我们之前使用过的 DBUtils 风格非常接近。 第一步：引入 JDBC 和 MySQL 的相关依赖。 1234567891011121314151617181920212223242526272829303132333435&lt;!-- Spring jdbc相关依赖--&gt;&lt;!-- spring-jdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- spring-tx: 事务相关 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- spring-orm: 整合Mybatis等框架需要 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- druid连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.20&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.19&lt;/version&gt;&lt;/dependency&gt; 第二步：开启组件扫描。 12&lt;!-- 开启组件扫描 --&gt;&lt;context:component-scan base-package=&quot;cn.xisun.spring&quot;/&gt; 第三步：配置数据库连接池。 12345678&lt;!-- 配置数据库连接池 --&gt;&lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;prop.driverClass&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;prop.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;prop.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;prop.password&#125;&quot;/&gt;&lt;/bean&gt; 1234prop.driverClass=com.mysql.cj.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/userDbprop.userName=rootprop.password=root 第四步：配置 JdbcTemplate 对象，注入 DataSource。 12345&lt;!-- 配置JdbcTemplate对象 --&gt;&lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt; 第五步：创建 dao 类，在 dao 注入 jdbcTemplate 对象；创建 service 类，在 service 类注入 dao 对象。 12public interface UserDao &#123;&#125; 12345678/** * dao类 */@Repositorypublic class UserDaoImpl implements UserDao &#123; @Autowired private JdbcTemplate jdbcTemplate;// 注入JdbcTemplate&#125; 12345678/** * service类 */@Servicepublic class UserService &#123; @Autowired private UserDao userDao;// 注入dao&#125; JdbcTemplate 操作数据库 — 添加、修改、删除。 创建对应数据库表的实体类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class User &#123; private String userId; private String userName; private String userStatus; public User() &#123; &#125; public User(String userId, String userName, String userStatus) &#123; this.userId = userId; this.userName = userName; this.userStatus = userStatus; &#125; public String getUserId() &#123; return userId; &#125; public void setUserId(String userId) &#123; this.userId = userId; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getUserStatus() &#123; return userStatus; &#125; public void setUserStatus(String userStatus) &#123; this.userStatus = userStatus; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (!Objects.equals(userId, user.userId)) &#123; return false; &#125; if (!Objects.equals(userName, user.userName)) &#123; return false; &#125; return Objects.equals(userStatus, user.userStatus); &#125; @Override public int hashCode() &#123; int result = userId != null ? userId.hashCode() : 0; result = 31 * result + (userName != null ? userName.hashCode() : 0); result = 31 * result + (userStatus != null ? userStatus.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;userId=&#x27;&quot; + userId + &#x27;\\&#x27;&#x27; + &quot;, userName=&#x27;&quot; + userName + &#x27;\\&#x27;&#x27; + &quot;, userStatus=&#x27;&quot; + userStatus + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 在 dao 中调用 JdbcTemplate 对象里面的 update() 进行数据库添加、修改和删除操作。 1234567public interface UserDao &#123; void add(User user); void update(User user); void delete(String userId);&#125; 12345678910111213141516171819202122232425262728293031@Repositorypublic class UserDaoImpl implements UserDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public void add(User user) &#123; // 1.创建sql语句 String sql = &quot;insert into t_user values(?, ?, ?)&quot;; // 2.设置参数 Object[] args = &#123;user.getUserId(), user.getUserName(), user.getUserStatus()&#125;; // 3.调用方法实现 int update = jdbcTemplate.update(sql, args); System.out.println(update); &#125; @Override public void update(User user) &#123; String sql = &quot;update t_user set user_name = ?, user_status = ? where user_id = ?&quot;; Object[] args = &#123;user.getUserName(), user.getUserStatus(), user.getUserId()&#125;; int update = jdbcTemplate.update(sql, args); System.out.println(update); &#125; @Override public void delete(String userId) &#123; String sql = &quot;delete from t_user where user_id = ?&quot;; int update = jdbcTemplate.update(sql, userId); System.out.println(update); &#125;&#125; 1234567891011121314151617@Servicepublic class UserService &#123; @Autowired private UserDao userDao; public void addUser(User user) &#123; userDao.add(user); &#125; public void updateUser(User user) &#123; userDao.update(user); &#125; public void deleteUser(String userId) &#123; userDao.delete(userId); &#125;&#125; 测试方法，执行 service 类相应方法实现添加、修改和删除操作。 1234567891011121314151617181920public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); User user = new User(); user.setUserId(&quot;1000&quot;); user.setUserName(&quot;Tom&quot;); user.setUserStatus(&quot;ok&quot;); // 添加 userService.addUser(user); // 修改 user.setUserStatus(&quot;ng&quot;); userService.updateUser(user); // 删除 userService.deleteUser(&quot;1000&quot;); &#125;&#125; JdbcTemplate 操作数据库 — 查询返回某个值、查询返回对象、查询返回集合。 在 dao 中调用 JdbcTemplate 对象里面的 query() 和 queryForObject() 进行数据库相应查询操作。 12345678public interface UserDao &#123; Integer selectCount(); User findUser(String userId); List&lt;User&gt; findAllUser();&#125; 1234567891011121314151617181920212223@Repositorypublic class UserDaoImpl implements UserDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public Integer selectCount() &#123; String sql = &quot;select count(*) from t_user&quot;; return jdbcTemplate.queryForObject(sql, Integer.class); &#125; @Override public User findUser(String userId) &#123; String sql = &quot;select * from t_user where user_id = ?&quot;; return jdbcTemplate.queryForObject(sql, new BeanPropertyRowMapper&lt;&gt;(User.class), userId); &#125; @Override public List&lt;User&gt; findAllUser() &#123; String sql = &quot;select * from t_user&quot;; return jdbcTemplate.query(sql, new BeanPropertyRowMapper&lt;&gt;(User.class)); &#125;&#125; RowMapper 是一个函数式接口，其中只有一个方法：T mapRow(ResultSet rs, int rowNum) throws SQLException，该方法的具体作用是将查询得到的每行数据映射到 ResultSet 中。 BeanPropertyRowMapper 类实现了 RowMapper 接口，其功能是：将查询得到的结果集的值，注入到对象属性中。 1234567891011121314151617@Servicepublic class UserService &#123; @Autowired private UserDao userDao; public Integer selectCount() &#123; return userDao.selectCount(); &#125; public User findUser(String userId) &#123; return userDao.findUser(userId); &#125; public List&lt;User&gt; findAllUser() &#123; return userDao.findAllUser(); &#125;&#125; 测试方法，执行 service 类相应方法实现查询返回某个值、查询返回对象和查询返回集合操作。 12345678910111213141516171819public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); // 查询返回某个值 Integer number = userService.selectCount(); System.out.println(number); // 查询返回对象 User user = userService.findUser(&quot;1001&quot;); System.out.println(user); // 查询返回集合 List&lt;User&gt; allUser = userService.findAllUser(); System.out.println(allUser); &#125;&#125; JdbcTemplate 操作数据库 — 批量添加、修改和删除操作。 在 dao 中调用 JdbcTemplate 对象里面的 batchUpdate() 进行数据库批量添加、修改和删除操作。 12345678public interface UserDao &#123; void batchAddUser(List&lt;Object[]&gt; batchArgs); void batchUpdateUser(List&lt;Object[]&gt; batchArgs); void batchDeleteUser(List&lt;Object[]&gt; batchArgs);&#125; 1234567891011121314151617181920212223242526@Repositorypublic class UserDaoImpl implements UserDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public void batchAddUser(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;insert into t_user values(?, ?, ?)&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); &#125; @Override public void batchUpdateUser(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;update t_user set user_name = ?, user_status = ? where user_id = ?&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); &#125; @Override public void batchDeleteUser(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;delete from t_user where user_id = ?&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); &#125;&#125; 1234567891011121314151617@Servicepublic class UserService &#123; @Autowired private UserDao userDao; public void batchAddUser(List&lt;Object[]&gt; batchArgs) &#123; userDao.batchAddUser(batchArgs); &#125; public void batchUpdateUser(List&lt;Object[]&gt; batchArgs) &#123; userDao.batchUpdateUser(batchArgs); &#125; public void batchDeleteUser(List&lt;Object[]&gt; batchArgs) &#123; userDao.batchDeleteUser(batchArgs); &#125;&#125; 测试方法，执行 service 类相应方法实现批量添加、修改和删除操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); // 批量添加 List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;(); // 方式一 List&lt;User&gt; list = new ArrayList&lt;&gt;(10); list.add(new User(&quot;1001&quot;, &quot;Tom&quot;, &quot;ok&quot;)); list.add(new User(&quot;1002&quot;, &quot;Jerry&quot;, &quot;ok&quot;)); list.add(new User(&quot;1003&quot;, &quot;Mike&quot;, &quot;ok&quot;)); for (User user : list) &#123; batchArgs.add(new Object[]&#123;user.getUserId(), user.getUserName(), user.getUserStatus()&#125;); &#125; // 方式二 /*Object[] o1 = &#123;&quot;1001&quot;, &quot;Tom&quot;, &quot;ok&quot;&#125;; Object[] o2 = &#123;&quot;1002&quot;, &quot;Jerry&quot;, &quot;ok&quot;&#125;; Object[] o3 = &#123;&quot;1003&quot;, &quot;Mike&quot;, &quot;ok&quot;&#125;; batchArgs.add(o1); batchArgs.add(o2); batchArgs.add(o3);*/ userService.batchAddUser(batchArgs); // 批量修改 List&lt;Object[]&gt; batchArgs1 = new ArrayList&lt;&gt;(); Object[] o4 = &#123;&quot;1001&quot;, &quot;Tom&quot;, &quot;ng&quot;&#125;; Object[] o5 = &#123;&quot;1002&quot;, &quot;Jerry&quot;, &quot;ng&quot;&#125;; Object[] o6 = &#123;&quot;1003&quot;, &quot;Mike&quot;, &quot;ng&quot;&#125;; batchArgs1.add(o4); batchArgs1.add(o5); batchArgs1.add(o6); userService.batchUpdateUser(batchArgs1); // 批量删除 List&lt;Object[]&gt; batchArgs2 = new ArrayList&lt;&gt;(); Object[] o7 = &#123;&quot;1002&quot;&#125;; Object[] o8 = &#123;&quot;1003&quot;&#125;; batchArgs2.add(o7); batchArgs2.add(o8); userService.batchDeleteUser(batchArgs2); &#125;&#125; 事务操作 事务是数据库操作的最基本单元，是一组由于逻辑上紧密关联而合并成一个整体 (工作单元) 的多个数据库操作，这些操作要么都执行成功，如果有一个失败所有操作都失败。典型应用场景：银行转账。 事务的四个特性 (ACID)： 原子性 (atomicity)：原子的本意是不可再分，事务的原子性表现为一个事务中涉及到的多个操作在逻辑上缺一不可。事务的原子性要求事务中的所有操作要么都执行，要么都不执行。 一致性 (consistency)：一致指的是数据的一致，具体是指：所有数据都处于满足业务规则的一致性状态。一致性原则要求：一个事务中不管涉及到多少个操作，都必须保证事务执行之前数据是正确的，事务执行之后数据仍然是正确的。如果一个事务在执行的过程中，其中某一个或某几个操作失败了，则必须将其他所有操作撤销，将数据恢复到事务执行之前的状态，这就是回滚。 隔离性 (isolation)：在应用程序实际运行过程中，事务往往是并发执行的，所以很有可能有许多事务同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏。隔离性原则要求多个事务在并发执行过程中不会互相干扰。 持久性 (durability)：持久性原则要求事务执行完成后，对数据的修改永久的保存下来，不会因各种系统错误或其他意外情况而受到影响。通常情况下，事务对数据的修改应该被写入到持久化存储器中。 事务管理一般添加到 JavaEE 三层结构里面的 Service 层 (业务逻辑层)。 事务管理操作有两种方式： 编程式事务管理： 执行步骤 — 使用原生的 JDBC API 进行事务管理： 获取数据库连接Connection对象 取消事务的自动提交 执行操作 正常完成操作时手动提交事务 执行失败时回滚事务 关闭相关资源 使用原生的 JDBC API 实现事务管理是所有事务管理方式的基石，同时也是最典型的编程式事务管理。编程式事务管理需要将事务管理代码嵌入到业务方法中来控制事务的提交和回滚。在使用编程的方式管理事务时，必须在每个事务操作中包含额外的事务管理代码。相对于核心业务而言，事务管理的代码显然属于非核心业务，如果多个模块都使用同样模式的代码进行事务管理，显然会造成较大程度的代码冗余。 声明式事务管理： 大多数情况下声明式事务比编程式事务管理更好：它将事务管理代码从业务方法中分离出来，以声明的方式来实现事务管理。事务管理代码的固定模式作为一种横切关注点，可以通过 AOP 方法模块化，进而借助 Spring AOP 框架实现声明式事务管理。 Spring 既支持编程式事务管理，也支持声明式事务管理。 Spring 进行声明式事务管理，底层使用 AOP 原理。 Spring 在不同的事务管理 API 之上定义了一个抽象层，通过配置的方式使其生效，从而让应用程序开发人员不必了解事务管理 API 的底层实现细节，就可以使用 Spring 的事务管理机制。 Spring 的事务管理器： Spring 的核心事务管理抽象是 PlatformTransactionManager。它为事务管理封装了一组独立于技术的方法。无论使用 Spring 的哪种事务管理策略 (编程式或声明式)，事务管理器都是必须的。 DataSourceTransactionManager：在应用程序中只需要处理一个数据源，而且通过 JDBC 存取。 JtaTransactionManager：在 JavaEE 应用服务器上用 JTA (Java Transaction API) 进行事务管理。 HibernateTransactionManager：用 Hibernate 框架存取数据库。 事务管理器可以以普通的 bean 的形式声明在 Spring IOC 容器中。 Spring 声明式事务管理的两种实现方式： 基于 xml 配置文件方式 基于注解方式 (常用) Spring 基于注解实现声明式事务管理： 第一步：引入 jdbc 和 mysql 的相关依赖、开启组件扫描、配置数据库连接池、配置 JdbcTemplate 对象，注入 DataSource。具体操作见 JdbcTemplate。 第二步：在 Spring 配置文件中，配置事务管理器并注入数据源。 12345&lt;!-- 配置事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt; 事务管理器的名字一定要叫 transactionManager，不然会抛异常。 第三步：在 Spring 配置文件中，引入 tx 名称空间并开启事务注解。 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt;&lt;/beans&gt; 12&lt;!-- 开启事务注解 --&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; 第四步：创建 dao 类，在 dao 注入 jdbcTemplate 对象；创建 service 类，在 service 类注入 dao 对象。具体操作见 JdbcTemplate。 第五步：在需要进行事务控制的方法或类上添加 @Transactional 注解。 如果把 @Transactional 注解添加类上面，则这个类里面所有的方法都添加事务。 如果把 @Transactional 注解添加方法上面，则为这个方法添加事务。 代码一览： 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt; &lt;!-- 开启组件扫描 --&gt; &lt;context:component-scan base-package=&quot;cn.xisun.spring.dao,cn.xisun.spring.service&quot;/&gt; &lt;!-- 配置数据库连接池 --&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;prop.driverClass&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;prop.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;prop.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;prop.password&#125;&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置JdbcTemplate对象 --&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 开启事务注解 --&gt; &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt;&lt;/beans&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class Account &#123; private Integer accountId; private String accountName; private Integer accountBalance; public Account() &#123; &#125; public Account(Integer accountId, String accountName, Integer accountBalance) &#123; this.accountId = accountId; this.accountName = accountName; this.accountBalance = accountBalance; &#125; public Integer getAccountId() &#123; return accountId; &#125; public void setAccountId(Integer accountId) &#123; this.accountId = accountId; &#125; public String getAccountName() &#123; return accountName; &#125; public void setAccountName(String accountName) &#123; this.accountName = accountName; &#125; public Integer getAccountBalance() &#123; return accountBalance; &#125; public void setAccountBalance(Integer accountBalance) &#123; this.accountBalance = accountBalance; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; Account account = (Account) o; if (!Objects.equals(accountId, account.accountId)) &#123; return false; &#125; if (!Objects.equals(accountName, account.accountName)) &#123; return false; &#125; return Objects.equals(accountBalance, account.accountBalance); &#125; @Override public int hashCode() &#123; int result = accountId != null ? accountId.hashCode() : 0; result = 31 * result + (accountName != null ? accountName.hashCode() : 0); result = 31 * result + (accountBalance != null ? accountBalance.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;Account&#123;&quot; + &quot;accountId=&quot; + accountId + &quot;, accountName=&#x27;&quot; + accountName + &#x27;\\&#x27;&#x27; + &quot;, accountBalance=&quot; + accountBalance + &#x27;&#125;&#x27;; &#125;&#125; 123456789public interface AccountDao &#123; void reduceMoney(); void addMoney(); // 上面两个方法可以合并 int tranfer(String accountName, int money);&#125; 123456789101112131415161718192021222324252627282930313233@Repositorypublic class AccountDaoImpl implements AccountDao &#123; @Autowired private JdbcTemplate jdbcTemplate; // lucy少钱 @Override public void reduceMoney() &#123; String sql = &quot;update t_account set account_balance = account_balance - ? where account_name = ?&quot;; jdbcTemplate.update(sql, 100, &quot;lucy&quot;); &#125; // mary多钱 @Override public void addMoney() &#123; String sql = &quot;update t_account set account_balance = account_balance + ? where account_name = ?&quot;; jdbcTemplate.update(sql, 100, &quot;mary&quot;); &#125; // 上面两个方法可以合并 @Override public int tranfer(String accountName, int money) &#123; // 创建 SQL 语句 String sql = &quot;update t_account set account_balance = account_balance - ? where account_name = ?&quot;; // SQL 语句参数 Object[] args = &#123;money, accountName&#125;; // 执行 SQL 语句 int insertRows = jdbcTemplate.update(sql, args); return insertRows; &#125;&#125; 123456789101112131415161718192021@Service@Transactionalpublic class AccountService &#123; @Autowired private AccountDao accountDao; // 转账的方法一 public void accountMoney() &#123; // lucy 少 100 accountDao.reduceMoney(); // mary 多 100 accountDao.addMoney(); &#125; // 转账的方法二 public void transfer(String srcAccountName, String destAccountName, int money) &#123; accountDao.tranfer(srcAccountName, money); accountDao.tranfer(destAccountName, -money); System.out.println(srcAccountName + &quot; 向 &quot; + destAccountName + &quot; 转账 &quot; + money + &quot; 元&quot;); &#125;&#125; 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); AccountService accountService = context.getBean(&quot;accountService&quot;, AccountService.class); // 测试方法一 accountService.accountMoney(); // 测试方法二 accountService.transfer(&quot;lucy&quot;, &quot;mary&quot;, 100); &#125;&#125; Spring 声明式事务管理参数配置： @Transactional 注解里面可以配置事务的相关参数。 123456789101112131415161718192021222324252627@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Transactional &#123; @AliasFor(&quot;transactionManager&quot;) String value() default &quot;&quot;; @AliasFor(&quot;value&quot;) String transactionManager() default &quot;&quot;; Propagation propagation() default Propagation.REQUIRED; Isolation isolation() default Isolation.DEFAULT; int timeout() default -1; boolean readOnly() default false; Class&lt;? extends Throwable&gt;[] rollbackFor() default &#123;&#125;; String[] rollbackForClassName() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] noRollbackFor() default &#123;&#125;; String[] noRollbackForClassName() default &#123;&#125;;&#125; propagation：事务传播行为。 对数据库表数据进行变化的操作叫事务方法。当一个事务方法被另一个事务方法调用时，必须指定事务应该如何传播。 事务的传播行为可以由传播属性指定，Spring 中定义了 7 种传播行为： REQUIRED 和 REQUIRED_NEW 是常用的两种事务传播行为。REQUIRED 是默认的事务传播行为。 REQUIRED 和 REQUIRED_NEW 的区别示例如下： Spring 中，可以通过指定 @Transactional 注解的 propagation 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 propagation 属性值，设置事务传播行为： 1@Transactional(propagation = Propagation.REQUIRES_NEW) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; propagation=&quot;REQUIRES_NEW&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; isolation：事务隔离级别。 事务的特性之一是隔离性，能够使得多事务在执行过程中，不会互相干扰。 但是，如果不考虑事务的隔离性，会产生三个读的问题：脏读、不可重复读、幻 (虚) 读。 脏读：一个未提交的事务读取到另一个事务未提交的数据。通俗点说：事务 A 更新了数据，但事务 A 还未提交，数据就被事务 B 读取了。 不可重复读：一个未提交的事务读取到另一个已提交事务修改的数据。通俗点说：一个事务中多次读取一个数据的结果不一致。 幻 (虚) 读：一个未提交的事务读取到另一个已提交事务新增的数据。通俗点说：一个事务多次读取同一个条件的数据时，数据的总条目不一致。 举例说明，假设现在有两个事务：Transaction01 和 Transaction02 并发执行。 ① 脏读： [1] Transaction01 将某条记录的 AGE 值从 20 修改为 30，但还未提交。 [2] Transaction02 读取了 Transaction01 更新后的值：30。 [3] Transaction01 回滚，AGE 值恢复到了 20。 [4] Transaction02 读取到的 30 就是一个无效的值。 ② 不可重复读： [1] Transaction01 读取了 AGE 值为 20。 [2] Transaction02 将 AGE 值修改为 30 并提交。 [3] Transaction01 再次读取 AGE 值为 30，和第一次读取不一致。 ③ 幻 (虚) 读： [1] Transaction01 读取了 STUDENT 表中的一部分数据。 [2] Transaction02 向 STUDENT 表中插入了新的行。 [3] Transaction01 同一条件下再次读取 STUDENT 表时，多出了一些行。 通过设置事务隔离级别，解决读问题： 数据库系统必须具有隔离并发运行各个事务的能力，使它们不会相互影响，避免各种并发问题。一个事务与其他事务隔离的程度称为隔离级别。SQL标准中规定了多种事务隔离级别，不同隔离级别对应不同的干扰程度，隔离级别越高，数据一致性就越好，但并发性越弱。 各个隔离级别解决并发问题的能力： 隔离级别 脏读 不可重复读 幻 (虚) 读 READ UNCOMMITTED (读未提交) 有 有 有 READ COMMITTED (读已提交) 无 有 有 REPEATABLE READ (可重复读) 无 无 有 SERIALIZABLE (串行化) 无 无 无 各种数据库产品对事务隔离级别的支持程度： 隔离级别 Oracle MySQL READ UNCOMMITTED × √ READ COMMITTED √ √ REPEATABLE READ × √ (默认) SERIALIZABLE √ √ Spring 中，可以通过指定 @Transactional 注解的 isolation 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 isolation 属性值，设置事务隔离级别： 1@Transactional(isolation = Isolation.REPEATABLE_READ) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; isolation=&quot;REPEATABLE_READ&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; timeout：事务超时时间。 事务需要在一定时间内进行提交，如果不提交则进行回滚。 默认值是 -1，设置时间以秒为单位。 Spring 中，可以通过指定 @Transactional 注解的 timeout 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 timeout 属性值，设置事务超时时间： 1@Transactional(timeout = 20) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; timeout=&quot;20&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; readOnly：事务是否只读。 读：查询操作，写：添加、修改、删除操作。 由于事务可以在行和表上获得锁，因此长事务会占用资源，并对整体性能产生影响。如果一个事物只读取数据但不做修改，数据库引擎可以对这个事务进行优化。 readOnly 默认值为 false，表示可以查询，也可以添加、修改和删除。 若设置 readOnly 值是 true，表示这个事务只读取数据但不更新数据, 这样可以帮助数据库引擎优化事务。 Spring 中，可以通过指定 @Transactional 注解的 readOnly 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 read-only 属性值，设置事务超时时间： 1@Transactional(readOnly = true) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; read-only=&quot;true&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; rollbackFor：事务回滚触发条件。 设置出现哪些异常时，必须进行事务回滚，可以为多个。 Spring 中，可以通过指定 @Transactional 注解的 rollbackFor 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 rollback-for 属性值，设置事务超时时间： 1@Transactional(rollbackFor = &#123;IOException.class, SQLException.class&#125;) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; rollback-for=&quot;java.io.IOException, java.sql.SQLException&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; noRollbackFor：事务不回滚触发条件。 设置出现哪些异常时，不进行事务回滚，可以为多个。 Spring 中，可以通过指定 @Transactional 注解的 noRollbackFor 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 no-rollback-for 属性值，设置事务超时时间： 1@Transactional(noRollbackFor = &#123;ArithmeticException.class&#125;) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; no-rollback-for=&quot;java.lang.ArithmeticException&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; Spring 基于 xml 配置文件实现声明式事务管理 (了解)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 配置数据库连接池 --&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;prop.driverClass&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;prop.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;prop.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;prop.password&#125;&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置JdbcTemplate对象 --&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置通知 --&gt; &lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;!-- 配置事务参数 --&gt; &lt;tx:attributes&gt; &lt;!-- 指定哪种规则的方法上面添加事务 --&gt; &lt;tx:method name=&quot;accountMoney&quot; propagation=&quot;REQUIRED&quot; no-rollback-for=&quot;java.lang.ArithmeticException&quot;/&gt; &lt;!-- 下面的配置含义是account开头的方法 --&gt; &lt;!--&lt;tx:method name=&quot;account*&quot;/&gt;--&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 配置切入点和切面 --&gt; &lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* cn.xisun.spring.service.AccountService.*(..))&quot;/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref=&quot;accountService&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;/aop:config&gt;&lt;/beans&gt; Spring 基于完全注解实现声明式事务管理： 方式一： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Configuration@ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;)@EnableTransactionManagementpublic class SpringConfig &#123; /** * 创建数据库连接池 * 从jdbc.properties配置文件中获取数据库连接信息 * * Bean注解：该注解只能写在方法上，表明使用此方法创建一个对象，并且放入Spring容器。 * name属性：给当前@Bean注解方法创建的对象指定一个名称(即bean的id)，默认bean的名称就是其方法名。 * * @return 向IOC容器注入一个name为dataSource的bean */ @Bean(name = &quot;dataSource&quot;) public DataSource createDataSource() &#123; Properties pros = new Properties(); try (InputStream resource = this.getClass().getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;)) &#123; pros.load(resource); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(pros.getProperty(&quot;prop.driverClass&quot;)); dataSource.setUrl(pros.getProperty(&quot;prop.url&quot;)); dataSource.setUsername(pros.getProperty(&quot;prop.userName&quot;)); dataSource.setPassword(pros.getProperty(&quot;prop.password&quot;)); return dataSource; &#125; /** * 创建JdbcTemplate对象 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为jdbcTemplate的bean */ @Bean(name = &quot;jdbcTemplate&quot;) public JdbcTemplate createJdbcTemplate(DataSource dataSource) &#123; JdbcTemplate jdbcTemplate = new JdbcTemplate(); jdbcTemplate.setDataSource(dataSource); return jdbcTemplate; &#125; /** * 创建事务管理器 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为dataSourceTransactionManager的bean */ @Bean(name = &quot;dataSourceTransactionManager&quot;) public DataSourceTransactionManager createDataSourceTransactionManager(DataSource dataSource) &#123; DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); dataSourceTransactionManager.setDataSource(dataSource); return dataSourceTransactionManager; &#125;&#125; @Configuration：标识这是一个配置类。 @ComponentScan(basePackages = &quot;cn.xisun.spring&quot;)：配置包扫描路径。 @EnableTransactionManagement：开启注解事务管理。 方式二： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class JdbcConfig &#123; /** * 创建数据库连接池 * 从jdbc.properties配置文件中获取数据库连接信息 * * Bean注解：该注解只能写在方法上，表明使用此方法创建一个对象，并且放入Spring容器。 * name属性：给当前@Bean注解方法创建的对象指定一个名称(即bean的id)，默认bean的名称就是其方法名。 * * @return 向IOC容器注入一个name为dataSource的bean */ @Bean(name = &quot;dataSource&quot;) public DataSource createDataSource() &#123; Properties pros = new Properties(); try (InputStream resource = this.getClass().getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;)) &#123; pros.load(resource); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(pros.getProperty(&quot;prop.driverClass&quot;)); dataSource.setUrl(pros.getProperty(&quot;prop.url&quot;)); dataSource.setUsername(pros.getProperty(&quot;prop.userName&quot;)); dataSource.setPassword(pros.getProperty(&quot;prop.password&quot;)); return dataSource; &#125; /** * 创建JdbcTemplate对象 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为jdbcTemplate的bean */ @Bean(name = &quot;jdbcTemplate&quot;) public JdbcTemplate createJdbcTemplate(DataSource dataSource) &#123; JdbcTemplate jdbcTemplate = new JdbcTemplate(); jdbcTemplate.setDataSource(dataSource); return jdbcTemplate; &#125; /** * 创建事务管理器 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为dataSourceTransactionManager的bean */ @Bean(name = &quot;dataSourceTransactionManager&quot;) public DataSourceTransactionManager createDataSourceTransactionManager(DataSource dataSource) &#123; DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); dataSourceTransactionManager.setDataSource(dataSource); return dataSourceTransactionManager; &#125;&#125; 1234567@Configuration @ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;) @EnableTransactionManagement @Import(JdbcConfig.class) public class SpringConfig &#123; &#125; @Import(JdbcConfig.class)：引入 JdbcConfig.class 配置文件。 方式三： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Configuration@ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;)@EnableTransactionManagement@PropertySource(value = &quot;classpath:jdbc.properties&quot;)public class SpringConfig &#123; @Value(&quot;$&#123;prop.driverClass&#125;&quot;) private String driverClass; @Value(&quot;$&#123;prop.url&#125;&quot;) private String url; @Value(&quot;$&#123;prop.userName&#125;&quot;) private String userName; @Value(&quot;$&#123;prop.password&#125;&quot;) private String password; /** * 创建数据库连接池 * 从jdbc.properties配置文件中获取数据库连接信息 * * Bean注解：该注解只能写在方法上，表明使用此方法创建一个对象，并且放入Spring容器。 * name属性：给当前@Bean注解方法创建的对象指定一个名称(即bean的id)，默认bean的名称就是其方法名。 * * @return 向IOC容器注入一个name为dataSource的bean */ @Bean(name = &quot;dataSource&quot;) public DataSource createDataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(driverClass); dataSource.setUrl(url); dataSource.setUsername(userName); dataSource.setPassword(password); return dataSource; &#125; /** * 创建JdbcTemplate对象 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为jdbcTemplate的bean */ @Bean(name = &quot;jdbcTemplate&quot;) public JdbcTemplate createJdbcTemplate(DataSource dataSource) &#123; JdbcTemplate jdbcTemplate = new JdbcTemplate(); jdbcTemplate.setDataSource(dataSource); return jdbcTemplate; &#125; /** * 创建事务管理器 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为dataSourceTransactionManager的bean */ @Bean(name = &quot;dataSourceTransactionManager&quot;) public DataSourceTransactionManager createDataSourceTransactionManager(DataSource dataSource) &#123; DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); dataSourceTransactionManager.setDataSource(dataSource); return dataSourceTransactionManager; &#125;&#125; @PropertySource(value = &quot;classpath:jdbc.properties&quot;)：标识 properties 配置文件的路径。 @Value：给当前属性赋值，取值来源于读取的 jdbc.properties 配置文件中的内容。 测试方法： 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new AnnotationConfigApplicationContext(SpringConfig.class); AccountService accountService = context.getBean(&quot;accountService&quot;, AccountService.class); // 测试方法一 accountService.accountMoney(); // 测试方法二 accountService.transfer(&quot;lucy&quot;, &quot;mary&quot;, 100); &#125;&#125; Spring5 框架部分新功能 整个 Spring5 框架的代码基于 JDK 8，运行时兼容 JDK 9，许多不建议使用的类和方法在代码库中被删除。 Spring5 框架自带了通用的日志封装。 Spring5 已经移除 Log4jConfigListener，官方建议使用 Log4j2。 Spring5 框架整合 Log4j2： 第一步：引入 jar 包。 第二步：创建 log4j2.xml 配置文件，名称只能是这个。 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!-- 日志级别以及优先级排序: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL --&gt;&lt;!-- Configuration后面的status用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，可以看到log4j2内部各种详细输出 --&gt;&lt;configuration status=&quot;INFO&quot;&gt; &lt;!-- 先定义所有的appender --&gt; &lt;appenders&gt; &lt;!-- 输出日志信息到控制台 --&gt; &lt;console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!-- 控制日志输出的格式 --&gt; &lt;PatternLayout pattern=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n&quot;/&gt; &lt;/console&gt; &lt;/appenders&gt; &lt;!-- 然后定义logger，只有定义了logger并引入的appender，appender才会生效 --&gt; &lt;!-- root：用于指定项目的根日志，如果没有单独指定Logger，则会使用root作为默认的日志输出 --&gt; &lt;loggers&gt; &lt;root level=&quot;info&quot;&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;/loggers&gt;&lt;/configuration&gt; Spring5 框架核心容器支持 @Nullable 注解。 @Nullable 注解可以使用在方法上面，属性上面，参数上面，表示方法返回值可以为空，属性值可以为空，参数值可以为空。 123// 使用在方法上面，表示方法返回值可以为空@NullableString getId(); 123// 使用在属性上面，表示属性值可以为空@Nullableprivate String bookName; 1234// 使用在参数上面，表示参数值可以为空public &lt;T&gt; void registerBean(@Nullable String beanName, Class&lt;T&gt; beanClass, @Nullable Supplier&lt;T&gt; supplier, BeanDefinitionCustomizer... customizers) &#123; this.reader.registerBean(beanClass, beanName, supplier, customizers);&#125; Spring5 核心容器支持函数式风格 GenericApplicationContext。 1234567891011121314public class SpringTest &#123; public static void main(String[] args) &#123; // 1.创建GenericApplicationContext对象 GenericApplicationContext context = new GenericApplicationContext(); // 2.调用context的方法注册对象 context.refresh();// 清空context中的内容 // context.registerBean(Account.class, Account::new);// 方式一：注册bean context.registerBean(&quot;account&quot;, Account.class, Account::new);// 方式二：注册bean // 3.获取在Spring中注册的对象 // Account account = (Account) context.getBean(&quot;cn.xisun.spring.entity.Account&quot;);// 方式一：获取bean Account account = (Account) context.getBean(&quot;account&quot;);// 方式二：获取bean System.out.println(account); &#125;&#125; Spring5 支持整合 JUnit5。 Spring5 整合 JUnit4。 第一步：引入 Spring 相关针对测试依赖。 第二步：创建测试类，使用注解方式完成。 1234567891011@RunWith(SpringJUnit4ClassRunner. class) //单元测试框架@ContextConfiguration( &quot;classpath:bean1.xml&quot;) //加载配置文件public class JTest4 &#123; @Autowired private UserService userService; @Test public void test1() &#123; userService.accountMoney(); &#125;&#125; Spring5 整合 JUnit5。 第一步：引入 JUnit5 的依赖。 12345678910111213&lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt; &lt;version&gt;5.6.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 第二步：创建测试类，使用注解 @ExtendWith 和 @ContextConfiguration 完成。 1234567891011121314151617181920212223242526package cn.xisun.spring.entity;import cn.xisun.spring.service.AccountService;import org.junit.jupiter.api.Test;import org.junit.jupiter.api.extension.ExtendWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit.jupiter.SpringExtension;import static org.junit.jupiter.api.Assertions.*;/** * @author XiSun * @Date 2021/4/23 14:03 */@ExtendWith(SpringExtension.class)@ContextConfiguration(&quot;classpath:spring.xml&quot;)class AccountTest &#123; @Autowired private AccountService accountService; @Test void test() &#123; accountService.transfer(&quot;Tom&quot;, &quot;Jerry&quot;, 100); &#125;&#125; 第三步：使用一个复合注解 @SpringJUnitConfig 替代上面两个注解完成整合。 1234567891011121314151617181920212223package cn.xisun.spring.entity;import cn.xisun.spring.service.AccountService;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.junit.jupiter.SpringJUnitConfig;import static org.junit.jupiter.api.Assertions.*;/** * @author XiSun * @Date 2021/4/23 14:03 */@SpringJUnitConfig(locations = &quot;classpath:spring.xml&quot;)class AccountTest &#123; @Autowired private AccountService accountService; @Test void test() &#123; accountService.transfer(&quot;Tom&quot;, &quot;Jerry&quot;, 100); &#125;&#125; 本文参考https://www.bilibili.com/video/BV1Vf4y127N5 https://blog.csdn.net/oneby1314/article/details/114259893 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"}]},{"title":"Java 新特性","slug":"java-newfeature","date":"2021-04-09T08:16:06.000Z","updated":"2021-04-13T01:01:56.012Z","comments":true,"path":"2021/04/09/java-newfeature/","link":"","permalink":"http://example.com/2021/04/09/java-newfeature/","excerpt":"","text":"Java 8 的新特性简介 Java 8 (又称为 jdk 1.8) 是 Java 语言开发的一个主要版本。Java 8 是 oracle 公司于 2014 年 3 月发布，可以看成是自 Java 5 以来最具革命性的版本。Java 8 为 Java 语言、编译器、类库、开发工具与 JVM 带来了大量新特性。 Java 8 新特性一览： 速度更快。 代码更少 (增加了新的语法：Lambda 表达式)。 强大的 Stream API。 便于并行。 最大化减少空指针异常：Optional。 Nashorn 引擎，允许在 JVM上运行 JS 应用。 并行流和串行流： 并行流就是把一个内容分成多个数据块，并用不同的线程分别处理每个数据块的流。相比较串行的流，并行的流可以很大程度上提高程序的执行效率。 Java 8 中将并行进行了优化，我们可以很容易的对数据进行并行操作。Stream API 可以声明性地通过 parallel() 与 sequential() 在并行流与顺序流之间进行切换。 Lambda 表达式 Lambda 是一个匿名函数，我们可以把 Lambda 表达式理解为是一段可以传递的代码 (将代码像数据一样进行传递)。使用它可以写出更简洁、更灵活的代码。作为一种更紧凑的代码风格，使 Java 的语言表达能力得到了提升。 Lambda 表达式：在 Java 8 语言中引入的一种新的语法元素和操作符。这个操作符为 “-&gt;”，该操作符被称为 Lambda 操作符或箭头操作符。它将 Lambda 分为两个部分： 左侧：指定了 Lambda 表达式需要的参数列表。 右侧：指定了 Lambda 体，是抽象方法的实现逻辑，也即 Lambda 表达式要执行的功能。 语法格式： 类型推断：上述 Lambda 表达式中的参数类型都是由编译器推断得出的。Lambda 表达式中无需指定类型，程序依然可以编译，这是因为 javac 根据程序的上下文，在后台推断出了参数的类型。Lambda 表达式的类型依赖于上下文环境，是由编译器推断出来的。这就是所谓的 **”类型推断”**。 123456789101112131415161718192021222324public class LambdaTest &#123; // 语法格式三：数据类型可以省略，因为可由编译器推断得出，称为&quot;类型推断&quot; @Test public void test3() &#123; Consumer&lt;String&gt; con1 = (String s) -&gt; &#123; System.out.println(s); &#125;; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con2 = (s) -&gt; &#123; System.out.println(s); &#125;; con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125; @Test public void test4() &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();// 类型推断，ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); int[] arr = &#123;1, 2, 3&#125;;// 类型推断，int[] arr = new int[]&#123;1, 2, 3&#125;; &#125;&#125; Lambda 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143/** * Lambda表达式的使用 * * 1.举例: (o1,o2) -&gt; Integer.compare(o1,o2); * 2.格式: * -&gt;: lambda操作符或箭头操作符 * -&gt;左边：lambda形参列表(其实就是接口中的抽象方法的形参列表) * -&gt;右边：lambda体(其实就是重写的抽象方法的方法体) * * 3.Lambda表达式的使用: (分为6种情况介绍) * * 总结: * -&gt;左边: lambda形参列表的参数类型可以省略(类型推断)；如果lambda形参列表只有一个参数，其一对()也可以省略，其他情况不能省略 * -&gt;右边: lambda体应该使用一对&#123;&#125;包裹；如果lambda体只有一条执行语句(也可能是return语句)，省略这一对&#123;&#125;和return关键字 * * 4.Lambda表达式的本质: 作为函数式接口的实例 * * 5.如果一个接口中，只声明了一个抽象方法，则此接口就称为函数式接口。我们可以在一个接口上使用@FunctionalInterface注解， * 这样做可以检查它是否是一个函数式接口。 * * 6.所有以前用匿名实现类表示的现在都可以用Lambda表达式来写 */public class LambdaTest &#123; // 语法格式一：无参，无返回值 @Test public void test1() &#123; Runnable r1 = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;我爱北京天安门&quot;); &#125; &#125;; r1.run(); System.out.println(&quot;***********************&quot;); Runnable r2 = () -&gt; &#123; System.out.println(&quot;我爱北京故宫&quot;); &#125; r2.run(); &#125; // 语法格式二：Lambda需要一个参数，但是没有返回值。 @Test public void test2() &#123; Consumer&lt;String&gt; con = new Consumer&lt;String&gt;() &#123; @Override public void accept(String s) &#123; System.out.println(s); &#125; &#125;; con.accept(&quot;谎言和誓言的区别是什么？&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con1 = (String s) -&gt; &#123; System.out.println(s); &#125; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125; // 语法格式三：数据类型可以省略，因为可由编译器推断得出，称为&quot;类型推断&quot; @Test public void test3() &#123; Consumer&lt;String&gt; con1 = (String s) -&gt; &#123; System.out.println(s); &#125;; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con2 = (s) -&gt; &#123; System.out.println(s); &#125;; con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125; // 语法格式四：Lambda若只需要一个参数时，参数的小括号可以省略 @Test public void test4() &#123; Consumer&lt;String&gt; con1 = (s) -&gt; &#123; System.out.println(s); &#125;; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con2 = s -&gt; &#123; System.out.println(s); &#125;; con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125; // 语法格式五：Lambda需要两个或以上的参数，多条执行语句，并且可以有返回值 @Test public void test5() &#123; Comparator&lt;Integer&gt; com1 = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; System.out.println(o1); System.out.println(o2); return o1.compareTo(o2); &#125; &#125;; System.out.println(com1.compare(12, 21)); System.out.println(&quot;*****************************&quot;); Comparator&lt;Integer&gt; com2 = (o1, o2) -&gt; &#123; System.out.println(o1); System.out.println(o2); return o1.compareTo(o2); &#125;; System.out.println(com2.compare(12, 6)); &#125; // 语法格式六：当Lambda体只有一条语句时，return与大括号若有，都可以省略 @Test public void test6() &#123; Comparator&lt;Integer&gt; com1 = (o1, o2) -&gt; &#123; return o1.compareTo(o2); &#125;; System.out.println(com1.compare(12, 6)); System.out.println(&quot;*****************************&quot;); Comparator&lt;Integer&gt; com2 = (o1, o2) -&gt; o1.compareTo(o2); System.out.println(com2.compare(12, 21)); &#125; @Test public void test7() &#123; Consumer&lt;String&gt; con1 = s -&gt; &#123; System.out.println(s); &#125;; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*****************************&quot;); Consumer&lt;String&gt; con2 = s -&gt; System.out.println(s); con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125;&#125; 函数式 (Functional) 接口 什么是函数式 (Functional) 接口： 只包含一个抽象方法的接口，称为函数式接口。 你可以通过 Lambda 表达式来创建该接口的对象。(若 Lambda 表达式抛出一个受检异常 (即：非运行时异常)，那么该异常需要在目标接口的抽象方法上进行声明。) 我们可以在一个接口上使用 @FunctionalInterface 注解，这样做可以检查它是否是一个函数式接口。同时 javadoc 也会包含一条声明，说明这个接口是一个函数式接口。 在 java.util.function 包下定义了 Java 8 的丰富的函数式接口。 如何理解函数式接口： Java 从诞生日起就是一直倡导 “一切皆对象”，在 Java 里面面向对象 (OOP) 编程是一切。但是随着 python、scala 等语言的兴起和新技术的挑战，Java 不得不做出调整以便支持更加广泛的技术要求，也即 Java 不但可以支持 OOP 还可以支持 OOF (面向函数编程)。 在函数式编程语言当中，函数被当做一等公民对待。在将函数作为一等公民的编程语言中，Lambda 表达式的类型是函数。但是在 Java 8 中，有所不同。在 Java 8 中，Lambda 表达式是对象，而不是函数，它们必须依附于一类特别的对象类型——函数式接口。 简单的说，在 Java 8 中，Lambda 表达式就是一个函数式接口的实例。这就是 Lambda 表达式和函数式接口的关系。也就是说，只要一个对象是函数式接口的实例，那么该对象就可以用 Lambda 表达式来表示。 所有以前用匿名实现类表示的现在都可以用 Lambda 表达式来写。 函数式接口举例： 自定义函数式接口： 函数式接口中不使用泛型： 函数式接口中使用泛型： 作为参数传递 Lambda 表达式： Java 内置四大核心函数式接口： 其他接口： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * java内置的4大核心函数式接口： * * 消费型接口 Consumer&lt;T&gt; void accept(T t) * 供给型接口 Supplier&lt;T&gt; T get() * 函数型接口 Function&lt;T,R&gt; R apply(T t) * 断定型接口 Predicate&lt;T&gt; boolean test(T t) */public class LambdaTest &#123; // 作为参数传递Lambda表达式 // happyTime()：将参数1传给函数式接口con，Consumer函数式接口包含唯一方法accept() public void happyTime(double money, Consumer&lt;Double&gt; con) &#123; con.accept(money); &#125; @Test public void test1() &#123; happyTime(500, new Consumer&lt;Double&gt;() &#123; @Override public void accept(Double aDouble) &#123;// 重写accept() System.out.println(&quot;学习太累了，去天上人间买了瓶矿泉水，价格为：&quot; + aDouble); &#125; &#125;); System.out.println(&quot;********************&quot;); happyTime(400, money -&gt; System.out.println(&quot;学习太累了，去天上人间喝了口水，价格为：&quot; + money)); &#125; // filterString()：根据给定的规则，过滤集合中的字符串。此规则由Predicate的方法决定 // Predicate函数式接口包含唯一方法test() public List&lt;String&gt; filterString(List&lt;String&gt; list, Predicate&lt;String&gt; pre) &#123; ArrayList&lt;String&gt; filterList = new ArrayList&lt;&gt;(); // 过滤list中的每一个元素，通过Predicate实例test()验证的，添加到filterList中并返回 for (String s : list) &#123; if (pre.test(s)) &#123; filterList.add(s); &#125; &#125; return filterList; &#125; @Test public void test2() &#123; List&lt;String&gt; list = Arrays.asList(&quot;北京&quot;, &quot;南京&quot;, &quot;天津&quot;, &quot;东京&quot;, &quot;西京&quot;, &quot;普京&quot;); List&lt;String&gt; filterStrs = filterString(list, new Predicate&lt;String&gt;() &#123; @Override public boolean test(String s) &#123;// 重写test() return s.contains(&quot;京&quot;); &#125; &#125;); System.out.println(filterStrs); System.out.println(&quot;********************&quot;); List&lt;String&gt; filterStrs1 = filterString(list, s -&gt; s.contains(&quot;京&quot;)); System.out.println(filterStrs1); &#125;&#125; 方法引用与构造器引用 方法引用 (Method References)： 当要传递给 Lambda 体的操作，已经有实现的方法了，可以使用方法引用！ 方法引用可以看做是 Lambda 表达式深层次的表达。换句话说，方法引用就是 Lambda 表达式，也就是函数式接口的一个实例，通过方法的名字来指向一个方法，可以认为是 Lambda 表达式的一个语法糖。 要求：实现接口的抽象方法的参数列表和返回值类型，必须与方法引用的方法的参数列表和返回值类型保持一致！(针对情况一和情况二) ClassName :: methodName：当函数式接口方法的第一个参数是方法引用的方法的调用者，并且第二个参数是方法引用的方法的参数 (或无参数/返回值类型) 时使用。(针对情况三) 格式：使用操作符 “::” 将类 (或对象)与方法名分隔开来。 方法引用有如下三种主要使用情况： 对象 :: 实例方法名 类 :: 静态方法名 类 :: 实例方法 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class Employee &#123; private int id; private String name; private int age; private double salary; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public double getSalary() &#123; return salary; &#125; public void setSalary(double salary) &#123; this.salary = salary; &#125; public Employee() &#123; System.out.println(&quot;Employee().....&quot;); &#125; public Employee(int id) &#123; this.id = id; System.out.println(&quot;Employee(int id).....&quot;); &#125; public Employee(int id, String name) &#123; this.id = id; this.name = name; &#125; public Employee(int id, String name, int age, double salary) &#123; this.id = id; this.name = name; this.age = age; this.salary = salary; &#125; @Override public String toString() &#123; return &quot;Employee&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, salary=&quot; + salary + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Employee employee = (Employee) o; if (id != employee.id) return false; if (age != employee.age) return false; if (Double.compare(employee.salary, salary) != 0) return false; return name != null ? name.equals(employee.name) : employee.name == null; &#125; @Override public int hashCode() &#123; int result; long temp; result = id; result = 31 * result + (name != null ? name.hashCode() : 0); result = 31 * result + age; temp = Double.doubleToLongBits(salary); result = 31 * result + (int) (temp ^ (temp &gt;&gt;&gt; 32)); return result; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155/** * 方法引用的使用 * * 1.使用情境：当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用！ * * 2.方法引用，本质上就是Lambda表达式，而Lambda表达式作为函数式接口的实例。所以 * 方法引用，也是函数式接口的实例。 * * 3. 使用格式： 类(或对象) :: 方法名 * * 4. 具体分为如下的三种情况： * 情况1 对象 :: 非静态方法 * 情况2 类 :: 静态方法 * * 情况3 类 :: 非静态方法 * * 5. 方法引用使用的要求：要求接口中的抽象方法的形参列表和返回值类型与方法引用的方法的 * 形参列表和返回值类型相同！（针对于情况1和情况2） */public class MethodRefTest &#123; // 情况一：对象 :: 实例方法 // Consumer中的void accept(T t) // PrintStream中的void println(T t) @Test public void test1() &#123; // System.out.println(str)这个方法体，在PrintStream中已经存在实现的方法 Consumer&lt;String&gt; con1 = str -&gt; System.out.println(str); con1.accept(&quot;北京&quot;); System.out.println(&quot;*******************&quot;); PrintStream ps = System.out;// 利用System.out的对象，调用其println()方法 Consumer&lt;String&gt; con2 = ps::println; con2.accept(&quot;beijing&quot;); &#125; // Supplier中的T get() // Employee中的String getName() @Test public void test2() &#123; Employee emp = new Employee(1001, &quot;Tom&quot;, 23, 5600); // emp.getName()这个方法体，对应的就是emp对象的getName()方法 Supplier&lt;String&gt; sup1 = () -&gt; emp.getName(); System.out.println(sup1.get());// 返回emp对象的name System.out.println(&quot;*******************&quot;); Supplier&lt;String&gt; sup2 = emp::getName; System.out.println(sup2.get()); &#125; // 情况二：类 :: 静态方法 // Comparator中的int compare(T t1,T t2) // Integer中的int compare(T t1,T t2) @Test public void test3() &#123; Comparator&lt;Integer&gt; com1 = (t1, t2) -&gt; Integer.compare(t1, t2); System.out.println(com1.compare(12, 21)); System.out.println(&quot;*******************&quot;); Comparator&lt;Integer&gt; com2 = Integer::compare; System.out.println(com2.compare(12, 3)); &#125; // Function中的R apply(T t) // Math中的Long round(Double d) @Test public void test4() &#123; Function&lt;Double, Long&gt; func = new Function&lt;Double, Long&gt;() &#123; @Override public Long apply(Double d) &#123; return Math.round(d); &#125; &#125;; System.out.println(&quot;*******************&quot;); Function&lt;Double, Long&gt; func1 = d -&gt; Math.round(d);// lambda表达式 System.out.println(func1.apply(12.3)); System.out.println(&quot;*******************&quot;); Function&lt;Double, Long&gt; func2 = Math::round;// 方法引用 System.out.println(func2.apply(12.6)); &#125; // 情况三：类 :: 实例方法 (有难度) // Comparator中的int comapre(T t1,T t2) // String中的int t1.compareTo(t2) @Test public void test5() &#123; Comparator&lt;String&gt; com1 = (s1, s2) -&gt; s1.compareTo(s2); System.out.println(com1.compare(&quot;abc&quot;, &quot;abd&quot;)); System.out.println(&quot;*******************&quot;); Comparator&lt;String&gt; com2 = String::compareTo; System.out.println(com2.compare(&quot;abd&quot;, &quot;abm&quot;)); &#125; // BiPredicate中的boolean test(T t1, T t2); // String中的boolean t1.equals(t2) @Test public void test6() &#123; // 原始写法 BiPredicate&lt;String, String&gt; pre = new BiPredicate&lt;String, String&gt;() &#123; @Override public boolean test(String s1, String s2) &#123; return s1.equals(s2); &#125; &#125;; System.out.println(pre.test(&quot;abc&quot;, &quot;abc&quot;)); System.out.println(&quot;*******************&quot;); // lambda表达式：lambda体是参数1调用一个方法，参数2是那个方法的入参 BiPredicate&lt;String, String&gt; pre1 = (s1, s2) -&gt; s1.equals(s2); System.out.println(pre1.test(&quot;abc&quot;, &quot;abc&quot;)); System.out.println(&quot;*******************&quot;); // 方法引用：String类的equals()符合上述lambda体的功能 BiPredicate&lt;String, String&gt; pre2 = String::equals; System.out.println(pre2.test(&quot;abc&quot;, &quot;abd&quot;)); &#125; // Function中的R apply(T t) // Employee中的String getName(); @Test public void test7() &#123; Employee employee = new Employee(1001, &quot;Jerry&quot;, 23, 6000); // 原始写法：lambda体是参数1调用一个方法，返回一个参数2类型的值 Function&lt;Employee, String&gt; func = new Function&lt;Employee, String&gt;() &#123; @Override public String apply(Employee employee) &#123; return employee.getName(); &#125; &#125;; System.out.println(&quot;*******************&quot;); // lambda表达式：Employee类的getName()符合上述lambda体的功能 Function&lt;Employee, String&gt; func1 = e -&gt; e.getName(); System.out.println(func1.apply(employee)); System.out.println(&quot;*******************&quot;); // 方法引用 Function&lt;Employee, String&gt; func2 = Employee::getName; System.out.println(func2.apply(employee)); &#125;&#125; 构造器引用： 格式：ClassName :: new 与函数式接口相结合，自动与函数式接口中方法兼容。可以把构造器引用赋值给定义的方法，要求构造器参数列表要与接口中抽象方法的参数列表一致，且方法的返回值即为构造器对应类的对象。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * 一、构造器引用 * 和方法引用类似，函数式接口的抽象方法的形参列表和构造器的形参列表一致。 * 抽象方法的返回值类型即为构造器所属的类的类型 */public class ConstructorRefTest &#123; // 构造器引用 // Supplier中的T get() // Employee的空参构造器：Employee() @Test public void test1() &#123; // 原始写法 Supplier&lt;Employee&gt; sup = new Supplier&lt;Employee&gt;() &#123; @Override public Employee get() &#123; return new Employee(); &#125; &#125;; System.out.println(sup.get()); System.out.println(&quot;*******************&quot;); // Lambda表达式 Supplier&lt;Employee&gt; sup1 = () -&gt; new Employee(); System.out.println(sup1.get()); System.out.println(&quot;*******************&quot;); // 方法引用：Employee的无参构造器符合上述Lambda体 Supplier&lt;Employee&gt; sup2 = Employee::new; System.out.println(sup2.get()); &#125; // Function中的R apply(T t) @Test public void test2() &#123; // 原始写法 Function&lt;Integer, Employee&gt; func = new Function&lt;Integer, Employee&gt;() &#123; @Override public Employee apply(Integer id) &#123; return new Employee(id); &#125; &#125;; Employee employee = func.apply(1000); System.out.println(employee); System.out.println(&quot;*******************&quot;); // Lambda表达式 Function&lt;Integer, Employee&gt; func1 = id -&gt; new Employee(id); Employee employee1 = func1.apply(1001); System.out.println(employee1); System.out.println(&quot;*******************&quot;); // 方法引用：Employee的带id的有参构造器符合上述Lambda体 Function&lt;Integer, Employee&gt; func2 = Employee::new; Employee employee2 = func2.apply(1002); System.out.println(employee2); &#125; // BiFunction中的R apply(T t,U u) @Test public void test3() &#123; // 原始写法 BiFunction&lt;Integer, String, Employee&gt; func = new BiFunction&lt;Integer, String, Employee&gt;() &#123; @Override public Employee apply(Integer id, String name) &#123; return new Employee(id, name); &#125; &#125;; System.out.println(func.apply(1000, &quot;Tom&quot;)); System.out.println(&quot;*******************&quot;); // Lambda表达式 BiFunction&lt;Integer, String, Employee&gt; func1 = (id, name) -&gt; new Employee(id, name); System.out.println(func1.apply(1001, &quot;Tom&quot;)); System.out.println(&quot;*******************&quot;); // 方法引用：Employee的带id和name的有参构造器符合上述Lambda体 BiFunction&lt;Integer, String, Employee&gt; func2 = Employee::new; System.out.println(func2.apply(1002, &quot;Tom&quot;)); &#125;&#125; 数组引用： 格式：type[] :: new 可以把数组看做是一个特殊的类，则写法与构造器引用一致。 实例： 12345678910111213141516171819202122232425262728293031323334/** * 二、数组引用 * 大家可以把数组看做是一个特殊的类，则写法与构造器引用一致。 */public class ConstructorRefTest &#123; // 数组引用 // Function中的R apply(T t) @Test public void test4() &#123; // 原始写法 Function&lt;Integer, String[]&gt; func = new Function&lt;Integer, String[]&gt;() &#123; @Override public String[] apply(Integer length) &#123; return new String[length]; &#125; &#125;; String[] arr = func.apply(1); System.out.println(Arrays.toString(arr)); System.out.println(&quot;*******************&quot;); // Lambda表达式 Function&lt;Integer, String[]&gt; func1 = length -&gt; new String[length]; String[] arr1 = func1.apply(5); System.out.println(Arrays.toString(arr1)); System.out.println(&quot;*******************&quot;); // 方法引用 Function&lt;Integer, String[]&gt; func2 = String[]::new; String[] arr2 = func2.apply(10); System.out.println(Arrays.toString(arr2)); &#125;&#125; 强大的 Stream API Java 8 中有两大最为重要的改变。第一个是 Lambda 表达式；另外一个则是 Stream API。 Stream API (java.util.stream) 把真正的函数式编程风格引入到 Java 中。这是目前为止对 Java 类库最好的补充，因为 Stream API 可以极大提供 Java 程序员的生产力，让程序员写出高效率、干净、简洁的代码。 Stream 是 Java 8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。 使用 Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简言之，Stream API 提供了一种高效且易于使用的处理数据的方式。 为什么要使用 Stream API： 实际开发中，项目中多数数据源都来自于 Mysql，Oracle 等。但现在数据源可以更多了，有 MongDB，Redis 等，而这些 NoSQL 的数据就需要 Java 层面去处理。 Stream 和 Collection 集合的区别：Collection 是一种静态的内存数据结构，而 Stream 是有关计算的。前者是主要面向内存，存储在内存中，后者主要是面向 CPU，通过 CPU 实现计算。 Stream 就是一个数据渠道，用于操作数据源 (集合、数组等) 所生成的元素序列。”集合讲的是数据，Stream 讲的是计算！” Stream 的特性： Stream 自己不会存储元素。 Stream 不会改变源对象。相反，他们会返回一个持有结果的新 Stream。 Stream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行。 Stream 操作的三个步骤： 1 - 创建 Stream 一个数据源 (如：集合、数组)，获取一个流。 2 - 中间操作 一个中间操作链，对数据源的数据进行处理。 3 - 终止操作 (终端操作) 一旦执行终止操作，就执行中间操作链，并产生结果。之后，不会再被使用。 步骤一：Stream 的四种创建方式。 方式一：通过集合 Java 8 中的 Collection 接口被扩展，提供了两个获取流的方法： **default Stream&lt;E&gt; stream()**：返回一个顺序流。 **default Stream&lt;E&gt; parallelStream()**：返回一个并行流。 方式二：通过数组 Java 8 中的 Arrays 类的静态方法 stream() 可以获取数组流： **static &lt;T&gt; Stream&lt;T&gt; stream(T[] array)**：返回一个特殊对象数组的流。 重载形式，能够处理对应基本类型的数组： public static IntStream stream(int[] array)：返回一个 int 数组的流。 public static LongStream stream(long[] array)：返回一个 long 数组的流。 public static DoubleStream stream(double[] array)：返回一个 double 数组的流。 方式三：通过 Stream 类的 of() 可以调用 Stream 类静态方法 of()，通过显示值创建一个流。它可以接收任意数量的参数。 **public static&lt;T&gt; Stream&lt;T&gt; of(T... values)**：返回一个流。 方式四：创建无限流 可以使用静态方法 Stream.iterate() 和 Stream.generate() 这两种方式，创建无限流。 迭代：public static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f) 生成：public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) 实例： 12345678910111213141516/** * 提供用于测试的数据 */public class EmployeeData &#123; public static List&lt;Employee&gt; getEmployees() &#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); list.add(new Employee(1001, &quot;马1&quot;, 34, 6000.38)); list.add(new Employee(1002, &quot;马2&quot;, 12, 9876.12)); list.add(new Employee(1003, &quot;刘&quot;, 33, 3000.82)); list.add(new Employee(1004, &quot;雷&quot;, 26, 7657.37)); list.add(new Employee(1005, &quot;李&quot;, 65, 5555.32)); list.add(new Employee(1006, &quot;比&quot;, 42, 9500.43)); list.add(new Employee(1007, &quot;任&quot;, 26, 4333.32)); return list; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 1. Stream关注的是对数据的运算，与CPU打交道 * 集合关注的是数据的存储，与内存打交道 * * 2. * ① Stream 自己不会存储元素。 * ② Stream 不会改变源对象。相反，他们会返回一个持有结果的新Stream。 * ③ Stream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行 * * 3.Stream 执行流程 * ① Stream的实例化 * ② 一系列的中间操作(过滤、映射、...) * ③ 终止操作 * * 4.说明： * 4.1 一个中间操作链，对数据源的数据进行处理 * 4.2 一旦执行终止操作，就执行中间操作链，并产生结果。之后，不会再被使用 * * 测试Stream的实例化 */public class StreamAPITest &#123; // 创建Stream方式一：通过集合 @Test public void test1() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // 方法一： // default Stream&lt;E&gt; stream() : 返回一个顺序流 Stream&lt;Employee&gt; stream = employees.stream(); // 方法二： // default Stream&lt;E&gt; parallelStream() : 返回一个并行流 Stream&lt;Employee&gt; parallelStream = employees.parallelStream(); &#125; // 创建Stream方式二：通过数组 @Test public void test2() &#123; int[] arr = new int[]&#123;1, 2, 3, 4, 5, 6&#125;; // 调用Arrays类的static &lt;T&gt; Stream&lt;T&gt; stream(T[] array): 返回一个流 IntStream stream = Arrays.stream(arr); Employee e1 = new Employee(1001, &quot;Tom&quot;); Employee e2 = new Employee(1002, &quot;Jerry&quot;); Employee[] arr1 = new Employee[]&#123;e1, e2&#125;; Stream&lt;Employee&gt; stream1 = Arrays.stream(arr1); &#125; // 创建Stream方式三：通过Stream的of() @Test public void test3() &#123; Stream&lt;Integer&gt; stream = Stream.of(1, 2, 3, 4, 5, 6); Stream&lt;String&gt; stringStream = Stream.of(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;); &#125; // 创建Stream方式四：创建无限流 --- 用的比较少 @Test public void test4() &#123; // 迭代 // public static&lt;T &gt; Stream &lt; T &gt; iterate( final T seed, final UnaryOperator&lt;T&gt; f) // 遍历前10个偶数 Stream.iterate(0, t -&gt; t + 2).limit(10).forEach(System.out::println);// 从0开始，后一个数是前一个数+2 // 生成 // public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) // 遍历前10个随机数 Stream.generate(Math::random).limit(10).forEach(System.out::println); &#125;&#125; 步骤二：Stream 的中间操作。 多个中间操作可以连接起来形成一个流水线，除非流水线上触发终止操作，否则中间操作不会执行任何的处理！而在终止操作时一次性全部处理，这称为 “惰性求值”。 操作 1 - 筛选与切片： 操作 2 - 映射： 操作 3 - 排序： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126/** * 测试Stream的中间操作 */public class StreamAPITest &#123; // 1-筛选与切片 @Test public void test1() &#123; List&lt;Employee&gt; list = EmployeeData.getEmployees(); // filter(Predicate p) --- 接收Lambda，从流中排除某些元素。 // 练习：查询员工表中薪资大于7000的员工信息 list.stream().filter(e -&gt; e.getSalary() &gt; 7000).forEach(System.out::println); System.out.println(&quot;************************&quot;); // limit(n) --- 截断流，使其元素不超过给定数量n。 // 练习：打印员工表中前三名的员工信息 list.stream().limit(3).forEach(System.out::println);// 前一个流已经关闭，必须重新建一个流 System.out.println(&quot;************************&quot;); // skip(n) --- 跳过元素，返回一个扔掉了前n个元素的流。若流中元素不足n个，则返回一个空流。与limit(n)互补。 // 练习：跳过员工表中前三名的员工信息，然后打印之后的每个员工的信息 list.stream().skip(3).forEach(System.out::println); System.out.println(&quot;************************&quot;); // distinct() --- 筛选，通过流所生成元素的hashCode()和equals()去除重复元素 list.add(new Employee(1010, &quot;刘强东&quot;, 40, 8000)); list.add(new Employee(1010, &quot;刘强东&quot;, 41, 8000)); list.add(new Employee(1010, &quot;刘强东&quot;, 40, 8000)); list.add(new Employee(1010, &quot;刘强东&quot;, 40, 8000)); list.add(new Employee(1010, &quot;刘强东&quot;, 40, 8000)); // System.out.println(list); list.stream().distinct().forEach(System.out::println); &#125; // 2-映射 @Test public void test2() &#123; // map(Function f) --- 接收一个函数作为参数，将元素转换成其他形式或提取信息， // 该函数会被应用到每个元素上，并将其映射成一个新的元素。 // ---&gt; 类似于List的add()：如果流的每个值转换成新流，则将每个新流作为一个元素组成新的流 // 即类似：[1, [1, 2], 5, [1, 3, 2, 5], 9] // 练习1：将list中的每一个元素变成大写并打印 List&lt;String&gt; list = Arrays.asList(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;); // list.stream().map(str -&gt; str.toUpperCase()).forEach(System.out::println); list.stream().map(String::toUpperCase).forEach(System.out::println); System.out.println(); // 练习2：获取员工姓名长度大于3的员工的姓名。 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;String&gt; namesStream = employees.stream().map(Employee::getName); namesStream.filter(name -&gt; name.length() &gt; 3).forEach(System.out::println); System.out.println(); // 练习3： Stream&lt;Stream&lt;Character&gt;&gt; streamStream = list.stream().map(StreamAPITest::fromStringToStream); // streamStream.forEach(System.out::println); // 体会下下面的写法与上面写法的区别 streamStream.forEach(s -&gt; &#123; s.forEach(System.out::println); &#125;); System.out.println(&quot;************************&quot;); // flatMap(Function f) --- 接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。 // ---&gt; 似于List的addAll()：如果流的每个值转换成新流，则将每个新流的值组合连接成一个流 // 即类似：[1, 1, 2, 5, 1, 3, 2, 5, 9] Stream&lt;Character&gt; characterStream = list.stream().flatMap(StreamAPITest::fromStringToStream); characterStream.forEach(System.out::println); &#125; // 将字符串中的多个字符构成的集合转换为对应的Stream的实例 public static Stream&lt;Character&gt; fromStringToStream(String str) &#123;// 如：aa---&gt;返回两个字符a组成的集合对应的流 ArrayList&lt;Character&gt; list = new ArrayList&lt;&gt;(); for (Character c : str.toCharArray()) &#123; list.add(c); &#125; return list.stream(); &#125; // 对比map()和flatmap()的区别 @Test public void test3() &#123; ArrayList list1 = new ArrayList(); list1.add(1); list1.add(2); list1.add(3); ArrayList list2 = new ArrayList(); list2.add(4); list2.add(5); list2.add(6); list1.add(list2);// [1, 2, 3, [4, 5, 6]] list1.addAll(list2);// [1, 2, 3, 4, 5, 6] System.out.println(list1); &#125; // 3-排序 @Test public void test4() &#123; // sorted() --- 自然排序 List&lt;Integer&gt; list = Arrays.asList(12, 43, 65, 34, 87, 0, -98, 7); list.stream().sorted().forEach(System.out::println); // 抛异常，原因: Employee没有实现Comparable接口 // List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // employees.stream().sorted().forEach(System.out::println); // sorted(Comparator com) --- 定制排序 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); employees.stream().sorted((e1, e2) -&gt; &#123; int ageValue = Integer.compare(e1.getAge(), e2.getAge());// 先按年龄 if (ageValue != 0) &#123; return ageValue; &#125; else &#123; return -Double.compare(e1.getSalary(), e2.getSalary());// 再按薪水 &#125; &#125;).forEach(System.out::println); &#125;&#125; 步骤三：Stream 的终止操作。 终端操作会从流的流水线生成结果。其结果可以是任何不是流的值，例如：List、Integer，甚至是 void。 流进行了终止操作后，不能再次使用。 操作 1 - 匹配与查找： 操作 2 - 归约： map 和 reduce 的连接通常称为 map-reduce 模式，因 Google 用它来进行网络搜索而出名。 map 是一对一映射，由 n 到 n；reduce 是多对一归约，由 n 到 1。 操作 3 - 收集： Collector 接口中方法的实现决定了如何对流执行收集的操作，如收集到 List、Set、Map 等。 Collectors 实用类提供了很多静态方法，可以方便地创建常见收集器实例 (Collector 实例)，具体方法与实例如下表： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 测试Stream的终止操作 */public class StreamAPITest &#123; // 1-匹配与查找 @Test public void test1() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // allMatch(Predicate p) --- 检查是否匹配所有元素。 // 练习：是否所有的员工的年龄都大于18 boolean allMatch = employees.stream().allMatch(e -&gt; e.getAge() &gt; 18); System.out.println(allMatch); // anyMatch(Predicate p) --- 检查是否至少匹配一个元素。 // 练习：是否存在员工的工资大于10000 boolean anyMatch = employees.stream().anyMatch(e -&gt; e.getSalary() &gt; 10000); System.out.println(anyMatch); // noneMatch(Predicate p) ---- 检查是否没有匹配的元素。如果有，返回false // 练习：是否存在员工姓&quot;雷&quot; boolean noneMatch = employees.stream().noneMatch(e -&gt; e.getName().startsWith(&quot;雷&quot;)); System.out.println(noneMatch); // findFirst() --- 返回第一个元素 Optional&lt;Employee&gt; employee = employees.stream().findFirst(); System.out.println(employee); // findAny() --- 返回当前流中的任意元素 Optional&lt;Employee&gt; employee1 = employees.parallelStream().findAny(); System.out.println(employee1); &#125; @Test public void test2() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // count --- 返回流中元素的总个数 // 练习：返回工资高于5000的员工个数 long count = employees.stream().filter(e -&gt; e.getSalary() &gt; 5000).count(); System.out.println(count); // max(Comparator c) --- 返回流中最大值 // 练习：返回最高的工资 Stream&lt;Double&gt; salaryStream = employees.stream().map(Employee::getSalary); Optional&lt;Double&gt; maxSalary = salaryStream.max(Double::compare); System.out.println(maxSalary); // min(Comparator c) --- 返回流中最小值 // 练习：返回最低工资的员工 Optional&lt;Employee&gt; employee = employees.stream().min((e1, e2) -&gt; Double.compare(e1.getSalary(), e2.getSalary())); System.out.println(employee); System.out.println(&quot;************************&quot;); // forEach(Consumer c) --- 内部迭代 employees.stream().forEach(System.out::println); // 外部迭代 Iterator&lt;Employee&gt; iterator = employees.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; // 使用集合的遍历操作方法 employees.forEach(System.out::println); &#125; // 2-归约 @Test public void test3() &#123; // reduce(T identity, BinaryOperator) --- 可以将流中元素反复结合起来，得到一个值。返回T // 练习1：计算1-10的自然数的和 List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); Integer sum = list.stream().reduce(0, Integer::sum);// 有一个初始值，在初始值基础上操作 System.out.println(sum); // reduce(BinaryOperator) --- 可以将流中元素反复结合起来，得到一个值。返回Optional&lt;T&gt; // 练习2：计算公司所有员工工资的总和 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Double&gt; salaryStream = employees.stream().map(Employee::getSalary); Optional&lt;Double&gt; sumMoney = salaryStream.reduce((d1, d2) -&gt; d1 + d2); // Optional&lt;Double&gt; sumMoney = salaryStream.reduce(Double::sum);// 方法引用 // Double sumMoney = salaryStream.reduce(0.0, Double::sum);// 也可以计算工资总和 System.out.println(sumMoney.get()); &#125; // 3-收集 @Test public void test4() &#123; // collect(Collector c) --- 将流转换为其他形式。接收一个Collector接口的实现，用于给Stream中元素做汇总的方法 // 练习：查找工资大于6000的员工，结果返回为一个List或Set List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // 返回List List&lt;Employee&gt; employeeList = employees.stream().filter(e -&gt; e.getSalary() &gt; 6000).collect(Collectors.toList()); employeeList.forEach(System.out::println); System.out.println(&quot;************************&quot;); // 返回Set Set&lt;Employee&gt; employeeSet = employees.stream().filter(e -&gt; e.getSalary() &gt; 6000).collect(Collectors.toSet()); employeeSet.forEach(System.out::println); &#125;&#125; Optional 类 到目前为止，臭名昭著的空指针异常是导致 Java 应用程序失败的最常见原因。以前，为了解决空指针异常，Google 公司著名的 Guava 项目引入了 Optional 类，Guava 通过使用检查空值的方式来防止代码污染，它鼓励程序员写更干净的代码。受到 Google Guava 的启发，Optional 类已经成为 Java 8 类库的一部分。 Optional&lt;T&gt; 类 (java.util.Optional) 是一个容器类，它可以保存类型 T 的值，代表这个值存在。或者仅仅保存 null，表示这个值不存在。原来用 null 表示一个值不存在，现在 Optional 可以更好的表达这个概念。并且可以避免空指针异常。 Optional 类的 Javadoc 描述如下：这是一个可以为 null 的容器对象。如果值存在则 isPresent() 会返回 true，调用 get() 会返回该对象。 Optional 类提供了很多有用的方法，这样我们就不用显式进行空值检测。 创建 Optional 类对象的方法： **Optional.of(T t)**：创建一个 Optional 实例，t 必须非空。否则，报 NullPointerException。 123456789public class OptionalTest &#123; @Test public void test() &#123; Optional&lt;Employee&gt; opt = Optional.of(new Employee(&quot;张三&quot;, 8888)); // 判断opt中员工对象是否满足条件，如果满足就保留，否则返回空 Optional&lt;Employee&gt; emp = opt.filter(e -&gt; e.getSalary() &gt; 10000); System.out.println(emp); &#125;&#125; 12345678910111213public class OptionalTest &#123; @Test public void test() &#123; Optional&lt;Employee&gt; opt = Optional.of(new Employee(&quot;张三&quot;, 8888)); // 如果opt中员工对象不为空，就涨薪10% Optional&lt;Employee&gt; emp = opt.map(e -&gt; &#123; e.setSalary(e.getSalary() % 1.1); return e; &#125;); System.out.println(emp); &#125;&#125; Optional.empty()：创建一个空的 Optional 实例 **Optional.ofNullable(T t)**：创建一个 Optional 实例，t 可以为 null。 判断 Optional 容器中是否包含对象： **boolean isPresent()**：判断是否包含对象 void ifPresent(Consumer&lt;? super T&gt; consumer)：如果有值，就执行 Consumer 接口的实现代码，并且该值会作为参数传给它。 123456789public class OptionalTest &#123; @Test public void test() &#123; Boy b = new Boy(&quot;张三&quot;); Optional&lt;Girl&gt; opt = Optional.ofNullable(b.getGrilFriend()); // 如果女朋友存在就打印女朋友的信息 opt.ifPresent(System.out::println); &#125;&#125; 获取 Optional 容器的对象： **T get()**：如果调用对象包含值，返回该值，否则抛异常。可以对应于 Optional.of(T t) 一起使用。 **T orElse(T other)**：如果有值则将其返回，否则返回指定的 other 对象。可以对应于 Optional.ofNullable(T t) 一起使用。 12345678910public class OptionalTest &#123; @Test public void test() &#123; Boy b = new Boy(&quot;张三&quot;); Optional&lt;Girl&gt; opt = Optional.ofNullable(b.getGrilFriend()); // 如果有女朋友就返回他的女朋友，否则只能欣赏“嫦娥”了 Girl girl = opt.orElse(new Girl(&quot;嫦娥&quot;)); System.out.println(&quot;他的女朋友是：&quot; + girl.getName()); &#125;&#125; T orElseGet(Supplier&lt;? extends T&gt; other)：如果有值则将其返回，否则返回由 Supplier 接口实现提供的对象。 T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier)：如果有值则将其返回，否则抛出由 Supplier 接口实现提供的异常。 实例： 12345678910111213141516171819202122232425public class Boy &#123; private Girl girl; public Girl getGirl() &#123; return girl; &#125; public void setGirl(Girl girl) &#123; this.girl = girl; &#125; public Boy() &#123; &#125; public Boy(Girl girl) &#123; this.girl = girl; &#125; @Override public String toString() &#123; return &quot;Boy&#123;&quot; + &quot;girl=&quot; + girl + &#x27;&#125;&#x27;; &#125;&#125; 12345678910111213141516171819202122232425public class Girl &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Girl() &#123; &#125; public Girl(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Girl&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * Optional类：为了在程序中避免出现空指针异常而创建的。 * * 常用的方法：ofNullable(T t) * orElse(T t) */public class OptionalTest &#123; /* Optional.of(T t): 创建一个Optional实例，t必须非空。否则，报NullPointerException Optional.empty(): 创建一个空的Optional实例 Optional.ofNullable(T t): t可以为null */ @Test public void test1() &#123; Girl girl = new Girl(); // girl = null; // of(T t): 保证t是非空的 Optional&lt;Girl&gt; optionalGirl = Optional.of(girl); &#125; @Test public void test2() &#123; Girl girl = new Girl(); // girl = null; // ofNullable(T t): t可以为null Optional&lt;Girl&gt; optionalGirl = Optional.ofNullable(girl); System.out.println(optionalGirl); // orElse(T t1): 如果当前的Optional内部封装的t是非空的，则返回内部的t。 // 如果内部的t是空的，则返回orElse()方法中的参数t1。 Girl girl1 = optionalGirl.orElse(new Girl(&quot;赵&quot;)); System.out.println(girl1); &#125; @Test public void test3() &#123; Boy boy = new Boy(); boy = null; String girlName = getGirlName(boy); // String girlName = getGirlName1(boy);// 不会出现NullPointerException System.out.println(girlName); &#125; @Test public void test4() &#123; Boy boy = null; boy = new Boy(); boy = new Boy(new Girl(&quot;苍&quot;)); String girlName = getGirlName2(boy); System.out.println(girlName); &#125; // 未优化代码，容易出现NullPointerException public String getGirlName(Boy boy) &#123; return boy.getGirl().getName(); &#125; // 优化以后的getGirlName(): public String getGirlName1(Boy boy) &#123; if (boy != null) &#123; Girl girl = boy.getGirl(); if (girl != null) &#123; return girl.getName(); &#125; &#125; return null; &#125; // 使用Optional类优化的getGirlName() public String getGirlName2(Boy boy) &#123; // boy可能为空 Optional&lt;Boy&gt; boyOptional = Optional.ofNullable(boy); // 此时的boy1一定非空 Boy boy1 = boyOptional.orElse(new Boy(new Girl(&quot;迪&quot;))); // girl可能为空 Girl girl = boy1.getGirl(); Optional&lt;Girl&gt; girlOptional = Optional.ofNullable(girl); // 此时的girl1一定非空 Girl girl1 = girlOptional.orElse(new Girl(&quot;古&quot;)); return girl1.getName(); &#125;&#125; Java 9 的新特性Java 10 的新特性Java 11 的新特性本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 反射机制","slug":"java-reflection","date":"2021-04-07T05:23:13.000Z","updated":"2021-04-09T08:08:10.654Z","comments":true,"path":"2021/04/07/java-reflection/","link":"","permalink":"http://example.com/2021/04/07/java-reflection/","excerpt":"","text":"Java 反射机制概述 Reflection (反射) 是被视为动态语言的关键，反射机制允许程序在执行期借助于 Reflection API 取得任何类的内部信息，并能直接操作任意对象的内部属性及方法。 动态语言：是一类在运行时可以改变其结构的语言。例如新的函数、对象、甚至代码可以被引进，已有的函数可以被删除或是其他结构上的变化。通俗点说就是在运行时代码可以根据某些条件改变自身结构。主要动态语言：Object-C、C#、JavaScript、PHP、Python、Erlang。 静态语言：与动态语言相对应的，运行时结构不可变的语言就是静态语言。如 Java、C、C++。 Java 不是动态语言，但 Java 可以称之为 “准动态语言”。即 Java 有一定的动态性，我们可以利用反射机制、字节码操作获得类似动态语言的特性。Java 的动态性让编程的时候更加灵活。 加载完类之后，在堆内存的方法区中就产生了一个 Class 类型的对象 (一个类只有一个 Class 对象)，这个对象就包含了完整的类的结构信息。我们可以通过这个对象看到类的结构。这个对象就像一面镜子，透过这个镜子看到类的结构，所以，我们形象的称之为： 反射。 Java 反射机制提供的功能： 在运行时判断任意一个对象所属的类。 在运行时构造任意一个类的对象。 在运行时判断任意一个类所具有的成员变量和方法。 在运行时获取泛型信息。 在运行时调用任意一个对象的成员变量和方法。 在运行时处理注解。 生成动态代理。 反射相关的主要 API： java.lang.Class：代表一 个类。 java.lang.reflect.Method：代表类的方法。 java.lang.reflect.Field：代表类的成员变量。 java.lang.reflect.Constructor：代表类的构造器。 理解 Class 类并获取 Class 类的实例 在 Object 类中定义了以下的方法，此方法将被所有子类继承： public final Class getClass() 以上的方法返回值的类型是一个 Class 类，此类是 Java 反射的源头，实际上所谓反射从程序的运行结果来看也很好理解，即：可以通过对象反射求出类的名称。 对象照镜子后可以得到的信息：某个类的属性、方法和构造器、某个类到底实现了哪些接口。对于每个类而言，JRE 都为其保留一个不变的 Class 类型的对象。一个 Class 对象包含了特定某个结构 (class/interface/enum/annotation/primitive type/void/[]) 的有关信息。 Class 本身也是一个类。 Class 对象只能由系统建立对象。 一个加载的类在 JVM 中只会有一个 Class 实例。 一个 Class 对象对应的是一个加载到 JVM 中的一个 .class 文件。 每个类的实例都会记得自己是由哪个 Class 实例所生成。 通过 Class 可以完整地得到一个类中的所有被加载的结构。 Class 类是 Reflection 的根源，针对任何你想动态加载、运行的类，唯有先获得相应的 Class 对象。 Class 类的常用方法： static Class forName(String name)：返回指定类名 name 的 Class 对象。 Object newInstance()：调用缺省构造函数，返回该 Class 对象的一个实例。 getName()：返回此 Class 对象所表示的实体 (类、接口、数组类、基本类型或 void) 名称。 Class getSuperClass()：返回当前 Class 对象的父类的 Class 对象。 Class [] getInterfaces()：获取当前 Class 对象的接口。 ClassLoader getClassLoader()：返回该类的类加载器。 Class getSuperclass()：返回表示此 Class 所表示的实体的超类的 Class。 Constructor[] getConstructors()：返回一个包含某些 Constructor 对象的数组。 Field[] getDeclaredFields()：返回 Field 对象的一个数组。 Method getMethod(String name,Class … paramTypes)：返回一个 Method 对象，此对象的形参类型为 paramType。 反射实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package cn.xisun.java.base.file;public class Person &#123; private String name; public int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public Person() &#123; System.out.println(&quot;Person()&quot;); &#125; private Person(String name) &#123; this.name = name; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public void show() &#123; System.out.println(&quot;你好，我是一个人&quot;); &#125; private String showNation(String nation) &#123; System.out.println(&quot;我的国籍是：&quot; + nation); return nation; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class ReflectionTest &#123; /* 反射之前，对于Person的操作 */ @Test public void test1() &#123; // 1.创建Person类的对象 Person p1 = new Person(&quot;Tom&quot;, 12); // 2.通过对象，调用其内部的属性、方法 p1.age = 10; System.out.println(p1.toString()); p1.show(); // 3.在Person类外部，不可以通过Person类的对象调用其内部私有结构。---封装性的限制 // 比如：name、showNation()以及私有的构造器 &#125; /* 反射之后，对于Person的操作 */ @Test public void test2() throws Exception &#123; Class clazz = Person.class; // 1.通过反射，创建Person类的对象 Constructor cons = clazz.getConstructor(String.class, int.class); Object obj = cons.newInstance(&quot;Tom&quot;, 12); Person p = (Person) obj; System.out.println(p.toString());// Person&#123;name=&#x27;Tom&#x27;, age=12&#125; // 2.通过反射，调用对象指定的属性、方法 // 2.1 调用属性 Field age = clazz.getDeclaredField(&quot;age&quot;); age.set(p, 10); System.out.println(p.toString());// Person&#123;name=&#x27;Tom&#x27;, age=10&#125; // 2.2 调用方法 Method show = clazz.getDeclaredMethod(&quot;show&quot;); show.invoke(p);// 你好，我是一个人 System.out.println(&quot;*******************************&quot;); // 3.通过反射，可以调用Person类的私有结构的。比如：私有的构造器、方法、属性 // 3.1 调用私有的构造器 Constructor cons1 = clazz.getDeclaredConstructor(String.class); cons1.setAccessible(true); Person p1 = (Person) cons1.newInstance(&quot;Jerry&quot;); System.out.println(p1);// Person&#123;name=&#x27;Jerry&#x27;, age=0&#125; // 3.2 调用私有的属性 Field name = clazz.getDeclaredField(&quot;name&quot;); name.setAccessible(true); name.set(p1, &quot;HanMeimei&quot;); System.out.println(p1);// Person&#123;name=&#x27;HanMeimei&#x27;, age=0&#125; // 3.3 调用私有的方法 Method showNation = clazz.getDeclaredMethod(&quot;showNation&quot;, String.class); showNation.setAccessible(true); String nation = (String) showNation.invoke(p1, &quot;中国&quot;);// 相当于String nation = p1.showNation(&quot;中国&quot;) System.out.println(nation); &#125; // 疑问1：通过直接new的方式或反射的方式都可以调用公共的结构，开发中到底用那个？ // 建议：直接new的方式。 // 什么时候会使用：反射的方式。---&gt; 根据反射的特征：动态性，进行考虑 // 疑问2：反射机制与面向对象中的封装性是不是矛盾的？如何看待两个技术？ // 不矛盾。封装性是给出了一种建议，不应该调用私有的结构，但如果有调用私有结构的需求，则可以通过反射机制做到。&#125; 获取 Class 类的实例的四种方法： 若已知具体的类，则通过类的 class 属性获取，该方法最为安全可靠，程序性能最高。比如：Class clazz = String.class;。 若已知某个类的实例，则调用该实例的 getClass() 获取 Class 对象。比如：Class clazz = &quot;Hello,World!&quot;.getClass();。 若已知一个类的全类名，且该类在类路径下，可通过 Class 类的静态方法 forName() 获取，可能抛出 ClassNotFoundException。比如：Class clazz = Class.forName(&quot;java.lang.String&quot;);。— 最常用，体现了反射的动态性 使用类的加载器 ClassLoader，比如： 12ClassLoader cl = this.getClass().getClassLoader();Class clazz4 = cl.loadClass(&quot;类的全类名&quot;); 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ReflectionTest &#123; /* 关于java.lang.Class类的理解 1.类的加载过程： 程序经过javac.exe命令以后，会生成一个或多个字节码文件(.class结尾)。 接着我们使用java.exe命令对某个字节码文件进行解释运行。相当于将某个字节码文件 加载到内存中。这个过程就称为类的加载。加载到内存中的类，我们称为运行时类，此 运行时类，就作为Class的一个实例。 (万事万物皆对象：一方面，通过对象.xxx的方式调用方法、属性等；另一方面，在反射机制中，类本身也是Class的对象) 2.换句话说，Class的实例就对应着一个运行时类。 3.加载到内存中的运行时类，会缓存一定的时间。在此时间之内，我们可以通过不同的方式 来获取此运行时类。 */ /* 获取Class的实例的方式（前三种方式需要掌握） */ @Test public void test3() throws ClassNotFoundException &#123; // 方式一：调用运行时类的属性：.class Class clazz1 = Person.class; System.out.println(clazz1);// class cn.xisun.java.base.file.Person // 方式二：通过运行时类的对象，调用getClass() Person p1 = new Person(); Class clazz2 = p1.getClass(); System.out.println(clazz2);// class cn.xisun.java.base.file.Person // 方式三：调用Class的静态方法：forName(String classPath) Class clazz3 = Class.forName(&quot;cn.xisun.java.base.file.Person&quot;);// 指明类的全类名 // clazz3 = Class.forName(&quot;java.lang.String&quot;); System.out.println(clazz3);// class cn.xisun.java.base.file.Person System.out.println(clazz1 == clazz2);// true System.out.println(clazz1 == clazz3);// true // 方式四：使用类的加载器：ClassLoader (了解) ClassLoader classLoader = ReflectionTest.class.getClassLoader(); Class clazz4 = classLoader.loadClass(&quot;cn.xisun.java.base.file.Person&quot;); System.out.println(clazz4);// class cn.xisun.java.base.file.Person System.out.println(clazz1 == clazz4);// true &#125;&#125; 哪些类型可以有 Class 对象： class：外部类，成员 (成员内部类，静态内部类)，局部内部类，匿名内部类。 interface：接口。 []：数组. enum：枚举。 annotation：注解 @interface。 primitive type：基本数据类型。 void 实例： 123456789101112131415161718192021222324public class ReflectionTest &#123; /* Class实例可以是哪些结构的说明 */ @Test public void test4() &#123; Class c1 = Object.class; Class c2 = Comparable.class; Class c3 = String[].class; Class c4 = int[][].class;// 二维数组 Class c5 = ElementType.class;// 枚举类 Class c6 = Override.class;// 注解 Class c7 = int.class; Class c8 = void.class; Class c9 = Class.class; int[] a = new int[10]; int[] b = new int[100]; Class c10 = a.getClass(); Class c11 = b.getClass(); // 只要数组的元素类型与维度一样，就是同一个Class System.out.println(c10 == c11);// true &#125;&#125; 类的加载与 ClassLoader 的理解 类的加载过程：当程序主动使用某个类时，如果该类还未被加载到内存中，则系统会通过如下三个步骤来对该类进行初始化： 加载：将 class 文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后生成一个代表这个类的 java.lang.Class 对象，作为方法区中类数据的访问入口 (即引用地址)。所有需要访问和使用类数据的地方，只能通过这个 Class 对象。这个加载的过程需要类加载器参与。 链接：将 Java 类的二进制代码合并到 JVM 的运行状态之中的过程。 验证：确保加载的类信息符合 JVM 规范，例如：以 cafe 开头，没有安全方面的问题。 准备：正式为类变量 (static) 分配内存并设置类变量默认初始值的阶段，这些内存都将在方法区中进行分配。 解析：虚拟机常量池内的符号引用 (常量名) 替换为直接引用 (地址) 的过程。 初始化： 执行类构造器 &lt;clinit&gt;() 方法的过程。类构造器 &lt;clinit&gt;() 方法是由编译期自动收集类中所有类变量的赋值动作和静态代码块中的语句合并产生的。(类构造器是构造类信息的，不是构造该类对象的构造器) 当初始化一个类的时候，如果发现其父类还没有进行初始化，则需要先触发其父类的初始化。 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境中被正确加锁和同步。 代码图示： 什么时候会发生类的初始化： 类的主动引用 (一定会发生类的初始化)： 当虚拟机启动，先初始化 main 方法所在的类。 new 一个类的对象。 调用类的静态成员 (除了 final 常量) 和静态方法。 使用 java.lang.reflect 包的方法对类进行反射调用。 当初始化一个类，如果其父类没有被初始化，则先会初始化它的父类。 类的被动引用 (不会发生类的初始化)： 当访问一个静态域时，只有真正声明这个域的类才会被初始化。 当通过子类引用父类的静态变量，不会导致子类初始化。 通过数组定义类引用，不会触发此类的初始化。 引用常量不会触发此类的初始化 (常量在链接阶段就存入调用类的常量池中了)。 实例： 123456789101112131415161718192021222324252627282930313233343536public class ClassLoadingTest &#123; public static void main(String[] args) throws ClassNotFoundException &#123; // 主动引用：一定会导致A和Father的初始化 A a = new A(); System.out.println(A.m); Class.forName(&quot;cn.xisun.java.base.file.A&quot;); // 被动引用 A[] array = new A[5];// 不会导致A和Father的初始化 System.out.println(A.b);// 只会初始化Father System.out.println(A.M);// 不会导致A和Father的初始化 &#125; static &#123; System.out.println(&quot;main所在的类&quot;); &#125;&#125;class Father &#123; static int b = 2; static &#123; System.out.println(&quot;父类被加载&quot;); &#125;&#125;class A extends Father &#123; static &#123; System.out.println(&quot;子类被加载&quot;); m = 300; &#125; static int m = 100; static final int M = 1;&#125; 类加载器的作用： 类加载的作用：将 class 文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后在堆中生成一个代表这个类的 java.lang.Class 对象，作为方法区中类数据的访问入口。 类缓存：标准的 Java SE 类加载器可以按要求查找类，但一旦某个类被加载到类加载器中，它将维持加载 (缓存) 一段时间。不过 JVM 垃圾回收机制可以回收这些 Class 对象。 JVM 规范定义了如下类型的类的加载器： 引导类加载器 扩展类加载器 系统类加载器 自定义类加载器 实例： 123456789101112131415161718192021222324252627282930313233/** * 了解类的加载器 */public class ClassLoaderTest &#123; @Test public void test1() &#123; // 对于自定义类，使用系统类加载器进行加载 ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); System.out.println(classLoader);// sun.misc.Launcher$AppClassLoader@18b4aac2 // 调用系统类加载器的getParent()：获取扩展类加载器 ClassLoader classLoader1 = classLoader.getParent(); System.out.println(classLoader1);// sun.misc.Launcher$ExtClassLoader@21588809 // 调用扩展类加载器的getParent()：无法获取引导类加载器 // 引导类加载器主要负责加载java的核心类库，无法加载自定义类的。 ClassLoader classLoader2 = classLoader1.getParent(); System.out.println(classLoader2);// null ClassLoader classLoader3 = String.class.getClassLoader(); System.out.println(classLoader3);// null，String的加载器是引导类加载器，无法获取 // 测试当前类由哪个类加载器进行加载 ClassLoader classLoader4 = Class.forName(&quot;cn.xisun.java.base.file.ClassLoaderTest&quot;).getClassLoader(); System.out.println(classLoader4);// sun.misc.Launcher$AppClassLoader@18b4aac2 // 测试JDK提供的Object类由哪个类加载器加载 ClassLoader classLoader5 = Class.forName(&quot;java.lang.Object&quot;).getClassLoader(); System.out.println(classLoader5);// null，Object的加载器是引导类加载器，无法获取 // 关于类加载器的一个主要方法：getResourceAsStream(String str):获取类路径下的指定文件的输入流 InputStream in = this.getClass().getClassLoader().getResourceAsStream(&quot;test.properties&quot;); System.out.println(in); &#125;&#125; 类加载器读取配置文件： 1234567891011121314151617181920212223242526public class ClassLoaderTest &#123; /* Properties：用来读取配置文件。 注意：配置文件的路径问题 */ @Test public void test2() throws Exception &#123; Properties pros = new Properties(); // 读取配置文件的方式一： // 此时的文件默认在当前的module下 /*FileInputStream fis = new FileInputStream(&quot;jdbc1.properties&quot;); // FileInputStream fis = new FileInputStream(&quot;src\\\\jdbc1.properties&quot;);// 等同于方式二 pros.load(fis);*/ // 读取配置文件的方式二：使用ClassLoader // 此时的配置文件默认识别为：当前module的src下 ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); InputStream is = classLoader.getResourceAsStream(&quot;jdbc1.properties&quot;); pros.load(is); String user = pros.getProperty(&quot;user&quot;); String password = pros.getProperty(&quot;password&quot;); System.out.println(&quot;user = &quot; + user + &quot;, password = &quot; + password); &#125;&#125; 创建运行时类的对象 当拿到了运行时类的 Class 对象后，就可以创建该运行时类的对象，这是反射机制应用最多的地方。 通过 Class 对象的 newInstance() 创建： 运行时类必须有一个无参数的构造器。 运行时类的构造器的访问权限需要足够。 12345678910111213141516171819202122/** * 通过反射创建对应的运行时类的对象 */public class NewInstanceTest &#123; @Test public void test1() throws IllegalAccessException, InstantiationException &#123; Class&lt;Person&gt; clazz = Person.class; /* newInstance(): 调用此方法，创建对应的运行时类的对象。内部调用了运行时类的空参的构造器。 要想此方法正常的创建运行时类的对象，要求： 1.运行时类必须提供空参的构造器 2.空参的构造器的访问权限得够。通常，设置为public。 在javabean中要求提供一个public的空参构造器。原因： 1.便于通过反射，创建运行时类的对象 2.便于子类继承此运行时类时，默认调用super()时，保证父类有此构造器 */ Person obj = clazz.newInstance(); System.out.println(obj); &#125;&#125; 通过 Class 对象的 getDeclaredConstructor(Class … parameterTypes) 创建： 先向构造器的形参中传递一个对象数组进去，里面包含了构造器中所需的各个参数。 再通过 Constructor 实例化对象。 体会反射的动态性： 123456789101112131415161718192021222324252627282930313233343536373839public class NewInstanceTest &#123; /* 体会反射的动态性：以下程序只有在运行时，才能确定到底创建哪个对象 */ @Test public void test2() &#123; for (int i = 0; i &lt; 100; i++) &#123; int num = new Random().nextInt(3);// 0,1,2 String classPath = &quot;&quot;; switch (num) &#123; case 0: classPath = &quot;java.util.Date&quot;; break; case 1: classPath = &quot;java.lang.Object&quot;; break; case 2: classPath = &quot;cn.xisun.java.base.file.Person&quot;; break; &#125; try &#123; Object obj = getInstance(classPath); System.out.println(obj); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /* 创建一个指定类的对象。 classPath: 指定类的全类名 */ public Object getInstance(String classPath) throws Exception &#123; Class clazz = Class.forName(classPath); return clazz.newInstance(); &#125;&#125; 获取运行时类的完整结构 类的完整结构： Field 、Method 、Constructor 、Superclass 、Interface 、Annotation。 全部的 Field。 全部的方法。 全部的构造器。 所继承的父类。 实现的全部接口。 注解。 定义 Person 类和相关接口、注解类： 123456789101112public class Creature&lt;T&gt; implements Serializable &#123; private char gender; public double weight; private void breath() &#123; System.out.println(&quot;生物呼吸&quot;); &#125; public void eat() &#123; System.out.println(&quot;生物吃东西&quot;); &#125;&#125; 123public interface MyInterface &#123; void info();&#125; 12345@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotation &#123; String value() default &quot;hello&quot;;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@MyAnnotation(value = &quot;hi&quot;)public class Person extends Creature&lt;String&gt; implements Comparable&lt;String&gt;, MyInterface &#123; private String name; int age; public int id; public Person() &#123; &#125; @MyAnnotation(value = &quot;abc&quot;) private Person(String name) &#123; this.name = name; &#125; Person(String name, int age) &#123; this.name = name; this.age = age; &#125; @MyAnnotation private String show(String nation) &#123; System.out.println(&quot;我的国籍是：&quot; + nation); return nation; &#125; public String display(String interests, int age) throws NullPointerException, ClassCastException &#123; return interests + age; &#125; @Override public void info() &#123; System.out.println(&quot;我是一个人&quot;); &#125; @Override public int compareTo(String o) &#123; return 0; &#125; private static void showDesc() &#123; System.out.println(&quot;我是一个可爱的人&quot;); &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, id=&quot; + id + &#x27;&#125;&#x27;; &#125;&#125; 使用反射获得全部的 Field： public Field[] getFields()：返回此 Class 对象所表示的类或接口的 public 的 Field (包括父类)。 public Field[] getDeclaredFields()：返回此 Class 对象所表示的类或接口的全部 Field (不包括父类)。 Field 类中的方法： public int getModifiers()：以整数形式返回此 Field 的修饰符。 public Class&lt;?&gt; getType()：得到 Field 的属性类型。 public String getName()：返回 Field 的名称。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class FieldTest &#123; @Test public void test1() &#123; Class clazz = Person.class; // 获取属性结构 // getFields(): 获取当前运行时类及其父类中声明为public访问权限的属性 Field[] fields = clazz.getFields(); for (Field f : fields) &#123; System.out.println(f); &#125; System.out.println(); // getDeclaredFields(): 获取当前运行时类中声明的所有属性。（不包含父类中声明的属性） Field[] declaredFields = clazz.getDeclaredFields(); for (Field f : declaredFields) &#123; System.out.println(f); &#125; &#125; /* 权限修饰符 数据类型 变量名 */ @Test public void test2() &#123; Class clazz = Person.class; Field[] declaredFields = clazz.getDeclaredFields(); for (Field f : declaredFields) &#123; // 1.权限修饰符 int modifier = f.getModifiers(); System.out.print(Modifier.toString(modifier) + &quot;, &quot;); // 2.数据类型 Class type = f.getType(); System.out.print(type.getName() + &quot;, &quot;); // 3.变量名 String fName = f.getName(); System.out.print(fName); System.out.println(); &#125; &#125;&#125;test1输出结果：public int cn.xisun.java.base.file.Person.idpublic double cn.xisun.java.base.file.Creature.weightprivate java.lang.String cn.xisun.java.base.file.Person.nameint cn.xisun.java.base.file.Person.agepublic int cn.xisun.java.base.file.Person.idtest2输出结果：private, java.lang.String, name, int, agepublic, int, id 使用反射获得全部的 Method： public Method[] getMethods()：返回此 Class 对象所表示的类或接口的 public 的 Method (包括父类)。 public Method[] getDeclaredMethods()：返回此 Class 对象所表示的类或接口的全部 Method (不包括父类)。 Method 类中的方法： public Class&lt;?&gt;[] getParameterTypes()：取得全部的参数。 public int getModifiers()：取得修饰符。 public Class&lt;?&gt; getReturnType()：取得全部的返回值。 public Class&lt;?&gt;[] getExceptionTypes()：取得异常信息。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class MethodTest &#123; @Test public void test1() &#123; Class clazz = Person.class; // getMethods(): 获取当前运行时类及其所有父类中声明为public权限的方法 Method[] methods = clazz.getMethods(); for (Method m : methods) &#123; System.out.println(m); &#125; System.out.println(); // getDeclaredMethods(): 获取当前运行时类中声明的所有方法。（不包含父类中声明的方法） Method[] declaredMethods = clazz.getDeclaredMethods(); for (Method m : declaredMethods) &#123; System.out.println(m); &#125; &#125; /* @Xxxx 权限修饰符 返回值类型 方法名(参数类型1 形参名1, ...) throws XxxException&#123;&#125; */ @Test public void test2() &#123; Class clazz = Person.class; Method[] declaredMethods = clazz.getDeclaredMethods(); for (Method m : declaredMethods) &#123; // 1.获取方法声明的注解 Annotation[] annos = m.getAnnotations(); for (Annotation a : annos) &#123; System.out.print(a + &quot;, &quot;); &#125; // 2.权限修饰符 System.out.print(Modifier.toString(m.getModifiers()) + &quot;, &quot;); // 3.返回值类型 System.out.print(m.getReturnType().getName() + &quot;, &quot;); // 4.方法名 System.out.print(m.getName()); // 5.形参列表 System.out.print(&quot;(&quot;); Class[] parameterTypes = m.getParameterTypes(); if (!(parameterTypes == null &amp;&amp; parameterTypes.length == 0)) &#123; for (int i = 0; i &lt; parameterTypes.length; i++) &#123; if (i == parameterTypes.length - 1) &#123; System.out.print(parameterTypes[i].getName() + &quot; args_&quot; + i); break; &#125; System.out.print(parameterTypes[i].getName() + &quot; args_&quot; + i + &quot;, &quot;); &#125; &#125; System.out.print(&quot;), &quot;); // 6.抛出的异常 Class[] exceptionTypes = m.getExceptionTypes(); if (exceptionTypes.length &gt; 0) &#123; System.out.print(&quot;throws &quot;); for (int i = 0; i &lt; exceptionTypes.length; i++) &#123; if (i == exceptionTypes.length - 1) &#123; System.out.print(exceptionTypes[i].getName()); break; &#125; System.out.print(exceptionTypes[i].getName() + &quot;, &quot;); &#125; &#125; System.out.println(); &#125; &#125;&#125; 使用反射获得全部的构造器： public Constructor&lt;T&gt;[] getConstructors()：返回此 Class 对象所表示的类的所有 public 构造方法 (没有父类)。 public Constructor&lt;T&gt;[] getDeclaredConstructors()：返回此 Class 对象表示的类声明的所有构造方法 (没有父类)。 Constructor 类中的方法： public int getModifiers()：取得修饰符。 public String getName()：取得方法名称。 public Class&lt;?&gt;[] getParameterTypes()：取得参数的类型。 使用反射获得实现的全部接口： public Class&lt;?&gt;[] getInterfaces()：确定此对象所表示的类或接口实现的接口。 使用反射获得所继承的父类 public Class&lt;? Super T&gt; getSuperclass()：返回表示此 Class 所表示的实体 (类、接口、基本类型) 的父类的 Class。 使用反射获得泛型相关： Type getGenericSuperclass()：获取父类泛型类型。 泛型类型：ParameterizedType。 getActualTypeArguments()：获取实际的泛型类型参数数组。 使用反射获得 Annotation 相关 get Annotation(Class&lt;T&gt; annotationClass) getDeclaredAnnotations() 使用反射获得类所在的包： Package getPackage() 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106public class OtherTest &#123; /* 获取构造器结构 */ @Test public void test1() &#123; Class&lt;Person&gt; clazz = Person.class; // getConstructors(): 获取当前运行时类中声明为public的构造器 Constructor&lt;?&gt;[] constructors = clazz.getConstructors(); for (Constructor&lt;?&gt; c : constructors) &#123; System.out.println(c); &#125; System.out.println(); // getDeclaredConstructors(): 获取当前运行时类中声明的所有的构造器 Constructor&lt;?&gt;[] declaredConstructors = clazz.getDeclaredConstructors(); for (Constructor&lt;?&gt; c : declaredConstructors) &#123; System.out.println(c); &#125; &#125; /* 获取运行时类的父类 */ @Test public void test2() &#123; Class&lt;Person&gt; clazz = Person.class; Class&lt;? super Person&gt; superclass = clazz.getSuperclass(); System.out.println(superclass); &#125; /* 获取运行时类的带泛型的父类 */ @Test public void test3() &#123; Class&lt;Person&gt; clazz = Person.class; Type genericSuperclass = clazz.getGenericSuperclass(); System.out.println(genericSuperclass); &#125; /* 获取运行时类的带泛型的父类的泛型 代码：逻辑性代码 vs 功能性代码 */ @Test public void test4() &#123; Class&lt;Person&gt; clazz = Person.class; Type genericSuperclass = clazz.getGenericSuperclass(); ParameterizedType paramType = (ParameterizedType) genericSuperclass; // 获取泛型类型 Type[] actualTypeArguments = paramType.getActualTypeArguments(); System.out.println(actualTypeArguments[0].getTypeName());// 方式一 System.out.println(((Class) actualTypeArguments[0]).getName());// 方式二 &#125; /* 获取运行时类实现的接口 */ @Test public void test5() &#123; Class&lt;Person&gt; clazz = Person.class; Class&lt;?&gt;[] interfaces = clazz.getInterfaces(); for (Class&lt;?&gt; c : interfaces) &#123; System.out.println(c); &#125; System.out.println(); // 获取运行时类的父类实现的接口 Class&lt;?&gt;[] interfaces1 = clazz.getSuperclass().getInterfaces(); for (Class&lt;?&gt; c : interfaces1) &#123; System.out.println(c); &#125; &#125; /* 获取运行时类声明的注解 */ @Test public void test7() &#123; Class&lt;Person&gt; clazz = Person.class; Annotation[] annotations = clazz.getAnnotations(); for (Annotation annos : annotations) &#123; System.out.println(annos); &#125; &#125; /* 获取运行时类所在的包 */ @Test public void test6() &#123; Class&lt;Person&gt; clazz = Person.class; Package pack = clazz.getPackage(); System.out.println(pack); &#125;&#125; 调用运行时类的指定结构 调用指定属性： 在反射机制中，可以直接通过 Field 类操作类中的属性，通过 Field 类提供的 set() 和 get() 就可以完成设置和取得属性内容的操作。 public Field getField(String name)：返回此 Class 对象表示的类或接口的指定的 public 的Field。 **public Field getDeclaredField(String name)**：返回此 Class 对象表示的类或接口的指定的 Field。 在 Field 中： public void set(Object obj,Object value)：设置指定对象 obj 上此 Field 的属性内容。 public Object get(Object obj)：取得指定对象 obj 上此 Field 的属性内容。 调用指定方法： 通过反射，调用类中的方法，通过 Method 类完成。步骤： 通过 Class 类的 getDeclaredMethod(String name,Class…parameterTypes) 取得一个 Method 对象，并设置此方法操作时所需要的参数类型。 之后使用 Object invoke(Object obj, Object[] args) 进行调用，并向方法中传递要设置的 obj 对象的参数信息。 关于 setAccessible(true) 的使用： Method 和 Field、Constructor 对象都有 setAccessible()。 setAccessible() 启动和禁用访问安全检查的开关。 参数值为 true 则指示反射的对象在使用时应该取消 Java 语言访问检查。 提高反射的效率。如果代码中必须用反射，而该句代码需要频繁的被调用，那么请设置为 true。 使得原本无法访问的私有成员也可以访问。 参数值为 false 则指示反射的对象应该实施 Java 语言访问检查。 关于 Object invoke(Object obj, Object … args) 的使用： Object 对应原方法的返回值，若原方法无返回值，此时返回 null。 若原方法为静态方法，则形参 obj 为运行时类本身或者 null。 若原方法形参列表为空，则形参 args 为 null。 若原方法声明为 private，则需要在调用此 invoke() 前，显式调用方法对象的 setAccessible(true)，即可访问 private 的方法。(一般来说，不论调用的是什么权限的方法，都可显示调用方法对象的 setAccessible(true)。) 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126public class ReflectionTest &#123; /* 不需要掌握，因为只能获取public的,通常不采用此方法 */ @Test public void testField() throws Exception &#123; Class&lt;Person&gt; clazz = Person.class; // 创建运行时类的对象 Person p = clazz.newInstance(); // 获取指定的属性：要求运行时类中属性声明为public Field id = clazz.getField(&quot;id&quot;); /* 设置当前属性的值 set(): 参数1：指明设置哪个对象的属性 参数2：将此属性值设置为多少 */ id.set(p, 1001); /* 获取当前属性的值 get(): 参数1：获取哪个对象的当前属性值 */ int pId = (int) id.get(p); System.out.println(pId);// 1001 &#125; /* 如何操作运行时类中的指定的属性 --- 需要掌握 */ @Test public void testField1() throws Exception &#123; Class clazz = Person.class; // 创建运行时类的对象 Person p = (Person) clazz.newInstance(); // 1. getDeclaredField(String fieldName): 获取运行时类中指定变量名的属性 Field name = clazz.getDeclaredField(&quot;name&quot;); // 2.保证当前属性是可访问的 name.setAccessible(true); // 3.获取、设置指定对象的此属性值 name.set(p, &quot;Tom&quot;); System.out.println(name.get(p)); System.out.println(&quot;*************如何调用静态属性*****************&quot;); // public static String national = &quot;中国&quot;; Field national = clazz.getDeclaredField(&quot;national&quot;); national.setAccessible(true); System.out.println(national.get(Person.class));// 中国 &#125; /* 如何操作运行时类中的指定的方法 --- 需要掌握 */ @Test public void testMethod() throws Exception &#123; Class&lt;Person&gt; clazz = Person.class; // 创建运行时类的对象 Person p = clazz.newInstance(); /* 1.获取指定的某个方法 getDeclaredMethod(): 参数1 ：指明获取的方法的名称 参数2：指明获取的方法的形参列表 */ Method show = clazz.getDeclaredMethod(&quot;show&quot;, String.class); // 2.保证当前方法是可访问的 show.setAccessible(true); /* 3.调用方法的invoke(): 参数1：方法的调用者 参数2：给方法形参赋值的实参 invoke()的返回值即为对应类中调用的方法的返回值 */ Object returnValue = show.invoke(p, &quot;CHN&quot;); // String nation = p.show(&quot;CHN&quot;); System.out.println(returnValue);// CHN，返回的returnValue是一个String，可以强转 System.out.println(&quot;*************如何调用静态方法*****************&quot;); // private static void showDesc()&#123;&#125; Method showDesc = clazz.getDeclaredMethod(&quot;showDesc&quot;); showDesc.setAccessible(true); // 如果调用的运行时类中的方法没有返回值，则此invoke()返回null // Object returnVal = showDesc.invoke(null);// 参数写null也可以，因为静态方法的调用者只有类本身 Object returnVal = showDesc.invoke(Person.class);// 静态方法的调用者就是当前类 System.out.println(returnVal);// null &#125; /* 如何调用运行时类中的指定的构造器 --- 不常用 经常调用类的空参构造器创建类的对象：Person p = clazz.newInstance(); */ @Test public void testConstructor() throws Exception &#123; Class&lt;Person&gt; clazz = Person.class; //private Person(String name) /* 1.获取指定的构造器 getDeclaredConstructor(): 参数：指明构造器的参数列表 */ // private Person(String name) &#123;this.name = name;&#125; Constructor&lt;Person&gt; constructor = clazz.getDeclaredConstructor(String.class); // 2.保证此构造器是可访问的 constructor.setAccessible(true); // 3.调用此构造器创建运行时类的对象 Person per = constructor.newInstance(&quot;Tom&quot;); System.out.println(per); &#125;&#125; 反射的应用：动态代理 代理设计模式的原理：使用一个代理将对象包装起来，然后用该代理对象取代原始对象。任何对原始对象的调用都要通过代理。代理对象决定是否以及何时将方法调用转到原始对象上。 代理过程：代理类和被代理类实现共同的接口，重写接口的方法 a。被代理类中，在方法 a 中实现自身需要完成的逻辑。代理类中，提供被代理类的实例，并在方法 a 中，调用该实例对象的方法 a，同时，在代理类的方法 a 中，也可以添加一些不同代理类需要实现的公共的方法。 代理分为静态代理和动态代理。 静态代理的特征是代理类和目标对象的类都是在编译期间确定下来。静态代理不利于程序的扩展。同时，每一个代理类只能为一个接口服务，这样一来程序开发中必然产生过多的代理。 最好可以通过一个代理类完成全部的代理功能。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 静态代理举例 * * 特点：代理类和被代理类在编译期间，就确定下来了。 */interface ClothFactory &#123; void produceCloth();&#125;/** * 被代理类1 */class AntaClothFactory implements ClothFactory &#123; @Override public void produceCloth() &#123; System.out.println(&quot;Anta工厂生产一批运动服&quot;); &#125;&#125;/** * 被代理类2 */class LiningClothFactory implements ClothFactory &#123; @Override public void produceCloth() &#123; System.out.println(&quot;Lining工厂生产一批运动服&quot;); &#125;&#125;/** * 代理类 --- 只能代理实现了ClothFactory这个接口的被代理类，其他类型的被代理类不能使用 */class ProxyClothFactory implements ClothFactory &#123; // 用被代理类对象进行实例化 private ClothFactory factory; public ProxyClothFactory(ClothFactory factory) &#123; this.factory = factory; &#125; @Override public void produceCloth() &#123; System.out.println(&quot;代理类做一些公共的准备工作&quot;); factory.produceCloth();// 此方法由具体的被代理类自己实现 System.out.println(&quot;代理类做一些公共的收尾工作&quot;); &#125;&#125;public class StaticProxyTest &#123; public static void main(String[] args) &#123; // 创建被代理类的对象 ClothFactory anta = new AntaClothFactory(); // 创建代理类的对象 ClothFactory proxyClothFactory = new ProxyClothFactory(anta); proxyClothFactory.produceCloth(); System.out.println(&quot;******************************&quot;); proxyClothFactory = new ProxyClothFactory(new LiningClothFactory()); proxyClothFactory.produceCloth(); &#125;&#125; 动态代理是指客户通过代理类来调用其它对象的方法，并且是在程序运行时根据需要动态创建目标类的代理对象。 动态代理使用场合： 调试 远程方法调用 动态代理相比于静态代理的优点：抽象角色中 (接口) 声明的所有方法都被转移到调用处理器一个集中的方法中处理，这样，我们可以更加灵活和统一的处理众多的方法。 一个动态代理类能做到所有被代理类的工作，在运行时，会根据传入的被代理类的对象，动态的创建一个对应的代理对象。 Java 动态代理相关的 API： Proxy：专门完成代理的操作类，是所有动态代理类的父类。通过此类为一个或多个接口动态地生成实现类。 提供用于创建动态代理类和动态代理对象的静态方法： static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces)：创建一个动态代理类所对应的Class对象 static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h)：直接创建一个动态代理对象 动态代理步骤： 创建一个实现接口 InvocationHandler 的类，它必须实现 invoke()，以完成代理的具体操作： 创建被代理的类以及接口： 通过 Proxy 的静态方法 newProxyInstance(ClassLoader loader, Class[] interfaces, InvocationHandler h) 创建一个 Subject 接口代理： 12345RealSubject target = new RealSubject();// Create a proxy to wrap the original implementationDebugProxy proxy = new DebugProxy(target);// Get a reference to the proxy through the Subject interfaceSubject sub = (Subject) Proxy.newProxyInstance(Subject.class.getClassLoader(),new Class[] &#123; Subject.class &#125;, proxy); 通过 Subject 代理调用 RealSubject 实现类的方法： 12String info = sub.say(&quot;Peter&quot;, 24);System.out.println(info); 实例： 1234567891011121314151617181920212223/** * 被代理类型一 */interface Human &#123; String getBelief(); void eat(String food);&#125;/** * 被代理类 */class SuperMan implements Human &#123; @Override public String getBelief() &#123; return &quot;I believe I can fly!&quot;; &#125; @Override public void eat(String food) &#123; System.out.println(&quot;超人喜欢吃&quot; + food); &#125;&#125; 12345678910111213141516/** * 被代理类型二 */interface ClothFactory &#123; void produceCloth();&#125;/** * 被代理类 */class AntaClothFactory implements ClothFactory &#123; @Override public void produceCloth() &#123; System.out.println(&quot;Anta工厂生产一批运动服&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/*要想实现动态代理，需要解决的问题？问题一：如何根据加载到内存中的被代理类，动态的创建一个代理类及其对象。问题二：当通过代理类的对象调用方法a时，如何动态的去调用被代理类中的同名方法a。 *//** * 动态代理类 */class ProxyFactory &#123; // 调用此方法，返回一个代理类的对象。---&gt; 解决问题一 // 返回的可能是不同类型的代理类对象，因此返回Object，然后根据传参obj的类型，再进行强转 public static Object getProxyInstance(Object obj) &#123;// obj:被代理类的对象 // 创建InvocationHandler接口的实例，并赋值被代理类的对象 MyInvocationHandler handler = new MyInvocationHandler(); handler.bind(obj); /* 参数1：被代理类obj的类加载器 参数2：被代理类obj实现的接口 参数3：实现InvocationHandler接口的handler，包含被代理类执行方法调用的逻辑 */ return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), handler); &#125;&#125;/** * 实现InvocationHandler接口 ---&gt; 解决问题二 */class MyInvocationHandler implements InvocationHandler &#123; // 需要使用被代理类的对象进行赋值 private Object obj; // 赋值操作，也可以通过构造器进行赋值 public void bind(Object obj) &#123; this.obj = obj; &#125; // 当我们通过代理类的对象，调用方法a时，就会自动的调用如下的方法：invoke() // 将被代理类要执行的方法a的功能就声明在invoke()中 // proxy：代理类的对象 // method：代理类和被代理类共同实现的接口中的重写的方法 // args：该重写方法需要传入的参数 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; // method: 即为代理类对象调用的方法，此方法也就作为了被代理类对象要调用的方法 // obj: 被代理类的对象 Object returnValue = method.invoke(obj, args); // 上述方法的返回值returnValue就作为当前类中的invoke()的返回值。 // 实际上也就是被代理类所调用方法的返回值 return returnValue; &#125;&#125;/** * 测试方法 */public class ProxyTest &#123; public static void main(String[] args) &#123; // 被代理类型一 SuperMan superMan = new SuperMan(); // proxyHuman: 代理类的对象 // 在动态代理中，proxyHuman代表的是代理类的对象，不应该被强转为SuperMan，因为SuperMan是被代理类 // 但可以被强转为共同的接口Human。其他类型的代理类和被代理类同理 Human proxyHuman = (Human) ProxyFactory.getProxyInstance(superMan); // 当通过代理类对象调用方法时，会自动的调用被代理类中同名的方法 String belief = proxyHuman.getBelief();// 方法一：getBelief()有返回值 System.out.println(belief); proxyHuman.eat(&quot;四川麻辣烫&quot;);// 方法二：eat()没有返回值 System.out.println(&quot;*****************************&quot;); // 被代理类型二 AntaClothFactory antaClothFactory = new AntaClothFactory();// 创建被代理类 ClothFactory proxyClothFactory = (ClothFactory) ProxyFactory.getProxyInstance(antaClothFactory);// 创建代理类 proxyClothFactory.produceCloth();// 执行方法 &#125;&#125; 动态代理与 AOP (Aspect Orient Programming)： 前面介绍的 Proxy 和 InvocationHandler，很难看出这种动态代理的优势，下面介绍一种更实用的动态代理机制。 改进前： 改进后的说明：代码段 1、代码段 2、代码段 3 和深色代码段分离开了，但代码段 1、2、3 又和一个特定的方法 A 耦合了！最理想的效果是：代码块 1、2、3 既可以执行方法A ，又无须在程序中以硬编码的方式直接调用深色代码的方法。 AOP 实例，参看 ProxyUtil 的定义和使用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/** * 被代理类型一 */interface Human &#123; String getBelief(); void eat(String food);&#125;/** * 被代理类 */class SuperMan implements Human &#123; @Override public String getBelief() &#123; return &quot;I believe I can fly!&quot;; &#125; @Override public void eat(String food) &#123; System.out.println(&quot;超人喜欢吃&quot; + food); &#125;&#125;/** * 被代理类型二 */interface ClothFactory &#123; void produceCloth();&#125;/** * 被代理类 */class AntaClothFactory implements ClothFactory &#123; @Override public void produceCloth() &#123; System.out.println(&quot;Anta工厂生产一批运动服&quot;); &#125;&#125;/** * 不同被代理类都需要执行的通用方法，比如日志等 --- AOP的应用 */class ProxyUtil &#123; public void method1() &#123; System.out.println(&quot;====================通用方法一====================&quot;); &#125; public void method2() &#123; System.out.println(&quot;====================通用方法二====================&quot;); &#125;&#125;/** * 动态代理类 */class ProxyFactory &#123; public static Object getProxyInstance(Object obj) &#123; MyInvocationHandler handler = new MyInvocationHandler(); handler.bind(obj); return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), handler); &#125;&#125;class MyInvocationHandler implements InvocationHandler &#123; private Object obj; public void bind(Object obj) &#123; this.obj = obj; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; ProxyUtil util = new ProxyUtil(); // 执行通用方法一 util.method1(); // 执行被代理类的相应方法 Object returnValue = method.invoke(obj, args); // 执行通用方法二 util.method2(); return returnValue; &#125;&#125;public class ProxyTest &#123; public static void main(String[] args) &#123; // 被代理类型一 SuperMan superMan = new SuperMan(); Human proxyHuman = (Human) ProxyFactory.getProxyInstance(superMan); String belief = proxyHuman.getBelief();// getBelief()有返回值 System.out.println(belief); proxyHuman.eat(&quot;四川麻辣烫&quot;);// eat()没有返回值 System.out.println(&quot;*****************************&quot;); // 被代理类型二 AntaClothFactory antaClothFactory = new AntaClothFactory(); ClothFactory proxyClothFactory = (ClothFactory) ProxyFactory.getProxyInstance(antaClothFactory); proxyClothFactory.produceCloth(); &#125;&#125; 使用 Proxy 生成一个动态代理时，往往并不会凭空产生一个动态代理，这样没有太大的意义。通常都是为指定的目标对象生成动态代理。 这种动态代理在 AOP 中被称为 AOP 代理，AOP 代理可代替目标对象，AOP 代理包含了目标对象的全部方法。但 AOP 代理中的方法与目标对象的方法存在差异：AOP 代理里的方法可以在执行目标方法之前、之后插入一些通用处理。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 网络编程","slug":"java-network","date":"2021-04-05T04:39:24.000Z","updated":"2021-04-09T07:59:26.744Z","comments":true,"path":"2021/04/05/java-network/","link":"","permalink":"http://example.com/2021/04/05/java-network/","excerpt":"","text":"网络编程概述 Java 是 Internet 上的语言，它从语言级上提供了对网络应用程序的支持，程序员能够很容易开发常见的网络应用程序。 Java 提供的网络类库，可以实现无痛的网络连接，联网的底层细节被隐藏在 Java 的本机安装系统里，由 JVM 进行控制。并且 Java 实现了一个跨平台的网络库， 程序员面对的是一个统一的网络编程环境。 计算机网络：把分布在不同地理区域的计算机与专门的外部设备用通信线路互连成一个规模大、功能强的网络系统，从而使众多的计算机可以方便地互相传递信息、共享硬件、软件、数据信息等资源。 网络编程的目的：直接或间接地通过网络协议与其它计算机实现数据交换，进行通讯。 网络编程中有两个主要的问题： 如何准确地定位网络上一台或多台主机，以及定位主机上的特定的应用。 找到主机后如何可靠高效地进行数据传输。 网络通信要素概述 IP 和端口号 网络通信协议 如何实现网络中的主机互相通信： 通信双方地址： IP 端口号 一定的规则 (即：网络通信协议。有两套参考模型)： OSI 参考模型：模型过于理想化，未能在因特网上进行广泛推广。 TCP/IP 参考模型 (或叫 TCP/IP 协议)：事实上的国际标准。 网络中数据传输过程： 通信要素 1：IP 和端口号 IP 地址：Java 中，一个 InetAddress 类的实例对象，就代表一个 IP 地址。 唯一的标识 Internet 上的计算机 (通信实体)。 本地回环地址 (hostAddress)：127.0.0.1，主机名 (hostName)：localhost。 IP 地址分类方式 1：IPV4 和 IPV6。 IPV4：4 个字节组成，4 个 0 ~ 255。大概 42 亿，30 亿都在北美，亚洲 4亿。2011 年初已经用尽。以点分十进制表示，如 192.168.0.1。 IPV6：128 位，16个字节，写成 8 个无符号整数，每个整数用四个十六进制位表示，数之间用冒号 : 分开，如：3ffe:3201:1401:1280:c8ff:fe4d:db39:1984。 IP 地址分类方式 2： 公网地址 (万维网使用) 和私有地址 (局域网使用)。192.168. 开头的就是私有址址，范围为 192.168.0.0 ~ 192.168.255.255，专门为组织机构内部使用。 特点：不易记忆。 端口号：标识正在计算机上运行的进程 (程序)。 不同的进程有不同的端口号。 被规定为一个 16 位的整数 0 ~ 65535。 端口分类： 公认端口：0 ~ 1023。被预先定义的服务通信占用，如：HTTP 占用端口 80，FTP 占用端口 21，Telnet 占用端口 23 等。 注册端口：1024 ~ 49151。分配给用户进程或应用程序，如：Tomcat 占用端口 8080，MySQL 占用端口 3306，Oracle 占用端口 1521 等。 动态/私有端口：49152 ~ 65535。 端口号与 IP 地址的组合得出一个网络套接字：Socket。 InetAddress 类： Internet 上的主机有两种方式表示地址： 域名 (hostName)，如：www.atguigu.com。 IP 地址 (hostAddress)，如：202.108.35.210。 InetAddress 类主要表示 IP 地址，两个子类：Inet4Address、Inet6Address。 InetAddress 类对象含有一个 Internet 主机地址的域名和 IP 地址，如：www.atguigu.com 和 202.108.35.210。 域名容易记忆，当在连接网络时输入一个主机的域名后，域名服务器 (DNS) 负责将域名转化成 IP 地址，这样才能和主机建立连接。这就是域名解析。 域名解析时，先找本机 hosts 文件，确定是否有输入的域名地址，如果没有，再通过 DNS 服务器，找到对应的主机。 InetAddress 类没有提供公共的构造器，而是提供了如下几个静态方法来获取 InetAddress 实例： public static InetAddress getLocalHost() public static InetAddress getByName(String host) InetAddress 类提供了如下几个常用的方法： public String getHostAddress()：返回 IP 地址字符串，以文本表现形式。 public String getHostName()：获取此 IP 地址的主机名。 public boolean isReachable(int timeout)：测试是否可以达到该地址。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 一、网络编程中有两个主要的问题： * 1.如何准确地定位网络上一台或多台主机；定位主机上的特定的应用 * 2.找到主机后如何可靠高效地进行数据传输 * * 二、网络编程中的两个要素： * 1.对应问题一：IP和端口号 * 2.对应问题二：提供网络通信协议：TCP/IP参考模型（应用层、传输层、网络层、物理+数据链路层） * * * 三、通信要素一：IP和端口号 * * 1. IP: 唯一的标识 Internet 上的计算机（通信实体） * 2. 在Java中使用InetAddress类代表IP * 3. IP分类：IPv4 和 IPv6 ; 万维网 和 局域网 * 4. 域名: www.baidu.com www.mi.com www.sina.com www.jd.com www.vip.com * 5. 本地回路地址：127.0.0.1 对应着：localhost * * 6. 如何实例化InetAddress:两个方法：getByName(String host) 、 getLocalHost() * 两个常用方法：getHostName() / getHostAddress() * * 7. 端口号：正在计算机上运行的进程。 * 要求：不同的进程有不同的端口号 * 范围：被规定为一个 16 位的整数 0~65535。 * * 8. 端口号与IP地址的组合得出一个网络套接字：Socket */public class InetAddressTest &#123; public static void main(String[] args) &#123; try &#123; InetAddress inet1 = InetAddress.getByName(&quot;192.168.10.14&quot;); System.out.println(inet1);// /192.168.10.14 InetAddress inet2 = InetAddress.getByName(&quot;www.atguigu.com&quot;); System.out.println(inet2);// www.atguigu.com/58.215.145.131 InetAddress inet3 = InetAddress.getByName(&quot;127.0.0.1&quot;); System.out.println(inet3);// /127.0.0.1 // 获取本地ip InetAddress inet4 = InetAddress.getLocalHost(); System.out.println(inet4); // getHostAddress() System.out.println(inet2.getHostAddress());// 58.215.145.131 // getHostName() System.out.println(inet2.getHostName());// www.atguigu.com // isReachable(int timeout) try &#123; System.out.println(inet2.isReachable(10));// true &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; catch (UnknownHostException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 通信要素 2：网络通信协议 网络通信协议：计算机网络中实现通信必须有一些约定，即通信协议，对速率、传输代码、代码结构、传输控制步骤、出错控制等制定标准。 问题：网络协议太复杂。计算机网络通信涉及内容很多，比如指定源地址和目标地址、加密解密、压缩解压缩、差错控制、流量控制、路由控制等，如何实现如此复杂的网络协议呢？ 通信协议分层的思想：在制定协议时，把复杂成份分解成一些简单的成份，再将它们复合起来。最常用的复合方式是层次方式，即同层间可以通信、上一层可以调用下一层，而与再下一层不发生关系。各层互不影响，利于系统的开发和扩展。 TCP/IP 协议簇： TCP/IP 协议簇以其两个主要协议传输控制协议 (TCP) 和网络互联协议 (IP) 而得名，实际上是一组协议，包括多个具有不同功能且互为关联的协议。 传输层协议中两个非常重要的协议： 传输控制协议 TCP (Transmission Control Protocol)。 用户数据报协议 UDP (User Datagram Protocol)。 网络层的主要协议： 网络互联协议 IP (Internet Protocol)，其支持网间互连的数据通信。 TCP/IP 协议模型从更实用的角度出发，形成了高效的四层体系结构，即物理链路层、IP 层、传输层和应用层。 TCP 协议： 使用 TCP 协议前，须先建立 TCP 连接，形成传输数据通道。 传输前，采用 “三次握手” 方式，点对点通信，是可靠的。 TCP 协议进行通信的两个应用进程：客户端、服务端。 在连接中可进行大数据量的传输。 传输完毕，采用 “四次挥手” 方式，释放已建立的连接，效率相对低。 UDP 协议： 将数据、源、目的封装成数据包，不需要建立连接。 每个数据报的大小限制在 64 K 内。 发送不管对方是否准备好，接收方收到也不确认，故是不可靠的。 可以广播发送。 发送数据结束时无需释放资源，开销小，速度相对快。 TCP 网络编程 Socket 类实现了基于 TCP 协议的网络编程。 Socket 类： 利用套接字 (Socket) 开发网络应用程序早已被广泛的采用，以至于成为事实上的标准。 网络上具有唯一标识的 IP 地址和端口号组合在一起，才能构成唯一能识别的标识符套接字。 通信的两端都要有 Socket，是两台机器间通信的端点。 网络通信其实就是 Socket 间的通信。 Socket 允许程序把网络连接当成一个流，数据在两个 Socket 间通过 IO 传输。 一般主动发起通信的应用程序属客户端，等待通信请求的为服务端。 Socket 类： 流套接字 (stream socket)：使用 TCP 提供可依赖的字节流服务。 数据报套接字 (datagram socket)：使用 UDP 提供 “尽力而为” 的数据报服务。 Socket 类的常用构造器 ： public Socket(InetAddress address,int port)：创建一个流套接字并将其连接到指定 IP 地址的指定端口号。 public Socket(String host,int port)：创建一个流套接字并将其连接到指定主机上的指定端口号。 Socket 类的常用方法： public InputStream getInputStream()：返回此套接字的输入流。可以用于接收网络消息。 public OutputStream getOutputStream()：返回此套接字的输出流。可以用于发送网络消息。 public InetAddress getInetAddress()：返回此套接字连接到的远程 IP 地址；如果尚未连接套接字，则返回 null。 public InetAddress getLocalAddress()：返回此套接字绑定的本地地址，即本端的 IP 地址。 public int getPort()：返回此套接字连接到的远程端口号；如果尚未连接套接字，则返回 0。 public int getLocalPort()：返回此套接字绑定的本地端口，即本端的端口号。如果尚未绑定套接字，则返回 -1。 public void close()：关闭此套接字。套接字被关闭后，便不可在以后的网络连接中使用 (即无法重新连接或重新绑定)。需要创建新的套接字对象。关闭此套接字也将会关闭该套接字的 InputStream 和 OutputStream。 public void shutdownInput()：关闭此套接字的输入流。如果在套接字上调用 shutdownInput() 后再从套接字输入流读取内容，则流将返回 EOF (文件结束符)，即不能再从此套接字的输入流中接收任何数据。 public void shutdownOutput()：关闭此套接字的输出流。对于 TCP 套接字，任何以前写入的数据都将被发送，并且后跟 TCP 的正常连接终止序列。 如果在套接字上调用 shutdownOutput() 后再写入套接字输出流，则该流将抛出 IOException，即不能再通过此套接字的输出流发送任何数据。 Java 语言的基于套接字编程分为服务端编程和客户端编程，其通信模型如图所示： 服务端 Scoket 的工作过程包含以下四个基本的步骤： 调用 ServerSocket(int port)：创建一个服务器端套接字，并绑定到指定端口上。用于监听客户端的请求。 调用 accept()：监听连接请求，如果客户端请求连接，则接受连接，并返回通信套接字对象。 调用该 Socket 类对象的 getOutputStream() 和 getInputStream()：获取输出流和输入流，开始网络数据的发送和接收。 关闭 ServerSocket 和 Socket 对象：客户端访问结束，关闭通信套接字。 客户端 Socket 的工作过程包含以下四个基本的步骤： 创建 Socket：根据指定服务端的 IP 地址或端口号构造 Socket 类对象。若服务器端响应，则建立客户端到服务器的通信线路。若连接失败，会出现异常。 打开连接到 Socket 的输入/输出流：使用 getInputStream() 获得输入流，使用 getOutputStream() 获得输出流，进行数据传输。 按照一定的协议对 Socket 进行读/写操作：通过输入流读取服务器放入通信线路的信息 (但不能读取自己放入通信线路的信息)，通过输出流将信息写入通信线路。 关闭 Socket：断开客户端到服务器的连接，释放通信线路。 服务器建立 ServerSocket 对象： ServerSocket 对象负责等待客户端请求建立套接字连接，类似邮局某个窗口中的业务员。也就是说，服务器必须事先建立一个等待客户请求建立套接字连接的 ServerSocket 对象。 所谓 “接收” 客户的套接字请求，就是 accept() 会返回一个 Socket 对象。 123456789ServerSocket ss = new ServerSocket(9999);Socket s = ss.accept();InputStream in = s.getInputStream();byte[] buf = new byte[1024];int num = in.read(buf);String str = new String(buf,0,num);System.out.println(s.getInetAddress().toString()+”:”+str);s.close();ss.close(); 客户端创建 Socket 对象： 客户端程序可以使用 Socket 类创建对象，创建的同时会自动向服务器方发起连接。Socket 的构造器是： Socket(String host,int port)throws UnknownHostException,IOException：向服务器 (域名为 host，端口号为 port) 发起 TCP 连接，若成功，则创建 Socket 对象，否则抛出异常。 Socket(InetAddress address,int port)throws IOException：根据 InetAddress 对象所表示的 IP 地址以及端口号 port 发起连接。 客户端建立 socketAtClient 对象的过程就是向服务器发出套接字连接请求。 1234Socket s = new Socket(&quot;192.168.40.165&quot;,9999);OutputStream out = s.getOutputStream();out.write(&quot; hello&quot;.getBytes());s.close(); 流程示意图： 实例 1：客户端发送信息给服务端，服务端将数据显示在控制台上。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * 实现TCP的网络编程 * 实例1：客户端发送信息给服务端，服务端将数据显示在控制台上 */public class TCPTest &#123; /* 客户端 */ @Test public void client() &#123; Socket socket = null; OutputStream os = null; try &#123; // 1.创建Socket对象，指明服务器端的ip和端口号 InetAddress inet = InetAddress.getByName(&quot;127.0.0.1&quot;);// 本机 socket = new Socket(inet, 8879); // 2.获取一个输出流，用于输出数据 os = socket.getOutputStream(); // 3.写出数据的操作 os.write(&quot;你好，我是客户端&quot;.getBytes()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭资源 if (os != null) &#123; try &#123; os.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (socket != null) &#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 服务端 */ @Test public void server() &#123; ServerSocket ss = null; Socket socket = null; InputStream is = null; ByteArrayOutputStream baos = null; try &#123; // 1.创建服务器端的ServerSocket，指明自己的端口号 ss = new ServerSocket(8879); // 2.调用accept()表示接收来自于客户端的socket socket = ss.accept(); // 3.获取输入流 is = socket.getInputStream(); // 4.读取输入流中的数据 // 不建议这样写，可能会有乱码(字节流读取中文) /*byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123; String str = new String(buffer, 0, len); System.out.print(str); &#125;*/ baos = new ByteArrayOutputStream(); byte[] buffer = new byte[5]; int len; while ((len = is.read(buffer)) != -1) &#123; // 将输入流中的数据都读到ByteArrayOutputStream中，读完之后再转换 baos.write(buffer, 0, len); &#125; System.out.println(&quot;收到了来自于：&quot; + socket.getInetAddress().getHostAddress() + &quot;的数据&quot;);// 客户端信息 System.out.println(baos.toString());// 客户端发送的数据 &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 5.关闭资源 if (baos != null) &#123; try &#123; baos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (is != null) &#123; try &#123; is.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (socket != null) &#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (ss != null) &#123; try &#123; ss.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 实例 2：客户端发送文件给服务端，服务端将文件保存在本地。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * * 实现TCP的网络编程 * 实例2：客户端发送文件给服务端，服务端将文件保存在本地。 */public class TCPTest &#123; /* 客户端 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void client() throws IOException &#123; // 1.创建Socket对象，指明服务器端的ip和端口号 Socket socket = new Socket(InetAddress.getByName(&quot;127.0.0.1&quot;), 9090); // 2.获取一个输出流，用于输出数据 OutputStream os = socket.getOutputStream(); // 3.创建输入流，可以使用BufferedInputStream包装 FileInputStream fis = new FileInputStream(new File(&quot;beauty.jpg&quot;)); // 4.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = fis.read(buffer)) != -1) &#123; os.write(buffer, 0, len); &#125; // 5.关闭资源 fis.close(); os.close(); socket.close(); &#125; /* 服务端 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void server() throws IOException &#123; // 1.创建服务器端的ServerSocket，指明自己的端口号 ServerSocket ss = new ServerSocket(9090); // 2.调用accept()表示接收来自于客户端的socket Socket socket = ss.accept(); // 3.获取输入流 InputStream is = socket.getInputStream(); // 4.创建输出流，可以使用BufferedOutputStream包装 FileOutputStream fos = new FileOutputStream(new File(&quot;beauty1.jpg&quot;)); // 5.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; // 6.关闭资源 fos.close(); is.close(); socket.close(); ss.close(); &#125;&#125; 实例 3：从客户端发送文件给服务端，服务端保存到本地，然后返回 “发送成功” 给客户端，并关闭相应的连接。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * 实现TCP的网络编程 * 实例3：从客户端发送文件给服务端，服务端保存到本地，然后返回&quot;发送成功&quot;给客户端，并关闭相应的连接。 */public class TCPTest &#123; /* 客户端 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void client() throws IOException &#123; // 1.创建Socket对象，指明服务器端的ip和端口号 Socket socket = new Socket(InetAddress.getByName(&quot;127.0.0.1&quot;), 9090); // 2.获取一个输出流，用于输出数据 OutputStream os = socket.getOutputStream(); // 3.创建输入流，可以使用BufferedInputStream包装 FileInputStream fis = new FileInputStream(new File(&quot;beauty.jpg&quot;)); // 4.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = fis.read(buffer)) != -1) &#123; os.write(buffer, 0, len); &#125; // 关闭数据的输出，表示客服端数据传输已经完成，提醒服务端不必继续等待 // 如果不执行此操作，服务器端会一直阻塞 socket.shutdownOutput(); // 5.接收来自于服务器端的数据，并显示到控制台上 InputStream is = socket.getInputStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte[] bufferr = new byte[20]; int len1; while ((len1 = is.read(buffer)) != -1) &#123; baos.write(buffer, 0, len1); &#125; System.out.println(baos.toString()); // 6.关闭资源 baos.close(); fis.close(); os.close(); socket.close(); &#125; /* 服务端 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void server() throws IOException &#123; // 1.创建服务器端的ServerSocket，指明自己的端口号 ServerSocket ss = new ServerSocket(9090); // 2.调用accept()表示接收来自于客户端的socket Socket socket = ss.accept(); // 3.获取输入流 InputStream is = socket.getInputStream(); // 4.创建输出流，可以使用BufferedOutputStream包装 FileOutputStream fos = new FileOutputStream(new File(&quot;beauty1.jpg&quot;)); // 5.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123;// read()是一个阻塞式方法 fos.write(buffer, 0, len); &#125; System.out.println(&quot;图片传输完成&quot;); // 6.服务器端给予客户端反馈 OutputStream os = socket.getOutputStream(); os.write(&quot;你好，客户端，照片已收到！&quot;.getBytes()); // 7.关闭资源 os.close(); fos.close(); is.close(); socket.close(); ss.close(); &#125;&#125; UDP 网络编程 DatagramSocket 类和 DatagramPacket 类实现了基于 UDP 协议的网络编程。 UDP 数据报通过数据报套接字 DatagramSocket 发送和接收，系统不保证 UDP 数据报一定能够安全送到目的地，也不能确定什么时候可以抵达。 DatagramPacket 对象封装了 UDP 数据报，在数据报中包含了发送端的 IP 地址和端口号以及接收端的 IP 地址和端口号。 UDP 协议中每个数据报都给出了完整的地址信息，因此无须建立发送方和接收方的连接。如同发快递包裹一样。 DatagramSocket 类的常用方法： public DatagramSocket(int port)：创建数据报套接字并将其绑定到本地主机上的指定端口。套接字将被绑定到通配符地址，IP 地址由内核来选择。 public DatagramSocket(int port,InetAddress laddr)：创建数据报套接字，将其绑定到指定的本地地址。本地端口必须在 0 到 65535 之间 (包括两者)。如果 IP 地址为 0.0.0.0，套接字将被绑定到通配符地址，IP 地址由内核选择。 public void close()：关闭此数据报套接字。 public void send(DatagramPacket p)：从此套接字发送数据报包。DatagramPacket 包含的信息指示：将要发送的数据、数据长度、远程主机的 IP 地址和远程主机的端口号。 public void receive(DatagramPacket p)：从此套接字接收数据报包。当此方法返回时，DatagramPacket 的缓冲区填充了接收的数据。数据报包也包含发送方的 IP 地址和发送方机器上的端口号。此方法在接收到数据报前一直阻塞。数据报包对象的 length 字段包含所接收信息的长度。如果信息比包的长度长，该信息将被截短。 public InetAddress getLocalAddress()：获取套接字绑定的本地地址。 public int getLocalPort()：返回此套接字绑定的本地主机上的端口号。 public InetAddress getInetAddress()：返回此套接字连接的地址。如果套接字未连接，则返回 null。 public int getPort()：返回此套接字的端口。如果套接字未连接，则返回 -1。 DatagramPacket 类的常用方法： public DatagramPacket(byte[] buf,int length)：构造 DatagramPacket，用来接收长度为 length 的数据包。 length 参数必须小于等于 buf.length()。 public DatagramPacket(byte[] buf,int length,InetAddress address,int port)：构造数据报包，用来将长度为 length 的包发送到指定主机上的指定端口号。length 参数必须小于等于 buf.length()。 public InetAddress getAddress()：返回某台机器的 IP 地址，此数据报将要发往该机器或者是从该机器接收到的。 public int getPort()：返回某台远程主机的端口号，此数据报将要发往该主机或者是从该主机接收到的。 public byte[] getData()：返回数据缓冲区。接收到的或将要发送的数据从缓冲区中的偏移量 offset 处开始，持续 length 长度。 public int getLength()：返回将要发送或接收到的数据的长度。 UDP 网络通信流程： DatagramSocket 与 DatagramPacket。 建立发送端，接收端， 发送端与接收端是两个独立的运行程序。 建立数据包。 调用 Socket 的发送、接收方法。 关闭 Socket。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class UDPTest &#123; /* 发送端 注意：发送端发送数据，是不管接收端能不能收到，为了保证接收端能收到数据，应该先启动接收端。 */ @Test public void sender() &#123; DatagramSocket socket = null; try &#123; socket = new DatagramSocket(); String str = &quot;我是UDP方式发送的数据&quot;; byte[] data = str.getBytes(); InetAddress inet = InetAddress.getLocalHost(); // 封装数据报，发送到本机的9090端口 DatagramPacket packet = new DatagramPacket(data, 0, data.length, inet, 9090); socket.send(packet); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (socket != null) &#123; socket.close(); &#125; &#125; &#125; /* 接收端 注意：在接收端，要指定监听的端口。 */ @Test public void receiver() &#123; DatagramSocket socket = null; try &#123; socket = new DatagramSocket(9090); byte[] buffer = new byte[100]; DatagramPacket packet = new DatagramPacket(buffer, 0, buffer.length); socket.receive(packet); System.out.println(new String(packet.getData(), 0, packet.getLength())); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (socket != null) &#123; socket.close(); &#125; &#125; &#125;&#125; URL 网络编程 URL (Uniform Resource Locator)：统一资源定位符，它表示 Internet 上某一资源的地址。 它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。 通过 URL 我们可以访问 Internet 上的各种网络资源，比如最常见的 www，ftp 站点。浏览器通过解析给定的 URL 可以在网络上查找相应的文件或其他资源。 URL 的基本结构由 5 部分组成：**&lt;传输协议&gt;://&lt; 主机名&gt;:&lt; 端口号&gt;/&lt; 文件名&gt;#片段名?参数列表**。 例如：http://192.168.1.100:8080/helloworld/index.jsp#a?username=shkstart&amp;password=123 #片段名：即锚点，例如看小说，直接定位到章节 参数列表格式：参数名=参数值&amp;参数名=参数值…. 为了表示 URL，java.net 中实现了类 URL。我们可以通过下面的构造器来初始化一个 URL 对象： **public URL (String spec)**：通过一个表示 URL 地址的字符串可以构造一个 URL 对象。例如：URL url = new URL(&quot;http://www. atguigu.com/&quot;);。 **public URL(URL context, String spec)**：通过基 URL 和相对 URL 构造一个 URL 对象。例如：URL downloadUrl = new URL(url, &quot;download.html&quot;);。 public URL(String protocol, String host, String file)：例如：new URL(&quot;http&quot;,&quot;www.atguigu.com&quot;, “download. html&quot;);。 public URL(String protocol, String host, int port, String file)：例如：URL gamelan = new URL(&quot;http&quot;, &quot;www.atguigu.com&quot;, 80, “download.html&quot;);。 URL 类的构造器都声明抛出非运行时异常，必须要对这一异常进行处理，通常使用 try - catch 语句进行捕获。 一个 URL 对象生成后，其属性是不能被改变的，但可以通过它给定的方法来获取这些属性： public String getProtocol()：获取该 URL 的协议名 public String getHost()：获取该 URL 的主机名。 public String getPort()：获取该 URL 的端口号。 public String getPath()：获取该 URL 的文件路径。 public String getFile()：获取该 URL 的文件名。 public String getQuery()：获取该 URL 的查询名。 实例： 1234567891011121314151617181920212223242526272829/** * URL网络编程 * 1.URL：统一资源定位符，对应着互联网的某一资源地址 * 2.格式： * http://localhost:8080/examples/beauty.jpg?username=Tom * 协议 主机名 端口号 资源地址 参数列表 */public class URLTest &#123; public static void main(String[] args) &#123; try &#123; URL url = new URL(&quot;http://localhost:8080/examples/beauty.jpg?username=Tom&quot;); // public String getProtocol(): 获取该URL的协议名 System.out.println(url.getProtocol());// http // public String getHost(): 获取该URL的主机名 System.out.println(url.getHost());// localhost // public String getPort(): 获取该URL的端口号 System.out.println(url.getPort());// 8080 // public String getPath(): 获取该URL的文件路径 System.out.println(url.getPath());// /examples/beauty.jpg // public String getFile(): 获取该URL的文件名 System.out.println(url.getFile());// /examples/beauty.jpg?username=Tom // public String getQuery(): 获取该URL的查询名 System.out.println(url.getQuery());// username=Tom &#125; catch (MalformedURLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; URL 的方法 openStream()：能从网络上读取数据。 若希望输出数据，例如向服务器端的 CGI (公共网关接口 Common Gateway Interface 的简称，是用户浏览器和服务器端的应用程序进行连接的接口) 程序发送一些数据，则必须先与 URL 建立连接，然后才能对其进行读写，此时需要使用 URLConnection 类。 URLConnection：表示到 URL 所引用的远程对象的连接。当与一个 URL 建立连接时，首先要在一个 URL 对象上通过方法 openConnection() 生成对应的 URLConnection 对象。如果连接过程失败，将产生 IOException。比如： 12URL netchinaren = new URL (&quot;http://www.atguigu.com/index.shtml&quot;);URLConnectonn u = netchinaren.openConnection(); 通过 URLConnection 对象获取的输入流和输出流，即可以与现有的 CGI 程序进行交互。 public Object getContent() throws IOException public int getContentLength() public String getContentType() public long getDate() public long getLastModified() public InputStream getInputStream()throws IOException public OutputSteram getOutputStream()throws IOException 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class URLTest &#123; public static void main(String[] args) &#123; HttpURLConnection urlConnection = null; InputStream is = null; FileOutputStream fos = null; try &#123; URL url = new URL(&quot;http://localhost:8080/examples/beauty.jpg&quot;); urlConnection = (HttpURLConnection) url.openConnection(); urlConnection.connect(); is = urlConnection.getInputStream(); fos = new FileOutputStream(&quot;day10\\\\beauty3.jpg&quot;); byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; System.out.println(&quot;下载完成&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭资源 if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (is != null) &#123; try &#123; is.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (urlConnection != null) &#123; urlConnection.disconnect(); &#125; &#125; &#125;&#125; URI 、URL 和URN的区别： URI，是 uniform resource identifier，统一资源标识符，用来唯一的标识一个资源。而 URL 是 uniform resource locator，统一资源定位符，它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。而 URN，是 uniform resource name，统一资源命名，是通过名字来标识资源，比如 mailto:&#x6a;&#97;&#x76;&#x61;&#x2d;&#x6e;&#101;&#116;&#x40;&#x6a;&#x61;&#118;&#97;&#x2e;&#115;&#117;&#110;&#46;&#x63;&#x6f;&#109;。也就是说，URI 是以一种抽象的，高层次概念定义统一资源标识，而 URL 和 URN 则是具体的资源标识的方式。URL 和 URN 本身也都是一种 URI。 在 Java 的 URI 中，一个 URI 实例可以代表绝对的，也可以是相对的，只要它符合 URI 的语法规则。而 URL 类则不仅符合语义，还包含了定位该资源的信息，因此它不能是相对的。 总结 位于网络中的计算机具有唯一的 IP 地址，这样不同的主机可以互相区分。 客户端－服务器是一种最常见的网络应用程序模型。服务器是一个为其客户端提供某种特定服务的硬件或软件。客户机是一个用户应用程序，用于访问某台服务器提供的服务。端口号是对一个服务的访问场所，它用于区分同一物理计算机上的多个服务。套接字用于连接客户端和服务器，客户端和服务器之间的每个通信会话使用一个不同的套接字。TCP 协议用于实现面向连接的会话。 Java 中有关网络方面的功能都定义在 java.net 程序包中。Java 用 InetAddress 对象表示 IP 地址，该对象里有两个字段：主机名 (String) 和 IP 地址 (int)。 类 Socket 和 ServerSocket 实现了基于 TCP 协议的客户端－服务器程序。Socket 是客户端和服务器之间的一个连接，连接创建的细节被隐藏了。这个连接提供了一个安全的数据传输通道，这是因为 TCP 协议可以解决数据在传送过程中的丢失、损坏、重复、乱序以及网络拥挤等问题，它保证数据可靠的传送。 类 URL 和 URLConnection 提供了最高级网络应用。URL 的网络资源的位置来统一标识 Internet 上各种网络资源。通过 URL 对象可以创建当前应用程序和 URL 表示的网络资源之间的连接，这样当前程序就可以读取网络资源数据，或者把自己的数据传送到网络上去。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的泛型","slug":"java-generic","date":"2021-03-26T09:16:16.000Z","updated":"2021-04-09T08:00:45.251Z","comments":true,"path":"2021/03/26/java-generic/","link":"","permalink":"http://example.com/2021/03/26/java-generic/","excerpt":"","text":"泛型的设计背景集合容器类在设计阶段/声明阶段不能确定这个容器到底实际存的是什么类型的对象，所以在 JDK 1.5 之前只能把元素类型设计为 Object，JDK 1.5 之后使用泛型来解决。因为这个时候除了元素的类型不确定，其他的部分是确定的，例如关于这个元素如何保存，如何管理等是确定的，因此此时把元素的类型设计成一个参数，这个类型参数叫做泛型。Collection&lt;E&gt;，List&lt;E&gt;，ArrayList&lt;E&gt; 中的这个 &lt;E&gt; 就是类型参数，即泛型。 泛型的概念 所谓泛型，就是允许在定义类、接口时通过一个标识表示类中某个属性的类型或者是某个方法的返回值及参数类型。这个类型参数将在使用时（例如，继承或实现这个接口，用这个类型声明变量、创建对象时）确定（即传入实际的类型参数，也称为类型实参）。 从 JDK 1.5 以后，Java 引入了 “参数化类型” (Parameterized type) 的概念，允许在创建集合时指定集合元素的类型，如：List&lt;String&gt;，表明该 List 只能保存字符串类型的对象。 JDK 1.5 改写了集合框架中的全部接口和类，为这些接口、类增加了泛型支持，从而可以在声明集合变量、创建集合对象时传入类型实参。 在实例化集合类时，可以指明具体的泛型类型。指明完以后，在集合类或接口中凡是定义类或接口时，内部结构 (比如：方法、构造器、属性等) 使用到类的泛型的位置，都指定为实例化的泛型类型。比如：add(E e) —&gt; 实例化以后：add(Integer e)。 在实例化集合类时，如果没有指明泛型的类型，默认类型为 java.lang.Object 类型。 泛型的类型必须是类，不能是基本数据类型。需要用到基本数据类型的位置，拿包装类替换。 使用泛型的必要性： 解决元素存储的安全性问题。 解决获取数据元素时，需要类型强制转换的问题。 Java 泛型可以保证如果程序在编译时没有发出警告，运行时就不会产生 ClassCastException 异常。同时，代码更加简洁、健壮。 使用泛型的主要优点是能够在编译时而不是在运行时检测错误 在集合中使用泛型之前的情况： 12345678910111213141516171819 public class Test &#123; public static void main(String[] args) &#123; // 在集合中使用泛型之前的情况： ArrayList list = new ArrayList(); // 需求：存放学生的成绩 list.add(78); list.add(76); list.add(89); list.add(88); // 问题一：类型不安全 // list.add(&quot;Tom&quot;); for (Object score : list) &#123; // 问题二：强转时，可能出现ClassCastException int stuScore = (Integer) score; System.out.println(stuScore); &#125; &#125;&#125; 在集合中使用泛型的情况，以 ArrayList 为例： 123456789101112131415161718192021222324252627 public class Test &#123; public static void main(String[] args) &#123; // ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); // jdk7新特性：类型推断 ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(78); list.add(87); list.add(99); list.add(65); // 编译时，就会进行类型检查，类型不一致时编译不通过，保证数据的安全 // list1.add(&quot;Tom&quot;); // 方式一： for (Integer score : list) &#123; // 避免了强转操作 int stuScore = score; System.out.println(stuScore); &#125; // 方式二： Iterator&lt;Integer&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; int stuScore = iterator.next(); System.out.println(stuScore); &#125; &#125;&#125; 在集合中使用泛型的情况，以 HashMap 为例： 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; // Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); // jdk7新特性：类型推断 Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;Tom&quot;, 87); map.put(&quot;Jerry&quot;, 87); map.put(&quot;Jack&quot;, 67); // map.put(123,&quot;ABC&quot;); // 泛型的嵌套 Set&lt;Map.Entry&lt;String, Integer&gt;&gt; entries = map.entrySet(); Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; iterator = entries.iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; entry = iterator.next(); String key = entry.getKey(); Integer value = entry.getValue(); System.out.println(key + &quot;----&quot; + value); &#125; &#125;&#125; 自定义泛型结构 自定义泛型类和接口： 泛型类和接口的声明：class GenericClass&lt;K, V&gt; 和 interface GenericInterface&lt;T&gt;。其中，K，V，T 不代表值，而是表示类型，可以使用任意字母，常用 T 表示，是 Type 的缩写。 123456789101112131415161718192021public class Person&lt;T&gt; &#123; // 使用T类型定义变量 private T info; // 使用T类型定义一般方法 public T getInfo() &#123; return info; &#125; public void setInfo(T info) &#123; this.info = info; &#125; // 使用T类型定义构造器 public Person() &#123; &#125; public Person(T info) &#123; this.info = info; &#125;&#125; 泛型类和接口可能有多个参数，此时应将多个参数一起放在尖括号内，以逗号隔开。比如：&lt;E1, E2, E3&gt;。 泛型类的构造器如下：public GenericClass()&#123;&#125;。而下面是错误的：public GenericClass&lt;E&gt;()&#123;&#125;。 泛型类的实例化：如果定义的类是带泛型的，在实例化时应该指明类的泛型。如：List&lt;String&gt; strList = new ArrayList&lt;String&gt;();。 泛型如果不指定，将被擦除，泛型对应的类型均按照 Object 处理，但不等价于 Object。经验：泛型要使用一路都用。要不用，一路都不要用。 指定泛型时，不能使用基本数据类型，可以使用包装类替换。 把一个集合中的内容限制为一个特定的数据类型，这就是 Generic 背后的核心思想。 泛型类实例化后，操作原来泛型位置的结构必须与指定的泛型类型一致。 泛型不同的引用不能相互赋值。 尽管在编译时 ArrayList&lt;String&gt; 和 ArrayList&lt;Integer&gt; 是两种类型，但是，在运行时只有一个 ArrayList 被加载到 JVM 中。 如果泛型结构是一个接口或抽象类，则不可创建泛型类的对象。 JDK 7.0 开始，泛型的简化操作：ArrayList&lt;Fruit&gt; flist = new ArrayList&lt;&gt;();，类型推断。 在类/接口上声明的泛型，在本类或本接口中即代表某种类型，可以作为非静态属性的类型、非静态方法的参数类型、非静态方法的返回值类型。但在静态方法中不能使用类的泛型。 1234567891011121314151617public class Order&lt;T&gt; &#123; String orderName; int orderId; // 类的内部结构就可以使用类的泛型 T orderT; public Order(String orderName, int orderId, T orderT) &#123; this.orderName = orderName; this.orderId = orderId; this.orderT = orderT; &#125; // 静态方法中不能使用类的泛型，编译不通过 /*public static void show(T orderT) &#123; System.out.println(orderT); &#125;*/&#125; 异常类不能声明为泛型类。 12// 异常类不能声明为泛型类，编译不通过public class MyException&lt;T&gt; extends Exception &#123;&#125; 12345678public class Order&lt;T&gt; &#123; public void show() &#123; // try-catch结构中不能使用类的泛型，编译不通过 try &#123; &#125; catch (T t) &#123; &#125; &#125;&#125; 不能使用 new E[]，但是可以：E[] elements = (E[])new Object[capacity];。参考 ArrayList 源码中声明：Object[] elementData，而非泛型参数类型数组。 12345678public class Order&lt;T&gt; &#123; public Order() &#123; // 编译不通过 // T[] arr = new T[10]; // 编译通过 T[] arr = (T[]) new Object[10]; &#125;&#125; 父类有泛型，子类可以选择保留泛型也可以选择指定泛型类型： 子类不保留父类的泛型：按需实现。 没有类型 擦除。 具体类型。 子类保留父类的泛型：泛型子类。 全部保留。 部分保留。 子类除了指定或保留父类的泛型，还可以增加自己的泛型。 实例： 子类不增加自己的泛型： 123456789101112131415class Father&lt;T1, T2&gt; &#123;&#125;// 子类不保留父类的泛型：// 1)没有类型 擦除class Son1 extends Father &#123;&#125;// 等价于class Son1 extends Father&lt;Object, Object&gt;// 2)具体类型class Son2 extends Father&lt;Integer, String&gt; &#123;&#125;// 子类保留父类的泛型：// 1)全部保留class Son3&lt;T1, T2&gt; extends Father&lt;T1, T2&gt; &#123;&#125;// 2)部分保留class Son4&lt;T2&gt; extends Father&lt;Integer, T2&gt; &#123;&#125; 子类增加自己的泛型： 123456789101112131415class Father&lt;T1, T2&gt; &#123;&#125;// 子类不保留父类的泛型：// 1)没有类型 擦除class Son1&lt;A, B&gt; extends Father &#123;&#125;//等价于class Son extends Father&lt;Object, Object&gt;// 2)具体类型class Son2&lt;A, B&gt; extends Father&lt;Integer, String&gt; &#123;&#125; // 子类保留父类的泛型// 1)全部保留class Son3&lt;T1, T2, A, B&gt; extends Father&lt;T1, T2&gt; &#123;&#125;// 2)部分保留class Son4&lt;T2, A, B&gt; extends Father&lt;Integer, T2&gt; &#123;&#125; 如果子类在继承带泛型的父类时，指明了泛型类型，则实例化子类对象时，不再需要指明泛型 123public class Order&lt;T&gt; &#123;&#125;public class SubOrder extends Order&lt;Integer&gt; &#123;&#125;// SubOrder: 不是泛型类 如果子类在继承带泛型的父类时，未指明泛型类型，则实例化子类对象时，仍然需要指明泛型。 123public class Order&lt;T&gt; &#123;&#125;public class SubOrder1&lt;T&gt; extends Order&lt;T&gt; &#123;&#125;// SubOrder1&lt;T&gt;: 仍然是泛型类 自定义泛型方法 泛型方法的格式： 泛型方法的参数与类的泛型参数没有任何关系， 换句话说，泛型方法所属的类是不是泛型类都没有关系。泛型方法，可以声明为静态的。原因：泛型参数是在调用方法时确定的，并非在实例化类时确定。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class Order&lt;T&gt; &#123; String orderName; int orderId; // 类的内部结构就可以使用类的泛型 T orderT; // 如下的三个方法都不是泛型方法 public T getOrderT() &#123; return orderT; &#125; public void setOrderT(T orderT) &#123; this.orderT = orderT; &#125; @Override public String toString() &#123; return &quot;Order&#123;&quot; + &quot;orderName=&#x27;&quot; + orderName + &#x27;\\&#x27;&#x27; + &quot;, orderId=&quot; + orderId + &quot;, orderT=&quot; + orderT + &#x27;&#125;&#x27;; &#125; // 泛型方法：在方法中出现了泛型的结构，泛型参数与类的泛型参数没有任何关系。 // 换句话说，泛型方法所属的类是不是泛型类都没有关系。 public &lt;E&gt; List&lt;E&gt; copyFromArrayToList(E[] arr) &#123; ArrayList&lt;E&gt; list = new ArrayList&lt;&gt;(); list.addAll(Arrays.asList(arr)); return list; &#125; // 泛型方法，可以声明为静态的。原因：泛型参数是在调用方法时确定的。并非在实例化类时确定。 public static &lt;T&gt; void fromArrayToCollection(T[] a, Collection&lt;T&gt; c) &#123; Collections.addAll(c, a); System.out.println(c); &#125; public static void main(String[] args) &#123; Order&lt;String&gt; order = new Order&lt;&gt;(); Integer[] arr = new Integer[]&#123;1, 2, 3, 4&#125;; // 泛型方法在调用时，指明泛型参数的类型 List&lt;Integer&gt; list = order.copyFromArrayToList(arr); System.out.println(list);// [1, 2, 3, 4] ArrayList&lt;String&gt; str = new ArrayList&lt;&gt;(); String[] strings = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;&#125;; fromArrayToCollection(strings, str);// [A, B, C, D] Object[] ao = new Object[100]; Collection&lt;Object&gt; co = new ArrayList&lt;&gt;(); fromArrayToCollection(ao, co); String[] sa = new String[20]; Collection&lt;String&gt; cs = new ArrayList&lt;&gt;(); fromArrayToCollection(sa, cs); Collection&lt;Double&gt; cd = new ArrayList&lt;&gt;(); // 下面代码中T是Double类，但sa是String类型，编译错误。 // fromArrayToCollection(sa, cd); // 下面代码中T是Object类型，sa是String类型，可以赋值成功。 fromArrayToCollection(sa, co); &#125;&#125; 泛型方法声明泛型时也可以指定上限： 父类： 1234567891011121314151617181920212223242526272829public class DAO&lt;T&gt; &#123;// 不同表的共性操作的DAO，DAO：data(base) access object // 添加一条记录 public void add(T t) &#123;&#125; // 删除一条记录 public boolean remove(int index) &#123; return false; &#125; // 修改一条记录 public void update(int index, T t) &#123;&#125; // 查询一条记录 public T getIndex(int index) &#123; return null; &#125; // 查询多条记录 public List&lt;T&gt; getForList(int index) &#123; return null; &#125; // 泛型方法：因为返回内容在DAO类中无法确定，由子类自己指定 // 举例：获取表中一共有多少条记录？获取最大的员工入职时间？ public &lt;E&gt; E getValue() &#123; return null; &#125;&#125; 子类 StudentDao： 1public class StudentDAO extends DAO&lt;Student&gt; &#123;&#125;// 只能操作Student表的DAO 子类 CustomerDao： 1public class CustomerDAO extends DAO&lt;Customer&gt;&#123;&#125;// 只能操作Customer表的DAO 泛型在继承上的体现 如果 B 是 A 的一个子类型 (子类或者子接口)，而 G 是具有泛型声明的类或接口，则 G&lt;B&gt; 并不是 G&lt;A&gt; 的子类型，二者是并列关系。如果类 A 是类 B 的父类，则 A&lt;G&gt; 是 B&lt;G&gt; 的父类。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test &#123; public static void show(List&lt;Object&gt; list) &#123; &#125; public static void show1(List&lt;String&gt; list) &#123; &#125; public static void main(String[] args) &#123; // 子类对象赋值给父类对象---&gt;编译通过 Object obj = null; String str = null; obj = str;// Object是String的父类 // 子类对象数组赋值给父类对象数组---&gt;编译通过 Object[] arr1 = null; String[] arr2 = null; arr1 = arr2;// Object[]是String[]的父类 // 虽然类A是类B的父类，但是G&lt;A&gt;和G&lt;B&gt;二者不具备子父类关系，二者是并列关系 List&lt;Object&gt; list1 = null; List&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); // 编译不通过此时的list1和list2的类型不具有子父类关系 // list1 = list2; /* 反证法： 假设list1 = list2; list1.add(123);导致混入非String的数据。出错。 */ show(list1); show1(list2); // 补充：类A是类B的父类，则A&lt;G&gt;是B&lt;G&gt;的父类 List&lt;String&gt; list3 = null; AbstractList&lt;String&gt; list4 = null; ArrayList&lt;String&gt; list5 = null; list3 = list5; list4 = list5; &#125;&#125; 通配符的使用 通配符：？。 如果类 A 是类 B 的父类，G&lt;A&gt; 和 G&lt;B&gt; 是没有关系的，二者共同的父类是：G&lt;?&gt;。比如：List&lt;?&gt;，Map&lt;?, ?&gt;。其中，List&lt;?&gt; 是 List&lt;String&gt;、List&lt;Object&gt; 等各种泛型 List 的父类，Map&lt;?, ?&gt; 是各种泛型 Map 的父类。 12345678910111213141516171819202122public class Test &#123; public static void print(List&lt;?&gt; list) &#123; Iterator&lt;?&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; Object obj = iterator.next(); System.out.println(obj); &#125; &#125; public static void main(String[] args) &#123; List&lt;?&gt; list; List&lt;Object&gt; list1 = null; List&lt;String&gt; list2 = null; list = list1; list = list2; // 编译通过 print(list1); print(list2); &#125;&#125; 对于 List&lt;?&gt;，不能向其内部添加元素，因为不知道 List 中存储的元素的类型。 唯一的例外是 null，它是所有类型的成员。 读取 List&lt;?&gt; 中的元素时，永远是安全的，因为不管 List 中元素的真实类型是什么，都是一个 Object。 实例： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) &#123; List&lt;?&gt; list; List&lt;String&gt; list1 = new ArrayList&lt;&gt;(); list1.add(&quot;AA&quot;); list1.add(&quot;BB&quot;); list1.add(&quot;CC&quot;); list = list1; // 添加(写入)：对于List&lt;?&gt;不能向其内部添加元素，因为不知道List中存储的元素的类型 // 除了添加null之外，其他的都无法添加，编译不通过 // list.add(&quot;DD&quot;); // list.add(&#x27;?&#x27;); list.add(null); // 获取(读取)：允许读取List&lt;?&gt;元素，因为读取的元素，不论其类型为什么，其父类都是Object Object o = list.get(0); System.out.println(o); &#125;&#125; 通配符使用的注意事项： 有限制的通配符： 通配符指定上限 extends：使用时指定的类型必须是继承某个类，或者实现某个接口，即 &lt;=。 ? extends A：G&lt;? extends A&gt; 可以作为 G&lt;A&gt; 和 G&lt;B&gt; 的父类，其中 B 是 A 的子类。 通配符指定下限 super：使用时指定的类型不能小于操作的类，即 &gt;=。 ? super A：G&lt;? super A&gt; 可以作为 G&lt;A&gt; 和 G&lt;B&gt; 的父类，其中 B 是 A 的父类。 实例： 1234567891011121314151617181920public class Test &#123; public static void printCollection3(Collection&lt;? extends Person&gt; coll) &#123; // Iterator只能用Iterator&lt;?&gt;或Iterator&lt;? extends Person&gt;.why? Iterator&lt;?&gt; iterator = coll.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125; public static void printCollection4(Collection&lt;? super Person&gt; coll) &#123; // Iterator只能用Iterator&lt;?&gt;或Iterator&lt;? super Person&gt;.why? Iterator&lt;?&gt; iterator = coll.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125;class Person &#123;&#125; 读取和添加元素： 1public class Person &#123;&#125; 1public class Student extends Person &#123;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041public class Test &#123; public static void main(String[] args) &#123; List&lt;? extends Person&gt; list1; List&lt;? super Person&gt; list2; List&lt;Student&gt; list3 = new ArrayList&lt;&gt;(); List&lt;Person&gt; list4 = new ArrayList&lt;&gt;(); List&lt;Object&gt; list5 = new ArrayList&lt;&gt;(); list1 = list3; list1 = list4; // 编译不通过，因为Object &gt; Person // list1 = list5; // 编译不通过，因为Student &lt; Person // list2 = list3; list2 = list4; list2 = list5; // 读取数据：读出的元素定义为最大的类型 list1 = list3; Person p = list1.get(0); // 编译不通过，因为读出来的元素不一定是Student，也可能是Student的父类，但肯定是Person的子类 // Student s = list1.get(0); list2 = list4; Object obj = list2.get(0); // 编译不通过，因为读出来的元素不一定是Person，也可能是Person的父类，但肯定是Object的子类 // Person obj = list2.get(0); // 写入数据： // 编译不通过，因为list1是(-∞,Person]，添加的元素，可能其类型比Person或Student小，因此除了null都不能添加 // list1.add(new Person()); // list1.add(new Student()); list1.add(null); // 编译通过，因为list2是[Person,+∞)，无论是什么元素，都肯定是Person或其父类，那么Person及其子类都能添加 list2.add(new Person()); list2.add(new Student()); &#125;&#125; 泛型嵌套： 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; HashMap&lt;String, ArrayList&lt;Person&gt;&gt; map = new HashMap&lt;&gt;(); ArrayList&lt;Person&gt; list = new ArrayList&lt;Person&gt;(); list.add(new Person(&quot;AA&quot;)); list.add(new Person(&quot;BB&quot;)); list.add(new Person(&quot;ab&quot;)); map.put(&quot;AA&quot;, list); Set&lt;Map.Entry&lt;String, ArrayList&lt;Person&gt;&gt;&gt; entrySet = map.entrySet(); Iterator&lt;Map.Entry&lt;String, ArrayList&lt;Person&gt;&gt;&gt; iterator = entrySet.iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;String, ArrayList&lt;Person&gt;&gt; entry = iterator.next(); String key = entry.getKey(); ArrayList&lt;Person&gt; value = entry.getValue(); System.out.println(&quot;户主：&quot; + key); System.out.println(&quot;家庭成员：&quot; + value); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123// 只有此接口的子类才是表示人的信息interface Info &#123;&#125;// 表示联系方式class Contact implements Info &#123; private String address;// 联系地址 private String telephone;// 联系方式 private String zipcode;// 邮政编码 public Contact(String address, String telephone, String zipcode) &#123; this.address = address; this.telephone = telephone; this.zipcode = zipcode; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public void setTelephone(String telephone) &#123; this.telephone = telephone; &#125; public void setZipcode(String zipcode) &#123; this.zipcode = zipcode; &#125; public String getAddress() &#123; return this.address; &#125; public String getTelephone() &#123; return this.telephone; &#125; public String getZipcode() &#123; return this.zipcode; &#125; @Override public String toString() &#123; return &quot;Contact [address=&quot; + address + &quot;, telephone=&quot; + telephone + &quot;, zipcode=&quot; + zipcode + &quot;]&quot;; &#125;&#125;// 表示个人信息class Introduction implements Info &#123; private String name;// 姓名 private String sex;// 性别 private int age;// 年龄 public Introduction(String name, String sex, int age) &#123; this.name = name; this.sex = sex; this.age = age; &#125; public void setName(String name) &#123; this.name = name; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return this.name; &#125; public String getSex() &#123; return this.sex; &#125; public int getAge() &#123; return this.age; &#125; @Override public String toString() &#123; return &quot;Introduction [name=&quot; + name + &quot;, sex=&quot; + sex + &quot;, age=&quot; + age + &quot;]&quot;; &#125;&#125;class Person&lt;T extends Info&gt; &#123; private T info; // 通过构造器设置信息属性内容 public Person(T info) &#123; this.info = info; &#125; public void setInfo(T info) &#123; this.info = info; &#125; public T getInfo() &#123; return info; &#125; @Override public String toString() &#123; return &quot;Person [info=&quot; + info + &quot;]&quot;; &#125;&#125;public class GenericPerson &#123; public static void main(String args[]) &#123; Person&lt;Contact&gt; per = null;// 声明Person对象 per = new Person&lt;Contact&gt;(new Contact(&quot;北京市&quot;, &quot;01088888888&quot;, &quot;102206&quot;)); System.out.println(per); Person&lt;Introduction&gt; per2 = null;// 声明Person对象 per2 = new Person&lt;Introduction&gt;(new Introduction(&quot;李雷&quot;, &quot;男&quot;, 24)); System.out.println(per2); &#125;&#125; 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的集合","slug":"java-collectionandmap","date":"2021-03-20T06:51:43.000Z","updated":"2021-04-09T08:00:04.931Z","comments":true,"path":"2021/03/20/java-collectionandmap/","link":"","permalink":"http://example.com/2021/03/20/java-collectionandmap/","excerpt":"","text":"java 集合框架概述 面向对象语言是以对象的形式来对事物进行体现，为了方便对多个对象的操作，就需要对对象进行存储。 在 java 语言中，数组 (Array) 和集合都是对多个数据进行存储操作的结构，简称 java 容器。此时的存储，主要指的是内存层面的存储，不涉及到持久化的存储。 数组在内存存储方面的特点： 数组一旦初始化以后，其长度就确定了。 数组一旦定义好，其元素的类型也就确定了。 数组在存储数据方面的弊端： 数组一旦初始化以后，其长度就不可修改，不便于扩展。 数组中提供的属性和方法少，不便于进行添加、删除、插入等操作，且效率不高。 数组中没有现成的属性和方法，去直接获取数组中已存储的元素的个数 (只能直接知道数组的长度)。 数组存储的数据是有序的、可重复的。对于无序、不可重复的需求，不能满足，即数组存储数据的特点比较单一。 java 集合类可以用于存储数量不等的多个对象，还可用于保存具有映射关系的关联数组。 java 集合框架可分为 Collection 和 Map 两种体系： Collection 接口 ：单列集合，用来存储一个一个的对象CD 。 List 接口：存储有序的、可重复的数据。—&gt; “动态” 数组 ArrayList、LinkedList、Vector Set 接口：存储无序的、不可重复的数据。—&gt; 高中 “集合” HashSet、LinkedHashSet、TreeSet Map 接口：双列集合，用来存储具有映射关系 “key - value 对” 的数据。—&gt; 高中 “函数” HashMap、LinkedHashMap、TreeMap、Hashtable、Properties Collection 接口继承树： Map 接口继承树： Collection 接口 Collection 接口是 List、Set 和 Queue 接口的父接口，该接口里定义的方法既可用于操作 Set 集合，也可用于操作 List 和 Queue 集合。 jdk 不提供此接口的任何直接实现，而是提供更具体的子接口实现，如：Set 和 List。 在 jdk 5.0 之前，java 集合会丢失容器中所有对象的数据类型，把所有对象都当成 Object 类型处理；从 jdk 5.0 增加了泛型以后，java 集合可以记住容器中对象的数据类型。 Collection 接口的方法： 添加元素： add(Object obj) addAll(Collection coll) 获取有效元素的个数： int size() 清空集合中的元素： void clear() 是否是空集合： boolean isEmpty() 是否包含某个元素： boolean contains(Object obj)：判断当前集合中是否包含 obj，通过 obj 的 equals() 来判断是否是同一个对象。 向 Collection 的实现类的对象中添加数据 obj 时，要求 obj 所在的类要重写 equals()，否则调用的是 Object 中的 equals()，即 ==。 boolean containsAll(Collection coll)：对两个集合的元素逐个比较，判断 coll 中的所有元素是否都存在于当前集合中，也是通过元素的 equals() 来比较的。 删除： boolean remove(Object obj)：从当前集合中移除 obj，通过 obj 的 equals() 判断是否是要删除的那个元素，只会删除找到的第一个元素。 boolean removeAll(Collection coll)：从当前集合中移除 coll 中的所有元素，即取当前集合的差集。 取两个集合的交集： boolean retainAll(Collection coll)：把交集的结果存在当前集合中，不影响 coll。 集合是否相等： boolean equals(Object obj)：如果返回 true，则 obj 首先得与当前集合类型相同。如果是 List，要求元素个数和顺序一致，如果是 Set，则不考虑顺序。 转成对象数组： Object[] toArray()：将当前集合转换为数组。 拓展：将数组转换为集合，Arrays.asList()，例如：List&lt;String&gt; strings = Arrays.asList(new String[]&#123;&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;&#125;);。使用此方法时的注意事项： 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; // toArray()：集合转换为数组 Object[] objects = collection.toArray(); System.out.println(Arrays.toString(objects)); // 拓展：数组转换为集合 List&lt;String&gt; strings = Arrays.asList(new String[]&#123;&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;&#125;); List&lt;Person&gt; people = Arrays.asList(new Person(), new Person()); List&lt;int[]&gt; ints = Arrays.asList(new int[]&#123;1, 2, 3&#125;); System.out.println(ints.size());// 1，含有一个int[]数组的集合 List&lt;Integer&gt; integers = Arrays.asList(1, 2, 3); System.out.println(integers.size());// 3，含有三个Integer元素的集合 &#125;&#125; 获取集合对象的哈希值： hashCode() 遍历： iterator()：返回 Iterator 接口的实例，即迭代器对象，用于遍历集合的元素。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); // add(Object obj)：将元素obj添加到集合collection中 collection.add(&quot;AA&quot;); collection.add(&quot;bb&quot;); collection.add(123);// 自动装箱 collection.add(new Date()); // size()：获取添加的元素的个数 System.out.println(collection.size());// 4 // addAll(Collection c)：将c集合中的元素添加到当前的集合中 Collection&lt;Object&gt; collection2 = new ArrayList&lt;&gt;(); collection2.add(456); collection2.add(&quot;CC&quot;); collection.addAll(collection2); System.out.println(collection2.size()); // clear()：清空集合中的元素 collection.clear(); // isEmpty()：判断当前集合是否为空 System.out.println(collection.isEmpty());// true // contains(Object obj)：判断当前集合是否包含obj collection.add(new String(&quot;Tom&quot;)); System.out.println(collection.contains(new String(&quot;Tom&quot;)));// true，比较的是内容 collection.add(new Person(&quot;Jerry&quot;, 20)); // true，如果Person未重写equals()，调用的是Object的方法，即==，返回false System.out.println(collection.contains(new Person(&quot;Jerry&quot;, 20))); // containsAll(Collection coll)：判断coll中的所有元素是否都存在于当前集合中 collection.add(123); collection.add(&quot;bb&quot;); System.out.println(collection.containsAll(Arrays.asList(123, &quot;bb&quot;, new Person(&quot;Jerry&quot;, 20))));// true // remove(Object obj)：从当前集合中移除obj System.out.println(collection.remove(new Person(&quot;Jerry&quot;, 20)));// true System.out.println(collection);// [Tom, 123, bb] // removeAll(Collection coll)：从当前集合中移除coll中的所有元素 System.out.println(collection.removeAll(Arrays.asList(&quot;bb&quot;, &quot;BB&quot;)));// true，移除了一个bb System.out.println(collection);// [Tom, 123, bb] // retainAll(Collection coll)：获取当前集合与coll的交集，并返回给当前集合 System.out.println(collection.retainAll(Arrays.asList(123, new Person(&quot;Jerry&quot;, 20), &quot;BB&quot;)));// true System.out.println(collection);// [123] // equals(Object obj)： collection.add(&quot;BB&quot;); System.out.println(collection);// [123, BB] System.out.println(collection.equals(Arrays.asList(123, &quot;BB&quot;)));// true // false，因为collection是List，元素是有序的 System.out.println(collection.equals(Arrays.asList(&quot;BB&quot;, 123))); // hashCode()：返回当前集合的哈希值 System.out.println(collection.hashCode()); // toArray()：集合转换为数组 Object[] objects = collection.toArray(); System.out.println(Arrays.toString(objects)); // 拓展：数组转换为集合 List&lt;String&gt; strings = Arrays.asList(new String[]&#123;&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;&#125;); List&lt;Person&gt; people = Arrays.asList(new Person(), new Person()); List&lt;int[]&gt; ints = Arrays.asList(new int[]&#123;1, 2, 3&#125;); System.out.println(ints.size());// 1，含有一个int[]数组的集合 List&lt;Integer&gt; integers = Arrays.asList(1, 2, 3); System.out.println(integers.size());// 3，含有三个Integer元素的集合 &#125;&#125;class Person &#123; private String name; private int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; Person person = (Person) o; return age == person.age &amp;&amp; Objects.equals(name, person.name); &#125; @Override public int hashCode() &#123; return Objects.hash(name, age); &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; Iterator 迭代器接口 Iterator对象称为迭代器 (设计模式的一种)，主要用于遍历 Collection 集合中的元素。 GOF 给迭代器模式的定义为：提供一种方法访问一个容器 (container) 对象中各个元素，而又不需暴露该对象的内部细节。迭代器模式，就是为容器而生。 Collection 接口继承了 java.lang.Iterable 接口，该接口有一个 iterator()，所有实现了 Collection 接口的集合类都有一个 iterator()，用以返回一个实现了 Iterator 接口的类的对象。 Iterator 仅用于遍历集合，Iterator 本身并不提供承装对象的能力。如果需要创建 Iterator 对象，则必须有一个被迭代的集合。(不适用于 Map) 集合对象每次调用 iterator() 都得到一个全新的迭代器对象，默认游标都在集合的第一个元素之前。 Iterator 接口的方法： 迭代器的执行原理： 在调用 it.next() 之前必须要调用 it.hasNext() 进行检测。若不调用，且下一条记录无效，直接调用 it.next() 会抛出 NoSuchElementException 异常。 Iterator 接口的 remove()： Iterator 可以删除集合的元素，但是是在遍历过程中通过迭代器对象的 remove() 删除的，不是集合对象的 remove()。 如果还未调用 next()，或在上一次调用 next() 之后已经调用了 remove()，则再次调用 remove() 都会抛出 IllegalStateException 异常。 实例： 1234567891011121314151617181920212223242526public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); collection.add(123); collection.add(456); collection.add(new String(&quot;Tom&quot;)); collection.add(false); Iterator&lt;Object&gt; iterator = collection.iterator(); while (iterator.hasNext()) &#123; // iterator.remove();// 游标处于集合的第一个元素之前，java.lang.IllegalStateException Object obj = iterator.next(); if (&quot;Tom&quot;.equals(obj)) &#123;// Tom放在前面，可以防止obj为null时触发空指针异常 iterator.remove(); // iterator.remove();// 游标所处位置的元素已经被remove，在该位置再次调用remove发生异常，java.lang.IllegalStateException &#125; &#125; // 遍历集合 iterator = collection.iterator();// 重新获取collection的迭代器对象，不能使用原来的，因为其游标已经移到集合末尾了 while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125; 实例： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); collection.add(123); collection.add(456); collection.add(new String(&quot;Tom&quot;)); collection.add(false); Iterator&lt;Object&gt; iterator = collection.iterator(); // 遍历 // hasNext()：判断是否还有下一个元素 while (iterator.hasNext()) &#123; // next()：1.指针下移;2.将下移以后集合位置上的元素返回 System.out.println(iterator.next()); &#125; &#125;&#125; 错误写法： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); collection.add(123); collection.add(456); collection.add(new String(&quot;Tom&quot;)); collection.add(false); Iterator&lt;Object&gt; iterator = collection.iterator(); // 错误写法一：间隔的输出集合中的元素，也会出现java.util.NoSuchElementException异常 while (iterator.next()!=null)&#123;// 游标下移一次 System.out.println(iterator.next());// 游标下移两次 &#125; // 错误写法二：死循环 while (collection.iterator().hasNext()) &#123; System.out.println(collection.iterator().hasNext()); &#125; &#125;&#125; 补充： Enumeration 接口是 Iterator 迭代器的古老版本。 123456789public class Test &#123; public static void main(String[] args) &#123; Enumeration stringEnum = new StringTokenizer(&quot;a-b*c-d-e-g&quot;, &quot;-&quot;); while (stringEnum.hasMoreElements()) &#123; Object obj = stringEnum.nextElement(); System.out.println(obj); &#125; &#125;&#125; foreach 循环 jdk 5.0 提供了 foreach 循环迭代访问 Collection 和数组。格式如下： foreach 对 Collection 或数组的遍历操作，不需获取 Collection 和数组的长度，无需使用索引访问元素。 foreach 遍历 Collection 时，其底层仍然是调用 Iterator 来完成操作。 实例： 1234567891011121314151617181920public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); collection.add(123); collection.add(456); collection.add(new String(&quot;Tom&quot;)); collection.add(false); // 遍历集合：for (集合元素的类型 局部变量 : 集合对象)，底层仍然调用了迭代器 for (Object obj : collection) &#123; System.out.println(obj); &#125; int[] arr = new int[]&#123;1, 2, 3, 4, 5, 6&#125;; // 遍历数组：for(数组元素的类型 局部变量 : 数组对象) for (int i : arr) &#123; System.out.println(i); &#125; &#125;&#125; foreach 的使用注意事项： 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; String[] arr = new String[]&#123;&quot;MM&quot;, &quot;MM&quot;, &quot;MM&quot;&#125;; // 方式一：普通for赋值 for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = &quot;GG&quot;;// 能够赋值 &#125; for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i] + &quot; &quot;);// GG GG GG &#125; System.out.println(); arr = new String[]&#123;&quot;MM&quot;, &quot;MM&quot;, &quot;MM&quot;&#125;; // 方式二：增强for循环 for (String s : arr) &#123; s = &quot;GG&quot;;// 不能赋值，因为s是一个局部变量，foreach循环将arr数组的当前值赋给了s，然后循环中s被重新赋值为GG，不会影响到arr数组中的值 &#125; for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i] + &quot; &quot;);// MM MM MM &#125; &#125;&#125; List 接口 鉴于 java 中数组用来存储数据的局限性，我们通常使用 List 替代数组。 List 集合类中元素有序、且可重复，集合中的每个元素都有其对应的顺序索引。 List 容器中的元素都对应一个整数型的序号记载其在容器中的位置，可以根据序号存取容器中的元素。 JDK API 中 List 接口的实现类常用的有：ArrayList、LinkedList 和 Vector。 List 常用方法： List 除了从 Collection 集合继承的方法外，还添加了一些根据索引来操作集合元素的方法。 void add(int index, Object ele)：在 index 位置插入ele 元素。 boolean addAll(int index, Collection eles)：从 index 位置开始将 eles 中的所有元素添加进来。 Object get(int index)：获取指定 index 位置的元素。 int indexOf(Object obj)：返回 obj 在集合中首次出现的位置。 int lastIndexOf(Object obj)：返回 obj 在当前集合中末次出现的位置。 Object remove(int index)：移除指定 index 位置的元素，并返回此元素，区别于 Collection 接口中的 remove(Object obj)。 123456789101112131415public class Test &#123; private static void updateList(List list) &#123; list.remove(2);// 删除索引2 // list.remove(new Integer(2));// 删除对象2 &#125; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(1); list.add(2); list.add(3); updateList(list); System.out.println(list);// [1, 2] &#125;&#125; Object set(int index, Object ele)：设置指定 index 位置的元素为 ele。 List subList(int fromIndex, int toIndex)：返回当前集合从 fromIndex 到 toIndex 位置的子集合，前包后不包，当前集合不发生改变。 总结： 增：add(Object obj) 删：remove(int index) / remove(Object obj) 改：set(int index, Object ele) 查：get(int index) 插：add(int index, Object ele) 长度：size() 遍历：① Iterator 迭代器方式；② 增强 for 循环；③ 普通的循环。 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; ArrayList&lt;Object&gt; list = new ArrayList(); list.add(123); list.add(456); list.add(&quot;AA&quot;); // 方式一：Iterator迭代器方式 Iterator&lt;Object&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; System.out.println(&quot;***************&quot;); // 方式二：增强for循环 for (Object obj : list) &#123; System.out.println(obj); &#125; System.out.println(&quot;***************&quot;); // 方式三：普通for循环 for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125;&#125; 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Test &#123; public static void main(String[] args) &#123; ArrayList&lt;Object&gt; list = new ArrayList(); list.add(123); list.add(456); list.add(&quot;AA&quot;); list.add(456); System.out.println(list);// [123, 456, AA, 456] // void add(int index, Object ele): 在index位置插入ele元素 list.add(1, &quot;BB&quot;); System.out.println(list);// [123, BB, 456, AA, 456] // boolean addAll(int index, Collection eles): 从index位置开始将eles中的所有元素添加进来 List&lt;Integer&gt; list1 = Arrays.asList(1, 2, 3); list.addAll(list1); // list.add(list1);// 这是把list1当作一个元素添加到list中 System.out.println(list);// [123, BB, 456, AA, 456, 1, 2, 3] // Object get(int index): 获取指定index位置的元素 System.out.println(list.get(0));// 123 // int indexOf(Object obj): 返回obj在集合中首次出现的位置。如果不存在，返回-1。 int index = list.indexOf(4567); System.out.println(index);// -1 // int lastIndexOf(Object obj): 返回obj在当前集合中末次出现的位置。如果不存在，返回-1。 System.out.println(list.lastIndexOf(456));// 4 // Object remove(int index): 移除指定index位置的元素，并返回此元素 Object obj = list.remove(0); System.out.println(obj);// 123 System.out.println(list);// [BB, 456, AA, 456, 1, 2, 3] // Object set(int index, Object ele): 设置指定index位置的元素为ele list.set(1, &quot;CC&quot;); System.out.println(list);// [BB, CC, AA, 456, 1, 2, 3] // List subList(int fromIndex, int toIndex): 返回从fromIndex到toIndex位置的左闭右开区间的子集合 List&lt;Object&gt; subList = list.subList(2, 4); System.out.println(subList);// [AA, 456] System.out.println(list);// [BB, CC, AA, 456, 1, 2, 3] &#125;&#125; ArrayList ArrayList 是 List 接口的典型实现类、主要实现类。 本质上，ArrayList 是对象引用的一个 “变长” 数组。 ArrayList 的 JDK 1.8 之前与之后的实现区别？ JDK 1.7：ArrayList 类似于饿汉式，初始化时直接创建一个初始容量为 10 的数组。 JDK 1.8：ArrayList 类似于懒汉式，初始化时创建一个长度为 0 的数组，当添加第一个元素时再创建一个初始容量为 10 的数组。 Arrays.asList(…) 返回的 List 集合，既不是 ArrayList 实例，也不是 Vector 实例。Arrays.asList(…) 返回值是一个固定长度的 List 集合。 源码分析： JDK 7.0： ArrayList list = new ArrayList();，初始化时，底层创建了长度是 10 的 Object[] 数组 elementData。 12345/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. */private transient Object[] elementData; 123456/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this(10);// 初始化时，数组长度为10&#125; 1234567891011121314/** * Constructs an empty list with the specified initial capacity. * * @param initialCapacity the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity * is negative */public ArrayList(int initialCapacity) &#123;// 也可以直接指定ArrayList的容量 super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity];&#125; list.add(123);，等同于 elementData[0] = new Integer(123);。 list.add(11);，每次添加数据前，会验证数组容量，如果此次的添加导致底层 elementData 数组容量不够，则扩容。 123456789101112/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; // add()添加元素之前，先验证数组容量，size即为已添加的元素的数量 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e;// 添加元素到数组的size+1的位置 return true;&#125; 123456private void ensureCapacityInternal(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0)// 如果添加的元素的总数，已经超过了数组的长度，则进行扩容操作 grow(minCapacity);&#125; 默认情况下，扩容为原来的容量的 1.5 倍，同时需要将原有数组中的数据复制到新的数组中。 1234567891011121314151617/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);// 扩容后的新数组，其长度为原数组长度的1.5倍 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);// 将原数组中的元素，复制到新数组中&#125; 结论：建议开发中使用带参的构造器：ArrayList list = new ArrayList(int capacity);，按需求在初始化时就指定 ArrayList 的容量，以尽可能的避免扩容。 JDK 8.0： ArrayList list = new ArrayList();，底层 Object[] 数组 elementData 初始化为 {} (长度为 0 的空数组)，并没有创建长度为 10 的数组。 1234567/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */transient Object[] elementData; // non-private to simplify nested class access 123456/** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; 123456/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;// 初始化时，没有创建长度为10的数组&#125; list.add(123);，第一次调用 add() 时，底层才创建了长度为 10 的数组，并将数据 123 添加到 elementData[0]。 123456789101112/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; // 第一次添加元素，size=0，先初始化数组 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e;// 添加元素到数组的size+1的位置 return true;&#125; 123private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));// 得到数组的长度&#125; 1234567private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 第一次添加元素，elementData为&#123;&#125;，返回数组长度为DEFAULT_CAPACITY，即10 return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;// 不是第一次添加元素，elementData不为&#123;&#125;，直接返回下一个添加元素的数目&#125; 1234567private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0)// 如果添加的元素的总数，已经超过了数组的长度，则进行扩容操作 grow(minCapacity);&#125; 后续的添加和扩容操作与 JDK 7.0 无异。 小结： JDK 7.0 中的 ArrayList 的对象的创建，类似于单例的饿汉式，初始化时直接创建一个初始容量为 10 的数组。 JDK 8.0 中的 ArrayList 的对象的创建，类似于单例的懒汉式，延迟了数组的创建，节省内存。 添加数据时，如果底层的数组需要扩容，均扩容为原来的容量的 1.5 倍，同时将原有数组中的数据复制到新的数组中。 LinkedList 双向链表，内部定义了内部类 Node，作为 LinkedList 中保存数据的基本结构。LinkedList 内部没有声明数组，而是定义了 Node 类型的 first 和 last，用于记录首末元素。 对于频繁的插入或删除元素的操作，建议使用 LinkedList 类，效率较高。 新增方法： void addFirst(Object obj) void addLast(Object obj) Object getFirst() Object getLast() Object removeFirst() Object removeLast() 源码分析： LinkedList list = new LinkedList();，内部声明了 Node 类型的 first 和 last 属性，默认值为 null。 12/** * Pointer to first node. Invariant: (first == null &amp;&amp; last == null) || (first.prev == null &amp;&amp; first.item != null) /transient Node first; /** Pointer to last node. Invariant: (first == null &amp;&amp; last == null) || (last.next == null &amp;&amp; last.item != null) */ transient Node last; 12345678910111213 &#96;&#96;&#96;javaprivate static class Node&lt;E&gt; &#123; E item;&#x2F;&#x2F; 这个就是往LinkedList中添加的数据 Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item &#x3D; element; this.next &#x3D; next; this.prev &#x3D; prev; &#125; &#125; 12345/** * Constructs an empty list. */public LinkedList() &#123;&#125; list.add(123);，将 123 封装到 Node 中，创建了 Node 对象。 123456789101112/** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #addLast&#125;. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; linkLast(e); return true;&#125; 1234567891011121314/** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; Node 的定义体现了 LinkedList 的双向链表的说法，其除了保存数据，还定义了两个变量： prev：变量记录前一个元素的位置。 next：变量记录下一个元素的位置。 Vector Vector 是一个古老的集合，JDK 1.0 就有了。大多数操作与 ArrayList 相同，区别之处在于 Vector 是线程安全的。 在各种 List 中，最好把 ArrayList 作为缺省选择。当插入、删除频繁时，使用 LinkedList。Vector 总是比 ArrayList 慢，所以尽量避免使用。 新增方法： void addElement(Object obj) void insertElementAt(Object obj,int index) void setElementAt(Object obj,int index) void removeElement(Object obj) void removeAllElements() 源码分析： JDK 7.0 和 JDK 8.0 中，通过 new Vector() 构造器创建对象时，底层都创建了长度为 10 的数组。在扩容方面，默认扩容为原来的数组长度的 2 倍。 12345678/** * Constructs an empty vector so that its internal data array * has size &#123;@code 10&#125; and its standard capacity increment is * zero. */public Vector() &#123; this(10);// 初始化长度为10&#125; 1234567891011/** * Constructs an empty vector with the specified initial capacity and * with its capacity increment equal to zero. * * @param initialCapacity the initial capacity of the vector * @throws IllegalArgumentException if the specified initial capacity * is negative */public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125; 123456789101112131415161718/** * Constructs an empty vector with the specified initial capacity and * capacity increment. * * @param initialCapacity the initial capacity of the vector * @param capacityIncrement the amount by which the capacity is * increased when the vector overflows * @throws IllegalArgumentException if the specified initial capacity * is negative */public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity];// 创建长度为10的数组 this.capacityIncrement = capacityIncrement;&#125; add() 添加数据之前，先验证数组容量： 12345678910111213/** * Appends the specified element to the end of this Vector. * * @param e element to be appended to this Vector * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) * @since 1.2 */public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 12345678910111213/** * This implements the unsynchronized semantics of ensureCapacity. * Synchronized methods in this class can internally call this * method for ensuring capacity without incurring the cost of an * extra synchronization. * * @see #ensureCapacity(int) */private void ensureCapacityHelper(int minCapacity) &#123; // overflow-conscious code if (minCapacity - elementData.length &gt; 0)// 数组容量不够，扩容 grow(minCapacity);&#125; 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity);// 扩容到原来数组长度的二倍 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; ArrayList、LinkedL`ist、Vector三者的异同 相同点：三个类都实现了 List 接口，存储数据的特点相同，都是存储有序的、可重复的数据。 不同点： ArrayList：作为 List 接口的主要实现类；线程不安全的，效率高；底层使用 Object[] elementData 存储。 LinkedList：线程不安全的，对于频繁的插入、删除操作，使用此类效率比 ArrayList 高；底层使用双向链表存储。 Vector：作为 List 接口的古老实现类；线程安全的，效率低；底层使用 Object[] elementData 存储。 ArrayList 和 LinkedList 的异同： ArrayList 和 LinkedList 都线程不安全，相对线程安全的 Vector，二者执行效率更高。 ArrayList 底层是实现了基于动态数组的数据结构，LinkedList 底层是实现了基于链表的数据结构。 对于随机访问 get() 和 set()，ArrayList 优于LinkedList，因为 LinkedList 要移动指针。 对于新增和删除操作 add() (特指插入) 和 remove()，LinkedList 比较占优势，因为 ArrayList 要移动数据。 ArrayList 和 Vector 的区别： Vector 和 ArrayList 几乎是完全相同的，唯一的区别在于 Vector 是同步类，属于强同步类。因此开销就比 ArrayList 要大，访问要慢。 正常情况下，大多数的 java 程序员使用 ArrayList 而不是 Vector，因为同步完全可以由程序员自己来控制。 Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。Vector 还有一个子类 Stack。 Set 接口 Set 集合存储无序的、不可重复的数据，如果试把两个相同的元素加入同一个 Set 集合中，则添加操作失败。 无序性：不等于随机性。以 HashSet 为例，存储的数据在底层数组中并非按照数组索引的顺序添加，而是根据数据的哈希值决定的。 不可重复性：保证添加的元素按照 equals() 判断时，不能返回 true。即：相同的元素只能添加一个。 Set 接口是 Collection 的子接口，Set 接口没有提供额外的方法，使用的都是Collection中声明过的方法。 Set 判断两个对象是否相同不是使用 == 运算符，而是根据 equals()。 对于存放在 Set (主要指：HashSet、LinkedHashSet) 容器中的对象，其对应的类一定要重写 equals() 和 hashCode()，以实现对象相等规则。 要求：重写的 hashCode() 和 equals() 尽可能保持一致性，即：相等的对象必须具有相等的散列码。 如果不重写所添加元素所在类的 hashCode()，则会调用 Object 类的 hashCode()，该方法是产生一个随机数，因此，即使添加两个一样的元素，其 hashCode 值也可能不同，也就都能添加成功。 重写两个方法的小技巧：对象中用作 equals() 方法比较的 Field，都应该用来计算 hashCode 值。 TreeSet 比较两个元素是否相同的方法，不是 equals() 和 hashCode()，而是元素对应类的排序方法。 重写 hashCode() 方法的基本原则： 在程序运行时，同一个对象多次调用 hashCode() 方法应该返回相同的值。 当两个对象的 equals() 方法比较返回 true 时，这两个对象的 hashCode() 方法的返回值也应相等。 对象中用作 equals() 方法比较的 Field，都应该用来计算 hashCode 值。 重写 equals() 方法的基本原则，以自定义的 Customer 类为例，何时需要重写 equals()： 如果一个类有自己特有的 “逻辑相等” 概念，当重写 equals() 的时候，总是需要重写 hashCode()。因为根据一个类改写后的 equals()，两个截然不同的实例有可能在逻辑上是相等的，但是，根据 Object 类的 hashCode()，它们仅仅是两个对象。这种情况，违反了 “相等的对象必须具有相等的散列码” 的原则。 结论：重写 equals() 的时候，一般都需要同时重写 hashCode() 方法。通常参与计算 hashCode 的对象的属性也应该参与到 equals() 中进行计算。 Eclipse/IDEA 工具里 hashCode() 的重写，为什么会有 31 这个数字： 123456@Overridepublic int hashCode() &#123; int result = name.hashCode(); result = 31 * result + age; return result;&#125; 选择系数的时候要选择尽量大的系数，因为如果计算出来的 hashCode 值越大，所谓的冲突就越少，查找起来效率也会提高。—&gt; 减少冲突 31 只占用 5 bits，相乘造成数据溢出的概率较小。 31 可以由 i * 31 == (i &lt;&lt; 5) - 1 来表示，现在很多虚拟机里面都有做相关优化。—&gt; 提高算法效率 31 是一个素数，素数作用就是如果用一个数字来乘以这个素数，那么最终出来的结果只能被素数本身和被乘数还有 1 来整除！—&gt; 减少冲突 HashSet HashSet 是 Set 接口的典型实现，大多数时候使用 Set 集合时都使用这个实现类。 HashSet 按 Hash 算法来存储集合中的元素，因此具有很好的存取、查找、删除性能。 HashSet 具有以下特点： 不保证元素的排列顺序。 不是线程安全的。 集合元素可以是 null。 向 HashSet 中添加元素的过程： 当向 HashSet 集合中存入一个元素 a 时，首先会调用元素 a 所在类的 hashCode()，计算元素 a 的 hashCode 值，然后根据 hashCode 值，通过某种散列函数，计算出元素 a 在 HashSet 底层数组中的存储位置 (即为：索引位置，这个索引位置不是像 List 那样有顺序的，而是无序的)。 说明：这个散列函数会根据元素的 hashCode 值和底层数组的长度相计算，得到该元素在数组中的下标 (存储位置)，并且这种散列函数计算还尽可能保证能均匀存储元素，越是散列分布，该散列函数设计的越好。 向 List 中添加元素时，会按照索引位置的顺序在数组中逐个添加，这是一种有序性。而向 HashSet 中添加元素时，可能第一个元素的索引位置在数组的中间，第二个元素的索引位置在数组的头，第三个元素的索引位置在数组的尾，是按照一种无序的状态添加的，是为无序性。 计算出元素 a 的存储位置后，首先判断数组此位置上是否已经有元素： 如果此位置上没有其他元素，则元素 a 添加成功。—&gt; 情况1 如果此位置上有其他元素 b (或以链表形式存在的多个元素)，则比较元素 a 与元素 b (或以链表形式存在的多个元素) 的 hashCode 值： 如果 hashCode 值不相同，则元素 a 添加成功。—&gt; 情况2 如果 hashCode 值相同，进而需要调用元素 a 所在类的 equals()： equals() 返回 true，则元素 a 添加失败。 equals() 返回 false，则元素 a 添加成功。—&gt; 情况3 对于添加成功的情况 2 和情况 3 而言：元素 a 与已经存在指定索引位置上的元素以链表的方式存储。 JDK 7.0：元素 a 存放到底层数组中，指向原来的元素。 JDK 8.0：原来的元素存放到数组中，指向元素 a。 总结：七上八下。 由以上向 HashSet 添加元素的过程，可以看出 HashSet 的底层：数组 + 链表的结构。(前提：JDK 7.0，JDK 8.0 见 HashMap。) HashSet 底层结构： HashSet 集合判断两个元素相等的标准：两个对象通过 hashCode() 比较相等，并且两个对象的 equals() 返回值也相等。 利用 HashSet 去除 List 中的重复元素： 1234567891011121314151617181920public class Test &#123; public static List duplicateList(List list) &#123; HashSet set = new HashSet(); set.addAll(list); return new ArrayList(set); &#125; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(new Integer(1)); list.add(new Integer(2)); list.add(new Integer(2)); list.add(new Integer(4)); list.add(new Integer(4)); List list2 = duplicateList(list); for (Object integer : list2) &#123; System.out.println(integer); &#125; &#125;&#125; 实例： 12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; Set set = new HashSet(); set.add(456); set.add(123); set.add(123); set.add(&quot;AA&quot;); set.add(&quot;CC&quot;); set.add(129); Iterator iterator = set.iterator(); while(iterator.hasNext())&#123; System.out.print(iterator.next() + &quot; &quot;);// AA CC 129 456 123，不是按照元素添加的顺序进行输出的 &#125; &#125;&#125; LinkedHashSet LinkedHashSet 是 HashSet 的子类，不允许集合元素重复 LinkedHashSet 根据元素的 hashCode 值来决定元素的存储位置，但它同时使用双向链表维护元素的次序，这使得元素看起来是以插入顺序保存的。 遍历 LinkedHashSet 内部数据时，可以按照添加的顺序遍历。 LinkedHashSet 插入性能略低于 HashSet，但在迭代访问 Set 里的全部元素时有很好的性能。 对于频繁的遍历操作，LinkedHashSet 效率高于 HashSet。 LinkedHashSet 底层结： 实例： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; // LinkedHashSet在添加数据的同时，每个数据还维护了两个引用，记录此数据前一个数据和后一个数据 Set set = new LinkedHashSet(); set.add(456); set.add(123); set.add(123); set.add(&quot;AA&quot;); set.add(&quot;CC&quot;); set.add(129); Iterator iterator = set.iterator(); while (iterator.hasNext()) &#123; System.out.print(iterator.next() + &quot; &quot;);// 456 123 AA CC 129，按照元素添加的顺序进行输出的 &#125; &#125;&#125; 面试题： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Test &#123; public static void main(String[] args) &#123; HashSet set = new HashSet(); User p1 = new User(1001, &quot;AA&quot;); User p2 = new User(1002, &quot;BB&quot;); set.add(p1);// 假设p1添加到HashSet底层数组的位置7(hashCode值以1001和AA计算出来) set.add(p2);// 假设p2添加到HashSet底层数组的位置3(hashCode值以1002和BB计算出来) System.out.println(set);// 位置3和7处对应的2个User p1.name = &quot;CC&quot;;// 更改p1指向的User对象的name为CC set.remove(p1);// 以新的p1在HashSet底层数组查找，没有相同的对象(hashCode值以1001和CC计算出来) System.out.println(set);// 位置3和7处对应的2个User，但位置7指向的User对象的name为C，不是AA，位置3指向的User对象的name为BB set.add(new User(1001, &quot;CC&quot;));// 新new出来的User，hashCode值以1001和CC计算出来，不同于最初的p1，其位置不会在7处，也不会在3处，假设在11处 System.out.println(set);// 位置3、7和11处对应的3个User，其中，位置7和11对应的User的id和name都是1001和CC，但不是堆中的同一个对象 set.add(new User(1001, &quot;AA&quot;));// 新new出来的User，hashCode值以1001和AA计算出来，等于最初的p1，位置在7处，但因为现在7处User对象的name为CC，所以equals()不相同，这个User对象链接到7位置 System.out.println(set);// 位置3、7和11处对应的4个User &#125;&#125;class User &#123; int id; String name; public User(int id, String name) &#123; this.id = id; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (id != user.id) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; int result = id; result = 31 * result + (name != null ? name.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125;输出结果：[User&#123;id=1002, name=&#x27;BB&#x27;&#125;, User&#123;id=1001, name=&#x27;AA&#x27;&#125;][User&#123;id=1002, name=&#x27;BB&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;][User&#123;id=1002, name=&#x27;BB&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;][User&#123;id=1002, name=&#x27;BB&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;, User&#123;id=1001, name=&#x27;AA&#x27;&#125;] 解析： TreeSet TreeSet 是 SortedSet 接口的实现类，TreeSet 可以按照添加对象的指定属性，进行排序，确保集合元素处于排序状态。 TreeSet 特点：有序，查询速度比 List 快。 TreeSet 与 TreeMap 一样，底层使用红黑树结构存储数据。 红黑树参考：http://www.cnblogs.com/yangecnu/p/Introduce-Red-Black-Tree.html 新增方法： Comparator comparator() Object first() Object last() Object lower(Object e) Object higher(Object e) SortedSet subSet(fromElement, toElement) SortedSet headSet(toElement) SortedSet tailSet(fromElement) 向 TreeSet 中添加的数据，要求是相同类的对象。 TreeSet 两种排序方法：自然排序 (实现 Comparable 接口) 和定制排序 (Comparator)。默认情况下，TreeSet 采用自然排序。 在 TreeSet 中比较两个元素是否相同时，取决于使用的是自然排序还是定制排序，不再考虑 equals()，比如 add() 和 remove() 等方法，这点与 HashSet 不同。 自然排序： TreeSet 会调用集合元素的 compareTo(Object obj) 来比较元素之间的大小关系，然后将集合元素按升序 (默认情况) 排列。 如果试图把一个对象添加到 TreeSet 时，则该对象的类必须实现 Comparable 接口。 实现 Comparable 的类必须实现 compareTo(Object obj)，两个对象即通过 compareTo(Object obj) 的返回值来比较大小。 Comparable 的典型实现： BigDecimal、BigInteger 以及所有的数值型对应的包装类：按它们对应的数值大小进行比较。 Character：按字符的 unicode值来进行比较。 Boolean：true 对应的包装类实例大于 false 对应的包装类实例。 String：按字符串中字符的 unicode 值进行比较。 Date、Time：后边的时间、日期比前面的时间、日期大。 向 TreeSet 中添加元素时，只有第一个元素无须比较 compareTo()，后面添加的所有元素都会调用 compareTo() 进行比较。 因为只有相同类的两个实例才会比较大小，所以向 TreeSet 中添加的应该是同一个类的对象。 对于 TreeSet 集合而言，使用自然排序判断两个元素相等的标准是：两个元素通过 compareTo() 比较返回 0，不再是 equals()。 当需要把一个对象放入 TreeSet 中，在重写该对象对应的 equals() 时，应保证该方法与 compareTo() 有一致的结果：如果两个对象通过 equals() 比较返回 true，则通过 compareTo(Object obj) 比较应返回 0。否则，会让人难以理解。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class Test &#123; public static void main(String[] args) &#123; Set set = new TreeSet(); set.add(new User(&quot;Tom&quot;, 12)); set.add(new User(&quot;Jerry&quot;, 32)); set.add(new User(&quot;Jim&quot;, 2)); set.add(new User(&quot;Mike&quot;, 65)); set.add(new User(&quot;Jack&quot;, 33)); set.add(new User(&quot;Jack&quot;, 56)); Iterator iterator = set.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125;class User implements Comparable &#123; private String name; private int age; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; System.out.println(&quot;User equals()....&quot;); if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (age != user.age) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; //return name.hashCode() + age; int result = name != null ? name.hashCode() : 0; result = 31 * result + age; return result; &#125; // 按照姓名从大到小排列,年龄从小到大排列 @Override public int compareTo(Object o) &#123; if (o instanceof User) &#123; User user = (User) o; // return -this.name.compareTo(user.name); int compare = -this.name.compareTo(user.name); if (compare != 0) &#123; return compare; &#125; else &#123; return Integer.compare(this.age, user.age); &#125; &#125; else &#123; throw new RuntimeException(&quot;输入的类型不匹配&quot;); &#125; &#125;&#125; 定制排序： TreeSet 的自然排序要求元素所属的类实现 Comparable 接口，如果元素所属的类没有实现 Comparable 接口，或不希望按照升序 (默认情况 )的方式排列元素或希望按照其它属性大小进行排序，则考虑使用定制排序。定制排序，通过 Comparator 接口来实现。需要重写 compare() 方法。 利用 int compare(T o1,T o2) 方法，比较 o1 和 o2 的大小：如果方法返回正整数，则表示 o1 大于 o2；如果返回 0，表示相等；返回负整数，表示 o1 小于 o2。 要实现定制排序，需要将实现 Comparator 接口的实例作为形参传递给 TreeSet 的构造器。此时，仍然只能向 TreeSet 中添加类型相同的对象。否则会发生 ClassCastException 异常。 对于 TreeSet 集合而言，使用定制排序判断两个元素相等的标准是：两个元素通过 compare() 比较返回 0，不再是 equals()。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class Test &#123; public static void main(String[] args) &#123; // 定制排序 Comparator com = new Comparator() &#123; // 按照年龄从小到大排列 @Override public int compare(Object o1, Object o2) &#123; if (o1 instanceof User &amp;&amp; o2 instanceof User) &#123; User u1 = (User) o1; User u2 = (User) o2; return Integer.compare(u1.getAge(), u2.getAge()); &#125; else &#123; throw new RuntimeException(&quot;输入的数据类型不匹配&quot;); &#125; &#125; &#125;; TreeSet set = new TreeSet(com); set.add(new User(&quot;Tom&quot;, 12)); set.add(new User(&quot;Jerry&quot;, 32)); set.add(new User(&quot;Jim&quot;, 2)); set.add(new User(&quot;Mike&quot;, 65)); set.add(new User(&quot;Mary&quot;, 33)); set.add(new User(&quot;Jack&quot;, 33)); set.add(new User(&quot;Jack&quot;, 56)); Iterator iterator = set.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125;class User implements Comparable &#123; private String name; private int age; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; System.out.println(&quot;User equals()....&quot;); if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (age != user.age) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; //return name.hashCode() + age; int result = name != null ? name.hashCode() : 0; result = 31 * result + age; return result; &#125; // 按照姓名从大到小排列,年龄从小到大排列 @Override public int compareTo(Object o) &#123; if (o instanceof User) &#123; User user = (User) o; // return -this.name.compareTo(user.name); int compare = -this.name.compareTo(user.name); if (compare != 0) &#123; return compare; &#125; else &#123; return Integer.compare(this.age, user.age); &#125; &#125; else &#123; throw new RuntimeException(&quot;输入的类型不匹配&quot;); &#125; &#125;&#125; 实例： 1234567891011121314151617181920212223public class Test &#123; public static void main(String[] args) &#123; Set set = new TreeSet(); // 失败：不能添加不同类的对象 /*set.add(123); set.add(456); set.add(&quot;AA&quot;); set.add(new User(&quot;Tom&quot;,12));*/ // 举例：全部添加Integer对象 /*set.add(34); set.add(-34); set.add(43); set.add(11); set.add(8);*/ Iterator iterator = set.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125; Map 接口 Map 与 Collection 是并列存在，双列数据，用于保存具有映射关系的数据：key - value 对。 Map结构的理解： Map 中的 key：无序的、不可重复的，使用 Set 存储所有的 key。—&gt; key 所在的类要重写 hashCode() 和 equals() (以 HashMap 为例)。 Map 中的 value：无序的、可重复的，使用 Collection 存储所有的 value。—&gt; value 所在的类要重写 equals()。 一个键值对：key - value 构成了一个 entry 对象。 Map 中的映射关系的类型是 Map.Entry 类型，它是 Map 接口的内部接口。 Map 中的 entry：无序的、不可重复的，使用 Set 存储所有的 entry。 Map 中的 key 和 value 都可以是任何引用类型的数据。 常用 String 类作为 Map 的 key。 key 和 value 之间存在单向一对一关系，即通过指定的 key 总能找到唯一的、确定的 value。 Map 接口的常用实现类：HashMap、TreeMap、LinkedHashMap 和 Properties。其中，HashMap 是 Map 接口使用频率最高的实现类。 Map 常用方法： 添加、删除、修改操作： Object put(Object key, Object value)：将指定 key - value 对添加到 (或修改) 当前 map 对象中。 如果在 map 中已存在 key，则会用 value 替换 map 中该 key 对应的值。 void putAll(Map m)：将 m 中的所有 key - value 对存放到当前 map 中。 Object remove(Object key)：移除指定 key 的 key - value 对，并返回 value。若 key 不存在，返回 null。 void clear()：清空当前 map 中的所有数据，与 map = null; 操作不同。 实例： 12345678910111213141516171819202122232425262728293031public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); // put() map.put(&quot;AA&quot;, 123); map.put(45, 123); map.put(&quot;BB&quot;, 56); // 修改，key已存在，会替换其value map.put(&quot;AA&quot;, 87); System.out.println(map);// &#123;AA=87, BB=56, 45=123&#125; // putAll() Map map1 = new HashMap(); map1.put(&quot;CC&quot;, 123); map1.put(&quot;DD&quot;, 123); map.putAll(map1); System.out.println(map);// &#123;AA=87, BB=56, CC=123, DD=123, 45=123&#125; // remove(Object key) Object value = map.remove(&quot;CC&quot;); System.out.println(value);// 123 System.out.println(map);// &#123;AA=87, BB=56, DD=123, 45=123&#125; System.out.println(map.remove(&quot;EE&quot;));// key不存在，返回null // clear() map.clear();// 与map = null操作不同 System.out.println(map.size());// 0 System.out.println(map);// &#123;&#125; &#125;&#125; 元素查询的操作： Object get(Object key)：获取指定 key 对应的 value，如果 key 不存在，返回 null。 boolean containsKey(Object key)：是否包含指定的 key。 boolean containsValue(Object value)：是否包含指定的 value。 int size()：返回 map 中 key - value 对的个数。 boolean isEmpty()：判断当前 map 是否为空，以 size 是否为 0 判断。 boolean equals(Object obj)：判断当前 map 和参数对象 obj 是否相等。 实例： 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); map.put(&quot;AA&quot;, 123); map.put(45, 123); map.put(&quot;BB&quot;, 56); // Object get(Object key) System.out.println(map.get(45));// 123 System.out.println(map.get(43));// null // containsKey(Object key) boolean isExist = map.containsKey(&quot;BB&quot;); System.out.println(isExist);// true isExist = map.containsValue(123); System.out.println(isExist);// true map.clear(); System.out.println(map.isEmpty());// true &#125;&#125; 元视图操作的方法： Set keySet()：返回所有 key 构成的 Set 集合。 Collection values()：返回所有 value 构成的 Collection 集合。 Set entrySet()：返回所有 key - value 对构成的 Set 集合。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); map.put(&quot;AA&quot;, 123); map.put(45, 1234); map.put(&quot;BB&quot;, 56); // 遍历所有的key集：keySet() Set keys = map.keySet(); Iterator iterator = keys.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; System.out.println(); // 遍历所有的value集：values() Collection values = map.values(); for (Object obj : values) &#123; System.out.println(obj); &#125; System.out.println(); // 遍历所有的key-value // 方式一：entrySet() Set entrySet = map.entrySet(); Iterator iterator1 = entrySet.iterator(); while (iterator1.hasNext()) &#123; Object obj = iterator1.next(); // entrySet集合中的元素都是entry Map.Entry entry = (Map.Entry) obj; System.out.println(entry.getKey() + &quot;----&gt;&quot; + entry.getValue()); &#125; System.out.println(); // 方式二： Set keySet = map.keySet(); Iterator iterator2 = keySet.iterator(); while (iterator2.hasNext()) &#123; Object key = iterator2.next(); Object value = map.get(key); System.out.println(key + &quot;=====&quot; + value); &#125; &#125;&#125; 总结： 添加：put(Object key, Object value) 删除：remove(Object key) 修改：put(Object key, Object value) 查询：get(Object key) 长度：size() 遍历：keySet() / values() / entrySet() HashMap HashMap 是 Map 接口使用频率最高的实现类。 HashMap 允许使用 null 键和 null 值，与 HashSet 一样，不保证映射的顺序。 所有的 key 构成的集合是 Set：无序的、不可重复的。所以，key 所在的类要重写：hashCode() 和 equals()。 HashMap 判断两个 key 相等的标准是：两个 key 的 hashCode 值相等，同时通过 equals() 判断返回 true。 所有的 value 构成的集合是 Collection：无序的、可以重复的。所以，value 所在的类要重写：equals()。 HashMap 判断两个 value 相等的标准是：两个 value 通过 equals() 判断返回 true。 一个 key - value 对构成一个 entry，所有的 entry 构成的集合是 Set：无序的、不可重复的。 不要修改映射关系的 key： 映射关系存储到 HashMap 中时，会存储 key 的 hash 值，这样就不用在每次查找时重新计算每一个 Entry 或 Node (TreeNode) 的 hash 值了，因此如果已经 put 到 Map 中的映射关系，再修改 key 的属性，而这个属性又参与 hashcode 值的计算，那么会导致匹配不上。 HashMap 源码中的重要常量： DEFAULT_INITIAL_CAPACITY：HashMap 的默认容量，16。 1234/** * The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 MAXIMUM_CAPACITY：HashMap 的最大支持容量，2^30。 123456/** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; DEFAULT_LOAD_FACTOR：HashMap 的默认加载因子，0.75。不同于 ArrayList，HashMap 不是在底层数组全部填满时才进行扩容操作，因为数组上有一些位置可能会一直都没有添加元素，但其他位置上元素可能有很多，导致链表和二叉树结构变多。因此，会在元素添加到一定数量时，就执行扩容操作，即添加元素数量达到 threshold 值时扩容。默认加载因子如果过小，会导致数组还有很多空位置时扩容，数组利用率低；默认加载因子如果过大，会导致数组中存在很多元素时才扩容，链表和二叉树结构过多。因此，默认加载因子在 0.7 ~ 0.75 左右比较合适。 1234/** * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f; TREEIFY_THRESHOLD：Bucket 中链表存储的 Node 长度大于该默认值，判断是否转换为红黑树，默认为 8。Since JDK 8.0。 123456789/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8; UNTREEIFY_THRESHOLD：Bucket 中红黑树存储的 Node 长度小于该默认值，转换为链表，默认为 6，Since JDK 8.0。 123456/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6; MIN_TREEIFY_CAPACITY：桶中的 Node 被树化时最小的 hash 表容量，默认为 64。当桶中 Node 的数量大到需要变红黑树 (8) 时，若 hash 表容量小于 MIN_TREEIFY_CAPACITY，此时应执行 resize() 进行扩容操作。MIN_TREEIFY_CAPACITY 的值至少是 TREEIFY_THRESHOLD 的 4 倍。Since JDK 8.0。 1234567/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64; table ：存储元素的数组，长度总是 2 的 n 次幂。JDK 7.0 中是 transient Entry&lt;K, V&gt;[] table;，JDK 8.0 中是 transient Node&lt;K,V&gt;[] table;。 entrySet：存储具体元素的集。 size：HashMap 中已存储的键值对的数量。 modCount：HashMap 扩容和结构改变的次数。 threshold：扩容的临界值，其值一般等于容量 * 加载因子，(int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1);。扩容的操作不是当底层数组全部被填满后再扩容，而是达到临界值后的下一次添加操作进行扩容。 loadFactor：加载因子。 源码分析： JDK 7.0： 初始化操作，以无参构造器为例：HashMap hashMap = new HashMap();，在实例化以后，底层创建了长度是 16 的一维数组 Entry[] table： 1234567/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);// 默认初始化长度：16，加载因子：0.75。 &#125; 12345678910111213141516171819202122232425262728293031/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY)// map最大长度：1073741824 initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1;// map初始化时的长度，总是2的n次幂 this.loadFactor = loadFactor; threshold = (int)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1);// 扩容的临界值16*0.75=12 table = new Entry[capacity];// 底层创建了长度是16的一维数组Entry[] table useAltHashing = sun.misc.VM.isBooted() &amp;&amp; (capacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); init(); &#125; 向数组中添加数据操作，hashMap.put(key1, value1);： 123456789101112131415161718192021222324252627282930313233/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) &#123; if (key == null) return putForNullKey(value);// HashMap可以添加key为null的键值对 int hash = hash(key);// 计算key的hash值，中间调用了key的hashCode()方法 int i = indexFor(hash, table.length);// 获取当前数据在数组中的索引位置 // 取出数组的i位置上的元素，i位置上的元素可能不止一个，需要一个一个对比 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 如果i位置上有元素，对比该元素与当前key的hash值和equals()是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value;// i位置上元素与当前key相同，则将当前value替换i位置上原值 e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i);// 如果数组的i位置上没有元素，则直接添加当前key-value对在i位置上 return null;&#125; 计算 key 的 hash值： 123456789101112131415161718192021222324/** * Retrieve object hash code and applies a supplemental hash function to the * result hash, which defends against poor quality hash functions. This is * critical because HashMap uses power-of-two length hash tables, that * otherwise encounter collisions for hashCodes that do not differ * in lower bits. Note: Null keys always map to hash 0, thus index 0. */final int hash(Object k) &#123; int h = 0; if (useAltHashing) &#123; if (k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h = hashSeed; &#125; h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 获取位置： 123456/** * Returns index for hash code h. */static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 添加数据： 123456789101112131415161718/** * Adds a new entry with the specified key, value and hash code to * the specified bucket. It is the responsibility of this * method to resize the table if appropriate. * * Subclass overrides this to alter the behavior of put method. */void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 如果已添加的元素数量≥扩容的临界值，且即将添加元素的数组bucketIndex位置上已存在元素 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length);// 扩容为原来数组长度的的2倍，并将原有的数据复制到新数组中 hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length);// 重新计算当前key在新数组中的位置 &#125; // 不需要扩容，或扩容完成，将当前元素存放到数组的bucketIndex位置上 createEntry(hash, key, value, bucketIndex);&#125; 123456789101112131415/** * Like addEntry except that this version is used when creating entries * as part of Map construction or &quot;pseudo-construction&quot; (cloning, * deserialization). This version needn&#x27;t worry about resizing the table. * * Subclass overrides this to alter the behavior of HashMap(Map), * clone, and readObject. */void createEntry(int hash, K key, V value, int bucketIndex) &#123; // 取出bucketIndex位置上原有的元素 Entry&lt;K,V&gt; e = table[bucketIndex]; // 将当前的元素存放在bucketIndex位置上，并指向原有的元素 table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 总结，JDK 7.0 中 HashMap 的底层实现原理，以 HashMap map = new HashMap(); 为例说明： 在实例化以后，底层创建了长度是 16 的一维数组 Entry[] table。 执行 map.put(key1, value1) 操作，可能已经执行过多次 put： 首先，计算 key1 所在类的 hashCode() 以及其他操作计算 key1 的哈希值，此哈希值经过某种算法计算以后，得到在 Entry 数组中的存放位置。 如果此位置上的数据为空，此时的 key1 - value1 添加成功。—&gt; 情况 1 如果此位置上的数据不为空，(意味着此位置上存在一个或多个数据(以链表形式存在))，比较 key1 和已经存在的一个或多个数据的哈希值： 如果 key1 的哈希值与已经存在的数据的哈希值都不相同，此时 key1 - value1 添加成功。—&gt; 情况 2 如果 key1 的哈希值和已经存在的某一个数据 (key2 - value2) 的哈希值相同，则调用 key1 所在类的 equals(key2)，继续比较： 如果 equals() 返回 false：此时 key1 - value1 添加成功。—&gt; 情况 3 如果 equals() 返回 true：使用 value1 替换 value2。 补充：关于情况 2 和情况 3，此时 key1 - value1 和原来的数据以链表的方式存储。 存储结构：HashMap是数组+链表结构 HashMap 的内部存储结构其实是数组和链表的结合 (即为链地址法)。当实例化一个 HashMap 时，系统会创建一个长度为 Capacity 的 Entry 数组，这个长度在哈希表中被称为容量 (Capacity)，在这个数组中可以存放元素的位置我们称之为 “桶” (bucket)，每个 bucket 都有自己的索引，系统可以根据索引快速的查找 bucket 中的元素。 每个 bucket 中存储一个元素，即一个 Entry 对象，但每一个 Entry 对象可以带一个引用变量，用于指向下一个元素，因此，在一个桶中，就有可能生成一个 Entry 链，而且新添加的元素是整个链表的 head。 结构图示意： 扩容过程： 当 HashMap 中的元素越来越多的时候，hash 冲突的几率也就越来越高，因为底层数组的长度是固定的。所以为了提高查询的效率，就要对 HashMap 的底层数组进行扩容，而在 HashMap 数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是 resize()。 当 HashMap 中的元素个数超过数组大小 (数组总大小 length，不是数组中存储的元素个数 size) * loadFactor 时 ， 就 会 进 行 数 组 扩 容 。loadFactor 的 默 认 值为 0.75，这是一个折中的取值，默认情况下，数组大小为 16，那么当 HashMap 中元素个数 ≥ 16 * 0.75 = 12 (这个值就是代码中的 threshold 值，也叫做临界值) 且要存放的位置非空的时候，就把数组的大小扩展为 2 * 16 = 32，即扩大一倍，然后重新计算每个元素在数组中的位置，把原有的数据复制到新数组中。 扩容是一个非常消耗性能的操作，如果已经预知 HashMap 中元素的个数，那么预设元素的个数能够有效的提高 HashMap 的性能。 JDK 8.0： 初始化操作，以无参构造器为例：HashMap hashMap = new HashMap();，在实例化时，底层没有创建一个长度为 16 的数组，只是给加载因子赋值 0.75： 123public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125; 底层的数组是 Node[]，而非 Entry[]，但 Node 实现了 Entry 接口： 1transient Node&lt;K,V&gt;[] table; 1static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123;&#125; 首次调用 put() 方法时： 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) // 第一次进入put()，table还未初始化，为null，进入resize()，如果不是第一次put()，不会进入此逻辑 n = (tab = resize()).length; // 查看当前元素在新创建的数组中的位置i所在的位置的元素p，是否为null if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null);// 如果p为null，当前位置i没有元素，添加成功 ---&gt; 情况1 else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p;// 位置i上的元素，与当前待添加元素的key相同 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123;// 位置i上的元素，与当前待添加元素的key不同 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123;// 位置i上只有一个元素 // 位置i上的原元素指向当前待添加的元素，&quot;八下&quot;，添加成功 ---&gt; 情况2和3 p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 链表的长度超过8时，判断是否转为红黑树结构 treeifyBin(tab, hash); break; &#125; // 位置i上不止一个元素，依次获得该链表中的每一个元素，与待添加元素的key，对比hash值和equals() if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 替换操作 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 从 put() 第一次进入 resize()，底层创建了长度为 16 的 Node 数组： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table;// 从put()第一次进入resize()，table为null int oldCap = (oldTab == null) ? 0 : oldTab.length;// 0 int oldThr = threshold;// 此时扩容的临界值为0 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;// 默认数组长度16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);// 默认扩容的临界值：12 &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr;// 赋值扩容的临界值 @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];// 创建一个长度为16的Node数组 table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 计算 key 的 hash 值： 1234 static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 链表转红黑树： 1234567891011121314151617181920 final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize();// 如果底层数组的长度小玉64，只扩容，不转红黑树 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; 总结，JDK 8.0 相较于 JDK 7.0 在底层实现方面的不同： new HashMap() 时，底层没有创建一个长度为 16 的数组。 JDK 8.0 底层的数组是 Node[]，而非 Entry[]。 首次调用 put() 时，底层创建长度为 16 的数组。 JDK 7.0 底层结构只有：数组 + 链表。JDK 8.0 中底层结构是：数组 + 链表 + 红黑树。 形成链表时，”七上八下”： JDK 7.0 中新的元素指向旧的元素，JDK 8.0 中旧的元素指向新的元素。 当数组的某一个索引位置上的元素以链表形式存在的数据个数 &gt; 8 且当前数组的长度 &gt; 64时，此时此索引位置上的所数据改为使用红黑树存储。 存储结构： HashMap 的内部存储结构其实是数组+ 链表 + 树的结合。当实例化一个 HashMap 时，会初始化 initialCapacity 和 loadFactor，在 put 第一对映射关系时，系统会创建一个长度为 initialCapacity 的 Node 数组，这个长度在哈希表中被称为容量 (Capacity)，在这个数组中可以存放元素的位置我们称之为 “桶” (bucket)，每个 bucket 都有自己的索引，系统可以根据索引快速的查找 bucket 中的元素。 每个 bucket 中存储一个元素，即一个 Node 对象，但每一个 Node 对象可以带一个引用变量 next，用于指向下一个元素，因此，在一个桶中，就有可能生成一个 Node 链。也可能是一个一个 TreeNode 对象，每一个 TreeNode 对象可以有两个叶子结点 left 和 right，因此，在一个桶中，就有可能生成一个 TreeNode 树。而新添加的元素作为链表的 last，或树的叶子结点。 结构图示意： 扩容过程： 扩容过程与 JDK 7.0 相同。 树形化：当 HashMap 中的其中一个链的对象个数如果达到了 8 个，此时如果 capacity 没有达到 64，那么 HashMap 会先扩容解决，如果已经达到了 64，那么这个链会变成树，结点类型由 Node 变成 TreeNode 类型。当然，如果当映射关系被移除后，下次 resize() 时判断树的结点个数低于 6 个，也会把树再转为链表。 面试题 谈谈你对 HashMap 中 put() 和 get() 的认识？如果了解再谈谈 HashMap 的扩容机制？默认大小是多少？什么是负载因子 (或填充比)？什么是吞吐临界值 (或阈值、threshold)？ 负载因子值的大小，对 HashMap 有什么影响？ 负载因子的大小决定了 HashMap 的数据密度。 负载因子越大，数据密度越大，发生碰撞的几率越高，数组中的链表越容易长，造成查询或插入时的比较次数增多，性能会下降。 负载因子越小，就越容易触发扩容，数据密度也越小，意味着发生碰撞的几率越小，数组中的链表也就越短，查询和插入时比较的次数也越小，性能会更高。但是会浪费一定的内容空间，而且经常扩容也会影响性能，建议初始化预设大一点的空间。 按照其他语言的参考及研究经验，会考虑将负载因子设置为 0.7 ~ 0.75，此时平均检索长度接近于常数。 LinkedHashMap LinkedHashMap 是 HashMap 的子类。 在 HashMap 存储结构的基础上，使用了一对双向链表来记录添加元素的顺序。 HashMap 中的内部类 Node： 123456static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next;&#125; LinkedHashMap 中的内部类 Entry，用以替换 Node： 123456static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; LinkedHashMap 在原有的 HashMap 底层结构基础上，添加了一对指针 befor 和 after，指向当前元素的前一个和后一个元素。 与 LinkedHashSet 类似，LinkedHashMap 可以维护 Map 的迭代顺序：迭代顺序与 key - value 对的插入顺序一致。 LinkedHashMap 在遍历元素时，可以按照添加的顺序实现遍历。 实例： 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); map.put(123, &quot;AA&quot;); map.put(345, &quot;BB&quot;); map.put(12, &quot;CC&quot;); System.out.println(map);// &#123;345=BB, 123=AA, 12=CC&#125; map = new LinkedHashMap(); map.put(123, &quot;AA&quot;); map.put(345, &quot;BB&quot;); map.put(12, &quot;CC&quot;); System.out.println(map);// &#123;123=AA, 345=BB, 12=CC&#125; &#125;&#125; 对于频繁的遍历操作，此类执行效率高于 HashMap。 TreeMap TreeMap 存储 key - value 对时，需要根据 key - value 对进行排序。TreeMap 可以保证所有的 key - value 对处于有序状态。 TreeMap 底层使用红黑树结构存储数据。 TreeMap 的 key 的排序： 自然排序：TreeMap 的所有的 key 应该是同一个类的对象，否则将会抛出 ClasssCastException 异常，同时，key 所在的类需要实现 Comparable 接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106public class Test &#123; public static void main(String[] args) &#123; TreeMap map = new TreeMap(); User u1 = new User(&quot;Tom&quot;, 23); User u2 = new User(&quot;Jerry&quot;, 32); User u3 = new User(&quot;Jack&quot;, 20); User u4 = new User(&quot;Rose&quot;, 18); map.put(u1, 98); map.put(u2, 89); map.put(u3, 76); map.put(u4, 100); Set entrySet = map.entrySet(); Iterator iterator1 = entrySet.iterator(); while (iterator1.hasNext()) &#123; Object obj = iterator1.next(); Map.Entry entry = (Map.Entry) obj; System.out.println(entry.getKey() + &quot;----&gt;&quot; + entry.getValue()); &#125; &#125;&#125;class User implements Comparable &#123; private String name; private int age; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; System.out.println(&quot;User equals()....&quot;); if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (age != user.age) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; //return name.hashCode() + age; int result = name != null ? name.hashCode() : 0; result = 31 * result + age; return result; &#125; // 按照姓名从大到小排列，年龄从小到大排列 @Override public int compareTo(Object o) &#123; if (o instanceof User) &#123; User user = (User) o; // return -this.name.compareTo(user.name); int compare = -this.name.compareTo(user.name); if (compare != 0) &#123; return compare; &#125; else &#123; return Integer.compare(this.age, user.age); &#125; &#125; else &#123; throw new RuntimeException(&quot;输入的类型不匹配&quot;); &#125; &#125;&#125;输出结果：User&#123;name=&#x27;Tom&#x27;, age=23&#125;----&gt;98User&#123;name=&#x27;Rose&#x27;, age=18&#125;----&gt;100User&#123;name=&#x27;Jerry&#x27;, age=32&#125;----&gt;89User&#123;name=&#x27;Jack&#x27;, age=20&#125;----&gt;76 定制排序：创建 TreeMap 时，传入一个 Comparator 对象，该对象负责对 TreeMap 中的所有 key 进行排序。此时不需要 Map 的 key 实现 Comparable 接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public class Test &#123; public static void main(String[] args) &#123; // 按照年龄从小到大排序 TreeMap map = new TreeMap(new Comparator() &#123; @Override public int compare(Object o1, Object o2) &#123; if (o1 instanceof User &amp;&amp; o2 instanceof User) &#123; User u1 = (User) o1; User u2 = (User) o2; return Integer.compare(u1.getAge(), u2.getAge()); &#125; throw new RuntimeException(&quot;输入的类型不匹配！&quot;); &#125; &#125;); User u1 = new User(&quot;Tom&quot;, 23); User u2 = new User(&quot;Jerry&quot;, 32); User u3 = new User(&quot;Jack&quot;, 20); User u4 = new User(&quot;Rose&quot;, 18); map.put(u1, 98); map.put(u2, 89); map.put(u3, 76); map.put(u4, 100); Set entrySet = map.entrySet(); Iterator iterator1 = entrySet.iterator(); while (iterator1.hasNext()) &#123; Object obj = iterator1.next(); Map.Entry entry = (Map.Entry) obj; System.out.println(entry.getKey() + &quot;----&gt;&quot; + entry.getValue()); &#125; &#125;&#125;class User implements Comparable &#123; private String name; private int age; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; System.out.println(&quot;User equals()....&quot;); if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (age != user.age) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; //return name.hashCode() + age; int result = name != null ? name.hashCode() : 0; result = 31 * result + age; return result; &#125; // 按照姓名从大到小排列，年龄从小到大排列 @Override public int compareTo(Object o) &#123; if (o instanceof User) &#123; User user = (User) o; // return -this.name.compareTo(user.name); int compare = -this.name.compareTo(user.name); if (compare != 0) &#123; return compare; &#125; else &#123; return Integer.compare(this.age, user.age); &#125; &#125; else &#123; throw new RuntimeException(&quot;输入的类型不匹配&quot;); &#125; &#125;&#125;输出结果：User&#123;name=&#x27;Rose&#x27;, age=18&#125;----&gt;100User&#123;name=&#x27;Jack&#x27;, age=20&#125;----&gt;76User&#123;name=&#x27;Tom&#x27;, age=23&#125;----&gt;98User&#123;name=&#x27;Jerry&#x27;, age=32&#125;----&gt;89 TreeMap 判断两个 key 相等的标准：两个 key 通过 compareTo() 或者 compare() 返回 0。 Hashtable Hashtable 是个古老的 Map 实现类，JDK 1.0 就提供了。不同于 HashMap，Hashtable 是线程安全的，但效率低。 Hashtable 实现原理和 HashMap 相同，功能相同。底层都使用哈希表结构，查询速度快，很多情况下可以互用。 与 HashMap 不同，Hashtable 不允许使用 null 作为 key 和 value。 实例： 1234567891011public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); map.put(null, 123); map.put(123, null); map.put(null, null); System.out.println(map);// &#123;null=null, 123=null&#125; map = new Hashtable(); map.put(null, 123);// java.lang.NullPointerException &#125;&#125; 与 HashMap 一样，Hashtable 也不能保证其中 key - value 对的顺序。 Hashtable 判断两个 key 相等、两个 value 相等的标准，与 HashMap 一致。 Properties Properties 类是 Hashtable 的子类，常用于处理配置文件。 由于属性文件里的 key、value 都是字符串类型，所以 Properties 里的 key 和 value 都是字符串类型。 存取数据时，建议使用 setProperty(String key, String value) 和 getProperty(String key)。 public Object setProperty(String key, String value) ： 保存一对属性。 public String getProperty(String key) ：使用此属性列表中指定的键搜索属性值。 public Set&lt;String&gt; stringPropertyNames() ：所有键的名称的集合。 12345678910111213141516171819202122232425public class ProDemo &#123; public static void main(String[] args) &#123; // 创建属性集对象 Properties properties = new Properties(); // 添加键值对元素 properties.setProperty(&quot;filename&quot;, &quot;a.txt&quot;); properties.setProperty(&quot;length&quot;, &quot;209385038&quot;); properties.setProperty(&quot;location&quot;, &quot;D:\\\\a.txt&quot;); // 打印属性集对象 System.out.println(properties); // 通过键，获取属性值 System.out.println(properties.getProperty(&quot;filename&quot;)); System.out.println(properties.getProperty(&quot;length&quot;)); System.out.println(properties.getProperty(&quot;location&quot;)); // 遍历属性集，获取所有键的集合 Set&lt;String&gt; strings = properties.stringPropertyNames(); // 打印键值对 for (String key : strings) &#123; System.out.println(key + &quot; -- &quot; + properties.getProperty(key)); &#125; &#125;&#125; 与流有关的方法： public void load(InputStream inStream)： 从字节输入流中读取键值对。 public void load(Reader reader)：从字符输入流中读取键值对。 public void store(Writer writer,String comments) public void store(OutputStream out,String comments) 实例： 123456789101112Properties pros = new Properties();try (FileInputStream fis = new FileInputStream(&quot;jdbc.properties&quot;)) &#123; // 加载流对应的文件 pros.load(fis); // 遍历集合并打印 Set&lt;String&gt; strings = properties.stringPropertyNames(); for (String key : strings) &#123; System.out.println(key + &quot; -- &quot; + properties.getProperty(key)); &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; jdbc.properties 格式，以 = 连接，不要有空格： 12user=Tompassword=123qwe Collections 工具类 Collections 是一个操作 Set、List 和 Map 等集合的工具类。 操作数组的工具类：Arrays。 Collections 中提供了一系列静态的方法对集合元素进行排序、查询和修改等操作，还提供了对集合对象设置不可变、对集合对象实现同步控制等方法。 常用方法： 排序操作： reverse(List list)：反转 list 中元素的顺序。 shuffle(List list)：对 list 集合元素进行随机排序。 sort(List list)：根据元素的自然顺序对指定 list 集合元素按升序排序 (自然排序)。 sort(List list, Comparator comparator)：根据指定的 comparator 产生的顺序对 list 集合元素进行排序 (定制排序)。 swap(List list, int i, int j)：将指定 list 集合中的 i 处元素和 j 处元素进行交换。 实例： 1234567891011121314151617181920public class Test &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(765); list.add(765); list.add(-97); list.add(0); System.out.println(list);// [123, 43, 765, 765, 765, -97, 0] // Collections.reverse(list);// [0, -97, 765, 765, 765, 43, 123] // Collections.shuffle(list);// [765, 43, 123, 0, -97, 765, 765] // Collections.sort(list);// [-97, 0, 43, 123, 765, 765, 765] // Collections.swap(list, 1, 2);// [123, 765, 43, 765, 765, -97, 0] System.out.println(list); &#125;&#125; 查找和替换操作： Object max(Collection coll)：根据元素的自然顺序，返回给定集合 coll 中的最大元素 (自然排序)。 Object max(Collection coll, Comparator )：根据 comparator 指定的顺序，返回给定集合 coll 中的最大元素 (定制排序)。 Object min(Collection)：根据元素的自然顺序，返回给定集合 coll 中的最小元素 (自然排序)。 Object min(Collection，Comparator)：根据 comparator 指定的顺序，返回给定集合 coll 中的最小元素 (定制排序)。 int frequency(Collection coll, Object obj)：返回指定集合 coll 中指定元素 obj 出现的次数。 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(765); list.add(765); list.add(-97); list.add(0); int frequency = Collections.frequency(list, 123); System.out.println(frequency);// 1 &#125;&#125; void copy(List dest, List src)：将 src 中的内容复制到 dest 中。注意：需要将dest数组中填充元素，数量不低于src的长度。 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; List src = new ArrayList(); src.add(123); src.add(43); src.add(765); src.add(-97); src.add(0); // 错误的写法：java.lang.IndexOutOfBoundsException: Source does not fit in dest /*List dest = new ArrayList(); List dest = new ArrayList(src.size()); Collections.copy(dest, src);*/ // 正确的写法：需要将dest数组中填充元素，数量不低于src的长度 List dest = Arrays.asList(new Object[src.size()]);// 1.先填充dest集合 System.out.println(dest.size());// = list.size(); Collections.copy(dest, src);// 2.再复制src集合 System.out.println(dest);// [123, 43, 765, -97, 0] &#125;&#125; boolean replaceAll(List list, Object oldVal, Object newVal)：使用新值 newVal 替换 list 对象的所有旧值 oldVal。 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(765); list.add(765); list.add(-97); list.add(0); System.out.println(list);// [123, 43, 765, 765, 765, -97, 0] Collections.replaceAll(list, 765, 888); System.out.println(list);// [123, 43, 888, 888, 888, -97, 0] &#125;&#125; 同步控制操作： Collections 类中提供了多个 synchronizedXxx()，该方法可使将指定集合包装成线程同步的集合，从而可以解决多线程并发访问集合时的线程安全问题。有了这些方法，涉及到线程安全的集合，可以不需要使用 Vector 或 Hashtable。 12345678910111213public class Test &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(-97); list.add(0); // 返回的list1即为线程安全的List List list1 = Collections.synchronizedList(list); &#125;&#125; 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的枚举类和注解","slug":"java-enumandannonation","date":"2021-03-18T13:18:56.000Z","updated":"2021-04-09T08:00:30.844Z","comments":true,"path":"2021/03/18/java-enumandannonation/","link":"","permalink":"http://example.com/2021/03/18/java-enumandannonation/","excerpt":"","text":"枚举类 (Enum) 的使用枚举类的理解： 类的对象只有有限个，确定的，则此类称为枚举类。 当需要定义一组常量时，强烈建议使用枚举类。 如果枚举类中只有一个对象，则可以作为单例模式的实现方式。 jdk 5.0 之后，可以在 switch 表达式中使用 Enum 定义的枚举类的对象作为表达式，case 子句可以直接使用枚举值的名字，无需添加枚举类作为限定。 如何定义枚举类： 方式一：jdk 5.0 之前需要自定义枚举类。 私有化类的构造器，保证不能在类的外部创建其对象。 在类的内部创建枚举类的实例。声明为：public static final。 对象如果有实例变量，应该声明为 private final，并在构造器中初始化。 123456789101112131415161718192021222324252627282930313233343536373839404142public class Test &#123; public static void main(String[] args) &#123; System.out.println(Season.SPRING);// Season&#123;SEASONNAME=&#x27;春天&#x27;, SEASONDESC=&#x27;春暖花开&#x27;&#125; &#125;&#125;// 自定义枚举类class Season &#123; // 1.声明Season对象的属性：private final 修饰 --- 常量 private final String SEASONNAME;//季节的名称 private final String SEASONDESC;//季节的描述 // 2.私有化类的构造器，并给对象的属性赋值 private Season(String seasonName, String seasonDesc) &#123; this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; &#125; // 3.提供当前枚举类的多个对象 public static final Season SPRING = new Season(&quot;春天&quot;, &quot;春暖花开&quot;); public static final Season SUMMER = new Season(&quot;夏天&quot;, &quot;夏日炎炎&quot;); public static final Season AUTUMN = new Season(&quot;秋天&quot;, &quot;秋高气爽&quot;); public static final Season WINTER = new Season(&quot;冬天&quot;, &quot;白雪皑皑&quot;); // 4.其他诉求一：获取枚举类对象的属性 public String getSEASONNAME() &#123; return SEASONNAME; &#125; public String getSEASONDESC() &#123; return SEASONDESC; &#125; // 5.其他诉求二：提供toString() @Override public String toString() &#123; return &quot;Season&#123;&quot; + &quot;SEASONNAME=&#x27;&quot; + SEASONNAME + &#x27;\\&#x27;&#x27; + &quot;, SEASONDESC=&#x27;&quot; + SEASONDESC + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 方式二：jdk 5.0 之后，可以使用新增的 enum 关键字定义枚举类。 使用 enum 关键字定义的枚举类默认继承了 java.lang.Enum 类，因此不能再继承其他类。 枚举类的构造器只能使用 private 权限修饰符。 枚举类的所有实例必须在枚举类中显式列出，以 “,” 分隔，以 “;” 结尾。列出的实例系统会默认添加 public static final 修饰。 必须在枚举类的第一行声明枚举类对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Test &#123; public static void main(String[] args) &#123; System.out.println(Season.SPRING);// SPRING System.out.println(&quot;enum的父类是：&quot; + Season.class.getSuperclass());// class java.lang.Enum &#125;&#125;// 使用enum关键字定义枚举类enum Season &#123; // 1.首行提供当前枚举类的多个对象: 多个对象之间以&quot;,&quot;隔开，末尾的对象以&quot;;&quot;结束 SPRING(&quot;春天&quot;, &quot;春暖花开&quot;), SUMMER(&quot;夏天&quot;, &quot;夏日炎炎&quot;), AUTUMN(&quot;秋天&quot;, &quot;秋高气爽&quot;), WINTER(&quot;冬天&quot;, &quot;白雪皑皑&quot;); // 2.声明Season对象的属性：private final 修饰 --- 常量 private final String SEASONNAME;//季节的名称 private final String SEASONDESC;//季节的描述 // 3.私有化类的构造器，并给对象的属性赋值 private Season(String seasonName, String seasonDesc) &#123; this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; &#125; // 4.其他诉求一：获取枚举类对象的属性 public String getSEASONNAME() &#123; return SEASONNAME; &#125; public String getSEASONDESC() &#123; return SEASONDESC; &#125; // 5.其他诉求二：提供toString()，一般不重写 /*@Override public String toString() &#123; return &quot;Season&#123;&quot; + &quot;SEASONNAME=&#x27;&quot; + SEASONNAME + &#x27;\\&#x27;&#x27; + &quot;, SEASONDESC=&#x27;&quot; + SEASONDESC + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;*/&#125; Enum 类中的常用方法： values()：返回枚举类型的对象数组，该方法可以很方便地遍历枚举类的所有的枚举值。 12345678public class Test &#123; public static void main(String[] args) &#123; Thread.State[] values = Thread.State.values(); for (Thread.State value : values) &#123; System.out.print(value + &quot; &quot;);// NEW RUNNABLE BLOCKED WAITING TIMED_WAITING TERMINATED &#125; &#125;&#125; valueOf(String str)：可以把一个字符串转为对应的枚举类对象。要求字符串必须是枚举类对象的 “名字”，如不是，会抛出运行时异常：java.lang.IllegalArgumentException。 12345678public class Test &#123; public static void main(String[] args) &#123; Thread.State aNew = Thread.State.valueOf(&quot;NEW&quot;); System.out.println(aNew);// NEW Thread.State dead = Thread.State.valueOf(&quot;DEAD&quot;); System.out.println(dead);// java.lang.IllegalArgumentException: No enum constant java.lang.Thread.State.DEAD &#125;&#125; toString()：返回当前枚举类对象常量的名称。 使用 enum 关键字定义的枚举类实现接口的情况： 和普通 java 类一样，枚举类可以实现一个或多个接口。 若每个枚举值在调用实现的接口的方法时，如果呈现出相同的行为方式，则只要统一实现该方法即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Test &#123; public static void main(String[] args) &#123; Season spring = Season.SPRING; spring.show();// 这是一个季节 &#125;&#125;interface Info &#123; void show();&#125;// 使用enum关键字定义枚举类enum Season implements Info &#123; // 1.首行提供当前枚举类的多个对象: 多个对象之间以&quot;,&quot;隔开，末尾的对象以&quot;;&quot;结束 SPRING(&quot;春天&quot;, &quot;春暖花开&quot;), SUMMER(&quot;夏天&quot;, &quot;夏日炎炎&quot;), AUTUMN(&quot;秋天&quot;, &quot;秋高气爽&quot;), WINTER(&quot;冬天&quot;, &quot;白雪皑皑&quot;); // 2.声明Season对象的属性：private final 修饰 --- 常量 private final String SEASONNAME;//季节的名称 private final String SEASONDESC;//季节的描述 // 3.私有化类的构造器，并给对象的属性赋值 private Season(String seasonName, String seasonDesc) &#123; this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; &#125; // 4.其他诉求一：获取枚举类对象的属性 public String getSEASONNAME() &#123; return SEASONNAME; &#125; public String getSEASONDESC() &#123; return SEASONDESC; &#125; // 每个枚举值在调用实现的接口的方法时，呈现出相同的行为方式 @Override public void show() &#123; System.out.println(&quot;这是一个季节&quot;); &#125; // 5.其他诉求二：提供toString()，一般不重写 /*@Override public String toString() &#123; return &quot;Season&#123;&quot; + &quot;SEASONNAME=&#x27;&quot; + SEASONNAME + &#x27;\\&#x27;&#x27; + &quot;, SEASONDESC=&#x27;&quot; + SEASONDESC + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;*/&#125; 若需要每个枚举值在调用实现的接口的方法时，呈现出不同的行为方式，则可以让每个枚举值分别来实现该方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class Test &#123; public static void main(String[] args) &#123; Season spring = Season.SPRING; spring.show();// 现在是春天 Season winter = Season.WINTER; winter.show();// 现在是冬天 &#125;&#125;interface Info &#123; void show();&#125;// 使用enum关键字定义枚举类enum Season implements Info &#123; // 1.首行提供当前枚举类的多个对象: 多个对象之间以&quot;,&quot;隔开，末尾的对象以&quot;;&quot;结束 SPRING(&quot;春天&quot;, &quot;春暖花开&quot;) &#123; @Override public void show() &#123; System.out.println(&quot;现在是春天&quot;); &#125; &#125;, SUMMER(&quot;夏天&quot;, &quot;夏日炎炎&quot;) &#123; @Override public void show() &#123; System.out.println(&quot;现在是夏天&quot;); &#125; &#125;, AUTUMN(&quot;秋天&quot;, &quot;秋高气爽&quot;) &#123; @Override public void show() &#123; System.out.println(&quot;现在是秋天&quot;); &#125; &#125;, WINTER(&quot;冬天&quot;, &quot;白雪皑皑&quot;) &#123; @Override public void show() &#123; System.out.println(&quot;现在是冬天&quot;); &#125; &#125;; // 2.声明Season对象的属性：private final 修饰 --- 常量 private final String SEASONNAME;//季节的名称 private final String SEASONDESC;//季节的描述 // 3.私有化类的构造器，并给对象的属性赋值 private Season(String seasonName, String seasonDesc) &#123; this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; &#125; // 4.其他诉求一：获取枚举类对象的属性 public String getSEASONNAME() &#123; return SEASONNAME; &#125; public String getSEASONDESC() &#123; return SEASONDESC; &#125; // 5.其他诉求二：提供toString()，一般不重写 /*@Override public String toString() &#123; return &quot;Season&#123;&quot; + &quot;SEASONNAME=&#x27;&quot; + SEASONNAME + &#x27;\\&#x27;&#x27; + &quot;, SEASONDESC=&#x27;&quot; + SEASONDESC + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;*/&#125; 注解 (Annotation) 的使用注解的概述： 从 jdk 5.0 开始，java 增加了对元数据 (MetaData) 的支持，也就是 Annotation (注解)。 Annotation 其实就是代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取，并执行相应的处理。通过使用 Annotation，程序员可以在不改变原有逻辑的情况下，在源文件中嵌入一些补充信息。代码分析工具、开发工具和部署工具可以通过这些补充信息进行验证或者进行部署。 Annotation 可以像修饰符一样被使用，可用于修饰包、类、构造器、方法、 成员变量、参数、局部变量的声明，这些信息被保存在 Annotation 的 “name = value” 对中。 在 JavaSE 中，注解的使用目的比较简单，例如标记过时的功能，忽略警告等。在 JavaEE/Android 中注解占据了更重要的角色，例如用来配置应用程序的任何切面，代替 JavaEE 旧版中所遗留的繁冗代码和 XML 配置等。 未来的开发模式都是基于注解的，JPA 是基于注解的，Spring 2.5 以上都是基于注解的，Hibernate 3.x 以后也是基于注解的，现在的 Struts2 有一部分也是基于注解的。注解是一种趋势，一定程度上可以说：框架 = 注解 + 反射 + 设计模式。 常见的 Annotation 示例： 使用 Annotation 时要在其前面增加 @ 符号，并把该 Annotation 当成一个修饰符使用，用于修饰它支持的程序元素。 示例一：生成文档相关的注解。 @author：标明开发该类模块的作者，多个作者之间使用 “,”分割。 @version：标明该类模块的版本。 @see：参考转向，也就是相关主题。 @since：从哪个版本开始增加的。 @param：对方法中某参数的说明，如果没有参数就不能写。 @return：对方法返回值的说明，如果方法的返回值类型是 void 就不能写。 @exception：对方法可能抛出的异常进行说明 ，如果方法没有用 throws 显式抛出的异常就不能写其中。 @param、@return 和 @exception 这三个标记都是只用于方法的。 @param 的格式要求：@param 形参名 形参类型 形参说明。 @return 的格式要求：@return 返回值类型 返回值说明。 @exception 的格式要求：@exception 异常类型 异常说明。 @param 和 @exception 可以并列多个。 实例： 示例二： 在编译时进行格式查 (jdk 内置的三个基本注解)。 @Override：限定重写父类方法，该注解只能用于方法。 @Deprecated：用于表示所修饰的元素 (类，方法等) 已过时，通常是因为所修饰的结构危险或存在更好的选择。 @SuppressWarnings：抑制编译器警告。 实例： 示例三： 跟踪代码依赖性，实现替代配置文件功能。 Servlet 3.0 提供了注解 (annotation)，使得不再需要在 web.xml 文件中进行 Servlet 的部署： spring 框架中关于事务的管理： 自定义注解：参照 @SuppressWarnings定义。 注解声明为：@interface。 自定义注解自动继承了 java.lang.annotation.Annotation 接口。 Annotation 的成员变量在 Annotation 定义中以无参数方法的形式来声明。其方法名和返回值定义了该成员的名字和类型，我们称为配置参数。类型只能是八种基本数据类型、String 类型 、Class 类型 、Enum 类型 、Annotation 类型，以上所有类型的数组。 可以在定义 Annotation 的成员变量时为其指定初始值，指定成员变量的初始值可使用 default 关键字。 没有成员定义的 Annotation 称为标记 (是一个标识作用)；包含成员变量的 Annotation 称为元数据 Annotation。 如果注解只有一个成员，建议使用参数名为 value。 如果注解有成员，那么使用时必须指定参数值，除非它有默认值。格式是 “参数名 = 参数值”，如果只有一个参数成员，且名称为 value，可以省略 “value=”，直接写参数值。 自定义注解必须配上注解的信息处理流程才有意义 (使用反射，能够得到注解的内容，然后根据内容和注解的对象建立关系，比如 Servlet 的注解)。 实例： 123public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125; jdk 中的元注解： jdk 的元注解是用于修饰其他注解而定义的，即对现有注解进行解释说明的注解。 元数据：对数据进行修饰的数据。比如：String name = &quot;Tom&quot;;，Tom 是数据，而 String 和 name 就是修饰 Tom 的元数据。 jdk 5.0 提供了 4 个标准的 meta-annotation 类型，分别是： @Retention：只能用于修饰一个 Annotation 定义，用于指定该 Annotation 的生命周期。@Rentention 包含一个 RetentionPolicy 类型的成员变量，使用 @Rentention 时必须为该 value 成员变量指定值： 12345678910@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Retention &#123; /** * Returns the retention policy. * @return the retention policy */ RetentionPolicy value();&#125; 123456789101112131415161718192021public enum RetentionPolicy &#123; /** * Annotations are to be discarded by the compiler. */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior. */ CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. * * @see java.lang.reflect.AnnotatedElement */ RUNTIME&#125; RetentionPolicy.SOURCE：在源文件中有效，即源文件保留，编译器直接丢弃这种策略的注释。 RetentionPolicy.CLASS：在 class 文件 中有效，即 class 保留，当运行 java 程序时，JVM 不会保留注解。这是默认值。 RetentionPolicy.RUNTIME：在运行时有效，即运行时保留，当运行 java 程序时，JVM 会保留注释。程序可以通过反射获取该注释。 只有声明为 RUNTIME 生命周期的注解，才能通过反射获取。 实例： 1234@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125; @Target：用于修饰 Annotation 定义，用于指定被修饰的 Annotation 能用于修饰哪些程序元素。 @Target 也包含一个名为 value 的成员变量。 123456789101112@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Target &#123; /** * Returns an array of the kinds of elements an annotation type * can be applied to. * @return an array of the kinds of elements an annotation type * can be applied to */ ElementType[] value();&#125; 123456789101112131415161718192021222324252627282930313233343536373839public enum ElementType &#123; /** Class, interface (including annotation type), or enum declaration */ TYPE, /** Field declaration (includes enum constants) */ FIELD, /** Method declaration */ METHOD, /** Formal parameter declaration */ PARAMETER, /** Constructor declaration */ CONSTRUCTOR, /** Local variable declaration */ LOCAL_VARIABLE, /** Annotation type declaration */ ANNOTATION_TYPE, /** Package declaration */ PACKAGE, /** * Type parameter declaration * * @since 1.8 */ TYPE_PARAMETER, /** * Use of a type * * @since 1.8 */ TYPE_USE&#125; 各取值含义如下： 实例： 12345678910@Target(ElementType.TYPE)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125;@MyAnnotation(&quot;修改了默认值&quot;)class Person &#123; public Person() &#123; &#125;&#125; @Documented：用于指定被该元 Annotation 修饰的 Annotation 类将被 javadoc 工具提取成文档。默认情况下，javadoc 是不包括注解的。如果希望一个注解在被 javadoc 解析生成文档时能保存下来，需要添加此注解。 12345@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Documented &#123;&#125; 定义为 @Documented 的注解，必须设置 @Retention 值为 RUNTIME。 @Inherited：被它修饰的 Annotation 将具有继承性。如果某个类使用了被 @Inherited 修饰的 Annotation，则其子类将自动具有该注解。 比如：如果把标有 @Inherited 注解的自定义的注解标注在类级别上，子类则可以继承父类类级别的注解。 实际应用中，使用较少。 自定义注解时，通常都会指明 @Retention 和 @Target 这两个注解。 利用反射获取注解信息： jdk 5.0 在 java.lang.reflect 包下新增了 AnnotatedElement 接口，该接口代表程序中可以接受注解的程序元素。 当一个 Annotation 类型被定义为运行时 Annotation 后，该注解才是运行时可见，当 class 文件被载入时保存在 class 文件中的 Annotation 才会被虚拟机读取。 程序可以调用 AnnotatedElement 对象的如下方法来访问 Annotation 信息： 实例： 1234567891011121314151617181920212223242526@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125;@MyAnnotation(&quot;修改了默认值&quot;)class Person &#123; public Person() &#123; &#125; public static void main(String[] args) &#123; Class&lt;Person&gt; clazz = Person.class; Annotation[] annotations = clazz.getAnnotations(); for (Annotation annotation : annotations) &#123; System.out.println(annotation);// @cn.xisun.java.base.MyAnnotation(value=[修改了默认值]) &#125; Annotation annotation = clazz.getAnnotation(MyAnnotation.class); MyAnnotation myAnnotation = (MyAnnotation) annotation; String[] info = myAnnotation.value(); System.out.println(Arrays.toString(info));// [修改了默认值] &#125;&#125; jdk 8.0 中注解的新特性： java 8.0 对注解处理提供了两点改进：可重复的注解及可用于类型的注解。此外，反射也得到了加强，在 java 8.0 中能够得到方法参数的名称。这会简化标注在方法参数上的注解。 可重复注解： jdk 8.0 之前的写法： 123456789101112131415161718192021222324252627282930313233343536@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125;// 定义新数组，值为需要重复注解对象的数组 @Retention(RetentionPolicy.RUNTIME)@interface MyAnnotations &#123; MyAnnotation[] value();&#125;// jdk 8.0之前的写法：@MyAnnotations(&#123;@MyAnnotation(&quot;注解1&quot;), @MyAnnotation(&quot;注解2&quot;)&#125;)class Person &#123; public Person() &#123; &#125; public static void main(String[] args) &#123; Class&lt;Person&gt; clazz = Person.class; Annotation[] annotations = clazz.getAnnotations(); for (Annotation annotation : annotations) &#123; System.out.println(annotation); &#125; Annotation annotation = clazz.getAnnotation(MyAnnotations.class); MyAnnotations myAnnotation = (MyAnnotations) annotation; MyAnnotation[] info = myAnnotation.value(); System.out.println(Arrays.toString(info)); &#125;&#125;输出结果：@cn.xisun.java.base.MyAnnotations(value=[@cn.xisun.java.base.MyAnnotation(value=[注解1]), @cn.xisun.java.base.MyAnnotation(value=[=注解2])])[@cn.xisun.java.base.MyAnnotation(value=[注解1]), @cn.xisun.java.base.MyAnnotation(value=[注解2])] jdk 8.0 之后的写法：利用 @Repeatable。 12345678910111213141516171819202122232425262728293031323334@Repeatable(MyAnnotations.class)@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125;@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@interface MyAnnotations &#123; MyAnnotation[] value();&#125;@MyAnnotation(&quot;注解1&quot;)@MyAnnotation(&quot;注解2&quot;)class Person &#123; public Person() &#123; &#125; public static void main(String[] args) &#123; Class&lt;Person&gt; clazz = Person.class; Annotation[] annotations = clazz.getAnnotations(); for (Annotation annotation : annotations) &#123; System.out.println(annotation);// @cn.xisun.java.base.MyAnnotation(value=[修改了默认值]) &#125; Annotation annotation = clazz.getAnnotation(MyAnnotations.class); MyAnnotations myAnnotation = (MyAnnotations) annotation; MyAnnotation[] info = myAnnotation.value(); System.out.println(Arrays.toString(info));// [修改了默认值] &#125;&#125; 类型注解： jdk 8.0 之后，关于元注解 @Target 的参数类型 ElementType 枚举值多了两个：TYPE_PARAMETER 和 TYPE_USE。、 ElementType.TYPE_PARAMETER：表示该注解能写在类型变量的声明语句中，如：泛型声明。 123456789public class TestTypeDefine&lt;@TypeDefine() U&gt; &#123; private U u; public &lt;@TypeDefine() T&gt; void test(T t)&#123; &#125;&#125;@Target(&#123;ElementType.TYPE_PARAMETER&#125;)@interface TypeDefine&#123;&#125; ElementType.TYPE_USE：表示该注解能写在使用类型的任何语句中。 1234567891011121314151617181920@Target(ElementType.TYPE_USE)@interface MyAnnotation &#123;&#125;@MyAnnotationpublic class AnnotationTest&lt;U&gt; &#123; @MyAnnotation private String name;// 对属性添加注解 public static &lt;@MyAnnotation T&gt; void method(T t) &#123;&#125;// 对泛型添加注解 public static void test(@MyAnnotation String arg) throws @MyAnnotation Exception &#123;&#125;// 对异常添加注解 public static void main(String[] args) &#123; AnnotationTest&lt;@MyAnnotation String&gt; t = null; int a = (@MyAnnotation int) 2L; @MyAnnotation int b = 10; &#125;&#125; 在 java 8.0 之前，注解只能是在声明的地方所使用，从 java 8.0 开始，注解可以应用在任何地方。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中与数字计算相关的类","slug":"java-math","date":"2021-03-18T03:47:03.000Z","updated":"2021-04-09T08:01:11.261Z","comments":true,"path":"2021/03/18/java-math/","link":"","permalink":"http://example.com/2021/03/18/java-math/","excerpt":"","text":"java.lang.Math 类java.lang.Math 类提供了一系列静态方法用于科学计算。其方法的参数和返回值类型一般为 double 型。常用的方法有： abs()：绝对值。 acos()，asin()，atan()，cos()，sin()，tan()： 三角函数。 sqrt()：平方根。 pow(double a,doble b)：a 的 b 次幂。 log()：自然对数。 exp()：以 e 为底的指数。 max(double a,double b)：较大值。 min(double a,double b)：较小值。 random()：返回 0.0 到 1.0 的随机数。 long round(double a)：double 型数据 a 转换为 long 型 (四舍五入)。 toDegrees(double angrad)：弧度转换角度。 toRadians(double angdeg)：角度转换弧度。 java.math.BigInteger 类与 java.math.BigDecimal 类BigInteger Integer 类作为 int 的包装类，能存储的最大整型值为 2^31 - 1，Long 类也是有限的，最大为 2^63 - 1。如果要表示再大的整数，不管是基本数据类型还是他们的包装类都无能为力，更不用说进行运算了。 java.math 包的 BigInteger 类，可以表示不可变的任意精度的整数。BigInteger 提供所有 java 的基本整数操作符的对应物，并提供 java.lang.Math 的所有相关方法。另外，BigInteger 还提供以下运算：模算术、GCD 计算、质数测试、素数生成、位操作以及一些其他操作。 构造器： BigInteger(String val)：常用字符串构建 BigInteger 对象。 常用方法： public BigInteger abs()：返回此 BigInteger 的绝对值的 BigInteger。 BigInteger add(BigInteger val)：返回其值为 (this + val) 的 BigInteger。 BigInteger subtract(BigInteger val)：返回其值为 (this - val) 的 BigInteger。 BigInteger multiply(BigInteger val)：返回其值为 (this * val) 的 BigInteger。 BigInteger divide(BigInteger val)：返回其值为 (this / val) 的 BigInteger。整数相除只保留整数部分。 BigInteger remainder(BigInteger val)：返回其值为 (this % val) 的 BigInteger。 BigInteger[] divideAndRemainder(BigInteger val)：返回包含 (this / val) 后跟 (this % val) 的两个 BigInteger 的数组。 BigInteger pow(int exponent)：返回其值为 (this^exponent ) 的 BigInteger。 实例： 123456public class Test &#123; public static void main(String[] args) &#123; BigInteger bi = new BigInteger(&quot;12433241123223262154841264166142223&quot;); System.out.println(bi); &#125;&#125; BigDecimal 一般的 Float 类和 Double 类可以用来做科学计算或工程计算，但在商业计算中，要求数字精度比较高，故用到 java.math.BigDecimal 类。 BigDecimal 类支持不可变的、任意精度的有符号十进制定点数。 构造器： public BigDecimal(double val) public BigDecimal(String val) 常用方法： public BigDecimal add(BigDecimal augend)：加。 public BigDecimal subtract(BigDecimal subtrahend)：减。 public BigDecimal multiply(BigDecimal multiplicand)：乘。 public BigDecimal divide(BigDecimal divisor, int scale, int roundingMode)：除。 实例： 123456789public class Test &#123; public static void main(String[] args) &#123; BigDecimal bd = new BigDecimal(&quot;12435.351&quot;); BigDecimal bd2 = new BigDecimal(&quot;11&quot;); // System.out.println(bd.divide(bd2));// 未指定精度，如果除不尽，会报错 System.out.println(bd.divide(bd2, BigDecimal.ROUND_HALF_UP));// 四舍五入 System.out.println(bd.divide(bd2, 15, BigDecimal.ROUND_HALF_UP));// 保留15位小数 &#125;&#125; 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的比较器","slug":"java-compare","date":"2021-03-17T07:54:07.000Z","updated":"2021-04-09T08:00:12.063Z","comments":true,"path":"2021/03/17/java-compare/","link":"","permalink":"http://example.com/2021/03/17/java-compare/","excerpt":"","text":"在 java 中经常会涉及到对象数组等的排序问题，那么就涉及到对象之间的比较问题。 java 实现对象排序的方式有两种： 自然排序：java.lang.Comparable 定制排序：java.util.Comparator java.lang.Comparable — 自然排序 Comparable 接口强行对实现它的每个类的对象进行整体排序，这种排序被称为类的自然排序。 实现 Comparable 接口的类必须实现 compareTo(Object obj)，两个对象通过 compareTo(Object obj) 的返回值来比较大小。 重写 compareTo(Object obj) 的规则：如果当前对象 this 大于形参对象 obj，则返回正整数，如果当前对象 this 小于形参对象 obj，则返回负整数，如果当前对象 this 等于形参对象 obj，则返回零。 实现 Comparable 接口的对象列表或数组，可以通过 Collections.sort() (针对集合)或 Arrays.sort() (针对数组)进行自动排序。实现此接口的对象可以用作有序映射中的键或有序集合中的元素，无需指定比较器。 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; // 集合排序 List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;AA&quot;); list.add(&quot;VV&quot;); list.add(&quot;BB&quot;); list.add(&quot;AC&quot;); list.add(&quot;CC&quot;); list.add(&quot;EE&quot;); list.add(&quot;DE&quot;); for (String str : list) &#123; System.out.print(str + &quot; &quot;);// AA VV BB AC CC EE DE &#125; System.out.println(); Collections.sort(list); for (String str : list) &#123; System.out.print(str + &quot; &quot;);// AA AC BB CC DE EE VV &#125; System.out.println(); // 数组排序 String[] strings = &#123;&quot;AA&quot;, &quot;VV&quot;, &quot;BB&quot;, &quot;AC&quot;, &quot;CC&quot;, &quot;EE&quot;, &quot;DE&quot;&#125;; System.out.println(Arrays.toString(strings));// [AA, VV, BB, AC, CC, EE, DE] Arrays.sort(strings); System.out.println(Arrays.toString(strings));// [AA, AC, BB, CC, DE, EE, VV] &#125;&#125; 对于类 C 的每一个 e1 和 e2 来说，当且仅当 e1.compareTo(e2) == 0 与 e1.equals(e2) 具有相同的 boolean 值时，类 C 的自然排序才叫做与 equals 一致。建议 (虽然不是必需的) 最好使自然排序与 equals 一致。 Comparable 的典型实现：(默认都是从小到大排列的) String 类：按照字符串中字符的 Unicode 值进行比较。 Character 类：按照字符的 Unicode 值来进行比较。 数值类型对应的包装类以及 BigInteger 类、BigDecimal 类：按照它们对应的数值大小进行比较。 Boolean 类：true 对应的包装类实例大于 false 对应的包装类实例。 Date 类、Time 类等：后面的日期时间比前面的日期时间大。 对于自定义类来说，如果需要排序，我们可以让自定义类实现 Comparable 接口，并重写 compareTo(Object obj)，在 compareTo(Object obj) 中，指明如何排序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Test &#123; public static void main(String[] args) &#123; Goods[] arr = new Goods[5]; arr[0] = new Goods(&quot;lenovo&quot;, 34); arr[1] = new Goods(&quot;dell&quot;, 43); arr[2] = new Goods(&quot;xiaomi&quot;, 12); arr[3] = new Goods(&quot;huawei&quot;, 65); arr[4] = new Goods(&quot;microsoft&quot;, 43); System.out.println(&quot;排序前：&quot; + Arrays.toString(arr)); Arrays.sort(arr); System.out.println(&quot;排序后：&quot; + Arrays.toString(arr)); &#125;&#125;class Goods implements Comparable &#123; private String name; private double price; public Goods() &#123; &#125; public Goods(String name, double price) &#123; this.name = name; this.price = price; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; @Override public String toString() &#123; return &quot;Goods&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, price=&quot; + price + &#x27;&#125;&#x27;; &#125; // 先按照价格从低到高进行排序，再按照名称从高到低进行排序 @Override public int compareTo(Object o) &#123; if (o instanceof Goods) &#123; Goods goods = (Goods) o; if (this.price &gt; goods.price) &#123; return 1; &#125; else if (this.price &lt; goods.price) &#123; return -1; &#125; else &#123; return -this.name.compareTo(goods.name); &#125; // return Double.compare(this.getPrice(), goods.getPrice()); &#125; throw new RuntimeException(&quot;传入的数据类型有误&quot;); &#125;&#125; java.util.Comparator — 定制排序 当元素的类型没有实现 java.lang.Comparable 接口而又不方便修改代码，或者实现了 java.lang.Comparable 接口的排序规则不适合当前的操作，那么可以考虑使用 Comparator 的对象来排序，强行对多个对象进行整体排序的比较。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test &#123; public static void main(String[] args) &#123; // 集合排序 List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;AA&quot;); list.add(&quot;VV&quot;); list.add(&quot;BB&quot;); list.add(&quot;AC&quot;); list.add(&quot;CC&quot;); list.add(&quot;EE&quot;); list.add(&quot;DE&quot;); for (String str : list) &#123; System.out.print(str + &quot; &quot;);// AA VV BB AC CC EE DE &#125; System.out.println(); // 不再以String本身默认的从小到大排序，而是从大到小排序 Collections.sort(list, new Comparator&lt;String&gt;() &#123; @Override public int compare(String o1, String o2) &#123; return o2.compareTo(o1); &#125; &#125;); for (String str : list) &#123; System.out.print(str + &quot; &quot;);// VV EE DE CC BB AC AA &#125; System.out.println(); // 数组排序 String[] strings = &#123;&quot;AA&quot;, &quot;VV&quot;, &quot;BB&quot;, &quot;AC&quot;, &quot;CC&quot;, &quot;EE&quot;, &quot;DE&quot;&#125;; System.out.println(Arrays.toString(strings));// [AA, VV, BB, AC, CC, EE, DE] // 不再以String本身默认的从小到大排序，而是从大到小排序 Arrays.sort(strings, new Comparator&lt;String&gt;() &#123; @Override public int compare(String o1, String o2) &#123; return -o1.compareTo(o2); &#125; &#125;); System.out.println(Arrays.toString(strings));// [VV, EE, DE, CC, BB, AC, AA] &#125;&#125; 重写 compare(Object o1,Object o2)，比较 o1 和 o2 的大小： 如果方法返回正整数，则表示 o1 大于 o2；如果返回 0，表示相等；返回负整数，表示 o1 小于 o2。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class Test &#123; public static void main(String[] args) &#123; Goods[] arr = new Goods[5]; arr[0] = new Goods(&quot;lenovo&quot;, 34); arr[1] = new Goods(&quot;dell&quot;, 43); arr[2] = new Goods(&quot;xiaomi&quot;, 12); arr[3] = new Goods(&quot;huawei&quot;, 65); arr[4] = new Goods(&quot;lenovo&quot;, 43); System.out.println(&quot;排序前：&quot; + Arrays.toString(arr)); // 不以Goods本身的自然排序方式排序，更改为：按产品名称从低到高进行排序，再按照价格从高到低进行排序 Arrays.sort(arr, new Comparator&lt;Goods&gt;() &#123; @Override public int compare(Goods o1, Goods o2) &#123; if (!o1.getName().equals(o2.getName())) &#123; return o1.getName().compareTo(o2.getName()); &#125; else &#123; if (o1.getPrice() &lt; o2.getPrice()) &#123; return 1; &#125; else if (o1.getPrice() &gt; o2.getPrice()) &#123; return -1; &#125; &#125; return 0; &#125; &#125;); System.out.println(&quot;排序后：&quot; + Arrays.toString(arr)); &#125;&#125;class Goods implements Comparable &#123; private String name; private double price; public Goods() &#123; &#125; public Goods(String name, double price) &#123; this.name = name; this.price = price; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; @Override public String toString() &#123; return &quot;Goods&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, price=&quot; + price + &#x27;&#125;&#x27;; &#125; // 先按照价格从低到高进行排序，再按照名称从高到低进行排序 @Override public int compareTo(Object o) &#123; if (o instanceof Goods) &#123; Goods goods = (Goods) o; if (this.price &gt; goods.price) &#123; return 1; &#125; else if (this.price &lt; goods.price) &#123; return -1; &#125; else &#123; return -this.name.compareTo(goods.name); &#125; // return Double.compare(this.getPrice(), goods.getPrice()); &#125; throw new RuntimeException(&quot;传入的数据类型有误&quot;); &#125;&#125; 可以将 Comparator 传递给 sort()，比如：Collections.sort() 或 Arrays.sort()，从而允许在排序顺序上实现精确控制。 还可以使用 Comparator 来控制某些数据结构 (如有序 set 或有序映射) 的顺序，或者为那些没有自然顺序的对象 collection 提供排序。 Comparable 和 Comparator 的对比 Comparable 接口的方式一旦指定，能够保证 Comparable 接口实现类的对象在任何位置都可以比较大小。 Comparator 接口属于临时性的比较，什么时候需要什么时候实现。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的日期和时间","slug":"java-date","date":"2021-03-15T09:00:58.000Z","updated":"2021-08-05T14:06:26.691Z","comments":true,"path":"2021/03/15/java-date/","link":"","permalink":"http://example.com/2021/03/15/java-date/","excerpt":"","text":"jdk 8 之前的日期和时间 APIjava.lang.System 类 long currentTimeMillis()：返回当前时间与 1970 年 1 月 1 日 0 时 0 分 0 秒之间以毫秒为单位的时间差，也被称为时间戳。 System 类的其他说明： System 类代表系统，系统级的很多属性和控制方法都放置在该类的内部。 由于该类的构造器是 private 的，所以无法创建该类的对象，也就是无法实例化该类。其内部的成员变量和成员方法都是 static 的，所以也可以很方便的进行调用。 成员变量： System 类内部包含 in、out 和 err 三个成员变量，分别代表标准输入流 (键盘输入)，标准输出流 (显示器) 和标准错误输出流 (显示器)。 成员方法： native long currentTimeMillis()：该方法的作用是返回当前的计算机时间，时间的表达格式为当前计算机间和 GMT 时间 (格林威治时间) 1970 年 1 月 1 日 0 时 0 分 0 秒之间的毫秒数。 void exit(int status)：该方法的作用是退出程序。其中 status 的值为 0 代表正常退出，非零代表异常退出。使用该方法可以在图形界面编程中实现程序的退出功能等。 void gc()：该方法的作用是请求系统进行垃圾回收。至于系统是否立刻回收，则取决于系统中垃圾回收算法的实现以及系统执行时的情况。 String getProperty(String key)：该方法的作用是获得系统中属性名为 key 的属性对应的值。系统中常见的属性名以及属性的作用如下表所示： 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; String javaVersion = System.getProperty(&quot;java.version&quot;); System.out.println(&quot;java的version: &quot; + javaVersion); String javaHome = System.getProperty(&quot;java.home&quot;); System.out.println(&quot;java的home: &quot; + javaHome); String osName = System.getProperty(&quot;os.name&quot;); System.out.println(&quot;os的name: &quot; + osName); String osVersion = System.getProperty(&quot;os.version&quot;); System.out.println(&quot;os的version: &quot; + osVersion); String userName = System.getProperty(&quot;user.name&quot;); System.out.println(&quot;user的name: &quot; + userName); String userHome = System.getProperty(&quot;user.home&quot;); System.out.println(&quot;user的home: &quot; + userHome); String userDir = System.getProperty(&quot;user.dir&quot;); System.out.println(&quot;user的dir: &quot; + userDir); &#125;&#125; java.util.Date 类 两个构造器的使用： Date date = new Date();：创建一个对应当前时间的 Date 对象。 Date date = new Date(1615816891380L);：创建指定毫秒数的 Date 对象。 两个方法的使用： toString()：把此 Date 对象转换为以下形式的 String：dow mon dd hh:mm:ss zzz yyyy，其中： dow 是一周中的某一天 (Sun，Mon，Tue，Wed，Thu，Fri，Sat)，zzz 是时间标准。 getTime()：返回自 1970 年 1 月 1 日 00:00:00 GMT 以来，此 Date 对象表示的毫秒数，即时间戳。 其它很多方法都过时了，不建议使用。 区别于 java.sql.Date 类： java.sql.Date 继承于 java.util.Date，是后者的子类，用于数据库中的日期。 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; // 创建java.sal.Date对象 java.sql.Date date = new java.sql.Date(System.currentTimeMillis()); System.out.println(date);// 2021-03-15 // java.util.Date对象转换为java.sql.Date对象 // 情况一：多态 java.util.Date date1 = new java.sql.Date(System.currentTimeMillis()); java.sql.Date date2 = (java.sql.Date) date1; System.out.println(date2); // 情况二： java.util.Date date3 = new java.util.Date(); Date date4 = new Date(date3.getTime()); System.out.println(date4);// 2021-03-15 &#125;&#125; java.text.SimpleDateFormat 类 java.util.Date 类的 API 不易于国际化，大部分被废弃了，java.text.SimpleDateFormat 类是一个与语言环境无关的方式来格式化和解析日期的具体类。 它允许对 Date 类的格式化和解析。 格式化：日期 —&gt; 字符串。 解析：字符串 —&gt; 日期。 SimpleDateFormat 类的实例化 使用默认构造器 public SimpleDateFormat()： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; SimpleDateFormat sdf = new SimpleDateFormat(); // 格式化：日期---&gt;字符串 Date date = new Date(); String format = sdf.format(date); System.out.println(format);// 21-3-16 下午8:46，默认格式化后的输出结果 // 解析：字符串---&gt;日期 String str = &quot;21-3-16 下午8:46&quot;;// 默认能解析的格式 try &#123; Date parse = sdf.parse(str); System.out.println(parse); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 使用带参构造器 public SimpleDateFormat(String pattern)： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); // 格式化：日期---&gt;字符串 Date date = new Date(); String format = sdf.format(date); System.out.println(format);// 2021-03-16 21:59:37，按指定格式格式化后的输出结果 // 解析：字符串---&gt;日期 String str = &quot;2021-3-16 9:02:13&quot;;// 按照指定格式书写的日期字符串 try &#123; Date parse = sdf.parse(str); System.out.println(parse); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 对于带参的构造器，在解析的时候，字符串必须是符合该参数指定的格式，否则，会解析发生异常。 实例： 1234567891011121314151617181920212223242526272829303132// 一个人从1990-1-1开始，三天打鱼两天晒网，求指定时间是在打渔还是晒网。public class Test &#123; public static void main(String[] args) &#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); Date startDate = null; try &#123; startDate = sdf.parse(&quot;1990-1-1&quot;); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; Date nowDate = null; try &#123; nowDate = sdf.parse(&quot;1990-1-1&quot;); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; if (startDate != null &amp;&amp; nowDate != null) &#123; long time = nowDate.getTime() - startDate.getTime(); long day = time / 1000 / 60 / 60 / 24 + 1; long l = day % 5; System.out.println(day + &quot;, &quot; + l); if (l == 1 || l == 2 | l == 3) &#123; System.out.println(&quot;在打渔&quot;); &#125; if (l == 0 || l == 4) &#123; System.out.println(&quot;在晒网&quot;); &#125; &#125; &#125;&#125; java.util.Calendar 类 (日历类) Calendar 是一个抽象类，主用用于完成日期字段之间相互操作的功能。 1public abstract class Calendar implements Serializable, Cloneable, Comparable&lt;Calendar&gt; &#123;&#125; 获取 Calendar实例的方法： 创建它的子类 GregorianCalendar 的对象。 调用静态方法 Calendar.getInstance()。 一个 Calendar 的实例是系统当前时间的抽象表示，常用方法如下： int get(int field)：获取想要的时间信息。比如：YEAR、MONTH、DAY_OF_WEEK、HOUR_OF_DAY、MINUTE、SECOND 等。 获取月份时：一月是 0，二月是 1，以此类推，十二月是 11。 获取星期时：周日是 1，周二是 2，以此类推，周六是 7。 void set(int field,int value)：设置时间。 void add(int field,int amount)：当前时间基础上做增减。 final Date getTime()：Calendar 对象转换为 Date对象。 final void setTime(Date date)：Date 对象转换为 Calendar 对象。 实例： 123456789101112131415161718192021222324252627282930313233public class Test &#123; public static void main(String[] args) &#123; // 1.实例化 // 方式一：创建其子类(GregorianCalendar)的对象 // 方式二：调用其静态方法getInstance() Calendar calendar = Calendar.getInstance();// 当前时间 System.out.println(calendar.getClass());// class java.util.GregorianCalendar // 2.常用方法 // get() System.out.println(&quot;年：&quot; + calendar.get(Calendar.YEAR));// 2021 System.out.println(&quot;月：&quot; + (calendar.get(Calendar.MONTH) + 1));// 月份，0代表1月，1代表2月，类推 System.out.println(&quot;日：&quot; + calendar.get(Calendar.DAY_OF_MONTH));// 一个月中的第几天 System.out.println(&quot;时：&quot; + calendar.get(Calendar.HOUR)); System.out.println(&quot;分：&quot; + calendar.get(Calendar.MINUTE)); System.out.println(&quot;秒：&quot; + calendar.get(Calendar.SECOND)); System.out.println(&quot;星期：&quot; + (calendar.get(Calendar.DAY_OF_WEEK) - 1));// 一周中的第几天，1代表周日，2代表周一，类推 System.out.println(&quot;一年中第：&quot; + calendar.get(Calendar.DAY_OF_YEAR));// 一年中的第几天 // set() calendar.set(Calendar.YEAR, 2020);// 更改calendar本身 System.out.println(&quot;重设之后的年：&quot; + calendar.get(Calendar.YEAR));// 2020 // add() calendar.add(Calendar.YEAR, 2); System.out.println(&quot;加2年之后的年：&quot; + calendar.get(Calendar.YEAR));// 2022 calendar.add(Calendar.YEAR, -1); System.out.println(&quot;减1年之后的年：&quot; + calendar.get(Calendar.YEAR));// 2021 // getTime()：Calendar---&gt;java.util.Date Date date = calendar.getTime(); // setTime()：java.util.Date---&gt;Calendar Date date1 = new Date(234234235235L); calendar.setTime(date1);// 设置calendar为指定时间 &#125;&#125; jdk 8 之后的日期和时间 API如果我们可以跟别人说：”我们在 1502643933071 见面，别晚了！”那么就再简单不过了。但是我们希望时间与昼夜和四季有关，于是事情就变复杂了。jdk 1.0 中包含了一个 java.util.Date 类，但是它的大多数方法已经在 jdk 1.1 引入 Calendar 类之后被弃用了，但 Calendar 并不比 Date 好多少。它们面临的问题是： 可变性：像日期和时间这样的类应该是不可变的。 偏移性：Date 中的年份是从 1900 开始的，而月份都从 0 开始。 格式化：格式化只对 Date 有用，Calendar 则不行。 此外，它们也不是线程安全的，也不能处理闰秒等。 第三次引入的 API 是成功的，并且 java 8 中引入的 java.time API 已经纠正了过去的缺陷，将来很长一段时间内它都会为我们服务。 java 8 吸收了 Joda-Time 的精华，以一个新的开始为 java 创建优秀的 API。新的 java.time 中包含了所有关于本地日期 (LocalDate)、本地时间 (LocalTime)、本地日期时间 (LocalDateTime)、时区 (ZonedDateTime) 和持续时间 (Duration) 的类。历史悠久的 Date 类新增了 toInstant() 方法，用于把 Date 转换成新的表示形式。这些新增的本地化时间日期 API 大大简化了日期时间和本地化的管理。 新时间日期 API： java.time – 包含值对象的基础包。 java.time.chrono – 提供对不同的日历系统的访问。 java.time.format – 格式化和解析时间和日期。 java.time.temporal – 包括底层框架和扩展特性。 java.time.zone – 包含时区支持的类。 说明：大多数开发者只会用到基础包和 format 包，也可能会用到 temporal 包。因此，尽管有 68 个新的公开类型，大多数开发者，大概将只会用到其中的三分之一。 java.time.LocalDate、java.time.LocalTime 和 java.time.LocalDateTime 类 LocalDate、LocalTime、LocalDateTime 类是其中较重要的几个类，它们的实例是不可变的对象，分别表示使用 ISO-8601日历系统的日期、时间、日期和时间。它们提供了简单的本地日期或时间，并不包含当前的时间信息，也不包含与时区相关的信息。 LocalDate：代表 IOS 格式 (yyyy-MM-dd) 的日期，可以存储生日、纪念日等日期。 LocalTime：表示一个时间，而不是日期。 LocalDateTime：是用来表示日期和时间的，这是一个最常用的类之一。 ISO-8601日历系统是国际标准化组织制定的现代公民的日期和时间的表示法，也就是公历。 常用方法： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142public class Test &#123; public static void main(String[] args) &#123; // now()：获取当前的日期、时间、日期+时间 LocalDate date = LocalDate.now(); LocalTime time = LocalTime.now(); LocalDateTime dateTime = LocalDateTime.now(); System.out.println(date);// 2021-03-17 System.out.println(time);// 11:37:43.400 System.out.println(dateTime);// 2021-03-17T11:37:43.400 // of()：自定义指定的年、月、日、时、分、秒对应的时间对象，没有偏移量 LocalDate date1 = LocalDate.of(2020, 3, 17); LocalTime time1 = LocalTime.of(11, 4, 25); LocalDateTime dateTime1 = LocalDateTime.of(2020, 3, 17, 11, 05, 45); System.out.println(date1);// 2020-03-17 System.out.println(time1);// 11:04:25 System.out.println(dateTime1);// 2020-03-17T11:05:45 // getXxx()：获取指定的时间信息 System.out.println(&quot;年：&quot; + dateTime.getYear());// 2021 System.out.println(&quot;月：&quot; + dateTime.getMonth());// MARCH System.out.println(&quot;月份数值：&quot; + dateTime.getMonthValue());// 3 System.out.println(&quot;日：&quot; + dateTime.getDayOfMonth());// 17 System.out.println(&quot;星期：&quot; + dateTime.getDayOfWeek());// WEDNESDAY System.out.println(&quot;时：&quot; + dateTime.getHour());// 11 System.out.println(&quot;分：&quot; + dateTime.getMinute());// 37 System.out.println(&quot;秒：&quot; + dateTime.getSecond());// 18 // withXX()：设置时间为指定的值并返回新的对象---不可变性 LocalDateTime dateTime2 = dateTime.withYear(2022); System.out.println(dateTime);// 2021-03-17T11:37:43.400 System.out.println(dateTime2);// 2022-03-17T11:37:43.400 // plusXxx()：在当前时间基础上做增减操作并返回新的对象---不可变性 LocalDateTime dateTime3 = dateTime.plusYears(2);// 加2年 System.out.println(dateTime);// 2021-03-17T11:37:43.400 System.out.println(dateTime3);// 2023-03-17T11:37:43.400 LocalDateTime dateTime4 = dateTime.minusYears(2);// 减2年 System.out.println(dateTime);// 2021-03-17T11:37:43.400 System.out.println(dateTime4);// 2019-03-17T11:37:43.400 &#125;&#125; java.time.Instant 类 — 瞬时 Instant：时间线上的一个瞬时点，这可能被用来记录应用程序中的事件时间戳。 在处理时间和日期的时候，我们通常会想到年，月，日，时，分，秒。然而，这只是时间的一个模型，是面向人类的。第二种通用模型是面向机器的，或者说是连续的。在此模型中，时间线中的一个点表示为一个很大的数，这有利于计算机处理。在 UNIX 中，这个数从 1970 年开始，以秒为的单位；同样的，在 java 中，也是从 1970 年开始，但以毫秒为单位。 java.time 包通过值类型 Instant 提供机器视图，不提供处理人类意义上的时间单位。Instant 表示时间线上的一点，而不需要任何上下文信息，例如，时区。概念上讲，它只是简单的表示自 1970 年 01 月 01 日 00 时 00 分 00 秒 (UTC) 开始的秒数。因为 java.time 包是基于纳秒计算的，所以 Instant 的精度可以达到纳秒级。 1秒 = 1000 毫秒 = 10^6 微秒 = 10^9 纳秒，即：1 ns = 10^-9 s。 常用方法： 时间戳是指格林威治时间 1970 年 01 月 01 日 00 时 00 分 00 秒 (北京时间 1970 年 01 月 01日 08 时 00 分 00 秒) 起至现在的总秒数。 实例： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; Instant instant = Instant.now();// 默认UTC时区，本初子午线对应的标准时间 System.out.println(instant);// 2021-03-17T03:50:15.672Z // 添加时间的偏移量 OffsetDateTime now = instant.atOffset(ZoneOffset.ofHours(8));// 东八区时间，要加上8小时 System.out.println(now); // 获取自1970-01-01 00:00:00(UTC)到当前时间的毫秒数 ---&gt; Date类的getTime()方法 long milli = instant.toEpochMilli(); System.out.println(milli); // 通过给定的毫秒数，获取Instant实例 ---&gt; new Date(long millis); Instant instant1 = Instant.ofEpochMilli(1615953468824L); &#125;&#125; java.time.format.DateTimeFormatter 类 格式化日期或时间，类似 SimpleDateFormat。 常用方法： 实例化方式一：预定义的标准格式。如：ISO_LOCAL_DATE_TIME、ISO_LOCAL_DATE、ISO_LOCAL_TIME。 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; DateTimeFormatter formatter = DateTimeFormatter.ISO_LOCAL_DATE_TIME; // 格式化：日期 ---&gt; 字符串 LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime);// 2021-03-17T13:18:37.907 String str = formatter.format(localDateTime); System.out.println(str);// 2021-03-17T13:18:37.907 // 解析：字符串 ---&gt; 日期 String str1 = &quot;2021-03-17T13:17:33.274&quot;;// 只能解析此种格式的字符串 TemporalAccessor parse = formatter.parse(str1); System.out.println(parse);// &#123;&#125;,ISO resolved to 2021-03-17T13:17:33.274 &#125;&#125; 实例化方式二，本地化相关的格式： ofLocalizedDateTime()，三种格式：FormatStyle.LONG / FormatStyle.MEDIUM / FormatStyle.SHORT 适用于 LocalDateTime。 12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime);// 2021-03-17T13:29:37.732 DateTimeFormatter formatter1 = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.LONG); String str1 = formatter1.format(localDateTime); System.out.println(str1);// 2021年3月17日 下午01时29分37秒 DateTimeFormatter formatter2 = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.MEDIUM); String str2 = formatter2.format(localDateTime); System.out.println(str2);// 2021-3-17 13:29:37 DateTimeFormatter formatter3 = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.SHORT); String str3 = formatter3.format(localDateTime); System.out.println(str3);// 21-3-17 下午1:29 &#125;&#125; ofLocalizedDate()，四种格式：FormatStyle.FULL / FormatStyle.LONG / FormatStyle.MEDIUM / FormatStyle.SHORT 适用于LocalDate。 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; LocalDate localDate = LocalDate.now(); System.out.println(localDate);// 2021-03-17 DateTimeFormatter formatter1 = DateTimeFormatter.ofLocalizedDate(FormatStyle.FULL); String str1 = formatter1.format(localDate); System.out.println(str1);// 2021年3月17日 星期三 DateTimeFormatter formatter2 = DateTimeFormatter.ofLocalizedDate(FormatStyle.LONG); String str2 = formatter2.format(localDate); System.out.println(str2);// 2021年3月17日 DateTimeFormatter formatter3 = DateTimeFormatter.ofLocalizedDate(FormatStyle.MEDIUM); String str3 = formatter3.format(localDate); System.out.println(str3);// 2021-3-17 DateTimeFormatter formatter4 = DateTimeFormatter.ofLocalizedDate(FormatStyle.SHORT); String str4 = formatter4.format(localDate); System.out.println(str4);// 21-3-17 &#125;&#125; 实例化方式三：自定义的格式，最常用。如：ofPattern(&quot;yyyy-MM-dd hh:mm:ss&quot;)。 123456789public class Test &#123; public static void main(String[] args) &#123; DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;); String str = formatter.format(LocalDateTime.now()); System.out.println(str);// 2021-03-17 13:13:52 TemporalAccessor accessor = formatter.parse(&quot;2021-02-17 13:18:09&quot;);// 字符串需要严格匹配自定义的格式 System.out.println(accessor);// &#123;&#125;,ISO resolved to 2021-02-17T13:18:09 &#125;&#125; 其他 API java.time.ZoneId：该类中包含了所有的时区信息，一个时区的 ID，如 Europe/Paris。 1234567891011121314151617181920public class Test &#123; public static void main(String[] args) &#123; // ZoneId: 类中包含了所有的时区信息 // ZoneId的getAvailableZoneIds(): 获取所有的ZoneId Set&lt;String&gt; zoneIds = ZoneId.getAvailableZoneIds(); for (String s : zoneIds) &#123; System.out.println(s); &#125; // ZoneId的of(): 获取指定时区的时间 LocalDateTime localDateTime = LocalDateTime.now(ZoneId.of(&quot;Asia/Tokyo&quot;)); System.out.println(localDateTime); // ZonedDateTime: 带时区的日期时间 // ZonedDateTime的now(): 获取本时区的ZonedDateTime对象 ZonedDateTime zonedDateTime = ZonedDateTime.now(); System.out.println(zonedDateTime); // ZonedDateTime的now(ZoneId id): 获取指定时区的ZonedDateTime对象 ZonedDateTime zonedDateTime1 = ZonedDateTime.now(ZoneId.of(&quot;Asia/Tokyo&quot;)); System.out.println(zonedDateTime1); &#125;&#125; java.time.ZonedDateTime：一个在 ISO-8601日历系统时区的日期时间，如 2007-12-03T10:15:30+01:00 Europe/Paris。 其中每个时区都对应着 ID，地区 ID 都为 “{区域}/{城市}” 的格式，例如：Asia/Shanghai 等。 java.time.Clock：使用时区提供对当前即时、日期和时间的访问的时钟。 java.time.Duration：持续时间，用于计算两个 “时间” 间隔。 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; // Duration: 用于计算两个&quot;时间&quot;间隔，以秒和纳秒为基准 // between(): 静态方法，返回Duration对象，表示两个时间的间隔 LocalTime localTime = LocalTime.now(); LocalTime localTime1 = LocalTime.of(15, 23, 32); Duration duration = Duration.between(localTime1, localTime); System.out.println(duration); System.out.println(duration.getSeconds()); System.out.println(duration.getNano()); LocalDateTime localDateTime = LocalDateTime.of(2016, 6, 12, 15, 23, 32); LocalDateTime localDateTime1 = LocalDateTime.of(2017, 6, 12, 15, 23, 32); Duration duration1 = Duration.between(localDateTime1, localDateTime); System.out.println(duration1.toDays());// -365 &#125;&#125; java.time.Period：日期间隔，用于计算两个 “日期” 间隔。 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; // Period:用于计算两个&quot;日期&quot;间隔，以年、月、日衡量 LocalDate localDate = LocalDate.now();// 2021-3-17 LocalDate localDate1 = LocalDate.of(2028, 3, 18); Period period = Period.between(localDate, localDate1); System.out.println(period);// P7Y1D System.out.println(period.getYears());// 7 System.out.println(period.getMonths());// 0 System.out.println(period.getDays());// 1 Period period1 = period.withYears(2); System.out.println(period1); &#125;&#125; java.time.temporal.TemporalAdjuster：时间校正器。有时我们可能需要获取诸如将日期调整到 “下一个工作日” 等操作。 java.time.temporal.TemporalAdjusters：该类通过静态方法 firstDayOfXxx()/lastDayOfXxx()/nextXxx()，提供了大量的常用 TemporalAdjuster 的实现。 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; // TemporalAdjuster: 时间校正器 // 获取当前日期的下一个周日是哪天？当前日期：2021-3-17 TemporalAdjuster temporalAdjuster = TemporalAdjusters.next(DayOfWeek.SUNDAY); LocalDate localDateTime = LocalDate.now().with(temporalAdjuster);// LocalDateTime.now().with(temporalAdjuster) System.out.println(&quot;下一个周日是：&quot; + localDateTime);// 下一个周日是：2021-03-21 // 获取下一个工作日是哪天？ LocalDate localDate = LocalDate.now().with(new TemporalAdjuster() &#123; @Override public Temporal adjustInto(Temporal temporal) &#123; LocalDate date = (LocalDate) temporal; if (date.getDayOfWeek().equals(DayOfWeek.FRIDAY)) &#123; return date.plusDays(3); &#125; else if (date.getDayOfWeek().equals(DayOfWeek.SATURDAY)) &#123; return date.plusDays(2); &#125; else &#123; return date.plusDays(1); &#125; &#125; &#125;); System.out.println(&quot;下一个工作日是：&quot; + localDate);// 下一个工作日是：2021-03-18 &#125;&#125; 与传统日期处理的转换 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 线程","slug":"java-thread","date":"2021-03-04T12:17:34.000Z","updated":"2021-04-09T08:01:35.602Z","comments":true,"path":"2021/03/04/java-thread/","link":"","permalink":"http://example.com/2021/03/04/java-thread/","excerpt":"","text":"程序、进程和线程程序 (program)：是为完成特定任务、用某种语言编写的一组指令的集合。即指一段静态的代码，静态对象。 进程 (process)：是程序的一次执行过程，或是正在运行的一个程序。是一个动态的过程：有它自身的产生、存在和消亡的过程 —— 生命周期。如：运行中的QQ，运行中的MP3播放器。 程序是静态的，进程是动态的。 进程作为资源分配的单位，系统在运行时会为每个进程分配不同的内存区域。 线程 (thread)：进程可进一步细化为线程，是一个程序内部的一条执行路径。 若一个进程同一时间并行执行多个线程，就是支持多线程的。 线程作为调度和执行的单位，**每个线程拥有独立的运行栈和程序计数器 (pc)**，线程切换的开销小。 一个进程中的多个线程共享相同的内存单元 / 内存地址空间 (方法区、堆)：它们从同一堆中分配对象，可以访问相同的变量和对象。这就使得线程间通信更简便、高效。但多个线程操作共享的系统资源可能就会带来安全的隐患。 单核 CPU 和多核 CPU 的理解： 单核 CPU，其实是一种假的多线程，因为在一个时间单元内，也只能执行一个线程的任务。只是因为 CPU 时间单元特别短，因此感觉不出来。例如：虽然有多车道，但是收费站只有一个工作人员在收费，只有收了费才能通过，那么 CPU 就好比收费人员。如果有某个人没准备好交钱，那么收费人员可以把他 “挂起”，晾着他，等他准备好了钱，再去收费。 如果是多核的话，才能更好的发挥多线程的效率，现在的服务器基本都是多核的。 一个 java 应用程序 java.exe，其实至少有三个线程：main() 主线程，gc() 垃圾回收线程，异常处理线程。当然如果发生异常，会影响主线程。 并行与并发： 并行：多个 CPU 同时执行多个任务。 比如：多个人同时做不同的事。 并发：一个 CPU (采用时间片) 同时执行多个任务。比如：秒杀、多个人做同一件事。 多线程程序的优点： 以单核 CPU 为例，只使用单个线程先后完成多个任务 (调用多个方法)，肯定比用多个线程来完成用的时间更短 (因为单核 CPU，在多线程之间进行切换时，也需要花费时间 )，为何仍需多线程呢？ 提高应用程序的响应。对图形化界面更有意义，可增强用户体验。 提高计算机系统 CPU 的利用率。 改善程序结构。将既长又复杂的进程分为多个线程，独立运行，利于理解和修改。 何时需要多线程： 程序需要同时执行两个或多个任务。 程序需要实现一些需要等待的任务时，如用户输入、文件读写操作、网络操作、搜索等。 需要一些后台运行的程序时。 Thread 类java 语言的 JVM 允许程序运行多个线程，它通过 java.lang.Thread 类来体现。 Thread 类的特性： 每个线程都是通过某个特定 Thread 对象的 run() 方法来完成操作的，经常把 run() 方法的主体称为线程体。 通过该 Thread 对象的 start() 方法来启动这个线程，而非直接调用 run()。 如果手动调用 run()，那么就只是普通的方法，并没有启动多线程。 调用 start() 之后，run() 由 JVM 调用，什么时候调用以及执行的过程控制都由操作系统的 CPU 调度决定。 构造器： Thread() Thread(String threadname) Thread(Runnable target) Thread(Runnable target, String name) 方法： **void start()**：启动当前线程，并执行当前线程对象的 run() 方法。 **run()**：通常需要重写 Thread 类中的此方法，将创建的线程在被调度时需要执行的操作声明在此方法中。 **static Thread currentThread()**：静态方法，返回执行当前代码的线程。在 Thread 子类中就是 this，通常用于主线程和 Runnable 实现类。 12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; System.out.println(Thread.currentThread().getName());// main new MyThread().start(); &#125;&#125;class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName());// Thread-0 System.out.println(currentThread().getName());// Thread-0 System.out.println(this.currentThread().getName());// Thread-0，实际代码中，不应该通过类实例访问静态成员 &#125;&#125; **String getName()**：返回当前线程的名称。 **void setName(String name)**：设置当前线程的名称。 12345678910111213141516171819202122232425262728293031public class Test &#123; public static void main(String[] args) &#123; // 设置main线程的名字 Thread.currentThread().setName(&quot;主线程&quot;); System.out.println(Thread.currentThread().getName()); // 设置自定义线程的名字 MyThread myThread = new MyThread(); myThread.setName(&quot;自定义线程一&quot;); myThread.start(); // 构造器设置自定义线程的名字 new MyThread(&quot;自定义线程二&quot;).start(); &#125;&#125;class MyThread extends Thread &#123; public MyThread() &#123; &#125; public MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125;&#125; **static void yield()**：释放当前线程 CPU 的执行权，但有可能 CPU 再次分配资源时，仍然优先分配到当前线程。 **join()**：在某个线程 a 中调用线程 b 的 join() 方法时，调用线程 a 将进入阻塞状态，直到线程 b 执行完之后，线程 a 才结束阻塞状态，等待 CPU 重新分配资源执行剩下的任务。注意：调用 join() 方法之后，低优先级的线程也可以获得执行。 **static void sleep(long millis)**：让当前线程 “睡眠” 指定的 millis 毫秒时间，在指定的 millis 毫秒时间内，当前线程是阻塞状态。时间到达时，重新排队等待 CPU 分配资源。 **stop()**：强制结束当前线程，已过时。 **boolean isAlive()**：返回boolean，判断线程是否存活。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Test &#123; public static void main(String[] args) &#123; MyThread myThread = new MyThread(); myThread.start(); System.out.println(myThread.isAlive()); for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; if (i == 20) &#123; try &#123; myThread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; ; &#125; System.out.println(myThread.isAlive()); &#125;&#125;class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; if (i % 20 == 0) &#123; yield(); &#125; if (i % 30 == 0) &#123; try &#123; sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 线程线程的调度策略调度策略： 时间片策略： 抢占式策略：高优先级的线程抢占 CPU。 java 的调度方法： 同优先级线程，组成先进先出队列 (先到先服务)，使用时间片策略。 高优先级线程，使用优先调度的抢占式策略。 线程的优先级线程的优先级等级： MAX_PRIORITY：10，最大优先级。 MIN _PRIORITY：1，最小优先级。 NORM_PRIORITY：5，默认优先级。 涉及的方法： getPriority()：获取线程的优先值。 setPriority(int newPriority)：设置线程的优先级。 12System.out.println(Thread.currentThread().getPriority());Thread.currentThread().setPriority(8); 说明： 线程创建时继承父线程的优先级。 低优先级只是获得调度的概率低，但并非一定是在高优先级线程之后才被调用。 线程的分类java 中的线程分为两类：一种是用户线程，一种是守护线程。 用户线程和守护线程，几乎在每个方面都是相同的，唯一的区别是判断 JVM 何时离开。 守护线程是用来服务用户线程的，通过在 start() 方法前调用 thread.setDaemon(true) 可以把一个用户线程变成一个守护线程。 java 垃圾回收就是一个典型的守护线程。 若 JVM 中都是守护线程，当前 JVM 将退出。 线程的生命周期要想实现多线程，必须在主线程中创建新的线程对象。java 语言使用 Thread 类及其子类的对象来表示线程，并用 Thread.State 类定义了线程的几种状态，在它的一个完整的生命周期中通常要经历如下的五种状态： 新建：当一个 Thread 类或其子类的对象被声明并创建时，新生的线程对象处于新建状态。 就绪：处于新建状态的线程被 start() 后，将进入线程队列等待 CPU 时间片，此时它已具备了运行的条件，只是没分配到 CPU 资源。 运行：当就绪的线程被调度并获得 CPU 资源时,便进入运行状态，run() 定义了线程的操作和功能。 阻塞：在某种特殊情况下，被人为挂起或执行输入输出操作时，让出 CPU 并临时中止自己的执行，进入阻塞状态。 死亡：线程完成了它的全部工作或线程被提前强制性地中止或出现异常导致结束。 线程的创建线程创建的一般过程： 方式一：继承 Thread 类 创建一个继承 Thread 类的子类。 重写 Thread 类的 run() — 将此线程执行的操作声明在 run() 方法体中。 创建 Thread 类的子类的对象。 通过该对象调用 start()。 启动当前线程。 调用当前线程的 run()。 不能通过直接调用对象的 run() 的形式启动线程。 不能再次调用当前对象的 start() 去开启一个新的线程，否则报 java.lang.IllegalThreadStateException 异常 。 如果要启动一个新的线程，需要重新创建一个 Thread 类的子类的对象，并调用其 start()。 实例一： 1234567891011121314151617181920212223242526272829public class Test &#123; public static void main(String[] args) &#123; // 启动一个子线程 MyThread myThread = new MyThread(); myThread.start(); // 启动一个新的子线程，并执行run方法 MyThread myThread2 = new MyThread(); myThread2.start(); // main线程 for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125;&#125;class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125;&#125; 实例二： 1234567891011121314151617181920212223242526public class Test &#123; public static void main(String[] args) &#123; // 匿名内部类 new Thread()&#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125; &#125;.start(); new Thread()&#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125; &#125;.start(); &#125;&#125; 方式二：实现 Runnable 接口 创建一个实现了 Runnable 接口的类。 实现类去实现 Runnable 接口中的抽象方法：run()。 创建实现类的对象。 将此对象作为参数传递到 Thread 类的构造器中，然后创建 Thread 类的对象。 通过 Thread 类的对象，调用 start()，最终执行的是上面重写的 run()。 实例： 123456789101112131415161718192021222324252627282930313233public class Test &#123; public static void main(String[] args) &#123; // 启动多个子线程时，只需要创建一个Runnable接口实现类的对象 MyRunnable myRunnable = new MyRunnable(); // 启动一个子线程 Thread thread = new Thread(myRunnable); thread.start(); // 启动一个新的子线程，并执行run方法 Thread thread2 = new Thread(myRunnable); thread2.start(); // main线程 for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125;&#125;class MyRunnable implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125;&#125; 方式一和方式二的对比 开发中，优先选择实现 Runnable 接口的方式。 实现 Runnable 接口的方式，没有类的单继承性的局限性。 实现 Runnable 接口的方式，更适合处理多个线程有共享数据的情况。 Thread 类也实现了 Runnable 接口，无论是方式一，还是方式二，都需要重写 Runnable 接口的 run() 方法，并将创建的线程需要执行的逻辑声明在 run() 方法中。 方式三：实现 Callable 接口 从 JDK 5.0 开始。 创建一个实现 Callable 接口的实现类。 实现 call()，将此线程需要执行的操作声明在 call() 的方法体中。 创建 Callable 接口实现类的对象。 将此 Callable 接口实现类的对象作为参数传递到 FutureTask 的构造器中，创建 FutureTask 的对象。 Future 接口可以对具体 Runnable 或 Callable 任务的执行结果进行取消、查询是否完成、获取结果等操作。 FutrueTask 是 Futrue 接口的唯一的实现类。 FutureTask 同时实现了 Runnable 和 Future 接口。它既可以作为 Runnable 被线程执行，又可以作为 Future 得到 Callable 的返回值。 Runnable 接口的 run() 没有返回值。 Callable 接口的 call() 有返回值。 将 FutureTask 的对象作为参数传递到 Thread 类的构造器中，创建 Thread 类的对象，并调用 start()，启动线程。 根据实际需求，选择是否获得 Callable 中 call() 的返回值。 实例： 123456789101112131415161718192021222324252627282930313233public class Test &#123; public static void main(String[] args) &#123; // 3.创建Callable接口实现类的对象 MyCallable myCallable = new MyCallable(); // 4.将此Callable接口实现类的对象作为参数传递到FutureTask的构造器中，创建FutureTask的对象 FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(myCallable); // 5.将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread类的对象，并调用start()，启动线程 new Thread(futureTask).start(); // 6.获得Callable中call()的返回值 try &#123; // get()返回值即为FutureTask构造器参数Callable实现类重写的call()的返回值 Integer sum = futureTask.get(); System.out.println(&quot;100以内偶数的总和为：&quot; + sum); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125;// 1.创建一个实现Callable接口的实现类class MyCallable implements Callable&lt;Integer&gt; &#123; // 2.实现call()方法，将此线程需要执行的操作声明在call()的方法体中 @Override public Integer call() throws Exception &#123; int sum = 0; for (int i = 1; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; sum += i; &#125; &#125; return sum; &#125;&#125; 与使用 Runnable 接口相比，Callable 接口功能更强大些： 相比 run() 方法，call() 可以有返回值。 call() 可以抛出异常，能够被外面的操作捕获，获取异常的信息。 Callable 支持泛型的返回值。 Callable 需要借助 FutureTask 类，比如获取 call() 的返回结果。 方式四：线程池 背景： 经常创建和销毁、使用量特别大的资源，比如并发情况下的线程，会对性能影响很大。 思路：提前创建好多个线程，放入线程池中，使用时直接获取，使用完放回池中，这样可以避免频繁创建销毁，实现重复利用。类似生活中的公共交通工具。 好处： 提高响应速度，减少了创建新线程的时间。 降低资源消耗，重复利用线程池中线程，不需要每次都创建。 便于线程管理。 corePoolSize：核心池的大小。 maximumPoolSize：最大线程数。 keepAliveTime：线程没有任务时最多保持多长时间后会终止。 JDK 5.0 起，提供了线程池相关 API：ExecutorService 和 Executors。 ExecutorService：真正的线程池接口，常用子类 ThreadPoolExecutor。 void execute(Runnable command)：执行任务/命令，没有返回值，一般用来执行 Runnable。 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task)：执行任务，有返回值，一般用来执行 Callable。 void shutdown()：关闭连接池。 Executors：工具类、线程池的工厂类，用于创建并返回不同类型的线程池。 Executors.newCachedThreadPool()：创建一个可根据需要创建新线程的线程池。 Executors.newFixedThreadPool(n)：创建一个可重用固定线程数的线程池。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class ThreadPool &#123; public static void main(String[] args) &#123; // 1.提供指定线程数量的线程池 ExecutorService executorService = Executors.newFixedThreadPool(10); // 2.执行指定的线程的操作，需要提供实现Runnable接口或Callable接口的实现类的对象 // 2-1.execute()适合使用于Runnable executorService.execute(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + i); &#125; &#125; &#125; &#125;); executorService.execute(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + i); &#125; &#125; &#125; &#125;); // 2-2.submit()适合适用于Callable Future&lt;Integer&gt; evenSum = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; int evenSum = 0; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; evenSum += i; &#125; &#125; return evenSum; &#125; &#125;); try &#123; System.out.println(&quot;100以内的偶数和: &quot; + evenSum.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; Future&lt;Integer&gt; oddSum = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; int oddSum = 0; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; oddSum += i; &#125; &#125; return oddSum; &#125; &#125;); try &#123; System.out.println(&quot;100以内的奇数和: &quot; + oddSum.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; // 3.使用完线程池后，需要关闭线程池 executorService.shutdown(); &#125;&#125; Executors.newSingleThreadExecutor()：创建一个只有一个线程的线程池。 Executors.newScheduledThreadPool(n)：创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。 线程的同步线程的安全问题多线程安全问题实例，模拟火车站售票程序，开启三个窗口售票。 方式一：继承 Thread 类。 12345678910111213141516171819202122232425262728293031323334353637383940public class TestThread &#123; public static void main(String[] args) &#123; // 启动第一个售票窗口 TicketThread thread1 = new TicketThread(); thread1.setName(&quot;售票窗口一&quot;); thread1.start(); // 启动第二个售票窗口 TicketThread thread2 = new TicketThread(); thread2.setName(&quot;售票窗口二&quot;); thread2.start(); // 启动第三个售票窗口 TicketThread thread3 = new TicketThread(); thread3.setName(&quot;售票窗口三&quot;); thread3.start(); &#125;&#125;class TicketThread extends Thread &#123; // 总票数，必须定义为static，随类只加载一次，因为每新建一个线程，都需要new一次TicketThread private static int ticketNum = 100; @Override public void run() &#123; while (true) &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; &#125;&#125; 方式二：实现 Runnable 接口。 1234567891011121314151617181920212223242526272829303132333435363738public class Test &#123; public static void main(String[] args) &#123; TicketRunnable ticket = new TicketRunnable(); // 启动第一个售票窗口 Thread thread1 = new Thread(ticket, &quot;售票窗口1&quot;); thread1.start(); // 启动第二个售票窗口 Thread thread2 = new Thread(ticket, &quot;售票窗口2&quot;); thread2.start(); // 启动第三个售票窗口 Thread thread3 = new Thread(ticket, &quot;售票窗口3&quot;); thread3.start(); &#125;&#125;class TicketRunnable implements Runnable &#123; // 总票数，不必定义为static，因为只需要new一次TicketRunnable private int ticketNum = 100; @Override public void run() &#123; while (true) &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; &#125;&#125; 说明： 如上程序，在买票的过程中，出现了重票、错票，说明多线程的执行过程中，出现了安全问题。 问题的原因：当多条语句在操作同一个线程的共享数据时，当一个线程对多条语句只执行了一部分，还没有执行完时，另一个线程参与进来执行，从而导致了共享数据的错误。 解决办法：对多条操作共享数据的语句，让一个线程全部执行完，在执行的过程中，其他线程不可以参与执行。 线程的同步机制对于多线程的安全问题，java 提供了专业的解决方式：同步机制。实现同步机制的方式，有同步代码块、同步方法、Lock 锁等多种形式。 同步的范围 如何找问题，即代码是否存在线程安全？— 非常重要（1）明确哪些代码是多线程运行的代码。（2）明确多个线程是否有共享数据。（3）明确多线程运行代码中是否有多条语句操作共享数据。 如何解决呢？— 非常重要 对多条操作共享数据的语句，只能让一个线程都执行完，在执行过程中，其他线程不可以参与执行，即所有操作共享数据的这些语句都要放在同步范围中。 切记 ： 范围太小：没锁住所有有安全问题的代码。 范围太大：没发挥多线程的功能。 同步机制的特点 优点：同步的方式，能够解决线程的安全问题。 局限性：操作同步代码时，只能有一个线程参与，其他线程等待，相当于是一个单线程的过程，效率低。 需要被同步的代码：操作共享数据的代码。 共享数据：多个线程共同操作的变量。 同步监视器，俗称：锁。任何一个类的对象，都可以充当锁。 要求：多个线程必须要公用同一把锁！！！针对不同实现同步机制的方式，都要保证同步监视器是同一个！！！ 同步机制中的锁同步锁机制：在《Thinking in Java》中，是这么说的：对于并发工作，你需要某种方式来防止两个任务访问相同的资源 (其实就是共享资源竞争)。防止这种冲突的方法就是当资源被一个任务使用时，在其上加锁。第一个访问某项资源的任务必须锁定这项资源，使其他任务在其被解锁之前，就无法访问它了，而在其被解锁之时，另一个任务就可以锁定并使用它了。 synchronized 的锁是什么： 任意对象都可以作为同步锁，所有对象都自动含有单一的锁 (监视器)。 同步代码块的锁：自己指定，很多时候也是指定为 this 或 类名.class。 同步方法的锁：静态方法 — 类名.class、非静态方法 — this。 注意： 必须确保使用同一个资源的多个线程共用的是同一把锁，这个非常重要，否则就无法保证共享资源的安全。 一个线程类中的所有静态方法共用同一把锁 — 类名.class，所有非静态方法共用同一把锁 — this，同步代码块在指定锁的时候需谨慎。 能够释放锁的操作： 当前线程的同步方法、同步代码块执行结束。 当前线程在同步代码块、同步方法中遇到 break、return 终止了该代码块、该方法的继续执行。 当前线程在同步代码块、同步方法中出现了未处理的Error或Exception，导致异常结束。 当前线程在同步代码块、同步方法中执行了线程对象的wait()方法，当前线程暂停，并释放锁。 不会释放锁的操作： 线程执行同步代码块或同步方法时，程序调用 Thread.sleep()、Thread.yield() 暂停当前线程的执行。 线程执行同步代码块时，其他线程调用了该线程的 suspend() 将该线程挂起，该线程不会释放锁 (同步监视器)。 应尽量避免使用 suspend() 和 resume() 来控制线程。 同步机制一：同步代码块格式： 123synchronized (同步监视器)&#123; // 需要被同步的代码&#125; 继承 Thread 类方式的修正： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class TestThread &#123; public static void main(String[] args) &#123; // 启动第一个售票窗口 TicketThread thread1 = new TicketThread(); thread1.setName(&quot;售票窗口一&quot;); thread1.start(); // 启动第二个售票窗口 TicketThread thread2 = new TicketThread(); thread2.setName(&quot;售票窗口二&quot;); thread2.start(); // 启动第三个售票窗口 TicketThread thread3 = new TicketThread(); thread3.setName(&quot;售票窗口三&quot;); thread3.start(); &#125;&#125;class TicketThread extends Thread &#123; // 总票数，必须定义为static，随类只加载一次，因为每新建一个线程，都需要new一次TicketThread private static int ticketNum = 100; // 锁，必须定义为static private static Object obj = new Object(); @Override public void run() &#123; while (true) &#123; synchronized (obj) &#123;// 可以使用：synchronized (TicketThread.class)，不能建议使用：synchronized (this) if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; &#125; &#125;&#125; obj 可以使用 TicketThread.class (当前类) 替代，TicketThread 类只会加载一次，类也是对象。 实现 Runnable 接口方式的修正： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class TestRunnable &#123; public static void main(String[] args) &#123; TicketRunnable ticket = new TicketRunnable(); // 启动第一个售票窗口 Thread thread1 = new Thread(ticket, &quot;售票窗口1&quot;); thread1.start(); // 启动第二个售票窗口 Thread thread2 = new Thread(ticket, &quot;售票窗口2&quot;); thread2.start(); // 启动第三个售票窗口 Thread thread3 = new Thread(ticket, &quot;售票窗口3&quot;); thread3.start(); &#125;&#125;class TicketRunnable implements Runnable &#123; // 总票数，不必定义为static，因为只需要new一次TicketRunnable private int ticketNum = 100; // 锁，不必定义为static Object obj = new Object(); @Override public void run() &#123; while (true) &#123; synchronized (obj) &#123;// 可以使用：synchronized (this) if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; &#125; &#125;&#125; obj 对象可以使用 this 代替，指代唯一的 TicketRunnable 对象。 同步机制二：同步方法格式： 1修饰符 synchronized 返回值类型 方法名 (形参列表) &#123;&#125; 如果操作共享数据的代码，完整的声明在一个方法中，则可以将此方法声明为同步方法。 同步方法仍然涉及到同步监视器，只是不需要显示的声明： 非静态的同步方法，同步监视器是：this。 静态的同步方法，同步监视器是：当前类本身。 继承 Thread 类方式的修正： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class TestMethod1 &#123; public static void main(String[] args) &#123; // 启动第一个售票窗口 TicketMethod1 thread1 = new TicketMethod1(); thread1.setName(&quot;售票窗口一&quot;); thread1.start(); // 启动第二个售票窗口 TicketMethod1 thread2 = new TicketMethod1(); thread2.setName(&quot;售票窗口二&quot;); thread2.start(); // 启动第三个售票窗口 TicketMethod1 thread3 = new TicketMethod1(); thread3.setName(&quot;售票窗口三&quot;); thread3.start(); &#125;&#125;class TicketMethod1 extends Thread &#123; // 总票数，必须定义为static，随类只加载一次，因为每新建一个线程，都需要new一次TicketThread private static int ticketNum = 100; @Override public void run() &#123; while (true) &#123; handleTicket(); &#125; &#125; // 必须设置成static的，此时的同步监视器是TicketMethod1.class private static synchronized void handleTicket() &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125;&#125; 此时，同步方法要设置成 static 的，此时的同步监视器是 TicketMethod1.class (当前类)。 实现 Runnable 接口方式的修正： 123456789101112131415161718192021222324252627282930313233343536373839404142public class TestMethod2 &#123; public static void main(String[] args) &#123; TicketMethod2 ticket = new TicketMethod2(); // 启动第一个售票窗口 Thread thread1 = new Thread(ticket, &quot;售票窗口1&quot;); thread1.start(); // 启动第二个售票窗口 Thread thread2 = new Thread(ticket, &quot;售票窗口2&quot;); thread2.start(); // 启动第三个售票窗口 Thread thread3 = new Thread(ticket, &quot;售票窗口3&quot;); thread3.start(); &#125;&#125;class TicketMethod2 implements Runnable &#123; private int ticketNum = 100; @Override public void run() &#123;// 有时可以直接设置run方法为synchronized，但本例不行 while (true) &#123; handleTicket(); &#125; &#125; // 非静态同步方法中，同步监视器：this private synchronized void handleTicket() &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125;&#125; 此时，同步方法中的同步监视器是：this，即当前 TicketMethod2 类的对象。 同步机制三：Lock 锁 从 JDK 5.0 开始，java 提供了更强大的线程同步机制——通过显式定义同步锁对象来实现同步。同步锁使用 Lock 对象充当。 java.util.concurrent.locks.Lock 接口是控制多个线程对共享资源进行访问的工具。锁提供了对共享资源的独占访问，每次只能有一个线程对 Lock 对象加锁，线程开始访问共享资源之前应先获得 Lock 对象。 在实现线程安全的控制中，比较常用的是 ReentrantLock，ReentrantLock 类实现了 Lock 接口，它拥有与 synchronized 相同的并发性和内存语义，可以显式加锁、释放锁。 声明格式： 继承 Thread 类方式的修正： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class LockTest &#123; public static void main(String[] args) &#123; // 启动第一个售票窗口 Ticket thread1 = new Ticket(); thread1.setName(&quot;售票窗口一&quot;); thread1.start(); // 启动第二个售票窗口 Ticket thread2 = new Ticket(); thread2.setName(&quot;售票窗口二&quot;); thread2.start(); // 启动第三个售票窗口 Ticket thread3 = new Ticket(); thread3.setName(&quot;售票窗口三&quot;); thread3.start(); &#125;&#125;class Ticket extends Thread &#123; private static int ticketNum = 100; // 1.实例化静态ReentrantLock private static Lock lock = new ReentrantLock(); @Override public void run() &#123; while (true) &#123; // 2.调用锁定方法: lock() lock.lock(); try &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; finally &#123; lock.unlock();// 3.调用解锁方法: unlock() &#125; &#125; &#125;&#125; ReentrantLock 实例对象需要设置为 static。 实现 Runnable 接口方式的修正： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class LockTest &#123; public static void main(String[] args) &#123; Ticket ticket = new Ticket(); // 启动第一个售票窗口 Thread thread1 = new Thread(ticket, &quot;售票窗口1&quot;); thread1.start(); // 启动第二个售票窗口 Thread thread2 = new Thread(ticket, &quot;售票窗口2&quot;); thread2.start(); // 启动第三个售票窗口 Thread thread3 = new Thread(ticket, &quot;售票窗口3&quot;); thread3.start(); &#125;&#125;class Ticket implements Runnable &#123; private int ticketNum = 100; // 1.实例化ReentrantLock private Lock lock = new ReentrantLock(); @Override public void run() &#123; while (true) &#123; // 2.调用锁定方法: lock() lock.lock(); try &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; finally &#123; lock.unlock();// 3.调用解锁方法: unlock() &#125; &#125; &#125;&#125; synchronized 和 Lock 的对比 synchronized 是隐式锁，出了作用域自动释放同步监视器，而 Lock 是显式锁，需要手动开启和关闭锁。 synchronized 有代码块锁和方法锁，而 Lock 只有代码块锁。 使用 Lock 锁，JVM 将花费较少的时间来调度线程，性能更好，并且具有更好的扩展性 (Lock 接口能提供更多的实现类)。 优先使用顺序：Lock → 同步代码块 (已经进入了方法体，分配了相应资源) → 同步方法 (在方法体之外) 经典实例 银行有一个账户，有两个储户分别向这个账户存钱，每次存 1000，存 10 次，要求每次存完打印账户余额。 实现方式一： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class AccountTest &#123; public static void main(String[] args) &#123; // 一个账户 Account account = new Account(0.0); // 两个储户 Customer c1 = new Customer(account); Customer c2 = new Customer(account); c1.setName(&quot;甲&quot;); c2.setName(&quot;乙&quot;); c1.start(); c2.start(); &#125;&#125;class Account &#123; private double balance; public Account(double balance) &#123; this.balance = balance; &#125; public double getBalance() &#123; return balance; &#125; // 此时的锁是Accout的对象，本例的写法中，Account只有一个，所以两个线程公用的是一个同步锁 public synchronized void deposit(double amt) &#123; if (amt &gt; 0) &#123; balance += amt; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;存钱成功，余额为：&quot; + balance); &#125; &#125;&#125;class Customer extends Thread &#123; private Account account; public Customer(Account account) &#123; this.account = account; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; account.deposit(1000.0); &#125; &#125;&#125; 实现方式二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class AccountTest &#123; public static void main(String[] args) &#123; // 一个账户 Account account = new Account(0.0); // 两个储户 Customer c1 = new Customer(account); Customer c2 = new Customer(account); c1.setName(&quot;甲&quot;); c2.setName(&quot;乙&quot;); c1.start(); c2.start(); &#125;&#125;class Account &#123; private double balance; public Account(double balance) &#123; this.balance = balance; &#125; public double getBalance() &#123; return balance; &#125; public void deposit(double amt) &#123; if (amt &gt; 0) &#123; balance += amt; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;存钱成功，余额为：&quot; + balance); &#125; &#125;&#125;class Customer extends Thread &#123; private Account account; // static的Lock private static Lock lock = new ReentrantLock(); public Customer(Account account) &#123; this.account = account; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; lock.lock(); try &#123; account.deposit(1000.0); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 线程的通信 wait() 与 notify() 和 notifyAll() wait()：一旦执行此方法，当前线程就进入阻塞状态，并释放同步监视器。 当前线程排队等候其他线程调用 notify() 或 notifyAll() 方法唤醒，唤醒后等待重新获得对监视器的所有权后才能继续执行。 被唤醒的线程从断点处继续代码的执行。 notify()：一旦执行此方法，就会唤醒被 wait() 的一个线程。如果有多个线程被 wait()，则唤醒优先级高的。 notifyAll()：一旦执行此方法，就会唤醒所有被 wait() 的线程。 wait() 与 notify() 和 notifyAll() 这三个方法必须使用在同步代码块或同步方法中。 wait() 与 notify() 和 notifyAll() 这三个方法的调用者必须是同步代码块或同步方法中的同步监视器。 否则会出现 java.lang.IllegalMonitorStateException 异常。 wait() 与 notify() 和 notifyAll() 这三个方法是定义在 java.lang.Object 类中的。 因为这三个方法必须由同步监视器调用，而任意对象都可以作为同步监视器，因此这三个方法只能在 Object 类中声明。 实例一：使用两个线程打印 1 - 100，要求线程 1 和线程 2 交替打印。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class CommunicationTest &#123; public static void main(String[] args) &#123; Number number = new Number(); Thread t1 = new Thread(number); Thread t2 = new Thread(number); t1.setName(&quot;线程1&quot;); t2.setName(&quot;线程2&quot;); t1.start(); t2.start(); &#125;&#125;class Number implements Runnable &#123; private int number = 1; @Override public void run() &#123; while (true) &#123; synchronized (this) &#123; // 唤醒被wait()的一个线程 notify();// 等同于：this.notify(); if (number &lt;= 100) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + number); number++; try &#123; // 使调用wait()方法的线程进入阻塞状态 wait();// 等同于：this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; break; &#125; &#125; &#125; &#125;&#125; 实例二：生产者 / 消费者问题。 生产者 (Producer)将产品交给店员 (Clerk)，而消费者 (Customer) 从店员处取走产品，店员一次只能持有固定数量的产品 (比如 20)，如果生产者试图生产更多的产品，店员会叫生产者停一下，如果店中有空位放产品了再通知生产者继续生产；如果店中没有产品了，店员会告诉消费者等一下，如果店中有产品了再通知消费者来取走产品。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class ProductTest &#123; public static void main(String[] args) &#123; Clerk clerk = new Clerk(); Producer producer1 = new Producer(clerk); producer1.setName(&quot;生产者1&quot;); Consumer consumer1 = new Consumer(clerk); consumer1.setName(&quot;消费者1&quot;); Consumer consumer2 = new Consumer(clerk); consumer2.setName(&quot;消费者2&quot;); producer1.start(); consumer1.start(); consumer2.start(); &#125;&#125;class Clerk &#123; private int productCount = 0; public synchronized void produceProduct() &#123; if (productCount &lt; 20) &#123; productCount++; System.out.println(Thread.currentThread().getName() + &quot;开始生产第&quot; + productCount + &quot;个产品&quot;); notify(); &#125; else &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public synchronized void consumerProduct() &#123; if (productCount &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;开始消费第&quot; + productCount + &quot;个产品&quot;); productCount--; notify(); &#125; else &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;// 生产者class Producer extends Thread &#123; private Clerk clerk; public Producer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + &quot;开始生产产品...&quot;); while (true) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; clerk.produceProduct(); &#125; &#125;&#125;// 消费者class Consumer extends Thread &#123; private Clerk clerk; public Consumer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + &quot;开始消费产品...&quot;); while (true) &#123; try &#123; Thread.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; clerk.consumerProduct(); &#125; &#125;&#125; 面试题：sleep() 和 wait() 的异同。 相同点：一旦执行方法，都可以使得当前的线程进入阻塞状态。 不同点： 两个方法声明的位置不同：`sleep()` 声明在 Thread 类中，`wait()` 声明在 Object 类中。 调用的要求不同：`sleep()` 可以在任何需要的场景下调用，`wait()` 必须使用在同步代码块或同步方法中。 关于是否释放同步监视器：如果两个方法都是用在同步代码块或同步方法中，`sleep()` 不会释放锁，`wait()` 会释放锁。 线程的死锁问题死锁： 不同的线程分别占用对方需要的同步资源不放弃，都在等待对方放弃自己需要的同步资源，就形成了线程的死锁。 出现死锁后，不会出现异常，不会出现提示，只是所有的线程都处于阻塞状态，无法继续。 实例一： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class DeadLock &#123; public static void main(String[] args) &#123; StringBuilder s1 = new StringBuilder(); StringBuilder s2 = new StringBuilder(); // 继承Thread类 new Thread() &#123; @Override public void run() &#123; synchronized (s1) &#123; s1.append(&quot;a&quot;); s2.append(1); // 添加sleep()，增加死锁触发的概率 try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (s2) &#123; s1.append(&quot;b&quot;); s2.append(2); System.out.println(s1); System.out.println(s2); &#125; &#125; &#125; &#125;.start(); // 实现Runnable接口 new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (s2) &#123; s1.append(&quot;c&quot;); s2.append(3); // 添加sleep()，增加死锁触发的概率 try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (s1) &#123; s1.append(&quot;d&quot;); s2.append(4); System.out.println(s1); System.out.println(s2); &#125; &#125; &#125; &#125;).start(); &#125;&#125; 实例二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class A &#123; public synchronized void foo(B b) &#123;// 同步监视器：A的对象 System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot;, 进入了A实例的foo方法&quot;); // ① try &#123; Thread.sleep(200); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot;, 企图调用B实例的last方法&quot;); // ③ b.last(); &#125; public synchronized void last() &#123; System.out.println(&quot;进入了A类的last方法内部&quot;); &#125;&#125;class B &#123; public synchronized void bar(A a) &#123;// 同步监视器：B的对象 System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot;, 进入了B实例的bar方法&quot;); // ② try &#123; Thread.sleep(200); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot;, 企图调用A实例的last方法&quot;); // ④ a.last(); &#125; public synchronized void last() &#123; System.out.println(&quot;进入了B类的last方法内部&quot;); &#125;&#125;public class DeadLock implements Runnable &#123; A a = new A(); B b = new B(); public void init() &#123; Thread.currentThread().setName(&quot;主线程&quot;); // 调用a对象的foo方法 a.foo(b); System.out.println(&quot;进入了主线程之后&quot;); &#125; @Override public void run() &#123; Thread.currentThread().setName(&quot;副线程&quot;); // 调用b对象的bar方法 b.bar(a); System.out.println(&quot;进入了副线程之后&quot;); &#125; public static void main(String[] args) &#123; DeadLock deadLock = new DeadLock(); new Thread(deadLock).start(); deadLock.init(); &#125;&#125; 解决死锁的方法： 专门的算法、原则。 尽量减少同步资源的定义。 尽量避免嵌套同步。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的异常处理","slug":"java-exception","date":"2021-03-03T09:30:12.000Z","updated":"2021-04-09T08:00:36.326Z","comments":true,"path":"2021/03/03/java-exception/","link":"","permalink":"http://example.com/2021/03/03/java-exception/","excerpt":"","text":"在使用计算机语言进行项目开发的过程中，即使程序员把代码写得 尽善尽美，在系统的运行过程中仍然会遇到一些问题，因为很多问题不是靠代码能够避免的，比如：客户输入数据的格式，读取文件是否存在，网络是否始终保持通畅等等。 在 java 语言中，将程序执行中发生的不正常情况称为异常。注意：开发过程中的语法错误和逻辑错误不是异常。 对于这些错误，一般有两种解决方法：一是遇到错误就终止程序的运行；另一种方法是由程序员在编写程序时，就考虑到错误的检测、错误消息的提示，以及错误的处理。捕获错误最理想的是在编译期间，但有的错误只有在运行时才会发生。比如：除数为0，数组下标越界等。 异常体系结构 父类：java.lang.Throwable。常见的异常分类如下： java.lang.Error：java 虚拟机无法解决的严重问题。如：JVM 系统内部错误、资源耗尽等严重情况。比如：StackOverflowError 和 OutOfMemoryError (OOM)。一般不编写针对性的代码进行处理 (需要更改代码逻辑等去解决问题)。 123456789public class ErrorTest &#123; public static void main(String[] args) &#123; // 1.栈溢出：java.lang.StackOverflowError main(args); // 2.堆溢出：java.lang.OutOfMemoryError: Java heap space Integer[] arr = new Integer[1024 * 1024 * 1024]; &#125;&#125; java.lang.Exception：其它因编程错误或偶然的外在因素导致的一般性问题，可以使用针对性的代码进行处理。例如：空指针访问、试图读取不存在的文件、网络连接中断和数组角标越界等。 编译时异常：是指编译器要求必须处置的异常。即程序在运行时由于外界因素造成的一般性异常。编译器要求 java 程序必须捕获或声明所有编译时异常。对于这类异常，如果程序不处理，可能会带来意想不到的结果。 java.io.IOException 和 java.io.FileNotFoundException 1234567891011public class IOEx &#123; public static void main(String[] args) &#123; File file = new File(&quot;hello.txt&quot;); FileInputStream fis = new FileInputStream(file);// !java.io.FileNotFoundException int data; while ((data = fis.read()) != -1) &#123;// !java.io.IOException System.out.println((char) data); &#125; fis.close();// !java.io.IOException &#125;&#125; 如上代码，在编译期 (javac.exe) 就会出错，编译不通过，无法生成字节码文件。 运行时异常：是指编译器不要求强制处置的异常。一般是指编程时的逻辑错误，是程序员应该积极避免其出现的异常。java.lang.RuntimeException 类及它的子类都是运行时异常。对于这类异常，可以不作处理，因为这类异常很普遍，若全处理可能会对程序的可读性和运行效率产生影响。 java.lang.NullPointerException 123456789public class NullRef &#123; int i = 1; public static void main(String[] args) &#123; NullRef t = new NullRef(); t = null; System.out.println(t.i); &#125;&#125; java.lang.ArrayIndexOutOfBoundsException 123456789public class IndexOutExp &#123; public static void main(String[] args) &#123; String[] friends = &#123;&quot;lisa&quot;, &quot;bily&quot;, &quot;kessy&quot;&#125;; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(friends[i]); // friends[4]? &#125; System.out.println(&quot;\\nthis is the end&quot;); &#125;&#125; java.lang.ClassCastException 12345678public class Order &#123; public static void main(String[] args) &#123; Object obj = new Date(); Order order; order = (Order) obj; System.out.println(order); &#125;&#125; java.lang.NumberFormatException 1234567public class NumFormat &#123; public static void main(String[] args) &#123; String str = &quot;abc&quot;; int num = Integer.parseInt(str); System.out.println(&quot;num = &quot; + num); &#125;&#125; java.util.InputMismatchExcjavaeption 12345678public class NumFormat &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); int num = scanner.nextInt();// 输入的非整数 System.out.println(&quot;num = &quot; + num); scanner.close(); &#125;&#125; java.lang.ArithmeticException 12345678910public class DivideZero &#123; int x; public static void main(String[] args) &#123; DivideZero c = new DivideZero(); int y = 3 / c.x; System.out.println(&quot;y = &quot; + y); System.out.println(&quot;program ends ok!&quot;); &#125;&#125; 异常处理机制在编写程序时，经常要在可能出现错误的地方加上检测的代码，如进行 x / y 运算时，要检测分母为 0、数据为空、输入的不是数据而是字符等。而过多的 if - else 分支会导致程序的代码加长、臃肿，可读性差。因此采用异常处理机制。 java 采用的异常处理机制，是将异常处理的程序代码集中在一起，与正常的程序代码分开，使得程序简洁、优雅，并易于维护。 异常的处理：抓抛模型 过程一 — “抛”：程序在正常执行的过程中，一旦出现异常，就会在异常代码处生成一个对应异常类的对象，并将此对象抛出。一旦抛出对象以后，其后的代码就不再继续执行。 关于异常对象的产生： 系统自动生成的异常对象。 手动的生成一个异常对象，并抛出 (throw)。 过程二 — “抓”：可以理解为异常的处理方式，分为两种。 try -catch -finally throws try - catch - finally格式： try：捕获异常的第一步是用 try 语句块选定捕获异常的范围，将可能出现异常的代码放在 try 语句块中。 catch (ExceptionType e)：在 catch 语句块中是对异常对象进行处理的代码。 每个 try 语句块可以伴随一个或多个 catch 语句，用于处理可能产生的不同类型的异常对象，当异常对象匹配到某一个 catch 时，就进入该 catch 中进行异常的处理，一旦处理完成，就跳出当前的 try - catch，结构，继续执行 finally 结构和其后的代码。 如果明确知道产生的是何种异常，可以用该异常类作为 catch 的参数；也可以用其父类作为 catch 的参数。比如：可以用 ArithmeticException 类作为参数的地方，就可以用 RuntimeException 类作为参数，或者用所有异常的父类 Exception 类作为参数。但不能是与 ArithmeticException 类无关的异常，如 NullPointerException，此时 catch 中的语句将不会执行。 与其它对象一样，可以访问 catch 到的异常对象的成员变量或调用它的方法。 getMessage() 获取异常的说明信息，返回字符串。 printStackTrace() 获取异常类名和异常的说明信息，以及异常出现在程序中的位置，返回值 void。 在 try 结构中声明的变量，出了 try 结构以后，就不能再使用了。 finally：捕获异常的最后一步是通过 finally 语句为异常处理提供一个统一的出口，使得在控制流转到程序的其它部分以前，能够对程序的状态作统一的管理。 不论在 try 代码块中是否发生了异常事件，catch 语句是否执行，catch 语句是否有异常，try 或 catch 语句中是否有 return，finally 块中的语句都会被执行。 123456789101112131415161718192021222324public class ExceptionTest &#123; public int method() &#123; try &#123; int[] arr = new int[10]; System.out.println(arr[10]); return 1; &#125; catch (ArrayIndexOutOfBoundsException e) &#123; e.printStackTrace(); return 2; &#125; finally &#123; System.out.println(&quot;finally一定会被执行&quot;); return 3;// finally中不建议使用return &#125; &#125; public static void main(String[] args) &#123; ExceptionTest exceptionTest = new ExceptionTest(); int method = exceptionTest.method(); System.out.println(method); &#125;&#125;输出结果：finally一定会被执行3 由上面代码也可以看出，finally 语句的内容，会在 try 或 catch 的 return 语句之前执行。 像数据库连接、输入输出流、网络编程 Socket 等资源，JVM 是不能自动回收的，需要手动的进行资源的释放。此时的资源释放，就需要声明在 finally 中。 1234567891011121314151617181920212223public class ExceptionTest &#123; public static void main(String[] args) &#123; File file = new File(&quot;hello.txt&quot;); FileInputStream fis = null; try &#123; fis = new FileInputStream(file); int data; while ((data = fis.read()) != -1) &#123; System.out.println((char) data); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fis != null) &#123; fis.close(); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125;&#125; try - catch - finally 结构可以嵌套。 try - catch - finally 结构中，try 语句必须存在，catch 和 finally 语句，必须至少存在一个，其中，catch 语句可以出现多个。 使用 try - catch - finally 处理编译时异常，使得程序在编译时就不再报错，但在运行时仍可能报错。相当于使用 try - catch - finally 将一个编译时可能出现的异常，延迟到运行时出现。 开发中，由于运行时异常比较常见，通常不针对运行时异常编写 try - catch - finally，针对编译时异常，一定要考虑异常的处理。 try - catch - finally：真正的将异常进行了处理。 throws如果一个方法中的语句执行时可能生成某种异常，但是并不能确定如何处理这种异常，则此方法应显示地声明抛出异常，表明该方法将不对这些异常进行处理，而由该方法的调用者负责处理。在方法声明中用 throws 语句可以声明抛出异常的列表，throws 后面的异常类型可以是方法中产生的异常类型，也可以是它的父类。注意：不同于 try - catch - finally，throws 并没有真正的将异常进行了处理，而是抛给了方法的调用者去处理。 “throws + 异常类型”，写在方法的声明处。指明此方法执行时，可能会抛出的异常类型。一旦当方法体执行时出现异常，仍会在异常代码处生成一个异常类的对象，此对象满足 throws 后的异常类型时，就会被抛出。异常代码后续的代码，就不再被执行。 重写方法声明抛出异常的原则：子类重写的方法不能抛出比父类被重写的方法范围更大的异常类型。 1234567891011121314public class A &#123; public void methodA() throws IOException &#123; &#125;&#125;public class B1 extends A &#123; public void methodA() throws FileNotFoundException &#123; &#125;&#125;public class B2 extends A &#123; public void methodA() throws Exception &#123;// 报错 &#125;&#125; 开发中如何选择使用 try - catch -finally 和 throws 如果父类中被重写的方法没有 throws 方式处理异常，则子类重写的方法也不能使用 throws，意味着如果子类重写的方法中由异常，必须使用 try - catch -finally 方式处理。 执行的方法中，先后又调用了另外的几个方法，这几个方法是递进关系执行的，则建议这几个方法使用 throws 的方式进行处理。而执行的方法中，可以考虑使用 try - catch -finally 方式进行处理。 手动生成并抛出异常java 异常类对象除在程序执行过程中出现异常时由系统自动生成并抛出，也可根据需要使用人工创建并抛出 。 首先要生成异常类对象，然后通过 throw 语句实现抛出操作 (提交给 java 运行环境)。如：IOException e = new IOException(); throw e;。 可以抛出的异常必须是 Throwable 或其子类的实例。下面的语句在编译时将会产生语法错误：throw new String(&quot;want to throw&quot;);。 实例： 1234567891011121314151617181920212223242526272829303132public class Student &#123; public static void main(String[] args) &#123; Student student = new Student(); student.regist(-100); try &#123; student.regist2(-200); &#125; catch (Exception exception) &#123; System.out.println(exception.getMessage()); &#125; &#125; private int id; public void regist(int id) &#123; if (id &gt; 0) &#123; this.id = id; &#125; else &#123; // 手动抛出异常 throw new RuntimeException(&quot;输入的数据非法：&quot; + id); &#125; &#125; public void regist2(int id) throws Exception &#123; if (id &gt; 0) &#123; this.id = id; &#125; else &#123; // 手动抛出异常，需要在方法中声明 throw new Exception(&quot;输入的数据非法：&quot; + id); &#125; &#125;&#125; 执行 throw 后，后面代码还会执行吗？ 12345678910111213141516171819public class ExceptionTest &#123; public static void main(String[] args) &#123; int i = 1; if (i &gt; 5) &#123; System.out.println(i); &#125; else &#123; try &#123; throw new Exception(&quot;数据非法！&quot;);// 异常被try-catch &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;123&quot;); &#125; &#125;&#125;输出结果：java.lang.Exception: 数据非法！ at cn.xisun.java.base.ExceptionTest.main(ExceptionTest.java:17)123 1234567891011public class ExceptionTest &#123; public static void main(String[] args) throws Exception &#123; int i = 1; if (i &gt; 5) &#123; System.out.println(i); &#125; else &#123; throw new Exception(&quot;数据非法！&quot;);// 异常被throws System.out.println(&quot;123&quot;);// 编译不通过 &#125; &#125;&#125; 1234567891011121314public class ExceptionTest &#123; public static void main(String[] args) throws Exception &#123; int i = 1; if (i &gt; 5) &#123; System.out.println(i); &#125; else &#123; throw new Exception(&quot;数据非法！&quot;); &#125; System.out.println(&quot;123&quot;); &#125;&#125;输出结果：Exception in thread &quot;main&quot; java.lang.Exception: 数据非法！ at cn.xisun.java.base.ExceptionTest.main(ExceptionTest.java:16) 1234567891011121314151617181920212223public class ExceptionTest &#123; private static void get(int i) &#123; try &#123; int a = i / 0; System.out.println(a); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot;发生异常&quot;); &#125; System.out.println(&quot;123&quot;); &#125; public static void main(String[] args) &#123; for (int i = 1; i &lt; 4; i++) &#123; System.out.println(i); get(i); &#125; &#125;&#125;输出结果：1Exception in thread &quot;main&quot; java.lang.RuntimeException: 发生异常 at cn.xisun.java.base.ExceptionTest.get(ExceptionTest.java:16) at cn.xisun.java.base.ExceptionTest.main(ExceptionTest.java:24) 用户自定义异常类如何自定义异常类： 继承于现有的异常结构：RuntimeException、Exception 等。 提供全局变量：serialVersionUID，唯一的标识当前类。 提供重载的构造器。 异常类的名字应做到见名知义，当异常出现时，可以根据名字判断异常类型。 实例： 1234567891011121314151617181920212223242526272829303132333435363738public class MyException extends Exception &#123; static final long serialVersionUID = 13465653435L; public MyException() &#123; &#125; public MyException(String message) &#123; super(message); &#125;&#125;class MyExpTest &#123; public void regist(int num) throws MyException &#123; if (num &lt; 0) &#123; throw new MyException(&quot;人数为负值，不合理&quot;); &#125; else &#123; System.out.println(&quot;登记人数&quot; + num); &#125; &#125; public void manager() &#123; try &#123; regist(-100); &#125; catch (MyException e) &#123; System.out.print(&quot;登记失败，出错信息：&quot; + e.getMessage()); &#125; System.out.print(&quot;本次登记操作结束&quot;); &#125; public static void main(String args[]) &#123; MyExpTest t = new MyExpTest(); t.manager(); &#125;&#125;输出结果：登记失败，出错信息：人数为负值，不合理本次登记操作结束 12345678910111213141516171819202122232425262728293031323334public class ReturnExceptionDemo &#123; static void methodA() &#123; try &#123; System.out.println(&quot;进入方法A&quot;); throw new RuntimeException(&quot;制造异常&quot;); &#125; finally &#123; System.out.println(&quot;调用A方法的finally&quot;); &#125; &#125; static void methodB() &#123; try &#123; System.out.println(&quot;进入方法B&quot;); return; &#125; finally &#123; System.out.println(&quot;调用B方法的finally&quot;); &#125; &#125; public static void main(String[] args) &#123; try &#123; methodA(); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; methodB(); &#125;&#125;输出结果：进入方法A调用A方法的finally制造异常进入方法B调用B方法的finally 异常处理的 5 个关键字 面试题： final、finally 和 finalize 的区别？ finalize 是一个方法。 throw 和 throws 的区别？ throw 表示抛出一个异常类的对象，生成异常对象的过程，声明在方法体内。 throws 属于异常处理的一种方式，声明在方法的声明处。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 的设计模式","slug":"java-design-mode","date":"2021-03-01T02:23:19.000Z","updated":"2021-04-09T08:00:21.316Z","comments":true,"path":"2021/03/01/java-design-mode/","link":"","permalink":"http://example.com/2021/03/01/java-design-mode/","excerpt":"","text":"设计模式是在大量的实践中总结和理论化之后优选的代码结构、编程风格、以及解决问题的思考方式。设计模式就像是经典的棋谱，不同的棋局，我们用不同的棋谱，免去自己再思考和摸索。 单例 (Singleton) 设计模式所谓类的单例设计模式，就是采取一定的方法保证在整个的软件系统中，对某个类只能存在一个对象实例，并且该类只提供一个取得其对象实例的方法。如果我们要让类在一个虚拟机中只能产生一个对象，我们首先必须将类的构造器的访问权限设置为 private，这样，就不能用 new 操作符在类的外部产生类的对象了，但在类内部仍可以产生该类的对象。因为在类的外部开始还无法得到类的对象，只能调用该类的某个静态方法以返回类内部创建的对象，静态方法只能访问类中的静态成员变量，所以，指向类内部产生的该类对象的变量也必须定义成静态的。 单例设计模式的优点：由于单例模式只生成一个实例，减少了系统性能开销，当一个对象的产生需要比较多的资源时，如读取配置、产生其他依赖对象时，则可以通过在应用启动时直接产生一个单例对象，然后永久驻留内存的方式来解决。比如，java.lang.Runtime： 单例设计模式的应用场景： 网站的计数器，一般也是单例模式实现，否则难以同步。 应用程序的日志应用，一般都使用单例模式实现，这一般是由于共享的日志文件一直处于打开状态，因此只能有一个实例去操作，否则内容不好追加。 数据库连接池的设计一般也是采用单例模式，因为数据库连接是一种数据库资源。 项目中，读取配置文件的类，一般也只有一个对象。没有必要每次使用配置文件数据，都生成一个对象去读取。 Application 也是单例的典型应用。 Windows 的 Task Manager (任务管理器) 就是很典型的单例模式。 Windows 的 Recycle Bin (回收站) 也是典型的单例应用。在整个系统运行过程中，回收站一直维护着仅有的一个实例。 单例设计模式的实现方法： 饿汉式： 1234567891011121314151617181920212223public class SingletonTest &#123; public static void main(String[] args) &#123; Bank bank1 = Bank.getInstance(); Bank bank2 = Bank.getInstance(); System.out.println(bank1 == bank2);// true，二者指向同一个对象 &#125;&#125;class Bank &#123; // 1.私有化类的构造器 private Bank() &#123; &#125; // 2.内部创建类的对象 // 4.要求此对象也必须声明为静态的 private static Bank instance = new Bank(); // 3.提供公共的静态方法，返回类的对象 public static Bank getInstance() &#123; return instance; &#125;&#125; 123456789101112131415public class SingletonTest &#123; public static void main(String[] args) &#123; Bank bank1 = Bank.getInstance(); Bank bank2 = Bank.getInstance(); System.out.println(bank1 == bank2);// true，二者指向同一个对象 &#125;&#125;class Bank &#123; private Bank() &#123; &#125; public static final Bank instance = new Bank();// 添加final是防止instance属性被外部修改&#125; 懒汉式： 123456789101112131415161718192021222324252627282930313233343536373839public class SingletonTest &#123; public static void main(String[] args) &#123; Bank bank1 = Bank.getInstance(); Bank bank2 = Bank.getInstance(); System.out.println(bank1 == bank2);// true，二者指向同一个对象 &#125;&#125;class Bank &#123; // 1.私有化类的构造器 private Bank() &#123; &#125; // 2.内部声明类的对象，没有初始化 // 4.要求此对象也必须声明为静态的 private static Bank instance = null; // 3.提供公共的静态方法，返回类的对象 public static Bank getInstance() &#123; // 同步方式一：效率稍差，等同于在方法上直接添加synchronized /*synchronized (Bank.class) &#123; if (instance == null) &#123; instance = new Bank(); &#125; &#125; return instance;*/ // 同步方式二：效率稍好 if (instance == null) &#123; synchronized (Bank.class) &#123; if (instance == null) &#123; instance = new Bank(); &#125; &#125; &#125; return instance; &#125;&#125; 区分饿汉式和懒汉式： 饿汉式 好处：天然就是线程安全的。 坏处：类加载时就创建了对象，导致对象加载时间过长。 懒汉式 好处：延迟对象的创建。 坏处：不是线程安全的，多线程情况下需要考虑线程安全问题。 模板式方法设计模式 (TemplateMethod)抽象类体现的就是一种模板模式的设计，抽象类作为多个子类的通用模板，子类在抽象类的基础上进行扩展、改造，但子类总体上会保留抽象类的行为方式。 解决的问题： 当功能内部一部分实现是确定的，一部分实现是不确定的。这时可以把不确定的部分暴露出去，让子类去实现。 换句话说，在软件开发中实现一个算法时，整体步骤很固定、通用，这些步骤已经在父类中写好了。但是某些部分易变，易变部分可以抽象出来，供不同子类实现。这就是一种模板模式。 模板方法设计模式是编程中经常用得到的模式。各个框架、类库中都有他的影子，比如常见的有： 数据库访问的封装 Junit 单元测试 JavaWeb 的 Servlet 中关于 doGet/doPost 方法调用 Hibernate 中模板程序 Spring 中 JDBCTemlate、HibernateTemplate 等 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; SubTemplate subTemplate = new SubTemplate(); subTemplate.getTime(); &#125;&#125;abstract class Template &#123; // 计算一段代码的执行时间 public final void getTime() &#123; long start = System.currentTimeMillis(); code(); long end = System.currentTimeMillis(); System.out.println(&quot;执行时间是：&quot; + (end - start)); &#125; // 代码不确定，由子类自己实现 --- 不确定的、异变的部分 public abstract void code();&#125;class SubTemplate extends Template &#123; @Override public void code() &#123; for (int i = 0; i &lt; 10000; i++) &#123; System.out.println(i); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 抽象类的应用：模板方法的设计模式public class TemplateMethodTest &#123; public static void main(String[] args) &#123; BankTemplateMethod btm = new DrawMoney(); btm.process(); BankTemplateMethod btm2 = new ManageMoney(); btm2.process(); &#125;&#125;abstract class BankTemplateMethod &#123; // 具体方法 public void takeNumber() &#123; System.out.println(&quot;取号排队&quot;); &#125; public abstract void transact(); // 办理具体的业务 --- 钩子方法 public void evaluate() &#123; System.out.println(&quot;反馈评分&quot;); &#125; // 模板方法，把基本操作组合到一起，子类一般不能重写 public final void process() &#123; this.takeNumber(); this.transact();// 像个钩子，具体执行时，挂哪个子类，就执行哪个子类的实现代码 this.evaluate(); &#125;&#125;class DrawMoney extends BankTemplateMethod &#123; @Override public void transact() &#123; System.out.println(&quot;我要取款！！！&quot;); &#125;&#125;class ManageMoney extends BankTemplateMethod &#123; @Override public void transact() &#123; System.out.println(&quot;我要理财！我这里有2000万美元!!&quot;); &#125;&#125; 代理模式 (Proxy)应用场景： 安全代理：屏蔽对真实角色的直接访问。 远程代理：通过代理类处理远程方法调用 (RMI)。 延迟加载：先加载轻量级的代理对象，真正需要再加载真实对象。比如，要开发一个大文档查看软件，大文档中有大的图片，有可能一个图片有 100 MB，在打开文件时，不可能将所有的图片都显示出来，这样就可以使用代理模式，当需要查看图片时，用 proxy 来进行大图片的打开。 分类： 静态代理 (静态定义代理类) 动态代理 (动态生成代理类) JDK 自带的动态代理，需要反射等知识。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class NetWorkTest &#123; public static void main(String[] args) &#123; Server1 server1 = new Server1(); Server1 server2 = new Server1(); ProxyServer proxyServer1 = new ProxyServer(server1); ProxyServer proxyServer2 = new ProxyServer(server2); // 表面上是代理类执行了browse()方法，实际上是被代理类执行的browse()方法 proxyServer1.browse(); proxyServer2.browse(); &#125;&#125;interface Network &#123; public void browse();&#125;// 被代理类1class Server1 implements Network &#123; @Override public void browse() &#123; System.out.println(&quot;真实的服务器1访问网络&quot;); &#125;&#125;// 被代理类class Server2 implements Network &#123; @Override public void browse() &#123; System.out.println(&quot;真实的服务器2访问网络&quot;); &#125;&#125;// 代理类class ProxyServer implements Network &#123; private Network work; public ProxyServer(Network work) &#123; this.work = work; &#125; public void check() &#123; System.out.println(&quot;联网之前的检查工作&quot;); &#125; @Override public void browse() &#123; // 代理类除了执行核心功能外，还执行了其他的一些工作 // 被代理类不需要关系这些其他的工作，只需要完成核心功能即可 check(); work.browse(); &#125;&#125;输出结果：联网之前的检查工作真实的服务器1访问网络联网之前的检查工作真实的服务器1访问网络 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class StaticProxyTest &#123; public static void main(String[] args) &#123; Star s = new Proxy(new RealStar()); s.confer(); s.signContract(); s.bookTicket(); s.sing(); s.collectMoney(); &#125;&#125;interface Star &#123; void confer();// 面谈 void signContract();// 签合同 void bookTicket();// 订票 void sing();// 唱歌 void collectMoney();// 收钱&#125;class RealStar implements Star &#123; @Override public void confer() &#123; &#125; @Override public void signContract() &#123; &#125; @Override public void bookTicket() &#123; &#125; @Override public void sing() &#123; System.out.println(&quot;明星：歌唱~~~&quot;); &#125; @Override public void collectMoney() &#123; &#125;&#125;class Proxy implements Star &#123; private Star real; public Proxy(Star real) &#123; this.real = real; &#125; @Override public void confer() &#123; System.out.println(&quot;经纪人面谈&quot;); &#125; @Override public void signContract() &#123; System.out.println(&quot;经纪人签合同&quot;); &#125; @Override public void bookTicket() &#123; System.out.println(&quot;经纪人订票&quot;); &#125; @Override public void sing() &#123; real.sing(); &#125; @Override public void collectMoney() &#123; System.out.println(&quot;经纪人收钱&quot;); &#125;&#125;输出结果：经纪人面谈经纪人签合同经纪人订票明星：歌唱~~~经纪人收钱 工厂模式","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的排序算法","slug":"java-algorithm-sort","date":"2021-02-19T08:30:35.000Z","updated":"2021-04-09T07:59:44.704Z","comments":true,"path":"2021/02/19/java-algorithm-sort/","link":"","permalink":"http://example.com/2021/02/19/java-algorithm-sort/","excerpt":"","text":"算法的五大特征： 说明：满足确定性的算法也称为确定性算法。现在人们也关注更广泛的概念，例如考虑各种非确定性的算法，如并行算法、概率算法等。另外，人们也关注并不要求终止的计算描述，这种描述有时被称为过程 (procedure)。 排序：假设含有 n 个记录的序列为 {R1, R2, …, Rn}，其相应的关键字序列为 {K1, K2, …, Kn}。将这些记录重新排序为 {Ri1, Ri2, …, Rin}，使得相应的关键字值满足条 Ki1&lt;= Ki2 &lt;= … &lt;= Kin，这样的一种操作称为排序。通常来说，排序的目的是快速查找。 衡量排序算法的优劣： 时间复杂度：分析关键字的比较次数和记录的移动次数。 空间复杂度：分析排序算法中需要多少辅助内存。 稳定性：若两个记录 A 和 B 的关键字值相等，但排序后 A、B 的先后次序保持不变，则称这种排序算法是稳定的。 排序算法分类：内部排序和外部排序。 内部排序：整个排序过程不需要借助于外部存储器 (如磁盘等)，所有排序操作都在内存中完成。 外部排序：参与排序的数据非常多，数据量非常大，计算机无法把整个排序过程放在内存中完成，必须借助于外部存储器（如磁盘）。外部排序最常见的是多路归并排序。可以认为外部排序是由多次内部排序组成。 十大内部排序算法：","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 的基础知识","slug":"java-base","date":"2021-02-10T06:47:42.000Z","updated":"2021-04-09T07:59:53.474Z","comments":true,"path":"2021/02/10/java-base/","link":"","permalink":"http://example.com/2021/02/10/java-base/","excerpt":"","text":"bit 和 byte计算机本质是一系列的电路开关。每个开关存在两种状态：开 (on) 和关 (off)。如果电路是开的，它的值是 1，如果电路是关的，它的值是 0。 一个 0 或者一个 1 存储为一个比特 (bit)，是计算机中最小的存储单位。 计算机中最基本的存储单元是字节 (byte) 。每个字节由 8 个比特构成。 计算机的存储能力是以字节来衡量的。如下： 千字节 (kilobyte，KB) = 1024 B 兆字节 (megabyte，MB) = 1024 KB 千兆字节 (gigabyte，GB) = 1024 MB 万亿字节 (terabyte，TB) = 1024 GB JDK、JRE 和 JVM JDK = JRE + 开发工具集（例如 Javac 编译工具等） JRE = JVM + Java SE 标准类库 阶段 编写：编写的 java 代码保存在以 .java 为结尾的源文件中。 在一个 java 源文件中，可以声明多个 class 类，但是，只能最多有一个类声明为 public。而且，声明为 public 的类名，必须与源文件名相同。 编译：使用 javac.exe 命令编译 java 源文件。格式：javac 源文件名.java 编译之后，会生成一个或多个以 .class 结尾的字节码文件，字节码文件的文件名与 java 源文件中的类名相同，二者是一一对应的。 运行：使用 java.exe 命令解释运行字节码文件。格式：java 类名 运行的字节码文件，需要有入口函数 main() 方法，且书写格式是固定的。 编译完源文件后，生成一个或多个字节码文件。然后运行时，使用 JVM 中的类的加载器和解释器，对生成的字节码文件进行解释运行。即：此时，需要将字节码文件对应的类加载到内存中，这个过程涉及到内存解析。 注释单行注释：// 注释文字 多行注释： /* 注释文字 */ 文档注释：/** 注释文字 */ 对于单行注释和多行注释，被注释的文字，不会被 JVM 解释执行； 多行注释里面不允许有多行注释嵌套； 文档注释内容可以被 JDK 提供的工具 javadoc 所解析，生成一套以网页文件形式体现该程序的说明文档。 关键字和保留字关键字 (key word)定义：被 java 语言赋予了特殊含义，用做专门用途的字符串 (单词)。 特点：关键字中所有字母都为小写。 官方地址：https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html 保留字 (reserved word)现有 java 版本尚未使用，但以后版本可能会作为关键字使用。自己命名标识符时要避免使用这些保留字：goto 、const。 标识符java 对各种变量、方法和类等要素命名时使用的字符序列称为标识符。 技巧：凡是自己可以起名字的地方都叫标识符。 定义合法标识符规则： 由 26 个英文字母大小写，0 - 9，_ 或 $ 组成； 数字不可以开头； 不可以使用关键字和保留字，但能包含关键字和保留字； java 中严格区分大小写，长度无限制； 标识符不能包含空格。 如果不遵守以上规则，编译不通过。 名称命名规范： 包名：多单词组成时所有字母都小写：xxxyyyzzz； 类名、接口名：多单词组成时，所有单词的首字母大写：XxxYyyZzz； 变量名、方法名：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词首字母大写：xxxYyyZzz； 常量名：所有字母都大写。多单词时每个单词用下划线连接：XXX_YYY_ZZZ。 在命名时，为了提高阅读性，要尽量有意义，做到见名知意。 java 采用 unicode 字符集，因此标识符也可以使用汉字声明，但是不建议使用。 变量定义变量是内存中的一个存储区域，该区域的数据可以在同一类型范围内不断变化。 变量是程序中最基本的存储单元。包含变量类型、变量名和存储的值。 java 中每个变量必须先声明，后使用，使用变量名来访问这块区域的数据。 变量的作用域：其定义所在的一对 { } 内，变量只有在其作用域内才有效，在同一个作用域内，不能定义重名的变量。 1234567891011121314151617181920212223public class VariableTest&#123; public static void main(String[] args)&#123; // 变量的定义 int myAge = 12; // 变量的使用 System.out.println(myAge); // 编译错误：使用myNumber之前未定义myNumber // System.out.println(myNumber); // 变量的定义 int myNumber; // 编译错误：使用myNumber之前未赋值myNumber // System.out.println(myNumber); // 变量的赋值 myNumber = 1001; // 变量的使用 System.out.println(myNumber); &#125;&#125; 按数据类型分类java 是强类型语言，对于每一种数据都定义了明确的具体数据类型，并在内存中分配了不同大小的内存空间。 基本数据类型整数类型 java 各整数类型有固定的表数范围和字段长度，不受具体 OS 的影响，以保证 java 程序的可移植性。 java 的整型常量默认为 int 型，声明 long 型常量须后加 ‘l’ 或 ‘L’。java 程序中变量通常声明为 int 型，除非不足以表示较大的数，才使用 long。 浮点类型 与整数类型类似，java 浮点类型也有固定的表数范围和字段长度，不受具体操作系统的影响。 float：单精度，尾数可以精确到 7 位有效数字。很多情况下，精度很难满足需求。 double：双精度，精度是float的两倍。通常采用此类型。 java 的浮点型常量默认为 double 型，声明 float 型常量，须后加 ‘f’ 或 ‘F’。 字符类型char 型数据用来表示通常意义上的 “字符”，占用 2 个字节。char 类型是可以进行运算的。因为它都对应有 Unicode 码。 java 中的所有字符都使用Unicode编码，故一个字符可以存储一个字母，一个汉字，或其他书面语的一个字符。 字符型变量的三种表现形式： 字符常量是用单引号括起来的单个字符。例如：char c1 = &#39;a&#39;; char c2= &#39;中&#39;; char c3 = &#39;9&#39;;。 java 中还允许使用转义字符 \\ 来将其后的字符转变为特殊字符型常量。例如：char c3 = &#39;\\n&#39;; // &#39;\\n&#39;表示换行符。常用的转义字符如下： 直接使用 Unicode 值来表示字符型常量：’\\uXXXX’。其中，XXXX 代表一个十六进制整数。如：\\u000a 表示 \\n。 布尔类型boolean 类型用来判断逻辑条件，一般用于程序流程控制。 boolean 类型数据只允许取值 true 和 false，无 null。 java 虚拟机中没有任何供 boolean 值专用的字节码指令，java 语言表达所操作的 boolean 值，在编译之后都使用 java 虚拟机中的 int 数据类型来代替：true 用 1 表示，false 用 0 表示。———《java 虚拟机规范 8 版》 基本数据类型之间的转换自动类型转换：不同数据类型的变量做运算时，容量小的数据类型自动转换为容量大的数据类型。数据类型按容量大小排序为： 此处的容量大小，指的是该数据类型表示数的范围的大和小。 有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算。 byte，short 和 char 之间不会相互转换，他们三者在计算时首先转换为 int 类型。 boolean 类型不能与其它数据类型运算。 当把任何基本数据类型的值和字符串 (String) 进行连接运算时 (+)，基本数据类型的值将自动转化为字符串 (String) 类型。 强制类型转换：自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符：()，但可能造成精度降低或溢出。 通常，字符串不能直接转换为基本类型，但通过基本类型对应的包装类，可以实现把字符串转换成基本类型。如：String a = “43”; int i = Integer.parseInt(a);。 boolean 类型不可以转换为其它的数据类型。 引用数据类型StringString 不是基本数据类型，属于引用数据类型 (class)。使用方式与基本数据类型一致，例如：String str = “abcd”;。 一个字符串可以串接另一个字符串，也可以直接串接其他类型的数据。例如：str = str + “xyz” ; int n = 100; str = str + n;。 String 与 8 种基本数据类型做运算时，但只能是连接运算。 按声明位置分类成员变量：在方法体外，类体内声明的变量。 局部变量：在方法体内部声明的变量。 成员变量和局部变量在初始化值方面的异同：同：都有生命周期；异：局部变量除形参外，需显式初始化。 进制所有数字在计算机底层都以二进制形式存在。 对于整数，有四种表示方式： 二进制 (binary) ：0 - 1，满 2 进 1，以 0b 或 0B 开头表示。 十进制 (decimal) ：0 - 9，满 10 进 1。 八进制 (octal) ：0 - 7，满 8 进 1，以数字 0 开头表示。 十六进制 (hex) ：0 - 9 及 A - F，满 16 进 1，以 0x 或 0X 开头表示。此处的 A - F 不区分大小写。如：0x21AF +1= 0X21B0。 二进制java 整数常量默认是 int 类型，当用二进制定义整数时，其第 32 位是符号位；当是 long 类型时，二进制默认占 64 位，第 64 位是符号位。 二进制的整数有如下三种形式： 原码：直接将一个数值换成二进制数，最高位是符号位。 负数的反码：是对原码按位取反，但最高位 (符号位) 不变，确定为1。 负数的补码：其反码加 1。 正数的原码、反码、补码都相同。 计算机以二进制补码的形式保存所有的整数。 原码到补码的转换： 不同进制间转换十进制转二进制：除 2 取余的逆。 二进制和八进制、十六进制转换： 运算符运算符是一种特殊的符号，用以表示数据的运算、赋值和比较等。 算术运算符 如果对负数取模，可以把模数负号忽略不记，如：5 % -2 = 1。 如果被模数是负数，则不可忽略，如： -5 % 2 = -1。此外，取模运算的结果不一定总是整数。 对于除号 “/“，它的整数除和小数除是有区别的：整数之间做除法时，只保留整数部分而舍弃小数部分。 例如：int x = 3510; x = x / 1000 * 1000;，x 的结果是 3000。 “+” 除字符串相加功能外，还能把非字符串转换成字符串。例如：System.out.println(&quot;5 + 5 = &quot; + 5 + 5); ，打印结果是：5 + 5 = 55 。 赋值运算符符号：=。当 “=” 两侧数据类型不一致时，可以使用自动类型转换或使用强制类型转换原则进行处理。支持连续赋值。 扩展赋值运算符： +=，-=，*=，/=，%=。这几个赋值运算符不会改变变量本身的数据类型。 12345int i = 1;i *= 0.1;System.out.println(i);// 0i++;System.out.println(i);// 1 12345int m = 2;int n = 3;n *= m++;// n = n * m++;System.out.println(&quot;m = &quot; + m);// 3System.out.println(&quot;n = &quot; + n);// 6 123int n = 10;n += (n++) + (++n);// n = n + (n++) + (++n); → n = 10 + 10 + 12;System.out.println(n);// 32 比较运算符 (关系运算符) 比较运算符的结果都是 boolean 型。 逻辑运算符&amp;：逻辑与，|：逻辑或，!：逻辑非。&amp;&amp;：短路与，||：短路或，^：逻辑异或。 逻辑运算符用于连接布尔型表达式，在 java 中不可以写成 3 &lt; x &lt; 6，应该写成 x &gt; 3 &amp; x &lt; 6。 “&amp;” 和 “&amp;&amp;” 的区别：&amp; 表示，左边无论真假，右边都进行运算；&amp;&amp; 表示，如果左边为真，右边参与运算，如果左边为假，右边不参与运算。 “|” 和 “||” 的区别同理：| 表示，左边无论真假，右边都进行运算；||表示，如果左边为假，右边参与运算，如果左边为真，右边不参与运算。 异或 (^) 与或 (|) 的不同之处是：当左右都为 true 时，结果为 false。即：异或，追求的是异! 123456789101112131415161718192021222324252627int x = 1;int y = 1;if (x++ == 2 &amp; ++y == 2) &#123; x = 7;&#125;System.out.println(&quot;x = &quot; + x + &quot;, y = &quot; + y);// x = 2, y = 2x = 1;y = 1;if (x++ == 2 &amp;&amp; ++y == 2) &#123; x = 7;&#125;System.out.println(&quot;x = &quot; + x + &quot;, y = &quot; + y);// x = 2, y = 1x = 1;y = 1;if (x++ == 1 | ++y == 1) &#123; x = 7;&#125;System.out.println(&quot;x = &quot; + x + &quot;, y = &quot; + y);// x = 7, y = 2x = 1;y = 1;if (x++ == 1 || ++y == 1) &#123; x = 7;&#125;System.out.println(&quot;x = &quot; + x + &quot;, y = &quot; + y);// x = 7, y = 1 12345678910boolean x = true;boolean y = false;short z = 42;if ((z++ == 42) &amp;&amp; (y = true)) &#123; z++;&#125;if ((x = false) || (++z == 45)) &#123; z++;&#125;System.out.println(&quot;z = &quot; + z);// z = 46 位运算符 无 &lt;&lt;&lt;。 位运算是直接对整数的二进制进行的运算。 &lt;&lt; ：在一定范围内，每向左移一位，相当于乘以 2。 &gt;&gt;：在一定范围内，每向右移一位，相当于除以2。 面试题：最高效的计算 2 * 8。利用：2 &lt;&lt; 3，或者 8 &lt;&lt; 1。 交换两个数： 123456789101112131415161718192021222324252627282930313233int num1 = 10;int num2 = 20;System.out.println(num1 + &quot;, &quot; + num2);// 方式一int temp;temp = num1;num1 = num2;num2 = temp;System.out.println(num1 + &quot;, &quot; + num2);// 方式二num1 = 10;num2 = 20;num1 = num1 + num2;num2 = num1 - num2;num1 = num1 - num2;System.out.println(num1 + &quot;, &quot; + num2);// 方式三num1 = 10;num2 = 20;num1 = num1 ^ num2;num2 = num1 ^ num2;num1 = num1 ^ num2;System.out.println(num1 + &quot;, &quot; + num2);// 方式四num1 = 10;num2 = 20;num1 = num1 &lt;&lt; 1;num2 = num2 &gt;&gt; 1;System.out.println(num1 + &quot;, &quot; + num2); 三元运算符格式： 表达式 1 和表达式 2 要求类型是一致的，因为要与接受的参数类型相同。 凡是可以使用三元运算符的地方，都可以改写为 if - else 结构，反之，不成立。如果既可以使用三元运算符，又可以使用 if - else 结构，优先使用三元运算符，因为更简洁、效率更高。 三元运算符与 if - else 的联系与区别： 三元运算符可简化 if - else 语句。 三元运算符要求必须返回一个结果。 if 后的代码块可有多个语句。 运算符的优先级 运算符有不同的优先级，所谓优先级就是表达式运算中的运算顺序。如上表，上一行运算符总优先于下一行。 只有单目运算符、三元运算符、赋值运算符是从右向左运算的。 程序流程控制流程控制语句是用来控制程序中各语句执行顺序的语句，可以把语句组合成能完成一定功能的小逻辑模块。 流程控制方式采用结构化程序设计中规定的三种基本流程结构，即： 顺序结构：程序从上到下逐行地执行，中间没有任何判断和跳转。 分支结构：根据条件，选择性地执行某段代码。有 if - else 和 switch - case 两种分支语句。 循环结构：根据循环条件，重复性的执行某段代码。有while、do - while、for三种循环语句。 注：JDK 1.5 提供了 foreach 循环，方便遍历集合、数组元素。 if - else 结构 switch - case 结构 switch (表达式) 中表达式的值，必须是下述几种类型之一：byte ，short，char，int，枚举类 (jdk 5.0)，String 类 (jdk 7.0)。 case 子句中的值必须是常量，不能是变量名或不确定的表达式值。 同一个 switch 语句，所有 case 子句中的常量值互不相同。 break 语句用来在执行完一个 case 分支后使程序跳出 switch 语句块；如果没有 break，程序会顺序执行到 switch 结尾。 default 子句是可任选的。同时，位置也是灵活的。当没有匹配的 case 时，执行 default。 如果多个 case 的执行语句相同，则可以将其合并。 同等情况下，switch - case 结构比 if - else 结构的效率稍高。 123456789101112Scanner scanner = new Scanner(System.in);int num = scanner.nextInt();switch (num) &#123; case 0: System.out.println(0); case 1: System.out.println(1); case 2: System.out.println(2); default: System.out.println(&quot;other&quot;);&#125; 添加 break 和不添加 break 的结果是不同的。 12345678910111213141516Scanner scanner = new Scanner(System.in);int num = scanner.nextInt();switch (num) &#123; case 0: System.out.println(0); break; case 1: System.out.println(1); break; case 2: System.out.println(2); break; default: System.out.println(&quot;other&quot;); break;// default位于最后，此break可以不添加。&#125; 键盘输入一个月份和天数，判断其是一年中的第几天： 12345678910111213141516171819202122232425262728293031323334353637public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;请输入month：&quot;); int month = scanner.nextInt(); System.out.println(&quot;请输入day：&quot;); int day = scanner.nextInt(); int sumDays = 0; switch (month) &#123; case 12: sumDays += 30; case 11: sumDays += 31; case 10: sumDays += 30; case 9: sumDays += 31; case 8: sumDays += 31; case 7: sumDays += 30; case 6: sumDays += 31; case 5: sumDays += 30; case 4: sumDays += 31; case 3: sumDays += 28; case 2: sumDays += 31; case 1: sumDays += day; &#125; System.out.println(month + &quot;月&quot; + day + &quot;日，是当年的第&quot; + sumDays + &quot;天。&quot;);&#125; 键盘输入一个年份、月份和天数，判断其是该年中的第几天： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;请输入year：&quot;); int year = scanner.nextInt(); System.out.println(&quot;请输入month：&quot;); int month = scanner.nextInt(); System.out.println(&quot;请输入day：&quot;); int day = scanner.nextInt(); int sumDays = 0; switch (month) &#123; case 12: sumDays += 30; case 11: sumDays += 31; case 10: sumDays += 30; case 9: sumDays += 31; case 8: sumDays += 31; case 7: sumDays += 30; case 6: sumDays += 31; case 5: sumDays += 30; case 4: sumDays += 31; case 3: if ((year % 4 == 0 &amp;&amp; year % 100 != 0) || year % 400 == 0) &#123; sumDays += 29;// 闰年2月29天 &#125; else &#123; sumDays += 28;// 平年2月28天 &#125; case 2: sumDays += 31; case 1: sumDays += day; &#125; System.out.println(year + &quot;年&quot; + month + &quot;月&quot; + day + &quot;日，是当年的第&quot; + sumDays + &quot;天。&quot;);&#125; 判断一年是否是闰年的标准： 1）可以被 4 整除，但不可被 100 整除。 或 2）可以被 400 整除。 for 循环语法格式： 执行过程：① - ② - ③ - ④ - ② - ③ - ④ - ② - ③ - ④ - ….. - ② 说明： ② 循环条件部分为 boolean 类型表达式，当值为 false 时，退出循环。 ① 初始化部分可以声明多个变量，但必须是同一个类型，用逗号分隔。 ④ 迭代部分可以有多个变量更新，用逗号分隔。 键盘输入两个正整数，求他们的最大公约数和最小公倍数： 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;请输入第一个正整数：&quot;); int firstNum = scanner.nextInt(); System.out.println(&quot;请输入第二个正整数：&quot;); int secondNum = scanner.nextInt(); int min = Math.min(firstNum, secondNum); int max = Math.max(firstNum, secondNum); // 最大公约数 for (int i = min; i &gt;= 1; i--) &#123; if (firstNum % i == 0 &amp;&amp; secondNum % i == 0) &#123; System.out.println(firstNum + &quot;和&quot; + secondNum + &quot;的最大公约数为：&quot; + i); break; &#125; &#125; // 最小公倍数 for (int i = max; i &lt;= firstNum * secondNum; i++) &#123; if (i % firstNum == 0 &amp;&amp; i % secondNum == 0) &#123; System.out.println(firstNum + &quot;和&quot; + secondNum + &quot;的最小公倍数为：&quot; + i); break; &#125; &#125;&#125;请输入第一个正整数：12请输入第二个正整数：2012和20的最大公约数为：412和20的最小公倍数为：60 while 循环语法格式： 执行过程：① - ② - ③ - ④ - ② - ③ - ④ - ② - ③ - ④ - ….. - ② 说明： 注意不要忘记声明 ④ 迭代部分。否则，循环将不能结束，变成死循环。 for 循环和 while 循环可以相互转换。 do - while 循环语法格式： 执行过程：① - ③ - ④ - ② - ③ - ④ - ② - ③ - ④ - ② - ③ - ④ - ….. - ② 说明： do - while 循环至少执行一次循环体 。 嵌套循环 将一个循环放在另一个循环体内，就形成了嵌套循环。其中，for，while，do - while 均可以作为外层循环或内层循环。 实质上，嵌套循环就是把内层循环当成外层循环的循环体。当只有内层循环的循环条件为 false 时，才会完全跳出内层循环，才可结束外层的当次循环，开始下一次的循环。 设外层循环次数为 m 次，内层为 n 次，则内层循环体实际上需要执行 m * n 次。 九九乘法表： 123456789101112131415161718public static void main(String[] args) &#123; for (int i = 1; i &lt;= 9; i++) &#123; for (int j = 1; j &lt;= i; j++) &#123; System.out.print(j + &quot; * &quot; + i + &quot; = &quot; + (j * i) + &quot;\\t&quot;); &#125; System.out.println(); &#125;&#125;1 * 1 = 1 1 * 2 = 2 2 * 2 = 4 1 * 3 = 3 2 * 3 = 6 3 * 3 = 9 1 * 4 = 4 2 * 4 = 8 3 * 4 = 12 4 * 4 = 16 1 * 5 = 5 2 * 5 = 10 3 * 5 = 15 4 * 5 = 20 5 * 5 = 25 1 * 6 = 6 2 * 6 = 12 3 * 6 = 18 4 * 6 = 24 5 * 6 = 30 6 * 6 = 36 1 * 7 = 7 2 * 7 = 14 3 * 7 = 21 4 * 7 = 28 5 * 7 = 35 6 * 7 = 42 7 * 7 = 49 1 * 8 = 8 2 * 8 = 16 3 * 8 = 24 4 * 8 = 32 5 * 8 = 40 6 * 8 = 48 7 * 8 = 56 8 * 8 = 64 1 * 9 = 9 2 * 9 = 18 3 * 9 = 27 4 * 9 = 36 5 * 9 = 45 6 * 9 = 54 7 * 9 = 63 8 * 9 = 72 9 * 9 = 81 10000 以内所有的质数： 1234567891011121314151617181920212223242526// 方式一public static void main(String[] args) &#123; // 质数：素数，只能被1和它本身整除的自然数，2是最小的质数。 int count = 0; boolean ifFlag = true; for (int i = 2; i &lt;= 100000; i++) &#123; // 优化一：使用Math.sqrt(i)代替i，减少循环的次数 // i除以一个从2开始的小数，会得到一个从i-1开始的大数，因此，除以2开始的小数与除以从i-1开始的大数， // 可以省略一个，以减少次数，这样计算的中点是i开方的值。 for (int j = 2; j &lt;= Math.sqrt(i); j++) &#123; if (i % j == 0) &#123; ifFlag = false; // 优化二：使用break，跳出不必要的循环 break; &#125; &#125; if (ifFlag) &#123; // 优化三：不打印，i越大，打印的耗时越长 // System.out.println(&quot;质数：&quot; + i); count++; &#125; // 重置 ifFlag = true; &#125; System.out.println(&quot;质数的个数有：&quot; + count);// 质数的个数有：9592&#125; 123456789101112131415161718// 方式二public static void main(String[] args) &#123; // 质数：素数，只能被1和它本身整除的自然数，2是最小的质数。 int count = 0; label: for (int i = 2; i &lt;= 100000; i++) &#123; // 优化一：使用Math.sqrt(i)代替i，减少循环的次数 // i除以一个从2开始的小数，会得到一个从i-1开始的大数，因此，除以2开始的小数与除以从i-1开始的大数， // 可以省略一个，以减少次数，这样计算的中点是i开方的值。 for (int j = 2; j &lt;= Math.sqrt(i); j++) &#123; if (i % j == 0) &#123; continue label; &#125; &#125; count++; &#125; System.out.println(&quot;质数的个数有：&quot; + count);// 质数的个数有：9592&#125; break 和 continue break 使用在 switch - case 结构或者循环结构中。 continue 只能使用在循环结构中。 break 语句用于终止某个语句块的执行，跳出当前循环，continue 语句用于跳过其所在循环语句块的当次执行，继续下一次循环。 123456789public static void main(String[] args) &#123; for (int i = 1; i &lt;= 10; i++) &#123; if (i % 4 == 0) &#123; break;// 输出结果：1 2 3 continue;// 输出结果：1 2 3 5 6 7 9 10 &#125; System.out.print(i + &quot;\\t&quot;); &#125;&#125; break 语句出现在多层嵌套的语句块中时，可以通过标签指明要终止的是哪一层语句块 (默认跳出包裹 break 最近的一层循环)： continue 语句出现在多层嵌套的循环语句体中时，可以通过标签指明要跳过的是哪一层循环 (默认跳出包裹 continue 最近的一层循环)。 1234567891011121314151617public static void main(String[] args) &#123; label: for (int i = 1; i &lt;= 4; i++) &#123; for (int j = 1; j &lt;= 10; j++) &#123; if (j % 4 == 0) &#123; break label;// 结束指定标识label层的当前循环 continue label;// 结束指定标识label层的当次循环 &#125; System.out.print(j); &#125; System.out.println(); &#125;&#125;break label输出结果：1 2 3continue label输出结果：1 2 3 2 3 1 2 3 1 2 3 break 和 continue 关键字后面不能直接声明执行语句。 随机数获取 [a, b] 之间的随机数： 1int v = (int) (Math.random() * (b - a + 1) + a); 如获取 [10, 99] 之间的随机数： 1int v = (int) (Math.random() * 90 + 10); 数组数组 (Array)，是多个相同类型数据按一定顺序排列的集合，使用一个名字命名，并通过编号的方式对这些数据进行统一管理。 数组的相关概念： 数组名 元素 下标 (或索引) 数组的长度 数组的特点： 数组是有序排列的。 创建数组对象会在内存中开辟一整块连续的空间，而数组名中引用的是这块连续空间的首地址。 数组本身是引用数据类型的变量，而数组中的元素可以是任何数据类型，既可以是基本数据类型，也可以是引用数据类型。 可以直接通过下标 (或索引) 的方式调用指定位置的元素，速度很快。 数组的长度一旦确定，就不能修改。 数组的分类： 按照维度：一维数组、二维数组、三维数组、… 按照元素的数据类型分：基本数据类型元素的数组、引用数据类型元素的数组 (即对象数组)。 一维数组声明方式：type var[] 或 type[] var;。例如：int a[]; int[] a1; double b[]; String[] c;// 引用类型变量数组。 不同写法：int[] x;，int x[];。 java 语言中声明数组时，不能指定其长度 (数组中元素的数)， 例如：int a[5];// 非法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public static void main(String[] args) &#123; // 1-1.静态初始化，方式一 int[] ids = new int[]&#123;1001, 1002, 1003, 1004, 1005&#125;; // 1-2.静态初始化，方式二，类型推断 int[] ids2 = &#123;1001, 1002, 1003, 1004, 1005&#125;; // 2.动态初始化 String[] names = new String[5]; names[0] = &quot;Student A&quot;; names[1] = &quot;Student B&quot;; names[2] = &quot;Student C&quot;; names[3] = &quot;Student D&quot;; names[4] = &quot;Student E&quot;; // 3.数组的长度 System.out.println(&quot;ids 的长度：&quot; + ids.length);// 5 System.out.println(&quot;names 的长度：&quot; + names.length);// 5 // 4.遍历数组 for (int i = 0; i &lt; ids.length; i++) &#123; System.out.println(ids[i]); &#125; for (int i = 0; i &lt; names.length; i++) &#123; System.out.println(names[i]); &#125; // 5.简写方式遍历数组 for (int id : ids) &#123; System.out.println(id); &#125; for (String name : names) &#123; System.out.println(name); &#125; // 6.数组元素的默认初始化值 int[] arrs = new int[5]; for (int arr : arrs) &#123; System.out.println(arr);// 0 &#125; String[] arrs2 = new String[5]; for (String arr2 : arrs2) &#123; System.out.println(arr2);// null &#125;&#125; 静态初始化：数组的初始化，和数组元素的赋值操作同时进行。 动态初始化 ：数组的初始化，和数组元素的赋值操作分开进行。 定义数组并用运算符 new 为之分配空间后，才可以引用数组中的每个元素。 数组元素的引用方式：数组名[数组元素下标]。 数组元素下标从 0 开始，长度为 n 的数组的合法下标取值范围：0 — n-1。如：int a[] = new int[3];，则可引用的数组元素为 a[0]、a[1] 和 a[2]。 数组元素下标可以是整型常量或整型表达式。如 a[3]，b[i]，c[6*i]。 数组一旦初始化完成，其长度也随即确定，且长度不可变。每个数组都有一个属性 length 指明它的长度，例如：a.length 指明数组 a 的长度 (元素个数)。 数组是引用类型，它的元素相当于类的成员变量，因此数组一经分配空间，其中的每个元素也被按照成员变量同样的方式被隐式初始化。然后，再根据实际代码设置，将数组相应位置的元素进行赋值，即显示赋值。 对于基本数据类型而言，默认的初始化值各有不同；对于引用数据类型而言，默认的初始化值为 null。 char 类型的默认值是 0，不是 ‘0’，表现的是类似空格的一种效果。 一维数组内存解析： 一个计算联系方式的数组： 123456789public static void main(String[] args) &#123; int[] arr = new int[]&#123;8, 2, 1, 0, 3&#125;; int[] index = new int[]&#123;2, 0, 3, 2, 4, 0, 1, 3, 2, 3, 3&#125;; String tel = &quot;&quot;; for (int i = 0; i &lt; index.length; i++) &#123; tel += arr[index[i]]; &#125; System.out.println(&quot;联系方式：&quot; + tel);// 联系方式：18013820100&#125; 二维数组java 语言里提供了支持多维数组的语法。如果把一维数组当成几何中的线性图形，那么二维数组就相当于是一个表格。 对于二维数组的理解，可以看成是一维数组 array1，作为另一个一维数组 array2 的元素而存在。其实，从数组底层的运行机制来看，没有多维数组。 不同写法：int[][] x;，int[] x[];，int x[][];。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public static void main(String[] args) &#123; // 1-1.静态初始化，方式一 int[][] arr = new int[][]&#123;&#123;3, 8, 2&#125;, &#123;2, 7&#125;, &#123;9, 0, 1, 6&#125;&#125;; // 1-2.静态初始化，方式二，类型推断 int[][] arr2 = &#123;&#123;3, 8, 2&#125;, &#123;2, 7&#125;, &#123;9, 0, 1, 6&#125;&#125;; System.out.println(arr2[2][3]); // 2-1.动态初始化，方式一 /* 定义了名称为arr3的二维数组，二维数组中有3个一维数组，内层每一个一维数组中有2个元素 内层一维数组的名称分别为arr3[0]，arr3[1]，arr3[2]，返回的是地址值 给内层第一个一维数组1脚标位赋值为78写法是：arr3[0][1] = 78; */ int[][] arr3 = new int[3][2]; arr3[0][1] = 78; System.out.println(arr3[0]);// [I@78308db1 System.out.println(arr3[0][1]);// 78 // 2-2.动态初始化，方式二 /* 二维数组arr4中有3个一维数组，内层每个一维数组都是默认初始化值null(注意：区别于格式2-1) 可以对内层三个一维数组分别进行初始化 */ int[][] arr4 = new int[3][]; // 初始化第一个 arr4[0] = new int[3]; // 初始化第二个 arr4[1] = new int[1]; // 初始化第三个 arr4[2] = new int[2]; // 3.特殊写法 int[] x, y[];// x是一维数组，y是二维数组 x = new int[3]; y = new int[3][2]; // 4.获取数组长度 System.out.println(&quot;arr的长度：&quot; + arr.length);// 3 System.out.println(&quot;arr第一个元素的长度：&quot; + arr[0].length);// 3 // 5.遍历二维数组 for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = 0; j &lt; arr[i].length; j++) &#123; System.out.print(arr[i][j] + &quot;\\t&quot;);// 3 8 2 2 7 9 0 1 6 &#125; &#125; System.out.println(); // 6.简写遍历二维数组 for (int[] valueArr : arr) &#123; for (int value : valueArr) &#123; System.out.print(value + &quot;\\t&quot;);// 3 8 2 2 7 9 0 1 6 &#125; &#125; System.out.println(); // 7.二维数组元素的默认初始化值 int[][] arr5 = new int[3][2]; System.out.println(arr5);// [[I@27c170f0 System.out.println(arr5[1]);// [I@5451c3a8 System.out.println(arr5[1][1]);// 0 String[][] arr6 = new String[3][2]; System.out.println(arr6);// [[Ljava.lang.String;@2626b418 System.out.println(arr6[1]);// [Ljava.lang.String;@5a07e868 System.out.println(arr6[1][1]);// null String[][] arr7 = new String[3][]; System.out.println(arr7[1]);// null，因为内层数组未初始化 System.out.println(arr7[1][1]);// NullPointerException&#125; 动态初始化方式一，初始化时直接规定了内层一维数组的长度，动态初始化方式二，可以在使用过程中根据需要另行初始化内层一维数组的长度。利用动态初始化方式二时，必须要先初始化内层一维数组才能对其使用，否则报空指针异常。 int[][] arr = new int[][3]; 的方式是非法的。 注意特殊写法情况：int[] x,y[];// x是一维数组，y是二维数组。 java 中多维数组不必都是规则矩阵形式。 数组元素的默认初始化值：针对形如 int[][] arr = new int[4][3]; 的初始化方式，外层元素的初始化值为地址值，内层元素的初始化值与一维数组初始化情况相同；针对形如 int[][] arr = new int[4][]; 的初始化方式，外层元素的初始化值为 null，内层元素没有初始化，不能调用。 二维数组内存解析： 杨辉三角： 使用二维数组打印一个 10 行杨辉三角。 提示：1. 第一行有 1 个元素，第 n 行有 n 个元素；2. 每一行的第一个元素和最后一个元素都是 1；3. 从第三行开始，对于非第一个元素和最后一个元素的元素，有：yanghui[i][j] = yanghui[i-1][j-1] + yanghui[i-1][j];。 1234567891011121314151617181920212223public static void main(String[] args) &#123; // 1.声明二维数组并初始化 int[][] arrs = new int[10][]; for (int i = 0; i &lt; arrs.length; i++) &#123; System.out.print(&quot;[&quot; + i + &quot;]\\t&quot;); // 2.初始化内层数组，并给内层数组的首末元素赋值 arrs[i] = new int[i + 1]; arrs[i][0] = 1; arrs[i][arrs[i].length - 1] = 1; for (int j = 0; j &lt; arrs[i].length; j++) &#123; // 3.给从第三行开始内层数组的非首末元素赋值 if (i &gt;= 2 &amp;&amp; j &gt; 0 &amp;&amp; j &lt; arrs[i].length - 1) &#123; arrs[i][j] = arrs[i - 1][j - 1] + arrs[i - 1][j]; &#125; System.out.print(arrs[i][j] + &quot;\\t&quot;); &#125; System.out.println(); &#125; System.out.print(&quot;\\t&quot;); for (int i = 0; i &lt; arrs.length; i++) &#123; System.out.print(&quot;[&quot; + i + &quot;]\\t&quot;); &#125;&#125; 数组中涉及到的常见算法数组元素的赋值 (杨辉三角、回形数等) 创建一个长度为 6 的 int 型数组，要求数组元素的值都在 1 - 30 之间，且是随机赋值。同时，要求元素的值各不相同。 12345678910111213141516171819public static void main(String[] args) &#123; int[] arr = new int[6]; for (int i = 0; i &lt; arr.length; i++) &#123; // 获取一个1-30之间的随机数 arr[i] = (int) (Math.random() * 30 + 1); // 判断是否有相同的值 for (int item : arr) &#123; if (item == arr[i]) &#123; i--; break; &#125; &#125; &#125; // 遍历数组 for (int value : arr) &#123; System.out.println(value); &#125;&#125; 回形数 从键盘输入一个 1 - 20 的整数，然后以该数字为矩阵的大小，把 1，2，3 … n*n 的数字按照顺时针螺旋的形式填入其中。例如： 输入数字 2，则程序输出： 输入数字 3，则程序输出： 输入数字 4， 则程序输出： 方式一： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;输入一个数字&quot;); int len = scanner.nextInt(); int[][] arr = new int[len][len]; int s = len * len; /* * k = 1：向右，k = 2：向下，k = 3：向左，k = 4：向上 */ int k = 1; int i = 0, j = 0; for (int m = 1; m &lt;= s; m++) &#123; if (k == 1) &#123; if (j &lt; len &amp;&amp; arr[i][j] == 0) &#123; arr[i][j++] = m; &#125; else &#123; k = 2; i++; j--; m--; &#125; &#125; else if (k == 2) &#123; if (i &lt; len &amp;&amp; arr[i][j] == 0) &#123; arr[i++][j] = m; &#125; else &#123; k = 3; i--; j--; m--; &#125; &#125; else if (k == 3) &#123; if (j &gt;= 0 &amp;&amp; arr[i][j] == 0) &#123; arr[i][j--] = m; &#125; else &#123; k = 4; i--; j++; m--; &#125; &#125; else if (k == 4) &#123; if (i &gt;= 0 &amp;&amp; arr[i][j] == 0) &#123; arr[i--][j] = m; &#125; else &#123; k = 1; i++; j++; m--; &#125; &#125; &#125; // 遍历数组 for (int m = 0; m &lt; arr.length; m++) &#123; for (int n = 0; n &lt; arr[m].length; n++) &#123; System.out.print(arr[m][n] + &quot;\\t&quot;); &#125; System.out.println(); &#125;&#125; 方式二： 12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;输入一个数字&quot;); int n = scanner.nextInt(); int[][] arr = new int[n][n]; int count = 0;// 要显示的数据 int maxX = n - 1;// x轴的最大下标 int maxY = n - 1;// Y轴的最大下标 int minX = 0;// x轴的最小下标 int minY = 0;// Y轴的最小下标 while (minX &lt;= maxX) &#123; // 向右 for (int x = minX; x &lt;= maxX; x++) &#123; arr[minY][x] = ++count; &#125; minY++; // 向下 for (int y = minY; y &lt;= maxY; y++) &#123; arr[y][maxX] = ++count; &#125; maxX--; // 向左 for (int x = maxX; x &gt;= minX; x--) &#123; arr[maxY][x] = ++count; &#125; maxY--; // 向上 for (int y = maxY; y &gt;= minY; y--) &#123; arr[y][minX] = ++count; &#125; minX++; &#125; // 遍历数组 for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = 0; j &lt; arr.length; j++) &#123; String space = (arr[i][j] + &quot;&quot;).length() == 1 ? &quot;0&quot; : &quot;&quot;; System.out.print(space + arr[i][j] + &quot; &quot;); &#125; System.out.println(); &#125;&#125; 求数值型数组中元素的最大值、最小值、平均数、总和等 定义一个 int 型的一维数组，包含 10 个元素，分别赋一些随机整数，然后求出所有元素的最大值，最小值，和值，平均值，并输出出来 。要求：所有随机数都是两位数。 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; // 初始化及赋值 int[] arr = new int[10]; int length = arr.length; for (int i = 0; i &lt; length; i++) &#123; int value = (int) (Math.random() * 90 + 10); arr[i] = value; &#125; // 遍历 for (int value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 计算 int max = arr[0]; int min = arr[0]; int sum = 0; double average = 0; for (int value : arr) &#123; max = max &lt; value ? value : max; min = min &gt; value ? value : min; sum += value; &#125; average = sum / (length * 1.0); System.out.println(&quot;最大值：&quot; + max); System.out.println(&quot;最小值：&quot; + min); System.out.println(&quot;和值：&quot; + sum); System.out.println(&quot;平均值：&quot; + average);&#125; 数组的复制、反转、查找 (线性查找、二分法查找) 复制 虚假的复制： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) &#123; // 声明arr1和arr2 int[] arr1, arr2; arr1 = new int[]&#123;2, 3, 5, 7, 11, 13, 17, 19&#125;; // 遍历arr1 for (int value : arr1) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 赋值arr2变量等于arr1 // 不能称作数组的复制，实际上是把arr1指向的地址(以及其他一些信息)赋给了arr2，堆空间中只有一个数组对象 arr2 = arr1; // 遍历arr2 for (int value : arr2) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 更改arr2 for (int i = 0; i &lt; arr1.length; i++) &#123; if (i % 2 == 0) &#123; arr2[i] = i; continue; &#125; arr2[i] = arr1[i]; &#125; // 遍历arr2 for (int value : arr2) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 遍历arr1 for (int value : arr1) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println();&#125;输出结果：2 3 5 7 11 13 17 19 2 3 5 7 11 13 17 19 0 3 2 7 4 13 6 19 0 3 2 7 4 13 6 19 arr1 和 arr2 地址值相同，都指向了堆空间中唯一的一个数组实体： 真实的复制： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public static void main(String[] args) &#123; // 声明arr1和arr2 int[] arr1, arr2; arr1 = new int[]&#123;2, 3, 5, 7, 11, 13, 17, 19&#125;; // 遍历arr1 for (int value : arr1) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 数组的复制 arr2 = new int[arr1.length]; for (int i = 0; i &lt; arr1.length; i++) &#123; arr2[i] = arr1[i]; &#125; // 遍历arr2 for (int value : arr2) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 更改arr2 for (int i = 0; i &lt; arr1.length; i++) &#123; if (i % 2 == 0) &#123; arr2[i] = i; continue; &#125; arr2[i] = arr1[i]; &#125; // 遍历arr2 for (int value : arr2) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 遍历arr1 for (int value : arr1) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println();&#125;输出结果：2 3 5 7 11 13 17 19 2 3 5 7 11 13 17 19 0 3 2 7 4 13 6 19 2 3 5 7 11 13 17 19 arr1 和 arr2 地址值不同，指向了堆空间中两个不同的数组实体： 反转 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; String[] arr = &#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;&#125;; // 遍历arr for (String value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 反转，方式一 for (int i = 0; i &lt; arr.length / 2; i++) &#123; String temp = arr[i]; arr[i] = arr[arr.length - 1 - i]; arr[arr.length - 1 - i] = temp; &#125; for (String value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 反转，方式二 for (int i = 0, j = arr.length - 1; i &lt; j; i++, j--) &#123; String temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; for (String value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println();&#125; 线性查找 12345678910111213141516171819202122232425public static void main(String[] args) &#123; String[] arr = &#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;&#125;; // 遍历arr for (String value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); String dest = &quot;D&quot;; boolean isFlag = true; for (int i = 0; i &lt; arr.length; i++) &#123; if (dest.equals(arr[i])) &#123; System.out.println(&quot;找到了指定的元素：&quot; + dest + &quot;，位置为：&quot; + i); isFlag = false; break; &#125; &#125; if (isFlag) &#123; System.out.println(&quot;没找到指定的元素：&quot; + dest); &#125;&#125;输出结果：A B C D E F G 找到了指定的元素：D，位置为：3 二分法查找，前提：所要查找的数组必须有序。 12345678910111213141516171819202122232425262728293031323334public static void main(String[] args) &#123; int[] arr = &#123;2, 5, 7, 8, 10, 15, 18, 20, 22, 25, 28&#125;; // 遍历arr for (int value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); int dest = 10; // 初始的首索引 int head = 0; // 初始的末索引 int end = arr.length - 1; boolean isFlag = true; while (head &lt;= end) &#123; int middle = (head + end) / 2; if (dest == arr[middle]) &#123; System.out.println(&quot;找到了指定的元素：&quot; + dest + &quot;，位置为：&quot; + middle); isFlag = false; break; &#125; else if (dest &lt; arr[middle]) &#123; end = middle - 1; &#125; else &#123;// dest2 &gt; arr2[middle] head = middle + 1; &#125; &#125; if (isFlag) &#123; System.out.println(&quot;没找到指定的元素：&quot; + dest); &#125;&#125;输出结果：2 5 7 8 10 15 18 20 22 25 28 找到了指定的元素：10，位置为：4 数组元素的排序算法排序：假设含有 n 个记录的序列为 {R1, R2, …, Rn}，其相应的关键字序列为 {K1, K2, …, Kn}。将这些记录重新排序为 {Ri1, Ri2, …, Rin}，使得相应的关键字值满足条 Ki1&lt;= Ki2 &lt;= … &lt;= Kin，这样的一种操作称为排序。通常来说，排序的目的是快速查找。 衡量排序算法的优劣： 时间复杂度：分析关键字的比较次数和记录的移动次数。 空间复杂度：分析排序算法中需要多少辅助内存。 稳定性：若两个记录 A 和 B 的关键字值相等，但排序后 A、B 的先后次序保持不变，则称这种排序算法是稳定的。 排序算法分类：内部排序和外部排序。 内部排序：整个排序过程不需要借助于外部存储器 (如磁盘等)，所有排序操作都在内存中完成。 外部排序：参与排序的数据非常多，数据量非常大，计算机无法把整个排序过程放在内存中完成，必须借助于外部存储器 (如磁盘等)。外部排序最常见的是多路归并排序。可以认为外部排序是由多次内部排序组成。 十大内部排序算法： 排序算法性能对比： 从平均时间而言：快速排序最佳。但在最坏情况下时间性能不如堆排序和归并排序。 从算法简单性看：由于直接选择排序、直接插入排序和冒泡排序的算法比较简单，将其认为是简单算法。对于 Shell 排序、堆排序、快速排序和归并排序算法，其算法比较复杂，认为是复杂排序。 从稳定性看：直接插入排序、冒泡排序和归并排序时稳定的；而直接选择排序、快速排序、 Shell 排序和堆排序是不稳定排序。 从待排序的记录数 n 的大小看，n 较小时，宜采用简单排序；而 n 较大时宜采用改进排序。 排序算法的选择： 若 n 较小 (如 n ≤50)，可采用直接插入或直接选择排序。当记录规模较小时，直接插入排序较好；否则因为直接选择移动的记录数少于直接插入，应选直接选择排序为宜。 若文件初始状态基本有序 (指正序)，则应选用直接插入、 冒泡或随机的快速排序为宜。 若 n 较大，则应采用时间复杂度为 O(nlgn) 的排序方法： 快速排序、 堆排序或归并排序。 冒泡排序冒泡排序的原理非常简单，它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。 排序思想： 比较相邻的元素。如果第一个比第二个大 (升序)，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较为止。 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; int[] arr = new int[]&#123;43, 32, 76, -98, 0, 64, 33, -21, 32, 99&#125;; // 遍历arr for (int value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 冒泡排序 for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125; // 遍历arr for (int value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println();&#125;输出结果：43 32 76 -98 0 64 33 -21 32 99-98 -21 0 32 32 33 43 64 76 99 快速排序快速排序通常明显比同为 O(nlogn) 的其他算法更快，因此常被采用，而且快速排序采用了分治法的思想，所以在很多笔试面试中能经常看到快速排序的影子，可见掌握快速排序的重要性。 快速排序 (Quick Sort) 由图灵奖获得者 Tony Hoare 发明，被列为 20 世纪十大算法之一，是迄今为止所有内排序算法中速度最快的一种。 快速排序属于冒泡排序的升级版，交换排序的一种。快速排序的时间复杂度为 O(nlog(n))。 排序思想： 从数列中挑出一个元素，称为”基准” (pivot)。 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面 (相同的数可以到任一边)。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区 (partition) 操作。 递归地 (recursive) 把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代 (iteration) 中，它至少会把一个元素摆到它最后的位置去。 Arrays 工具类的使用java.util.Arrays 类为操作数组的工具类，包含了用来操作数组 (比如排序和搜索) 的各种方法。常用的方法有： 123456789101112131415161718192021222324252627282930313233343536public static void main(String[] args) &#123; // 1.boolean equals(int[] a,int[] b)：判断两个数组是否相等 int[] arr1 = new int[]&#123;1, 2, 3, 4&#125;; int[] arr2 = new int[]&#123;1, 3, 2, 4&#125;; boolean isEquals = Arrays.equals(arr1, arr2); System.out.println(&quot;arr1和arr2是否相等：&quot; + isEquals); // 2.String toString(int[] a)：遍历数组信息 System.out.println(&quot;arr1：&quot; + Arrays.toString(arr1)); // 3.void fill(int[] a,int val)：将指定值填充到数 组之中 Arrays.fill(arr1, 10); System.out.println(&quot;arr1填充后：&quot; + Arrays.toString(arr1)); // 4.void sort(int[] a)：对数组进行排序，底层使用的是快速排序 System.out.println(&quot;arr2排序前：&quot; + Arrays.toString(arr2)); Arrays.sort(arr2); System.out.println(&quot;arr2排序后：&quot; + Arrays.toString(arr2)); // 5.int binarySearch(int[] a,int key)：对排序后的数组进行二分法检索指定的值 int[] arr3 = new int[]&#123;-98, -34, 2, 34, 54, 66, 79, 105, 210, 333&#125;; int dest = 211; int index = Arrays.binarySearch(arr3, dest); if (index &gt;= 0) &#123; System.out.println(dest + &quot;在数组中的位置为：&quot; + index); &#125; else &#123; System.out.println(dest + &quot;在数组中未找到：&quot; + index); &#125;&#125;输出结果：arr1和arr2是否相等：falsearr1：[1, 2, 3, 4]arr1填充后：[10, 10, 10, 10]arr2排序前：[1, 3, 2, 4]arr2排序后：[1, 2, 3, 4]211在数组中未找到：-10 数组中的常见异常12345678910111213141516171819public static void main(String[] args) &#123; // ArrayIndexOutOfBoundsException int[] arr = new int[]&#123;7, 10&#125;; System.out.println(arr[2]);// 数组脚标越界 System.out.println(arr[-1]);// 访问了数组中不存在的脚标 // NullPointerException：空指针异常，arr引用没有指向实体，却被操作实体中的元素 // 情形一 int[] arr2 = null; System.out.println(arr2[0]); // 情形二 int[][] arr3 = new int[4][]; System.out.println(arr3[0]);// null System.out.println(arr3[0][0]);// NullPointerException // 情形三 String[] arr4 = new String[]&#123;&quot;AA&quot;, &quot;BB&quot;, &quot;CC&quot;&#125;; arr4[0] = null; System.out.println(arr4[0].toString());// 对null调用了方法&#125; ArrayIndexOutOfBoundsException 和 NullPointerException，在编译时，不报错！！ 面向对象三条主线： java 类及类的成员：属性、方法、构造器、代码块、内部类。 面向对象的三大特征：封装性、继承性、多态性、(抽象性)。 其他关键字：this、super、static、final、abstract、interface、package、import等。 面向过程 (POP) 与面向对象 (OOP)： 二者都是一种思想，面向对象是相对于面向过程而言的。 面向过程，强调的是功能行为，以函数为最小单位，考虑怎么做。面向对象，将功能封装进对象，强调具备了功能的对象，以类/对象为最小单位，考虑谁来做。 面向对象更加强调运用人类在日常的思维逻辑中采用的思想方法与原则，如抽象、分类、继承、聚合、多态等。 例如，人把大象装进冰箱： 面向对象的三大特征： 封装 (Encapsulation) 继承 (Inheritance) 多态 (Polymorphism) 面向对象的思想概述： 程序员从面向过程的执行者转化成了面向对象的指挥者。 面向对象分析方法分析问题的思路和步骤： 根据问题需要，选择问题所针对的现实世界中的实体。 从实体中寻找解决问题相关的属性和功能，这些属性和功能就形成了概念世界中的类。 把抽象的实体用计算机语言进行描述，形成计算机世界中类的定义。即借助某种程序语言，把类构造成计算机能够识别和处理的数据结构。 将类实例化成计算机世界中的对象。对象是计算机世界中解决问题的最终工具。 java基本元素：类和对象类 (Class) 和对象 (Object) 是面向对象的核心概念。 类是对一类事物的描述，是抽象的、概念上的定义。 对象是实际存在的该类事物的每个个体，因而也称为实例 (instance)。 常见的类的成员有： 属性：对应类中的成员变量。 方法：对应类中的成员方法。 类的成员构成 version 1.0： 类的成员构成 version 2.0： 类的语法格式： 创建 java 自定义类步骤： 定义类：考虑修饰符、类名。 编写类的属性：考虑修饰符、属性类型、属性名、初始化值。 编写类的方法：考虑修饰符、返回值类型、方法名、形参等。 类的访问机制： 在一个类中的访问机制：类中的方法可以直接访问类中的成员变量。例外：static 方法访问非 static 属性，编译不通过。 在不同类中的访问机制： 先创建要访问类的对象， 再用对象访问类中定义的成员。 对象的创建和使用： 创建对象语法： 使用 对象名.对象成员 的方式访问对象成员，包括属性和方法。 如果创建了一个类的多个对象，则每个对象都独立的拥有一套类的属性 (非 static 的)，即：修改一个对象的属性 a，不影响另外一个对象属性 a 的值。 对象的产生： 对象的使用： 对象的生命周期： 对象的内存解析： 例如，下面一段代码的内存图如下： 匿名对象： 不定义对象的句柄，而直接调用这个对象的方法，这样的对象叫做匿名对象。如：new Person().shout();。 使用情况：如果对一个对象只需要进行一次方法调用，那么就可以使用匿名对象。我们经常将匿名对象作为实参传递给一个方法调用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 二、创建类的对象 = 类的实例化 */public class PersonTest &#123; public static void main(String[] args) &#123; // 1.创建Person类的对象 Person person = new Person(); // 2.调用对象的结构：属性和方法 // 2-1.调用属性：&quot;对象.属性&quot; person.name = &quot;Tom&quot;; person.isMale = true; System.out.println(&quot;年龄：&quot; + person.age);// 1 // 2-2.调用方法：&quot;对象.方法&quot; person.eat();// 人可以吃饭 person.sleep();// 人可以睡觉 person.talk(&quot;Chinese&quot;);// 人可以说话，语言是：Chinese // 3.创建一个新的Person类的对象 Person person2 = new Person(); System.out.println(person2.name);// null，非Tom // 4.将person变量保存的地址值赋值给person3，此时，二者指向堆空间中的同一个对象实体 // 修改person和person3，效果相同 Person person3 = person; System.out.println(person3.name);// Tom person3.age = 10; System.out.println(person.age);// 10 &#125;&#125;/** * 一、类的设计，其实就是类的成员的设计： * 属性 = 成员变量 = Field = 域、字段 * 方法 = 成员方法 = 函数 = Method */class Person &#123; // 属性 String name; int age = 1; boolean isMale; // 方法 public void eat() &#123; System.out.println(&quot;人可以吃饭&quot;); &#125; public void sleep() &#123; System.out.println(&quot;人可以睡觉&quot;); &#125; public void talk(String language) &#123; System.out.println(&quot;人可以说话，语言是：&quot; + language); &#125;&#125; 类的成员之一：属性 (field)语法格式： 常用的权限修饰符有：private、缺省、protected、public。其他修饰符：static、final。 数据类型：任何基本数据类型 (如 int、boolean 等) 或任何引用数据类型。 属性名：属于标识符，符合命名规则和规范即可。 属性 (成员变量) 与局部变量的区别： 成员变量的默认初始化值：当一个对象被创建时，会对其中各种类型的成员变量自动进行初始化赋值。除了基本数据类型之外的变量类型都是引用类型。 局部变量的默认初始化值：局部变量声明后，没有默认初始化值，必须显式赋值，方可使用。特别的，形参在调用时，赋值即可。 成员变量 vs 局部变量的内存位置： 属性赋值的方式和先后顺序： 赋值的方式： ① 默认初始化 ② 显示初始化 ③ 构造器中初始化 ④ 通过 “对象.属性” 或 “对象.方法” 的方式赋值 ⑤ 在代码块中初始化 赋值的先后顺序：① - ② / ⑤ - ③ - ④ ② 和 ⑤，谁定义在前，谁先赋值： 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; Order order = new Order(); System.out.println(order.orderId);// 4 &#125;&#125;class Order &#123; int orderId = 3; &#123; orderId = 4; &#125;&#125; 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; Order order = new Order(); System.out.println(order.orderId);// 3 &#125;&#125;class Order &#123; &#123; orderId = 4; &#125; int orderId = 3;&#125; 总结：程序中成员变量赋值的执行顺序 类的成员之二：方法 (method)什么是方法 (method 、函数)： 方法是类或对象行为特征的抽象，用来完成某个功能操作。在某些语言中也称为函数或过程。 将功能封装为方法的目的是，可以实现代码重用，简化代码。 java 里的方法不能独立存在，所有的方法必须定义在类里。 声明格式： 权限修饰符：public，缺省，private，protected 等。 返回值类型： 没有返回值：使用void。 有返回值：在方法声明时，必须指定返回值的类型，同时，方法体中需要使用 return 关键字返回指定类型的变量或常量。 方法名 ：属于标识符，命名时遵循标识符命名规则和规范，能够见名知意。 形参列表：可以包含零个，一个或多个参数。多个参数时，中间用 “,” 隔开。 方法体程序代码：方法功能的具体实现。 返回值：方法在执行完毕后返还给调用它的程序的数据。 方法的分类：按照是否有形参及返回值。 方法的调用： 方法通过方法名被调用，且只有被调用才会执行。 方法调用的过程： 方法被调用一次，就会执行一次。 没有具体返回值的情况，返回值类型用关键字 void 表示，此时方法体中可以不必使用 return 语句。如果使用，表示用来结束方法。 定义方法时，方法的结果应该返回给调用者，交由调用者处理。 方法中可以调用当前类的属性或方法，不可以在方法内部定义方法。 方法的重载 (overload) 概念：在同一个类中，允许存在一个以上的同名方法，只要它们的参数个数或者参数类型不同即可。 特点：与方法的权限修饰符、返回值类型、形参变量名、方法体都无关，只看参数列表，且参数列表 (参数个数或参数类型) 必须不同。调用时，根据方法参数列表的不同来区别。 如果方法一不存在，main 方法依然正常执行，此时涉及到的是自动类型转换： 12345678910111213// 方法一public static int getSum(int m, int n) &#123; return m + n;&#125;// 方法二public static double getSum(double m, double n) &#123; return m + n;&#125;public static void main(String[] args) &#123; System.out.println(getSum(1, 2));&#125; 可变个数的形参： JavaSE 5.0 中提供了 Varargs (variable number of arguments) 机制，允许直接定义能和多个实参相匹配的形参。从而，可以用一种更简单的方式，来传递个数可变的实参。 声明格式：方法名(参数的类型名 … 参数名) 可变参数：方法参数部分指定类型的参数个数是可变多个 — 0个，1个或多个。 可变个数形参的方法与同名的方法之间，彼此构成重载。 可变参数方法的使用与方法参数部分使用数组是一致的，二者不共存。如下所示，方法二与方法三是相同的，不共存： 123456789// 方法二public static void show(int... m) &#123; System.out.println(Arrays.toString(m));// m参数等同于数组，与数组的使用方法相同。&#125;// 方法三public static void show(int[] m) &#123; System.out.println(m);&#125; 方法的参数部分有可变形参，需要放在形参声明的最后。 123456789// 合法public static void show(String str, int... m) &#123; System.out.println(Arrays.toString(m));&#125;// 不合法public static void show(int... m, String str) &#123; System.out.println(Arrays.toString(m));&#125; 在一个方法的形参位置，最多只能声明一个可变个数形参。 方法参数的值传递机制 方法，必须由其所在类或对象调用才有意义。若方法含有参数： 形参：方法声明时的参数。 实参：方法调用时实际传给形参的数据。 java 的实参值如何传入方法呢？ java 里方法的参数传递方式只有一种：值传递。 即将实际参数值的副本 (复制品) 传入方法内，而参数本身不受影响。 形参是基本数据类型：将实参基本数据类型变量的 “数据值” 传递给形参。 形参是引用数据类型：将实参引用数据类型变量的 “地址值” 传递给形参。 形参时基本数据类型与引用数据类型之间的区别: 1234567891011121314151617181920212223242526272829public class ValueTransferTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;***************基本数据类型***************&quot;); int m = 10; int n = m; System.out.println(&quot;m = &quot; + m + &quot;, n = &quot; + n); n = 20; System.out.println(&quot;m = &quot; + m + &quot;, n = &quot; + n); System.out.println(&quot;***************引用数据类型***************&quot;); Order o1 = new Order(); o1.orderId = 1001; Order o2 = o1;// 赋值后, o1和o2的地址值相同, 都指向了堆空间中的同一个实体 System.out.println(&quot;o1.orderId = &quot; + o1.orderId + &quot;, o2.orderId = &quot; + o2.orderId); o2.orderId = 1002; System.out.println(&quot;o1.orderId = &quot; + o1.orderId + &quot;, o2.orderId = &quot; + o2.orderId); &#125;&#125;class Order &#123; int orderId;&#125;输出结果：***************基本数据类型***************m = 10, n = 10m = 10, n = 20***************引用数据类型***************o1.orderId = 1001, o2.orderId = 1001o1.orderId = 1002, o2.orderId = 1002 对于基本数据类型，两个不同方法内的局部变量，互不影响，不因变量名相同而改变，因为是将实参基本数据类型变量的 “数据值” 传递给形参： 12345678910111213141516171819202122232425262728293031323334public class ValueTransferTest &#123; public void swap(int m, int n) &#123; System.out.println(&quot;swap方法中, 交换之前: m = &quot; + m + &quot;, n = &quot; + n); int temp = m; m = n; n = temp; System.out.println(&quot;swap方法中, 交换之后: m = &quot; + m + &quot;, n = &quot; + n); &#125; public static void main(String[] args) &#123; int m = 10; int n = 20; System.out.println(&quot;main方法中, 交换之前: m = &quot; + m + &quot;, n = &quot; + n); // 能够交换m和n的值 int temp = m; m = n; n = temp; System.out.println(&quot;main方法中, 交换之后: m = &quot; + m + &quot;, n = &quot; + n); // 不能够交换m和n的值 ValueTransferTest valueTransferTest = new ValueTransferTest(); System.out.println(&quot;main方法中, 调用swap方法之前: m = &quot; + m + &quot;, n = &quot; + n); valueTransferTest.swap(m, n);// // swap方法调用完成后, 该方法内的局部变量temp, 形参m和n从栈内存中弹出回收 System.out.println(&quot;main方法中, 调用swap方法之后: m = &quot; + m + &quot;, n = &quot; + n); &#125;&#125;输出结果：main方法中, 交换之前: m = 10, n = 20main方法中, 交换之后: m = 20, n = 10main方法中, 调用swap方法之前: m = 20, n = 10swap方法中, 交换之前: m = 20, n = 10swap方法中, 交换之后: m = 10, n = 20main方法中, 调用swap方法之后: m = 20, n = 10 内存解析图参考： 对于引用数据类型，两个不同方法的局部变量，会互相影响，因为是将实参引用数据类型变量的 “地址值” 传递给形参，二者指向的是堆内存中的同一个对象： 12345678910111213141516171819202122232425262728293031323334353637383940public class ValueTransferTest &#123; public void swap(Data data) &#123; System.out.println(&quot;swap方法中, 交换之前: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); int temp = data.m; data.m = data.n; data.n = temp; System.out.println(&quot;swap方法中, 交换之后: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); &#125; public static void main(String[] args) &#123; Data data = new Data(); data.m = 10; data.n = 20; System.out.println(&quot;main方法中, 交换之前: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); // 能够交换m和n的值 int temp = data.m; data.m = data.n; data.n = temp; System.out.println(&quot;main方法中, 交换之后: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); // 能够交换m和n的值 ValueTransferTest valueTransferTest = new ValueTransferTest(); System.out.println(&quot;main方法中, 调用swap方法之前: data.m = &quot; + data.m + &quot;, data. = &quot; + data.n); valueTransferTest.swap(data);// swap方法调用完成后, 该方法内的局部变量temp和形参data从栈内存中弹出回收 System.out.println(&quot;main方法中, 调用swap方法之后: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); &#125;&#125;class Data &#123; int m; int n;java&#125;输出结果：main方法中, 交换之前: data.m = 10, data.n = 20main方法中, 交换之后: data.m = 20, data.n = 10main方法中, 调用swap方法之前: data.m = 20, data. = 10swap方法中, 交换之前: data.m = 20, data.n = 10swap方法中, 交换之后: data.m = 10, data.n = 20main方法中, 调用swap方法之后: data.m = 10, data.n = 20 内存解析图参考： 实例一： 1234567891011121314151617181920212223242526public class ValueTransferTest &#123; public void first() &#123; int i = 5; Value v = new Value(); v.i = 25; second(v, i); System.out.println(v.i);// 20 &#125; public void second(Value v, int i) &#123; i = 0; v.i = 20; Value val = new Value(); v = val; System.out.println(v.i + &quot; &quot; + i);// 15 0 &#125; public static void main(String[] args) &#123; ValueTransferTest test = new ValueTransferTest(); test.first(); &#125;&#125;class Value &#123; int i = 15;&#125; 实例二： 方法一： 123456789101112131415public class Test &#123; public static void method(int a, int b) &#123; System.out.println(&quot;a = &quot; + a * 10); System.out.println(&quot;b = &quot; + b * 20); System.exit(0); &#125; public static void main(String[] args) &#123; int a = 10; int b = 10; method(a, b); System.out.println(&quot;a = &quot; + a); System.out.println(&quot;b = &quot; + b); &#125;&#125; 方法二：重写 PrintStream 的 println 方法。 123456789101112131415161718192021222324public class Test &#123; public static void method(int a, int b) &#123; PrintStream printStream = new PrintStream(System.out) &#123; @Override public void println(String x) &#123; if (&quot;a = 10&quot;.equals(x)) &#123; x = &quot;a = 100&quot;; &#125; else if (&quot;b = 10&quot;.equals(x)) &#123; x = &quot;b = 200&quot;; &#125; super.println(x); &#125; &#125;; System.setOut(printStream); &#125; public static void main(String[] args) &#123; int a = 10; int b = 10; method(a, b); System.out.println(&quot;a = &quot; + a); System.out.println(&quot;b = &quot; + b); &#125;&#125; 实例三： 定义一个 int 型的数组：int[] arr = new int[]&#123;12,3,3,34,56,77,432&#125;;，让数组的每个位置上的值去除以首位置的元素，得到的结果，作为该位置上的新值，然后遍历新的数组。 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;12, 3, 3, 34, 56, 77, 432&#125;; System.out.println(&quot;计算前: &quot; + Arrays.toString(arr)); // 正确写法一 int temp = arr[0]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = arr[i] / temp; &#125; // 正确写法二 for (int i = arr.length - 1; i &gt;= 0; i--) &#123; arr[i] = arr[i] / arr[0]; &#125; // 错误写法 /*for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = arr[i] / arr[0]; &#125;*/ System.out.println(&quot;计算后: &quot; + Arrays.toString(arr)); &#125;&#125; 实例四： 123456789101112public class Test &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;1, 2, 3&#125;; System.out.println(arr);// 地址值 char[] arr1 = new char[]&#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;; System.out.println(arr1);// 传入char数组时，println方法体内是遍历这个数组 &#125;&#125;输出结果：[I@78308db1abc 递归方法 (recursion)： 一个方法体内调用它自身。方法递归包含了一种隐式的循环，它会重复执行某段代码，但这种重复执行无须循环控制。 递归一定要向已知方向递归，否则这种递归就变成了无穷递归，类似于死循环 实例一： 1234567891011121314151617181920212223242526272829303132public class PassObject &#123; // 1-n之间所有自然数的和 public static int getSum(int n) &#123; if (n == 1) &#123; return 1; &#125; else &#123; return n + getSum(n - 1); &#125; &#125; // 1-n之间所有自然数的乘积 public static long getProduct(int n) &#123; if (n == 1) &#123; return 1; &#125; else &#123; return n * getProduct(n - 1); &#125; &#125; public static void main(String[] args) &#123; // 方式一：循环 int sum = 0; for (int i = 1; i &lt;= 100; i++) &#123; sum += i; &#125; System.out.println(&quot;1-100之间自然数的和: &quot; + sum); // 方式二：递归 System.out.println(&quot;1-100之间自然数的和: &quot; + getSum(100)); System.out.println(&quot;1-100之间自然数的积: &quot; + getProduct(5)); &#125;&#125; 实例二： 123456789101112131415161718192021/** * 已知有一个数列：f(0) = 1, f(1) = 4, f(n+2) = 2 * f(n+1) + f(n), 其中n是大于0的整数, 求f(10)的值。 */public class PassObject &#123; public static int f(int n) &#123; if (n == 0) &#123; return 1; &#125; else if (n == 1) &#123; return 4; &#125; else &#123; return 2 * f(n - 1) + f(n - 2); &#125; &#125; public static void main(String[] args) &#123; int f = f(10); System.out.println(f); &#125;&#125;输出结果：10497 实例三： 123456789101112131415161718192021/** * 已知一个数列: f(20) = 1, f(21) = 4, f(n+2) = 2 * f(n+1) + f(n), 其中n是大于0的整数, 求f(10)的值。 */public class PassObject &#123; public static int f(int n) &#123; if (n == 20) &#123; return 1; &#125; else if (n == 21) &#123; return 4; &#125; else &#123; return f(n + 2) - 2 * f(n + 1); &#125; &#125; public static void main(String[] args) &#123; int f = f(10); System.out.println(f); &#125;&#125;输出结果：-3771 实例四： 12345678910111213141516171819202122232425262728293031323334/** * 斐波那契数列: 1 1 2 3 5 8 13 21 34 55 ... * 规律: 一个数等于前两个数之和 * 要求：计算斐波那契数列(Fibonacci)的第n个值，并将整个数列打印出来 */public class PassObject &#123; public static int f(int n) &#123; if (n &lt;= 0 || n &gt;= 30) &#123; return 0; &#125; if (n == 1) &#123; return 1; &#125; else if (n == 2) &#123; return 1; &#125; else &#123; return f(n - 1) + f(n - 2); &#125; &#125; public static void main(String[] args) &#123; int[] arr = new int[5]; int sum = 0; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = f(i + 1); sum += arr[i]; &#125; System.out.println(Arrays.toString(arr)); System.out.println(&quot;和: &quot; + sum); &#125;&#125;输出结果：[1, 1, 2, 3, 5]和: 12 实例五： 1234567891011121314151617181920212223242526272829303132public class PassObject &#123; private static int count = 0; public static int recursion(int k) &#123; count++; System.out.println(&quot;count1: &quot; + count + &quot;, k: &quot; + k); if (k &lt;= 0) &#123; return 0; &#125; return recursion(k - 1) + recursion(k - 2); &#125; public static void main(String[] args) &#123; recursion(4); &#125;&#125;输出结果：count1: 1, k: 4count1: 2, k: 3count1: 3, k: 2count1: 4, k: 1count1: 5, k: 0count1: 6, k: -1count1: 7, k: 0count1: 8, k: 1count1: 9, k: 0count1: 10, k: -1count1: 11, k: 2count1: 12, k: 1count1: 13, k: 0count1: 14, k: -1count1: 15, k: 0 递归过程： 遍历过程相当于二叉树的前序遍历。 OOP 特征一：封装和隐藏封装性的设计思想：隐藏对象内部的复杂性，只对外公开简单的接口。便于外界调用，从而提高系统的可扩展性、可维护性。通俗的说，把该隐藏的隐藏起来，该暴露的暴露出来。 程序设计追求 “高内聚，低耦合”： 高内聚：类的内部数据操作细节自己完成，不允许外部干涉。 低耦合：仅对外暴露少量的方法用于使用。 信息的封装和隐藏： java 中通过将对象的属性声明为私有的 (private)，再提供公共的 (public) 方法 — getXxx() 和 setXxx()，来实现对属性的操作，并以此达到下述目的： 隐藏一个类中不需要对外提供的实现细节。 使用者只能通过事先定制好的方法来访问数据，可以方便地加入控制逻辑，限制对属性的不合理操作。 便于修改，增强代码的可维护性。 封装性的体现：属性私有、方法私有、构造器私有 (单例模式) 等。 封装性的体现，需要权限修饰符的配合。 四种权限修饰符 从小到大排列：private、缺省 (什么都不写)、protected、public。 权限修饰符置于类的成员定义前，用来限定对象对该类成员的访问权限： 权限修饰符可以用来修饰类及类的内部结构：属性、方法、构造器、内部类。 对于 class 的权限修饰只可以用 public 和 default (缺省)。 public 类可以在任意地方被访问。 default 类只可以被同一个包内部的类访问。 对于 class 的内部结构，四种权限修饰符都可以使用。 封装性总结：java 提供了 4 种权限修饰符来修饰类及类的内部结构，体现类及类的内部结构在被调用时的可见性的大小。 本类中任意调用： 123456789101112131415161718192021222324252627282930313233343536package cn.xisun.database;public class Order &#123; private int orderPrivate; int orderDefault; protected int orderProtected; public int orderPublic; private void methodPrivate() &#123; orderPrivate = 1; orderDefault = 2; orderProtected = 3; orderPublic = 4; &#125; void methodDefault() &#123; orderPrivate = 1; orderDefault = 2; orderProtected = 3; orderPublic = 4; &#125; protected void methodProtected() &#123; orderPrivate = 1; orderDefault = 2; orderProtected = 3; orderPublic = 4; &#125; public void methodPublic() &#123; orderPrivate = 1; orderDefault = 2; orderProtected = 3; orderPublic = 4; &#125;&#125; 同包中的其他类： 1234567891011121314151617181920package cn.xisun.database;public class OrderTest &#123; public static void main(String[] args) &#123; Order order = new Order(); order.orderDefault = 1; order.orderProtected = 2; order.orderPublic = 3; order.methodDefault(); order.methodProtected(); order.methodPublic(); // 同一个包中的其他类，不可以调用Order类中private的属性和方法 /*order.orderPrivate = 4; order.methodPrivate();*/ &#125;&#125; 不同包中的子类： 1234567891011121314151617181920package cn.xisun.database.postgresql;import cn.xisun.database.Order;public class SubOrder extends Order &#123; public void method() &#123; orderProtected = 1; orderPublic = 2; methodProtected(); methodPublic(); // 不同包的子类中，不可以调用Order类中private和缺省的属性和方法 /*orderPrivate = 3; orderDefault = 4; methodPrivate(); methodDefault();*/ &#125;&#125; 不同包中的其他类 (非子类)： 12345678910111213141516171819202122package cn.xisun.database.postgresql;import cn.xisun.database.Order;public class OtherOrderTest &#123; public static void main(String[] args) &#123; Order order = new Order(); order.orderPublic = 1; order.methodPublic(); // 不同包下的普通类(非子类)，不可以调用Order类中private、缺省和protected的属性和方法 /*order.orderPrivate = 2; order.orderDefault = 3; order.orderProtected = 4; order.methodPrivate(); order.methodDefault(); order.methodProtected();*/ &#125;&#125; 类的成员之三：构造器 (构造方法，constructor)构造器的作用： 创建对象；给对象进行初始化。如：Order o = new Order(); Person p = new Person(“Peter”, 15); 语法格式： 根据参数不同，构造器可以分为如下两类： 隐式无参构造器 (系统默认提供)。 显定义一个或多个构造器 (无参、有参)。 构造器的特征： 构造器具有与类相同的名称，不声明返回值类型，与声明为 void 不同。 java 语言中，每个类都至少有一个构造器。 如果没有显示的定义类的构造器，则系统默认提供一个无参构造器。一旦显式定义了构造器， 则系统不再提供默认构造器。 一般情况下，为了防止一些框架出异常，无论要不要自定义其他构造器，都应该把类的无参构造器显示的定义出来。 构造器的修饰符默认与所属类的修饰符一致，即：public 或 default (缺省)。 构造器不能被 static、final、synchronized、abstract、native 修饰，不能有 return 语句返回值。 一个类中定义的多个构造器，彼此构成重载。 父类的构造器不可被子类继承。 JavaBean JavaBean 是一种 java 语言写成的可重用组件。 所谓 JavaBean，是指符合如下标准的 java 类： 类是公共的。 有一个无参的公共的构造器。 有属性，且有对应的 get、set 方法。 用户可以使用 JavaBean 将功能、处理、值、数据库访问和其他任何可以用 java 代码创造的对象进行打包，并且其他的开发者可以通过内部的 JSP 页面、Servlet、其他 JavaBean、applet 程序或者应用来使用这些对象。用户可以认为 JavaBean 提供了一种随时随地的复制和粘贴的功能，而不用关心任何改变。 UML 类图 关键字：thisthis 关键字的使用： this 可以用来修饰或调用：属性、方法、构造器。 this 修饰属性和方法： this理解为：当前对象或当前正在创建的对象。 在类的方法中，可以使用 “this.属性” 或 “this.方法” 的方式，调用当前属性或方法。 通常情况下，可以省略 “this.”。 特殊情况下，如果方法的形参和类的属性同名，则必须显示的使用 “this.变量” 的方式，表明此变量是属性，而非形参。 在类的构造器中，可以使用 “this.属性” 或 “this.方法” 的方式，调用当前正在创建的对象的属性或方法。 通常情况下，可以省略 “this.”。 特殊情况下，如果构造器的形参和类的属性同名，则必须显示的使用 “this.变量” 的方式，表明此变量是属性，而非形参。 使用 this 访问属性和方法时，如果在本类中未找到，会从父类中查找。 this 调用构造器： 在类的构造器中，可以显示的使用 “this(形参列表)” 的方式，调用本类中的其他构造器。 存在构造器的多重调用时，创建的对象仍然是只有一个，而不是调用一个构造器就创造了一个新的对象，只有最开始被调用的构造器才创造了对象。 构造器中，不能使用 “this(形参列表)” 的方式调用自己。 如果一个类中有 n 个构造器，则最多有 n - 1 个构造器中使用了 “this(形参列表)”。 构造器在彼此调用时，不能形参一个封闭环，如：构造器 A 中调用了构造器 B，则在构造器 B 中不能再调用构造器 A，多构造器调用类推。 规定：”this(形参列表)” 必须声明在当前构造器的首行。 一个构造器内部，最多只能声明一个 “this(形参列表)”，即只能调用一个其他的构造器。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Person &#123; private String name; private int age; // 无参构造器 public Person() &#123; this.eat(); &#125; // 带name的构造器 public Person(String name) &#123; this();// 调用无参构造器 this.name = name; &#125; // 带name和age的构造器 public Person(String name, int age) &#123; this(name);// 调用带name的构造器 this.age = age; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return this.name;// 此处this可以省略 &#125; public void setAge(int age) &#123; this.age = age; &#125; public int getAge() &#123; return this.age;// 此处this可以省略 &#125; public void eat() &#123; System.out.println(&quot;人吃饭&quot;); this.study();// this调用方法，此处this可以省略 &#125; public void study() &#123; this.eat();// this调用方法，此处this可以省略 System.out.println(&quot;人学习&quot;); &#125; public static void main(String[] args) &#123; &#125;&#125; 实例二： 1234567891011121314151617181920212223242526272829303132333435363738public class Boy &#123; private String name; private int age; public Boy() &#123; &#125; public Boy(String name, int age) &#123; this.name = name; this.age = age; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setAge(int age) &#123; this.age = age; &#125; public int getAge() &#123; return age; &#125; public void marray(Girl girl) &#123; System.out.println(&quot;我想娶&quot; + girl.getName()); &#125; public void shout() &#123; System.out.println(&quot;我想找对象&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839public class Girl &#123; private String name; private int age; public Girl() &#123; &#125; public Girl(String name, int age) &#123; this.name = name; this.age = age; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setAge(int age) &#123; this.age = age; &#125; public int getAge() &#123; return age; &#125; public void marry(Boy boy) &#123; System.out.println(&quot;我想嫁给&quot; + boy.getName()); boy.marray(this);// 传入当前Girl对象 &#125; public int compare(Girl girl) &#123; return this.age - girl.age; &#125;&#125; 1234567891011public class Person &#123; public static void main(String[] args) &#123; Boy boy = new Boy(&quot;罗密欧&quot;, 20); boy.shout(); Girl girl = new Girl(&quot;朱丽叶&quot;, 18); girl.marry(boy); Girl girl2 = new Girl(&quot;祝英台&quot;, 19); System.out.println(&quot;年龄差：&quot; + girl.compare(girl2)); &#125;&#125; 关键字：packagepackage 语句作为 java 源文件的第一条语句，指明该文件中定义的类所在的包。若缺省该语句，则指定为无名包。 语法格式： 包对应于文件系统的目录，package 语句中，用 . 来指明包 (目录) 的层次。 包属于标识符，遵循标识符的命名规范，通常用小写单词标识。通常使用所在公司域名的倒置，如：com.atguigu.xxx。 同一个包下，不能命名同名的接口、类。不同的包下，可以命名同名的接口、类。 JDK 中主要的包介绍： java.lang —- 包含一些 java 语言的核心类，如 String、Math、Integer、 System 和 Thread，提供常用功能。 java.net —- 包含执行与网络相关的操作的类和接口。 java.io —- 包含能提供多种输入/输出功能的类。 java.util —- 包含一些实用工具类，如定义系统特性、接口的集合框架类、使用与日期日历相关的函数。 java.text —- 包含了一些 java 格式化相关的类。 java.sql —- 包含了 java 进行 JDBC 数据库编程的相关类/接口。 java.awt —- 包含了构成抽象窗口工具集 (abstract window toolkits) 的多个类，这些类被用来构建和管理应用程序的图形用户界面 (GUI)。(B/S 和 C/S) 关键字：import为使用定义在不同包中的 java 类，需用 import 语句来引入指定包层次下所需要的类或全部类 (.*)。import 语句告诉编译器到哪里去寻找类。 语法格式： 在源文件中使用 import 语句，可以显式的导入指定包下的类或接口。 声明在包的声明和类的声明之间。 如果需要导入多个类或接口，那么就并列显式声明多个 import 语句即可。 举例：可以使用 import java.util.*; 的方式，一次性导入 java.util 包下所有的类或接口。 如果导入的类或接口是 java.lang 包下的，或者是当前包下的，则可以省略此 import 语句。 如果在代码中使用不同包下的同名的类，那么就需要使用类的全类名的方式指明调用的是哪个类。 如果已经导入 java.a 包下的类，那么如果需要使用 a 包的子包下的类的话，仍然需要导入。 import static 组合的使用：导入指定类或接口下的静态的属性或方法。 1234567import static java.lang.System.*;public class Person &#123; public static void main(String[] args) &#123; out.println(&quot;打印方法&quot;);// 可以省略System &#125;&#125; OOP 特征二：继承性如果多个类中存在相同的属性和行为时，将这些内容抽取到单独一个类中，那么这多个类无需再定义这些属性和行为，只要继承那个抽出来的类即可。 此处的多个类称为子类 (派生类、subclass)**，单独的这个类称为父类 (基类、超类、superclass)**。可以理解为：”子类 is a 父类”。 类继承语法规则： 继承性的作用： 继承的出现减少了代码冗余，提高了代码的复用性。 继承的出现，更有利于功能的扩展。 继承的出现，让类与类之间产生了关系，提供了多态的前提。 继承性的特点： 子类继承了父类，就继承了父类中声明的所有属性和方法。特别的，父类中声明为 private 的属性和方法，子类继承父类以后，仍然认为子类获取了父类中私有的结构，只是因为封装性的影响，使得子类的实例不能直接调用父类的这些私有的结构而已 (事实上，父类的实例，也不能直接调用这些私有的结构)。 在子类中，可以使用父类中定义的方法和属性，也可以声明创建子类特有的属性和方法，以实现功能的扩展。 在 java 中，继承的关键字用的是 extends，即子类不是父类的子集，而是对父类的扩展。 继承性的规则： 子类不能直接访问父类中私有的 (private) 的成员变量和方法。 java 只支持单继承和多层继承，不允许多重继承。 一个子类只能有一个父类。 一个父类可以派生出多个子类。 此处强调的是 java 类的单继承性，java 中，接口是可以多继承的。 子类和父类是一个相对概念。子类直接继承的父类，称为直接父类，间接继承的父类，称为间接父类。 子类继承父类后，就获取了直接父类及所有间接父类中声明的属性和方法。 所有的 java 类 (除 java.lang.Object 类之外)，都直接或间接继承 java.lang.Object。 方法的重写 (override/overwrite)在子类中可以根据需要，对从父类中继承来的方法进行改造，也称为方法的重置、覆盖。在程序执行时，子类的方法将覆盖父类的方法。 重写的要求： 子类重写的方法必须和父类被重写的方法具有相同的方法名称、参数列表。 子类重写的方法使用的访问权限不能小于父类被重写的方法的访问权限 (权限修饰符)。 子类不能重写父类中声明为 private 权限的方法。 子类中可以声明与父类 private 方法相同名称和参数列表的方法，但不属于重写。 子类重写的方法的返回值类型不能大于父类被重写的方法的返回值类型。 父类被重写的方法的返回值类型是 void，则子类重写的方法的返回值类型只能是 void。 父类被重写的方法的返回值类型是 A 类型，则子类重写的方法的返回值类型可以是 A 类或 A 类的子类。 父类被重写的方法的返回值类型是基本数据类型 (比如：double)，则子类重写的方法的返回值类型必须是相同的基本数据类型 (即，只能是 double)。 子类重写的方法抛出的异常类型不能大于父类被重写的方法抛出的异常类型。 子类与父类中同名同参数的方法必须同时声明为非 static 的(此时属于重写)，或者同时声明为 static 的 (此时不属于重写)。因为 static 方法是属于类的，子类无法覆盖父类的方法。 实例一： 实例二： 方法重载与重写的区别： 二者的定义细节：略。 从编译和运行的角度看：重载，是指允许存在多个同名方法，而这些方法的参数不同。编译器根据方法不同的参数表，对同名方法的名称做修饰。对于编译器而言，这些同名方法就成了不同的方法。它们的调用地址在编译期就绑定了。java 的重载是可以包括父类和子类的，即子类可以重载父类的同名不同参数的方法。所以：对于重载而言，在方法调用之前，编译器就已经确定了所要调用的方法，这称为 “早绑定” 或 “静态绑定”**；而对于多态，只有等到方法调用的那一刻，解释运行器才会确定所要调用的具体方法，这称为 **”晚绑定” 或 **”动态绑定”**。引用一句 Bruce Eckel 的话：“不要犯傻，如果它不是晚绑定，它就不是多态。” 重载不表现为多态性，重写表现为多态性。 关键字：super super 理解为：父类的。 super 可以用来调用父类的：属性、方法、构造器。 在子类的方法或构造器中，可以通过使用 “super.属性” 或 “super.方法” 的形式，显示的调用父类中声明的属性或方法。 通常情况下，可以省略 “super.”。 特殊情况：当子类和父类中定义了同名的属性时，要想在子类中调用父类中声明的该属性，则必须显示的使用 “super.属性” 的方式，表明调用的是父类中声明的属性。 特殊情况：当子类重写了父类中的方法以后，要想在子类中调用父类中被重写的方法时，则必须显示的使用 “super.方法” 的方式，表明调用的是父类中被重写的方法。 在子类的构造器中，可以通过使用 “super(形参列表)” 的形式，显示的调用父类中声明的指定的构造器。 “super(形参列表)” 的使用，必须声明在子类构造器的首行。 在类的构造器中，针对于 “this(形参列表)” 或 “super(形参列表)”，只能二选一，不能同时出现。 在构造器的首行，如果没有现实的声明 “this(形参列表)” 或 “super(形参列表)”，则默认调用的是父类中空参的构造器，即：super();。 子类中所有的构造器默认都会访问父类中空参的构造器。 当父类中没有空参的构造器时，子类的构造器必须通过 “this(形参列表)” 或 “super(形参列表)” 语句，指定调用本类或者父类中相应的构造器。同时，只能二选一，且必须放在构造器的首行。 如果子类构造器中既未显式调用父类或本类的构造器，且父类中又没有无参的构造器，则编译出错。 在类的多个构造器中，至少有一个类的构造器中使用了 “super(形参列表)”，调用父类中的构造器。 实例： 父类： 1234567891011121314151617181920212223public class Person &#123; String name; int age; int id = 1000; public Person() &#123; System.out.println(&quot;父类的空参构造器&quot;); &#125; public Person(String name, int age, int id) &#123; this.name = name; this.age = age; this.id = id; &#125; public void eat() &#123; System.out.println(&quot;吃饭&quot;); &#125; public void sleep() &#123; System.out.println(&quot;睡觉&quot;); &#125;&#125; 子类： 1234567891011121314151617181920212223242526272829303132333435363738394041public class Student extends Person &#123;// String name;// 父类中已有的属性，可以省略// int age;// 父类中已有的属性，可以省略 String major; int id = 1001; public Student() &#123; &#125; public Student(String name, int age, String major) &#123; this.name = name; this.age = age; this.major = major; &#125; // 父类中已有的方法，可以省略，如有需要，可以重写，// public void eat() &#123;// System.out.println(&quot;吃饭&quot;);// &#125; // 重写父类的方法 @Override public void sleep() &#123; System.out.println(&quot;学生睡觉&quot;); &#125; public void study() &#123; System.out.println(&quot;学习&quot;); &#125; public void show() &#123; System.out.println(&quot;子类中的id: &quot; + this.id);// this可以省略，就近原则 System.out.println(&quot;父类中的id: &quot; + super.id);// 子类与父类有同名的属性id，此时super不可以省略 &#125; public static void main(String[] args) &#123; Student student = new Student(); student.show(); &#125;&#125; this 和 super 的区别： 思考： 为什么 “super(形参列表)” 和 “this(形参列表)” 调用语句不能同时在一个构造器中出现？ 因为 “super(形参列表)” 和 “this(形参列表)” 调用语句都必须出现在构造器中的首行。 为什么 “super(形参列表)” 和 “this(形参列表)” 只能作为构造器中的第一句出现？ 因为无论通过哪个构造器创建子类对象，都需要保证先初始化父类。这样做的目的是：当子类继承父类后，可以获得父类中所有的属性和方法，这样子类就有必要在一开始就知道父类是如何为对象进行初始化。 子类对象实例化过程 从结果上看： 子类继承父类之后，就获取了父类中声明的属性和方法。(继承性) 创建子类的对象，在堆空间中，就会加载所有父类中声明的属性。 从过程上看： 当通过子类的构造器创建子类对象时，一定会直接或间接的调用其父类的构造器，进而调用父类的父类的构造器，直到调用了 java.lang.Object 类中空参的构造器为止。正因为加载过所有的父类的结构，所以才可以看到内存中有父类中的结构，子类对象才能够进行调用。 明确：虽然创建子类对象时，调用了父类的构造器，但是自始至终只创建了一个对象，即为 new 出来的子类对象。 实例： 从输出结果可以看出，在创建 Man 的实例时，先进入了父类的空参构造器，然后执行子类的空参构造器。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Person &#123; String name; int age; public Person() &#123; System.out.println(&quot;父类空参构造器&quot;); &#125; public void eat() &#123; System.out.println(&quot;人吃饭&quot;); &#125; public void walk() &#123; System.out.println(&quot;人走路&quot;); &#125; public static void main(String[] args) &#123; Person person = new Man(); person.eat(); person.walk(); &#125;&#125;class Man extends Person &#123; boolean isSmoking; public void earnMoney() &#123; System.out.println(&quot;男人负责挣钱养家&quot;); &#125; @Override public void eat() &#123; System.out.println(&quot;男人多吃肉，长肌肉&quot;); &#125; @Override public void walk() &#123; System.out.println(&quot;男人霸气的走路&quot;); &#125;&#125;输出结果：父类空参构造器子类空参构造器男人多吃肉，长肌肉男人霸气的走路 OOP 特征三：多态性多态性，也叫对象的多态性：父类的引用指向子类的对象 (或子类的对象赋给父类的引用)。 一个变量只能有一种确定的数据类型。 一个引用类型变量可能指向 (引用) 多种不同类型的对象。 子类可看做是特殊的父类，所以父类类型的引用可以指向子类的对象：向上转型 (upcasting)。 一个引用类型变量如果声明为父类的类型，但实际引用的是子类对象，那么该变量就不能再访问子类中添加的属性和方法： 多态的使用： 虚拟方法调用。 有了对象的多态性以后，在编译期，只能调用父类中声明的方法，但在运行期，实际执行的是子类中重写的父类的方法。 编译，看左边；运行，看右边。 java 引用变量有两个类型：编译时类型和运行时类型。编译时类型由声明该变量时使用的类型决定，运行时类型由实际赋给该变量的对象决定。 若编译时类型和运行时类型不一致，就出现了对象的多态性。 多态情况下，看左边：看的是父类的引用 (父类中不具备子类特有的方法)，看右边：看的是子类的对象 (实际运行的是子类重写父类的方法)。 对象的多态性，只适用于方法，不适用于属性。对于属性，编译期和运行期，看的都是左边，即都是父类中声明的那个属性。 成员方法：编译时，要查看引用变量所声明的类中是否有所调用的方法。运行时，调用实际 new 的对象所属的类中的重写方法。 成员变量：不具备多态性，只看引用变量所声明的类。 子类继承父类： 若子类重写了父类方法，就意味着子类里定义的方法彻底覆盖了父类里的同名方法，系统将不可能把父类里的方法转移到子类中。 编译，看左边；运行，看右边。 对于实例变量则不存在这样的现象，即使子类里定义了与父类完全相同的实例变量，这个实例变量依然不可能覆盖父类中定义的实例变量。 编译，运行，都看左边。 实例： 父类： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Person &#123; String name; int age; public void eat() &#123; System.out.println(&quot;人吃饭&quot;); &#125; public void walk() &#123; System.out.println(&quot;人走路&quot;); &#125; public static void main(String[] args) &#123; // 对象的多态性：父类的引用指向子类的对象 Person person = new Man(); // 多态的使用：当调用子父类同名同参数的方法时，实际执行的是子类重写的父类的方法---虚拟方法调用 // 编译期，只能调用父类Person类中的方法；运行期，执行的是子类Man类中的方法。 person.eat(); person.walk(); // 不能调用子类特有的属性或方法，因为编译时，person是Person类型，而Person类中没有子类的这个特有属性或方法。 // 有了对象的多态性以后，内存中实际上是加载了子类特有的属性或方法的，但是由于变量声明为父类类型，导致编译时，只能调用父类中 // 声明的属性和方法，子类中特有的属性和方法不能调用。 // person.isSmoking = true; // person.earnMoney(); System.out.println(&quot;*********************************&quot;) // 如何才能使用子类特有的属性和方法？ // 向下转型：使用强制类型转换符 Man man = (Man) person; man.isSmoking = true; man.earnMoney(); // 使用强转时，可能出现java.lang.ClassCastException异常 Woman woman = (Woman) person; woman.goShopping(); &#125;&#125;输出结果：父类空参构造器子类空参构造器男人多吃肉，长肌肉男人霸气的走路*********************************男人负责挣钱养家 子类： 123456789101112131415161718public class Man extends Person &#123; boolean isSmoking; public void earnMoney() &#123; System.out.println(&quot;男人负责挣钱养家&quot;); &#125; @Override public void eat() &#123; System.out.println(&quot;男人多吃肉，长肌肉&quot;); &#125; @Override public void walk() &#123; System.out.println(&quot;男人霸气的走路&quot;); &#125;&#125; 实例二： 1234567891011121314151617181920212223242526272829public class FieldMethodTest &#123; public static void main(String[] args) &#123; Sub s = new Sub(); System.out.println(s.count);// 20 s.display();// 20 Base b = s; // 对于引用数据，==比较的是两个引用数据类型变量的地址值 System.out.println(b == s);// true System.out.println(b.count);// 10 b.display();// 20 &#125;&#125;class Base &#123; int count = 10; public void display() &#123; System.out.println(this.count); &#125;&#125;class Sub extends Base &#123; int count = 20; @Override public void display() &#123; System.out.println(this.count); &#125;&#125; 实例三： 12345678910111213141516171819202122232425262728public class InterviewTest1 &#123; public static void main(String[] args) &#123; Base base = new Sub(); base.add(1, 2, 3);// sub_1 Sub s = (Sub) base; s.add(1, 2, 3);// sub_2 &#125;&#125;class Base &#123; public void add(int a, int... arr) &#123; System.out.println(&quot;base&quot;); &#125;&#125;class Sub extends Base &#123; @Override public void add(int a, int[] arr) &#123; System.out.println(&quot;sub_1&quot;); &#125; // 这个方法没有重写，在Base类中不存在这样声明的方法， // 也就没有多态，所以base.add(1, 2, 3)方法输出sub_1 public void add(int a, int b, int c) &#123; System.out.println(&quot;sub_2&quot;); &#125;&#125; 多态性的使用前提： 有类的继承关系。 有方法的重写。 如果没有以上两个前提，就不存在多态。 多态性的优点： 提高了代码的通用性，常称作接口重用。 方法声明的形参类型为父类类型，可以使用子类的对象作为实参调用该方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class AnimalTest &#123; // 多态的使用：传入的是Animal对象，但实际传入的可以是Animal的子类 public void func(Animal animal) &#123; animal.eat(); animal.shout(); &#125; public static void main(String[] args) &#123; AnimalTest animalTest = new AnimalTest(); animalTest.func(new Dog()); animalTest.func(new Cat()); &#125;&#125;class Animal &#123; public void eat() &#123; System.out.println(&quot;动物：进食&quot;); &#125; public void shout() &#123; System.out.println(&quot;动物：叫&quot;); &#125;&#125;class Dog extends Animal &#123; @Override public void eat() &#123; System.out.println(&quot;狗吃骨头&quot;); &#125; @Override public void shout() &#123; System.out.println(&quot;汪！汪！汪！&quot;); &#125;&#125;class Cat extends Animal &#123; @Override public void eat() &#123; System.out.println(&quot;猫吃鱼&quot;); &#125; @Override public void shout() &#123; System.out.println(&quot;喵！喵！喵！&quot;); &#125;&#125;// 举例二class Order &#123; // 此方法可以传入任意对象，而不需要每个特定对象都创建一次method()方法 public void method(Object object) &#123; // 方法体 &#125;&#125; 抽象类、接口的使用。(抽象类和接口不能实例化，它们的使用也体现了多态) 虚拟方法调用： 正常的方法调用： 1234Person e = new Person();e.getInfo();Student e = new Student();e.getInfo(); 虚拟方法调用 (多态情况下) 子类中定义了与父类同名同参数的方法，在多态情况下，将此时父类的方法称为虚拟方法，父类根据赋给它的不同子类对象，动态调用属于子类的该方法。这样的方法调用在编译期是无法确定的。 12Person e = new Student();e.getInfo();// 调用Student类的getInfo()方法 编译时类型和运行时类型 上面代码中，编译时 e 为 Person 类型，而方法的调用是在运行时确定的，所以调用的是 Student 类的 getInfo() 方法 —— 动态绑定。 重写是多态，重载不是。 实例： 多态是编译时行为还是运行时行为? 多态是运行时行为，证明方法如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class InterviewTest &#123; public static Animal getInstance(int key) &#123; switch (key) &#123; case 0: return new Cat(); case 1: return new Dog(); default: return new Sheep(); &#125; &#125; public static void main(String[] args) &#123; // 因为key需要在运行时才能得到值，编译期时无法判断getInstance()方法输出什么 int key = new Random().nextInt(3); System.out.println(key); Animal animal = getInstance(key); animal.eat(); &#125;&#125;class Animal &#123; protected void eat() &#123; System.out.println(&quot;animal eat food&quot;); &#125;&#125;class Cat extends Animal &#123; @Override protected void eat() &#123; System.out.println(&quot;cat eat fish&quot;); &#125;&#125;class Dog extends Animal &#123; @Override public void eat() &#123; System.out.println(&quot;Dog eat bone&quot;); &#125;&#125;class Sheep extends Animal &#123; @Override public void eat() &#123; System.out.println(&quot;Sheep eat grass&quot;); &#125;&#125; 关键字：instanceofa instanceof A：检验对象 a 是否为类 A 的对象实例，如果是，返回 true，如果不是，返回 false。 使用情景：为了避免向下转型时出现 java.lang.ClassCastException，在向下转型之前，先进行 instanceof 判断，在返回 true 时，才进行向下转型。 要求 a 所属的类与类 A 必须是子类和父类的关系，否则编译错误。 如果 a 属于类 A 的子类 B，a instanceof A 的返回值也为 true。 对象类型转换 (casting) 基本数据类型的 Casting 自动类型转换：小的数据类型可以自动转换成大的数据类型。如 long g = 20; double d = 12.0f;。 强制类型转换：可以把大的数据类型强制转换 (casting) 成小的数据类型。如 float f = (float)12.0; int a = (int)1200L;。 对 java 对象的强制类型转换称为造型 从子类到父类的类型转换可以自动进行。 从父类到子类的类型转换必须通过造型 (强制类型转换) 实现。 无继承关系的引用类型间的转换是非法的。 在造型前可以使用 instanceof。 实例： 123456789101112131415public class ConversionTest &#123; public static void main(String[] args) &#123; double d = 13.4; long l = (long) d; System.out.println(l); int in = 5; // boolean b = (boolean)in; Object obj = &quot;Hello&quot;; String objStr = (String) obj; System.out.println(objStr); Object objPri = new Integer(5); // 下面代码运行时引发ClassCastException异常 String str = (String) objPri; &#125;&#125; 123456789101112131415public class Test &#123; public void method(Person e) &#123;// 设Person类中没有getschool()方法 // System.out.pritnln(e.getschool());// 非法，编译时错误 if (e instanceof Student) &#123; Student me = (Student) e;// 将e强制转换为Student类型 System.out.pritnln(me.getschool()); &#125; &#125; public static void main(String[] args) &#123; Test t = new Test(); Student m = new Student(); t.method(m); &#125;&#125; Object 类的使用 Object 类是所有 java 类的根父类。 如果在类的声明中未使用 extends 关键字指明其父类，则默认父类为 java.lang.Object 类。 验证方法： 123456789public class ObjectTest &#123; public static void main(String[] args) &#123; Base base = new Base(); System.out.println(&quot;父类：&quot; + base.getClass().getSuperclass());// 父类：class java.lang.Object &#125;&#125;class Base &#123;&#125; Object 类中的主要结构 == 操作符与 equals() 方法== 运算符： 如果比较的是基本数据类型变量：比较两个变量保存的数据是否相等，不一定类型要相同。 12345678910111213141516public static void main(String[] args) &#123; int i = 10; int j = 10; System.out.println(i == j);// true double k = 10.0; System.out.println(i == k);// true char c = 10; System.out.println(i == c);// true char c1 = &#x27;A&#x27;; char c2 = 65; System.out.println(c1 == c2);// true&#125; 如果比较的是引用数据类型变量：比较两个变量的地址值是否相同，即两个引用是否指向同一个对象实体。 12345678910// String类比较特殊，要注意。public static void main(String[] args) &#123; String s1 = &quot;javacdfa&quot;;// 这样写的javacdfa，位于常量池中 String s2 = &quot;javacdfa&quot;; System.out.println(s1 == s2);// true String s3 = new String(&quot;iam&quot;);// 这样new的，在堆内存中 String s4 = new String(&quot;iam&quot;); System.out.println(s3 == s4);// false&#125; 用 == 进行比较时，符号两边的数据类型必须兼容 (可自动转换的基本数据类型除外)，否则编译出错。 equals() 方法： 是一个方法，而非运算符，只能适用于引用数据类型。 使用格式：obj1.equals(obj2)。 所有类都继承了 Object，也就获得了 equals() 方法，也可以对其重写 。 Object 类中 equals() 方法的定义： 123public boolean equals(Object obj) &#123; return (this == obj);&#125; 说明：其作用与 == 相同, 比较是否指向同一个对象 。 像 File、String、Date 及包装类等，都重写了 Object 类中的 equals() 方法，重写以后，比较的不是两个引用对象的地址是否相同，而是比较两个引用对象的 “实体内容” 是否相同。 通常情况下，自定义的类使用 equals() 方法时，也是比较两个引用对象的 “实体内容” 是否相同。那么，就应该重写 equals() 方法。比如 String 类的 equals() 方法： 1234567891011121314151617181920212223public boolean equals(Object anObject) &#123; // 先判断地址 if (this == anObject) &#123; return true; &#125; // 再判断内容 if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false;&#125; 重写 equals() 方法的原则： 对称性：如果 x.equals(y) 返回是 true，那么 y.equals(x) 也应该返回是 true。 自反性：x.equals(x) 必须返回是 true。 传递性：如果 x.equals(y) 返回是 true，而且 y.equals(z) 返回是 true，那么 z.equals(x) 也应该返回是 true。 一致性：如果 x.equals(y) 返回是 true，只要 x 和 y 内容一直不变，不管重复 x.equals(y) 多少次，返回都是 true。 任何情况下，x.equals(null) 永远返回是 false；x.equals(和x不同类型的对象) 永远返回是 false。 面试题：== 和 equals() 的区别？ == 既可以比较基本类型也可以比较引用类型。对于基本类型是比较值，对于引用类型是比较内存地址。 equals() 方法属于 java.lang.Object 类里面的方法，如果该方法没有被重写过，默认也是 ==。 具体到特定自定义的类，要看该类里有没有重写 Object 的 equals() 方法以及重写的逻辑。 通常情况下，重写 equals() 方法，是比较类中的相应属性是否都相等。 toString() 方法 当输出一个对象的引用时，实际上就是调用当前对象的 toString() 方法。 1234567891011public class ObjectTest &#123; public static void main(String[] args) &#123; Order order = new Order(); System.out.println(order);// cn.xisun.database.Order@78308db1 System.out.println(order.toString());// cn.xisun.database.Order@78308db1 &#125;&#125;class Order &#123;&#125; Object 类中 toString() 方法的定义： 123public String toString() &#123; return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode());&#125; 像 File、String、Date 及包装类等，都重写了 Object 类中的 toString() 方法，使得在调用对象的 toString() 方法时，返回相应的 “实体内容”。 自定义类也可以重写 toString() 方法，当调用此方法时，返回 相应的 “实体内容”。比如 String 类的 equals() 方法： 123public String toString() &#123; return this;&#125; 基本类型数据转换为 String 类型时，调用了对应包装类的 toString() 方法。 面试题： 12345678public static void main(String[] args) &#123; char[] arr = new char[] &#123; &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27; &#125;; System.out.println(arr);// abc int[] arr1 = new int[] &#123; 1, 2, 3 &#125;; System.out.println(arr1);// [I@78308db1 double[] arr2 = new double[] &#123; 1.1, 2.2, 3.3 &#125;; System.out.println(arr2);// [D@27c170f0&#125; 包装类 (Wrapper) 的使用包装类：也叫封装类，是针对八种基本数据类型定义的相应的引用数据类型，以使得基本数据类型的变量具有类的特征。 JDK 1.5 之后，支持自动装箱，自动拆箱，但类型必须匹配。 基本类型、包装类与 String 类之间的转换： 基本数据类型转换成包装类 装箱：基本数据类型包装成包装类的实例，通过包装类的构造器实现。例如：int i = 500; Integer t = new Integer(i);。 自动装箱，例如：int i =500; Integer t = i;。 包装类转换成基本数据类型 拆箱：获得包装类对象中包装的基本类型变量，通过调用包装类的 .xxxValue() 方法。例如：boolean b = bObj.booleanValue();。 自动拆箱，例如：Integer t = 500; int i = t;。 基本数据类型/包装类转换成字符串 调用字符串重载的 valueOf() 方法，例如：String fstr = String.valueOf(2.34f);。 更直接的方式，连接运算，例如：String intStr = 5 + &quot;&quot;;。 字符串转换成基本数据类型/包装类 通过包装类的构造器实现，例如：int i = new Integer(&quot;12&quot;);。 通过包装类的 parseXxx(String s) 静态方法，例如：Float f = Float.parseFloat(“12.1”);。 面试题： 1234567891011121314public static void main(String[] args) &#123; // 三目运算符比较基本数据类型，在编译阶段自动拆箱为int和double类型，由于三目运算符要求表达式2和表达式3类型一致， // 所以在编译阶段自动类型提升(即int自动类型转换为double类型)，再自动装箱为Object，输出时使用多态调用重写 // 的toString()，即Double包装类的toString()方法 Object o1 = true ? new Integer(1) : new Double(2.0); System.out.println(o1);// 1.0 Object o2; if (true) o2 = new Integer(1); else o2 = new Double(2.0); System.out.println(o2);// 1&#125; 1234567891011public static void main(String[] args) &#123; Integer i = new Integer(1); Integer j = new Integer(1); System.out.println(i == j);// new了两个对象，false Integer m = 1; Integer n = 1; System.out.println(m == n);// 自动装箱，且在-128~127范围内，true Integer x = 128;// 相当于new Integer(128); Integer y = 128;// 相当于new Integer(128); System.out.println(x == y);// false&#125; Integer 类内部定义了 IntegerCache 结构，IntegerCache 中定义了一个 Integer[] 数组，保存了从 -128127 范围的整数。如果使用了自动装箱的方式，给 Integer 赋值在 -128127 范围内时，可以直接使用数组中的元素，不用 new。目的：提高效率。如果赋值超过了此范围，会 new 一个新对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 利用Vector代替数组处理：从键盘读入学生成绩(以负数代表输入结束)，找出最高分，并输出学生成绩等级。 * 提示：数组一旦创建，长度就固定不变，所以在创建数组前就需要知道它的长度。而向量类java.util.Vector可以根据需要动态伸缩。 *  创建Vector对象：Vector v=new Vector(); *  给向量添加元素：v.addElement(Object obj);// obj必须是对象 *  取出向量中的元素：Object obj=v.elementAt(0); *  注意第一个元素的下标是0，返回值是Object类型的。 *  计算向量的长度：v.size(); *  若与最高分相差10分内：A等；20分内：B等；30分内：C等；其它：D等。 */public class ScoreTest &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); Vector&lt;Object&gt; objects = new Vector&lt;&gt;(); int maxScore = -1; while (true) &#123; int nextScore = scanner.nextInt(); if (nextScore &lt; 0) &#123; break; &#125; if (nextScore &gt; 100) &#123; continue; &#125; objects.add(nextScore);// 自动装箱 if (maxScore &lt; nextScore) &#123; maxScore = nextScore; &#125; &#125; char level; for (int i = 0; i &lt; objects.size(); i++) &#123; Object object = objects.elementAt(i); int score = (Integer) object;// 自动拆箱 if (maxScore - score &lt; 10) &#123; level = &#x27;A&#x27;; &#125; else if (maxScore - score &lt; 20) &#123; level = &#x27;B&#x27;; &#125; else if (maxScore - score &lt; 30) &#123; level = &#x27;C&#x27;; &#125; else &#123; level = &#x27;D&#x27;; &#125; System.out.println(&quot;Student-&quot; + i + &quot; score is &quot; + score + &quot;, level is &quot; + level); &#125; &#125;&#125; 关键字：static当编写一个类时，其实就是在描述其对象的属性和行为，而并没有产生实质上的对象，只有通过 new 关键字才会产生出对象，这时系统才会分配内存空间给对象，其方法才可以供外部调用。有时候，希望无论是否产生了对象或无论产生了多少对象的情况下，某些特定的数据在内存空间里只有一份。例如：所有的中国人都有个国家名称，每一个中国人都共享这个国家名称，不必在每一个中国人的实例对象中都单独分配一个用于代表国家名称的变量。 实例变量： 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; Circle c1 = new Circle(2.0); // c1.radius=2.0 Circle c2 = new Circle(3.0); // c2.radius=3.0 &#125;&#125;class Circle &#123; private double radius; public Circle(double radius) &#123; this.radius = radius; &#125; public double findArea() &#123; return Math.PI * radius * radius; &#125;&#125; 上述代码中，c1 的 radius 独立于 c2 的 radius，存储在不同的空间。c1 中的 radius 变化不会影响 c2 的 radius，反之亦然。 像 Circle 类中的变量 radius 这样的，叫**实例变量 (instance variable)**，它属于类的每一个对象，不能被同一个类的不同对象所共享。 如果想让一个类的所有实例共享数据，就用类变量。类变量的定义，就需要用到 static 关键字。 static 关键字的使用： static：静态的。 static 可以用来修饰：属性、方法、代码块、内部类。 static 修饰后的成员具备以下特点： 随着类的加载而加载。 优先于对象存在。 修饰的成员，被所有对象所共享。 访问权限允许时，可不创建对象，直接被类调用。 使用 static 修饰属性：静态变量 (类变量/class variable) 属性，按是否使用 static 修饰，分为：静态属性和非静态属性 (实例变量)。 实例变量：当创建了类的多个对象，每个对象都独立的拥有一套类中的非静态属性。当修改其中一个对象的非静态属性时，不会导致其他对象中同样的属性值被修改。 静态变量：当创建了类的多个对象，每个对象都共用同一个静态变量。当通过某一个对象修改静态变量时，会导致其他对象调用此静态变量时，是修改之后的值。 注意：实际操作时，虽然编译能过通过，但不应该通过类的实例对象来访问静态成员。 静态变量随着类的加载而加载。可以通过 “类.静态变量”的方式进行调用。 静态变量的加载要早于对象的创建。(实例变量在创建对象的过程中，或创建对象之后，才创建。) 由于类只会加载一次，则静态变量在内存中也只会存在一份：保存在方法区的静态域中。 类可以访问静态变量，但不能访问实例变量 (实例变量在对象产生时才生成)，对象可以访问实例变量，也能访问静态变量 (不推荐)。 静态变量举例：System.out，Math.PI。 使用 static 修饰方法：静态方法 (类方法/class method) 随着类的加载而加载，可以通过 “类.静态方法” 的方式进行调用。 类可以访问静态方法，但不能访问非静态方法 (非静态方法在对象产生时才生成)，对象可以访问非静态方法，也能访问静态方法 (不推荐)。 静态方法中，只能调用静态属性或静态方法，它们的生命周期是一致的。非静态方法中，既可以调用非静态属性或非静态方法，也能调用静态属性或静态方法。 static 使用的注意点： 在静态方法内，不能使用 this 关键字、super 关键字。(this 和 super 指向当前类对象和父类对象，需要创建实例对象后才有这些概念。) 12345public static void show() &#123; // 省略的是Chiese.，而不是this. walk();// 等同于Chinese.walk(); System.out.println(&quot;nation: &quot; + nation);// 等同于System.out.println(Chinese.nation);&#125; static 修饰的方法不能被重写。 关于静态属性和静态方法的使用，从生命周期的角度去理解。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Test &#123; public static void main(String[] args) &#123; Chinese c1 = new Chinese(); c1.name = &quot;姚明&quot;; c1.age = 40; Chinese c2 = new Chinese(); c2.name = &quot;马龙&quot;; c2.age = 30; // 通过c1对象修改nation的值，c2对象也能获得 // 实际操作时，虽然编译能过通过，但不应该通过类的实例对象来访问静态成员 c1.nation = &quot;CHN&quot;; System.out.println(c2.nation); // 对象实例调用非静态方法 c1.eat(); // 类调用静态方法 Chinese.show(); // 通过c1对象也能调用非静态方法 // 实际操作时，虽然编译能过通过，但不应该通过类的实例对象来访问静态成员 c1.show(); &#125;&#125;class Chinese &#123; String name; int age; static String nation; public void eat() &#123; System.out.println(&quot;吃饭&quot;); // 调用非静态结构 this.info(); System.out.println(&quot;name: &quot; + this.name); // 调用静态结构 walk(); System.out.println(&quot;nation: &quot; + nation); &#125; public void info() &#123; System.out.println(&quot;name: &quot; + name + &quot;, age: &quot; + age + &quot;, nation: &quot; + nation); &#125; public static void show() &#123; System.out.println(&quot;我是中国人&quot;); // 不能调用非静态的结构 // eat(); // name = &quot;Tom&quot;; // 调用静态的结构 walk(); System.out.println(&quot;nation: &quot; + nation);// 省略的是Chiese.，而不是this.，等同于System.out.println(Chinese.nation); &#125; public static void walk() &#123; System.out.println(&quot;走路&quot;); &#125;&#125; 类变量和实例变量内存解析： 类属性、类方法的设计思想： 类属性作为该类各个对象之间共享的变量，在设计类时，分析哪些属性不因对象的不同而改变，将这些属性设置为类属性，相应的方法设置为类方法。 如果方法与调用者无关，则这样的方法通常被声明为类方法，由于不需要创建对象就可以调用类方法，从而简化了方法的调用。 类中的常量，通常也声明为 static 的。 操作静态属性的方法，通常设置为 static 的。 工具类中的方法，习惯上声明为 static 的。 main() 方法的语法由于 java 虚拟机需要调用类的 main() 方法，所以该方法的访问权限必须是 public，又因为 java 虚拟机在执行 main() 方法时不必创建对象，所以该方法必须是 static 的，该方法接收一个 String 类型的数组参数，该数组中保存执行 java 命令时传递给所运行的类的参数。 又因为 main() 方法是静态的，我们不能直接访问该类中的非静态成员，必须创建该类的一个实例对象后，才能通过这个对象去访问类中的非静态成员，这种情况，我们在之前的例子中多次碰到。 main() 方法的使用说明： main() 方法是程序的入口。 main() 方法也是一个普通的静态方法，在执行某个类的 main() 方法之前，需要先加载这个类，这个过程是早于 main() 方法中首行的执行语句的。 main() 方法可以作为程序与控制台交互的方式之一，其他的还可以使用 Scanner 类。 命令行参数用法举例： 类的成员之四：代码块 (或初始化块) 代码块的作用：对 java 类或对象进行初始化。 代码块的分类：一个类中代码块若有修饰符，则只能被 static 修饰，称为静态代码块 (static block)，没有使用 static 修饰的，为非静态代码块。 静态代码块： 内部可以有输出语句。 随着类的加载而执行，而且只执行一次。(不同于静态方法，静态方法必须在被类显示的调用后，才会执行方法内的语句。) 作用：初始化类的信息。 如果一个类定义了多个静态代码块，则按照声明的先后顺序来执行。一般情况下，不建议定义多个。 静态代码块的执行要优先于非静态代码块的执行，与声明的先后顺序无关。 静态代码块中，只能调用静态的属性、静态的方法，不能调用非静态的属性、非静态的方法。 非静态代码块： 内部可以有输出语句。 随着对象的创建而执行。(不同于非静态方法，非静态方法必须在被类的对象显示的调用后，才会执行方法内的语句。) 每创建一个对象，就执行一次非静态代码块。且先于构造器执行。 作用：可以再创建对象时，对对象的属性等进行初始化。 如果一个类定义了多个非静态代码块，则按照声明的先后顺序来执行。一般情况下，不建议定义多个。 非静态代码块中，可以调用静态的属性、静态的方法，也可以调用非静态的属性、非静态的方法。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class BlockTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;*********类加载*********&quot;); String desc = Person.desc; System.out.println(desc); System.out.println(&quot;*********对象加载*********&quot;); Person person = new Person(); &#125;&#125;class Person &#123; // 属性 String name; int age; static String desc = &quot;我是一个人&quot;; // 静态代码块 static &#123; System.out.println(&quot;我是一个静态代码块-1&quot;); // 调用静态结构 desc = &quot;我是一个中国人&quot;;// 对类的静态属性重新赋值 info(); // 不能调用非静态结构 // name = &quot;Tom&quot;; // eat(); &#125; static &#123; System.out.println(&quot;我是一个静态代码块-2&quot;); &#125; // 非静态代码块 &#123; System.out.println(&quot;我是一个非静态代码块-1&quot;); // 调用静态结构 desc = &quot;我是一个中国人&quot;; info(); // 调用非静态结构 name = &quot;Tom&quot;; eat(); &#125; &#123; System.out.println(&quot;我是一个非静态代码块-2&quot;); &#125; // 构造器 public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; // 静态方法 public static void info() &#123; System.out.println(&quot;我是一个静态方法&quot;); &#125; // 非静态方法 public void eat() &#123; System.out.println(&quot;我是一个非静态方法&quot;); &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 代码块及构造器的执行顺序： 由父及子，静态先行。 注意：调用 main() 方法时，需要先加载类，这个过程是早于 main() 方法中的首行执行语句的。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576class Root &#123; static &#123; System.out.println(&quot;Root的静态初始化块&quot;); &#125; &#123; System.out.println(&quot;Root的普通初始化块&quot;); &#125; public Root() &#123; System.out.println(&quot;Root的无参数的构造器&quot;); &#125;&#125;class Mid extends Root &#123; static &#123; System.out.println(&quot;Mid的静态初始化块&quot;); &#125; &#123; System.out.println(&quot;Mid的普通初始化块&quot;); &#125; public Mid() &#123; System.out.println(&quot;Mid的无参数的构造器&quot;); &#125; public Mid(String msg) &#123; // 通过this调用同一类中重载的构造器 this(); System.out.println(&quot;Mid的带参数构造器，其参数值：&quot; + msg); &#125;&#125;class Leaf extends Mid &#123; static &#123; System.out.println(&quot;Leaf的静态初始化块&quot;); &#125; &#123; System.out.println(&quot;Leaf的普通初始化块&quot;); &#125; public Leaf() &#123; // 通过super调用父类中有一个字符串参数的构造器 super(&quot;尚硅谷&quot;); System.out.println(&quot;Leaf的构造器&quot;); &#125;&#125;public class LeafTest &#123; public static void main(String[] args) &#123; new Leaf(); System.out.println(); new Leaf(); &#125;&#125;输出结果：Root的静态初始化块Mid的静态初始化块Leaf的静态初始化块Root的普通初始化块Root的无参数的构造器Mid的普通初始化块Mid的无参数的构造器Mid的带参数构造器，其参数值：尚硅谷Leaf的普通初始化块Leaf的构造器Root的普通初始化块Root的无参数的构造器Mid的普通初始化块Mid的无参数的构造器Mid的带参数构造器，其参数值：尚硅谷Leaf的普通初始化块Leaf的构造器 实例二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Father &#123; static &#123; System.out.println(&quot;11111111111&quot;); &#125; &#123; System.out.println(&quot;22222222222&quot;); &#125; public Father() &#123; System.out.println(&quot;33333333333&quot;); &#125; // main方法是一个静态方法，执行某个类的main方法之前，要先加载这个类，此处是Father类 public static void main(String[] args) &#123; System.out.println(&quot;77777777777&quot;); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Father(); &#125;&#125;class Son extends Father &#123; static &#123; System.out.println(&quot;44444444444&quot;); &#125; &#123; System.out.println(&quot;55555555555&quot;); &#125; public Son() &#123; System.out.println(&quot;66666666666&quot;); &#125; // main方法是一个静态方法，执行某个类的main方法之前，要先加载这个类，此处是先加载Son类 public static void main(String[] args) &#123; // 由父及子 静态先行 System.out.println(&quot;77777777777&quot;); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Father(); &#125;&#125;public class Test &#123; // main方法是一个静态方法，执行某个类的main方法之前，要先加载这个类，此处是先加载Test类 public static void main(String[] args) &#123; System.out.println(&quot;77777777777&quot;); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Father(); &#125;&#125; 调用 Father 类的 main() 方法，要先加载 Father 类。输出结果： 12345678910111213141516F: 11111111111F/m: 77777777777F/m: ************************S: 44444444444F: 22222222222F: 33333333333S: 55555555555S: 66666666666F/m: ************************F: 22222222222F: 33333333333S: 55555555555S: 66666666666F/m: ************************F: 22222222222F: 33333333333 调用 Son 类的 main() 方法，要先加载 Son 类。输出结果： 12345678910111213141516F: 11111111111S: 44444444444S/m: 77777777777S/m: ************************F: 22222222222F: 33333333333S: 55555555555S: 66666666666S/m: ************************F: 22222222222F: 33333333333S: 55555555555S: 66666666666S/m: ************************F: 22222222222F: 33333333333 调用 Test 类的 main() 方法，要先加载 Test 类。输出结果： 12345678910111213141516T/m: 77777777777T/m: ************************F: 11111111111S: 44444444444F: 22222222222F: 33333333333S: 55555555555S: 66666666666T/m: ************************F: 22222222222F: 33333333333S: 55555555555S: 66666666666T/m: ************************F: 22222222222F: 33333333333 关键字：finalfinal：最终的。 final 可以用来修饰的结构：类、方法、变量 (属性是成员变量，是变量的其中一种。)。 final 用来修饰类：此类不能被其他类所继承。例如：String 类、System 类、StringBuffer 类。 final 用来修饰方法：此方法不能被子类重写。例如：Object 类中的 getClass()。 final 用来修饰变量：此时的 “变量” 称为常量，名称大写，且只能被赋值一次。 final 修饰成员变量：必须在声明时或代码块中或在每个构造器中显式赋值，否则编译不通过。 1234567891011121314151617181920212223242526public class FinalTest &#123; // 1.显式初始化：所有对象的这个常量值都是相同的，可以考虑直接显式初始化 final int WIDTH = 0; // 2.代码块中初始化：如果涉及到调用方法，或赋值操作较多，可以考虑代码块中初始化 final int HEIGHT; &#123; HEIGHT = show(); &#125; // 3.构造器中初始化：如果涉及到调用方法，或赋值操作较多，可以考虑代码块中初始化 final int LEFT; public FinalTest() &#123; LEFT = show(); &#125; public int show() &#123; return 0; &#125; public static void main(String[] args) &#123; FinalTest finalTest = new FinalTest(); &#125;&#125; final 修饰局部变量：修饰方法内局部变量时，表明该变量是一个常量，不能被修改；修饰形参时，表明此形参是一个常量，当调用此方法时，给常量形参赋一个实参，一旦赋值以后，就只能在方法体内使用此形参，但不能被修改。 123456789101112131415161718public class FinalTest &#123; public int show() &#123; // 1.修饰方法内局部变量：常量，不能被再次更改。 final int NUM = 10; return NUM; &#125; // 2.修饰形参：当方法被调用时，传入的实参，不能被再次更改。 public void show(final int num) &#123; System.out.println(num); &#125; public static void main(String[] args) &#123; FinalTest finalTest = new FinalTest(); finalTest.show(20); &#125;&#125; static final 用来修饰属性：全局常量。 12345678910111213141516171819202122public class FinalTest &#123; static final int WIDTH = 0; static final int HEIGHT; static &#123; HEIGHT = show(); &#125; public FinalTest() &#123; &#125; public static int show() &#123; return 0; &#125; public static void main(String[] args) &#123; FinalTest finalTest = new FinalTest(); &#125;&#125; 面试题： 123456public class Something &#123; public int addOne(final int x) &#123; // return ++x;// 编译不通过 return x + 1;// 正常 &#125;&#125; 123456789101112131415public class Something &#123; public void addOne(final Other o) &#123; // o = new Other();// 编译不通过 o.i++;// 正常 &#125; public static void main(String[] args) &#123; Other o = new Other(); new Something().addOne(o); &#125;&#125;class Other &#123; public int i;&#125; 抽象类和抽象方法随着继承层次中一个个新子类的定义，类变得越来越具体，而父类则更一般，更通用。类的设计应该保证父类和子类能够共享特征。有时将一个父类设计得非常抽象，以至于它没有具体的实例，这样的类叫做抽象类。 抽象类应用：抽象类是用来模型化那些父类无法确定全部实现，而是由其子类提供具体实现的对象的类。 abstract 关键字的使用： abstract：抽象的。 abstract 可以用来修饰的结构：类、方法。 abstract 修饰类：抽象类。 抽象类不能实例化。 抽象类中一定有构造器 (抽象类本身不能使用构造器)，便于子类实例化时调用。 开发中，会提供抽象类的子类，让子类对象实例化，完成相关操作。 abstract 修饰方法：抽象方法。 抽象方法只有方法声明，没有方法体，以分号结束。比如：public abstract void talk(); 包含抽象方法的类，一定是一个抽象类。反之，抽象类中可以没有抽象方法。 若子类重写了父类 (不仅包括直接父类，也包括间接父类) 中的所有的抽象方法后，此子类方可实例化；若子类没有重写父类中的所有的抽象方法，则次子类也是一个抽象类，需要使用 abstract 修饰。 abstract 不能修饰变量、代码块、构造器。 abstract 不能修饰私有方法、静态方法、final 的方法、final 的类。 抽象类的匿名子类对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Test &#123; public static void method(Student student) &#123; &#125; public static void method1(Person person) &#123; person.eat(); &#125; public static void main(String[] args) &#123; // 匿名对象 method(new Student()); // 1.创建非匿名类的非匿名的对象 Worker worker = new Worker(); method1(worker); // 2.创建非匿名类的匿名的对象 method1(new Worker()); // 3.创建匿名子类的非匿名的对象：p Person p = new Person() &#123; @Override public void eat() &#123; // 重写方法体 System.out.println(&quot;...&quot;); &#125; &#125;; method1(p); // 4.创建匿名子类的匿名对象 method1(new Person() &#123; @Override public void eat() &#123; // 重写方法体 System.out.println(&quot;,,,&quot;); &#125; &#125;); &#125;&#125;abstract class Person &#123; public abstract void eat();&#125;class Student &#123;&#125;class Worker extends Person &#123; @Override public void eat() &#123; // 重写方法体 System.out.println(&quot;、、、&quot;); &#125;&#125; 接口 (interface)一方面，有时必须从几个类中派生出一个子类，继承它们所有的属性和方法。但是，java 不支持多重继承。有了接口，就可以得到多重继承的效果。另一方面，有时必须从几个类中抽取出一些共同的行为特征，而它们之间又没有 is - a 的关系，仅仅是具有相同的行为特征而已。例如：鼠标、键盘、打印机、扫描仪、摄像头、充电器、MP3 机、手机、数码相机、移动硬盘等都支持 USB 连接。 接口就是规范，定义的是一组规则，体现了现实世界中 “如果你是/要…则必须能…” 的思想。继承是一个 “是不是” 的关系，而接口实现则是 “能不能” 的关系。 接口的本质是契约，标准，规范，就像我们的法律一样，制定好后大家都要遵守。 接口的使用： 接口使用 interface 定义。 java 中，接口和类是并列的两个结构，或者可以理解为一种特殊的类。从本质上讲，接口是一种特殊的抽象类。 如何定义接口：定义接口中的成员 JDK 7 及以前：只能定义全局常量和抽象方法。 全局常量：接口中的所有成员变量都默认是由 public static final 修饰的。书写时，可以省略，但含义不变，常量不能被更改。 抽象方法：接口中的所有抽象方法都默认是由 public abstract 修饰的。 JDK 8：除了定义全局常量和抽象方法之外，还可以定义静态方法、默认方法。 静态方法：使用 static 关键字修饰，默认为 public 的。 只能通过接口直接调用，并执行其方法体。 默认方法：使用 default 关键字修饰，默认为 public 的。 可以通过实现类的对象来调用，如果实现类重写了接口中的默认方法，调用时，执行的是重写后的方法。 如果子类 (或实现类) 继承的父类和实现的接口中，声明了同名同参数的mo人方法，那么子类在没有重写此方法的情况下， 默认调用的是父类中的同名同参数的方法 — 类优先原则。如果重写了，调用子类重写的方法。 如果实现类实现了多个接口，而多个接口中定义了同名同参数的默认方法，那么在实现类没有重写此方法的情况下，编译不通过 — 接口冲突。如果要避免接口冲突，则在实现类中，必须重写此方法。 在子类 (或实现类) 的方法中，使用 “super.方法名” 调用父类的方法，使用 “接口名.super.方法名” 调用接口中的方法。 实例： 1234567891011121314151617181920212223 public interface InterfaceA &#123; // 静态方法 static void method1() &#123; System.out.println(&quot;接口A：静态方法1&quot;); &#125; // 默认方法 default void method2() &#123; System.out.println(&quot;接口A：默认方法2&quot;); &#125; default void method3() &#123; System.out.println(&quot;接口A：默认方法3&quot;); &#125; default void method4() &#123; System.out.println(&quot;接口A：默认方法4&quot;); &#125; default void method5() &#123; System.out.println(&quot;接口A：默认方法5&quot;); &#125;&#125; 12345 public interface InterfaceB &#123; default void method5() &#123; System.out.println(&quot;接口B：默认方法5&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 public class SubClassTest &#123; public static void main(String[] args) &#123; // 1.静态方法 InterfaceA.method1(); SubClass subClass = new SubClass(); // 2.默认方法 subClass.method2(); // 3.重写的默认方法 subClass.method3(); // 4.调用的是父类中的method4() subClass.method4(); &#125; &#125; class SuperClass &#123; public void method4() &#123; System.out.println(&quot;父类：方法4&quot;); &#125; &#125; class SubClass extends SuperClass implements InterfaceA, InterfaceB &#123; // 重写接口InterfaceA中的method3() @Override public void method3() &#123; System.out.println(&quot;实现类：方法3&quot;); &#125; // 重写了父类SuperClass的method4() @Override public void method4() &#123; System.out.println(&quot;实现类：方法4&quot;); &#125; // InterfaceA和InterfaceB声明了同名同参的method5()，SubClass中必须重写此方法，否则接口冲突，编译不通过 // 如果继承的父类SuperClass中也声明了同名同参的method5()，则不会出现接口冲突 @Override public void method5() &#123; System.out.println(&quot;实现类：方法5&quot;); &#125; public void myMethod() &#123; method2();// InterfaceA的method2() method3();// 重写的InterfaceA的method3() InterfaceA.super.method3();// InterfaceA的method3() method4();// 重写的SuperClass的method4() super.method4();// 父类SuperClass的method4() InterfaceA.super.method5();// InterfaceA的method5() InterfaceB.super.method5();// InterfaceB的method5() &#125;&#125; 接口中不能定义构造器，意味着接口不可以实例化。 java 开发中，接口都通过让类去实现的方式 (implements) 来使用 (面向接口编程)。 如果实现类覆盖了接口中 (包括直接接口和间接接口) 的所有抽象方法，则此实现类可以实例化。如果实现类没有覆盖接口 (包括直接接口和间接接口) 中所有的抽象方法，则此实现类仍为一个抽象类。 1234567891011121314151617181920interface MyInterface&#123; String s = &quot;MyInterface&quot;; public void absM1();&#125;interface SubInterface extends MyInterface&#123; public void absM2();&#125;// 实现类SubAdapter必须给出接口SubInterface以及父接口MyInterface中所有方法的实现。// 否则，SubAdapter仍需声明为abstract的。public class SubAdapter implements SubInterface&#123; public void absM1()&#123; System.out.println(&quot;absM1&quot;); &#125; public void absM2()&#123; System.out.println(&quot;absM2&quot;); &#125;&#125; java 类可以实现多个接口，弥补了 java 单继承性的局限性。 格式：class SubClass extends SuperClass implements InterfaceA, InterfaceB, InterfaceC &#123;&#125; 接口与接口之间可以继承，而且可以多继承。 与继承关系类似，接口与实现类之间体现了多态性。 接口，实际上可以看作是一种规范。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class InterfaceTest &#123; public static void main(String[] args) &#123; System.out.println(Flyable.MAX_SPEED); System.out.println(Flyable.MIN_SPEED); Plane plane = new Plane(); plane.fly(); &#125;&#125;interface Flyable &#123; // 全局常量，可以省略 public static final int MAX_SPEED = 7900;// 第一宇宙速度 int MIN_SPEED = 1; // 抽象方法，可以省略 public abstract public abstract void fly(); void stop();&#125;interface Attackable &#123; void attack();&#125;// 全部实现接口中的方法，可以实例化class Plane implements Flyable &#123; @Override public void fly() &#123; System.out.println(&quot;飞机起飞&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;飞机降落&quot;); &#125;&#125;// 未全部实现接口中的方法，仍是一个抽象类abstract class Kite implements Flyable &#123; @Override public void fly() &#123; System.out.println(&quot;风筝在飞&quot;); &#125;&#125;// 实现多个接口class Bullet implements Flyable, Attackable &#123; @Override public void fly() &#123; System.out.println(&quot;子弹起飞&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;子弹停止&quot;); &#125; @Override public void attack() &#123; System.out.println(&quot;子弹具有攻击性&quot;); &#125;&#125; 面试题： 抽象类与接口有哪些异同？ 接口能继承接口； 抽象类能继承接口 (如不完全实现接口方法的类，还是抽象类)； 抽象类能继承非抽象类 (如抽象类的父类 Object)。 排错： 因为接口 A 和父类 B 是并列的，所以需要明确变量 x 的所属，如果 A 是 B 的父类，那么在 C 中就近原则，x 会认为是 B 的属性： 1234567891011121314151617181920212223interface A &#123; int x = 0; int x1 = 2;&#125;class B &#123; int x = 1; int x2 = 3;&#125;class C extends B implements A &#123; public void pX() &#123; System.out.println(x);// error: Reference to &#x27;x&#x27; is ambiguous, both &#x27;B.x&#x27; and &#x27;A.x&#x27; match // System.out.println(A.x);// 0 // System.out.println(super.x);// 1 System.out.println(x1);// 2 System.out.println(x2);// 3 &#125; public static void main(String[] args) &#123; new C().pX(); &#125;&#125; 接口中的所有成员变量都默认是 public static final 的，不能在实现类中被重写： 123456789101112131415161718192021222324252627282930interface Playable &#123; void play();&#125;interface Bounceable &#123; void play();&#125;interface Rollable extends Playable, Bounceable &#123; Ball BALL = new Ball(&quot;PingPang&quot;);&#125;class Ball implements Rollable &#123; private String name; public String getName() &#123; return name; &#125; public Ball(String name) &#123; this.name = name; &#125; // play()方法被认为是即重写了接口Playable，又重写了接口Bounceable @Override public void play() &#123; BALL = new Ball(&quot;Football&quot;);// error: Cannot assign a value to final variable &#x27;BALL&#x27; System.out.println(BALL.getName()); &#125;&#125; 接口匿名实现类的对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class InterfaceTest &#123; public static void main(String[] args) &#123; Computer computer = new Computer(); // 1.创建接口的非匿名实现类的非匿名对象 Flash flash = new Flash(); computer.transferData(flash); // 2.创建接口的非匿名实现类的匿名对象 computer.transferData(new Printer()); // 3.创建接口的匿名实现类的非匿名对象 USB phone = new USB() &#123; @Override public void start() &#123; System.out.println(&quot;手机开始工作&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;手机停止工作&quot;); &#125; &#125;; computer.transferData(phone); // 4.创建接口的匿名实现类的匿名对象 computer.transferData(new USB() &#123; @Override public void start() &#123; System.out.println(&quot;mp3开始工作&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;mp3停止工作&quot;); &#125; &#125;); &#125;&#125;class Computer &#123; public void transferData(USB usb) &#123; usb.start(); transferDetails(); usb.stop(); &#125; private void transferDetails() &#123; System.out.println(&quot;具体传输数据的细节&quot;); &#125;&#125;interface USB &#123; void start(); void stop();&#125;class Flash implements USB &#123; @Override public void start() &#123; System.out.println(&quot;U盘开启工作&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;U盘停止工作&quot;); &#125;&#125;class Printer implements USB &#123; @Override public void start() &#123; System.out.println(&quot;打印机开启工作&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;打印机停止工作&quot;); &#125;&#125; 类的成员之五：内部类当一个事物的内部，还有一个部分需要一个完整的结构进行描述，而这个内部的完整的结构又只为外部事物提供服务，那么整个内部的完整结构最好使用内部类。 在 java 中，允许一个类 A 声明在另一个类 B 的内部，则类 A 称为内部类，类 B 称为外部类。 Inner class一般用在定义它的类或语句块之内，在外部引用它时必须给出完整的名称。Inner class 的名字不能与包含它的外部类类名相同。 内部类的分类：成员内部类 (静态的、非静态的) vs 局部内部类 (代码块内、构造器内、方法内) 成员内部类： 一方面，作为外部类的成员： 调用外部类的结构，注意生命周期，如静态成员内部类不能调用外部类非静态的方法。 可以被 static 修饰，但此时就不能再使用外层类的非 static 的成员变量。注意，外部类不能被 static 修饰。 可以被 private、protected、缺省和 public 四种权限修饰符修饰。注意，外部类不能被 private 和 protected 修饰。 另一方面，作为一个类： 类内可以定义属性、方法、构造器、代码块、内部类等。 可以被 final 修饰，表示此类不能被继承，如果不使用 final，就可以被继承。 可以被 abstract 修饰，表示此类不能被实例化，可以被其它的内部类继承。 编译以后生成 OuterClass$InnerClass.class 字节码文件 (也适用于局部内部类)。 非 static 的成员内部类中的成员不能声明为 static 的，只有在外部类或 static 的成员内部类中才可声明 static 成员。 外部类访问成员内部类的成员，需要 “内部类.成员” 或 “内部类对象.成员” 的方式。 成员内部类可以直接使用外部类的所有成员，包括私有的数据。 当想要在外部类的静态成员部分使用内部类时，可以考虑内部类声明为静态的。 局部内部类： 局部内部类仍然是一个独立的类，在编译之后内部类会被编译成独立的 .class 文件，但是前面冠以外部类的类名和 $ 符号，以及数字编号。 只能在声明它的方法或代码块中使用，而且是先声明后使用，除此之外的任何地方都不能使用该类。 局部内部类的对象可以通过外部方法的返回值返回使用，返回值类型只能是局部内部类的父类或父接口类型。 局部内部类可以使用外部类的成员，包括私有的。 局部内部类可以使用外部方法的局部变量，但是必须是 final 的，final 可以省略 (jdk 8 及之后)，但这个局部变量赋值后不能有再次修改操作，否则编译不通过。这是因为局部内部类和局部变量的声明周期不同所致。 局部内部类和局部变量地位类似，不能使用 public，缺省，protected 和 private 修饰。 局部内部类不能使用static修饰，因此也不能包含静态成员。 关注如下的 3 个问题： 如何实例化成员内部类的对象？ 静态成员内部类：外部类.静态内部类 变量名 = new 外部类.静态内部类();。 非静态成员内部类：外部类.非静态内部类 变量名 = new 外部类().new 非静态内部类();。 如何在成员内部类中区分调用外部类的结构？ 静态成员内部类，参考： 12345public void show(int age) &#123; System.out.println(&quot;形参：&quot; + age); System.out.println(&quot;静态成员内部类的静态属性：&quot; + Brain.age); System.out.println(&quot;外部类的静态属性：&quot; + Person.age);&#125; 非静态成员内部类，参考： 12345public void show(String name) &#123; System.out.println(&quot;形参：&quot; + name); System.out.println(&quot;非静态成员内部类的非静态属性：&quot; + this.name);// 非静态成员内部类，不能定义static的变量 System.out.println(&quot;外部类的非静态属性：&quot; + Person.this.name);&#125; 开发者局部内部类的使用？ 12345678910111213141516171819202122232425262728293031public class InnerClassTest1 &#123; // 这种局部内部类，开发中很少见 public void method() &#123; class AA &#123; &#125; &#125; // 返回一个实现类Comparable接口的类的对象 public Comparable getComparable() &#123; // 创建一个实现了Comparable接口的类：局部内部类 // 方式一：创建Comparable接口的非匿名实现类的匿名对象 /*class MyComparable implements Comparable &#123; @Override public int compareTo(Object o) &#123; return 0; &#125; &#125; return new MyComparable();*/ // 方式二：创建Comparable接口的匿名实现类的匿名对象 return new Comparable() &#123; @Override public int compareTo(Object o) &#123; return 0; &#125; &#125;; &#125;&#125; 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class InnerClassTest &#123; public static void main(String[] args) &#123; // 1.创建Brain实例---静态的成员内部类 Person.Brain brain = new Person.Brain(); brain.think(); brain.show(8); // 2.创建Hand实例---非静态的成员内部类 Person.Hand hand = new Person().new Hand(); hand.grasp(); hand.show(&quot;外来手&quot;); &#125;&#125;class Person &#123; String name = &quot;小明&quot;; static int age = 8; // 静态成员内部类 static class Brain &#123; static int age = 8; public Brain() &#123; &#125; public void think() &#123; System.out.println(&quot;大脑想东西&quot;); &#125; public void show(int age) &#123; System.out.println(&quot;形参：&quot; + age); System.out.println(&quot;静态成员内部类的静态属性：&quot; + Brain.age); System.out.println(&quot;外部类的静态属性：&quot; + Person.age); &#125; &#125; // 非静态成员内部类 class Hand &#123; String name = &quot;内部手&quot;; public Hand() &#123; &#125; public void grasp() &#123; System.out.println(&quot;手抓东西&quot;); // 调用Person外部类的方法 Person.this.eat();// 等价于eat()，注意方法的生命周期 &#125; public void show(String name) &#123; System.out.println(&quot;形参：&quot; + name); System.out.println(&quot;非静态成员内部类的非静态属性：&quot; + this.name); System.out.println(&quot;外部类的非静态属性：&quot; + Person.this.name); &#125; &#125; static &#123; // 静态代码块内局部内部类 class AA &#123; &#125; &#125; &#123; // 非静态代码块内局部内部类 class BB &#123; &#125; &#125; public Person() &#123; // 构造器内局部内部类 class CC &#123; &#125; &#125; public static void method1() &#123; // 静态方法内局部内部类 class DD &#123; &#125; &#125; public void method() &#123; // 非静态方法内局部内部类 class EE &#123; &#125; &#125; public void eat() &#123; &#125;&#125; 匿名内部类： 匿名内部类不能定义任何静态成员、方法和类，只能创建匿名内部类的一个实例。一个匿名内部类一定是在 new 的后面，用其隐含实现一个接口或实现一个类。 格式： 特点： 匿名内部类必须继承父类或实现接口。 匿名内部类只能有一个对象。 匿名内部类对象只能使用多态形式引用 实例： 12345678910111213141516171819202122232425262728interface Product &#123; public double getPrice(); public String getName();&#125;public class AnonymousTest &#123; public void test(Product p) &#123; System.out.println(&quot;购买了一个&quot; + p.getName() + &quot;，花掉了&quot; + p.getPrice()); &#125; public static void main(String[] args) &#123; AnonymousTest ta = new AnonymousTest(); // 调用test方法时，需要传入一个Product参数， // 此处传入其匿名实现类的实例 ta.test(new Product() &#123; @Override public double getPrice() &#123; return 567.8; &#125; @Override public String getName() &#123; return &quot;AGP显卡&quot;; &#125; &#125;); &#125;&#125; 面试题： 1234567891011121314151617181920212223public class Test &#123; public Test() &#123; Inner s1 = new Inner(); s1.a = 10; Inner s2 = new Inner(); s2.a = 20; Test.Inner s3 = new Test.Inner(); System.out.println(s3.a); &#125; class Inner &#123; public int a = 5; &#125; public static void main(String[] args) &#123; Test t = new Test(); Inner r = t.new Inner(); System.out.println(r.a); &#125;&#125;输出结果：55 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"maven 的配置文件","slug":"maven-configfiles","date":"2021-01-23T02:30:26.000Z","updated":"2021-01-27T02:31:46.290Z","comments":true,"path":"2021/01/23/maven-configfiles/","link":"","permalink":"http://example.com/2021/01/23/maven-configfiles/","excerpt":"","text":"settings.xmlsettings.xml是Maven的全局配置文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Licensed to the Apache Software Foundation (ASF) under oneor more contributor license agreements. See the NOTICE filedistributed with this work for additional informationregarding copyright ownership. The ASF licenses this fileto you under the Apache License, Version 2.0 (the&quot;License&quot;); you may not use this file except in compliancewith the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing,software distributed under the License is distributed on an&quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANYKIND, either express or implied. See the License for thespecific language governing permissions and limitationsunder the License.--&gt;&lt;!-- | 官方文档：https://maven.apache.org/settings.html | | maven提供以下两种level的配置: | | 1. User Level. 当前用户独享的配置，通常在$&#123;user.home&#125;/.m2/settings.xml目录下。 | 可在CLI命令行中通过以下参数设置：-s /path/to/user/settings.xml | | 2. Global Level. 同一台计算机上的所有maven用户共享的全局配置。通常在$&#123;maven.home&#125;/conf/settings.xml目录下。 | 可在CLI命令行中通过以下参数设置：-gs /path/to/global/settings.xml | | 备注： | 优先级：User Level &gt; Global Level | 默认情况，$&#123;user.home&#125;/.m2目录下没有settings.xml文件，需手动复制$&#123;maven.home&#125;/conf/settings.xml。 |--&gt;&lt;!-- | This is the configuration file for Maven. It can be specified at two levels: | | 1. User Level. This settings.xml file provides configuration for a single user, | and is normally provided in $&#123;user.home&#125;/.m2/settings.xml. | | NOTE: This location can be overridden with the CLI option: | | -s /path/to/user/settings.xml | | 2. Global Level. This settings.xml file provides configuration for all Maven | users on a machine (assuming they&#x27;re all using the same Maven | installation). It&#x27;s normally provided in | $&#123;maven.conf&#125;/settings.xml. | | NOTE: This location can be overridden with the CLI option: | | -gs /path/to/global/settings.xml | | The sections in this sample file are intended to give you a running start at | getting the most out of your Maven installation. Where appropriate, the default | values (values used when the setting is not specified) are provided. | |--&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;!-- 本地仓库路径，默认值：$&#123;user.home&#125;/.m2/repository --&gt; &lt;!-- localRepositor y | The path to the local repository maven will use to store artifacts. | | Default: $&#123;user.home&#125;/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;localRepository&gt;D:\\Program Files\\Maven\\apache-maven-3.6.3-maven-repository&lt;/localRepository&gt; &lt;!-- 当maven需要输入值的时候，是否交由用户输入，默认为true；false情况下maven将根据使用配置信息进行填充。 --&gt; &lt;!-- interactiveMode | This will determine whether maven prompts you when it needs input. If set to false, | maven will use a sensible default value, perhaps based on some other setting, for | the parameter in question. | | Default: true &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; --&gt; &lt;!-- 是否支持联网进行artifact下载、部署等操作，默认false。 --&gt; &lt;!-- offline | Determines whether maven should attempt to connect to the network when executing a build. | This will have an effect on artifact downloads, artifact deployment, and others. | | Default: false &lt;offline&gt;false&lt;/offline&gt; --&gt; &lt;!-- | 搜索插件时，如果groupId没有显式提供时，则以此处配置的groupId为默认值， | 可以简单理解为默认导入这些groupId下的所有artifact(需要时才下载)。 | 默认情况下该列表包含了：org.apache.maven.plugins和org.codehaus.mojo。 | | 查看插件信息： | mvn help:describe -Dplugin=org.apache.maven.plugins:maven-compiler-plugin:3.5.1 -Ddetail |--&gt; &lt;!-- pluginGroups | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e. | when invoking a command line like &quot;mvn prefix:goal&quot;. Maven will automatically add the group identifiers | &quot;org.apache.maven.plugins&quot; and &quot;org.codehaus.mojo&quot; if these are not already contained in the list. |--&gt; &lt;pluginGroups&gt; &lt;!-- pluginGroup | Specifies a further group identifier to use for plugin lookup. | plugin 的 groupId &lt;pluginGroup&gt;com.your.plugins&lt;/pluginGroup&gt; --&gt; &lt;/pluginGroups&gt; &lt;!-- 用来配置不同的代理，多代理profiles可以应对笔记本或移动设备的工作环境：通过简单的设置profile id就可以很容易的更换整个代理配置。 --&gt; &lt;!-- proxies | This is a list of proxies which can be used on this machine to connect to the network. | Unless otherwise specified (by system property or command-line switch), the first proxy | specification in this list marked as active will be used. |--&gt; &lt;proxies&gt; &lt;!-- proxy | Specification for one proxy, to be used in connecting to the network. | | 代理元素包含配置代理时需要的信息 &lt;proxy&gt; | 代理的唯一定义符，用来区分不同的代理元素 &lt;id&gt;optional&lt;/id&gt; | 该代理是否是激活的那个。true则激活代理。当我们声明了一组代理，而某个时候只需要激活一个代理的时候，该元素就可以派上用处。 &lt;active&gt;true&lt;/active&gt; | 代理的协议 &lt;protocol&gt;http&lt;/protocol&gt; | 代理服务器认证的登录名 &lt;username&gt;proxyuser&lt;/username&gt; | 代理服务器认证登录密码 &lt;password&gt;proxypass&lt;/password&gt; | 代理的主机名 &lt;host&gt;proxy.host.net&lt;/host&gt; | 代理的端口 &lt;port&gt;80&lt;/port&gt; | 不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，使用逗号分隔也很常见。 &lt;nonProxyHosts&gt;local.net|some.host.com&lt;/nonProxyHosts&gt; &lt;/proxy&gt; --&gt; &lt;/proxies&gt; &lt;!-- 进行远程服务器访问时所需的授权配置信息。通过系统唯一的server-id进行唯一关联。 --&gt; &lt;!-- servers | This is a list of authentication profiles, keyed by the server-id used within the system. | Authentication profiles can be used whenever maven must make a connection to a remote server. |--&gt; &lt;servers&gt; &lt;!-- server | Specifies the authentication information to use when connecting to a particular server, identified by | a unique name within the system (referred to by the &#x27;id&#x27; attribute below). | | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are | used together. | | 方式一：使用用户名和密码 &lt;server&gt; | 当前server的id，该id与distributionManagement中repository元素的id相匹配。 &lt;id&gt;deploymentRepo&lt;/id&gt; | 鉴权用户名 &lt;username&gt;repouser&lt;/username&gt; | 鉴权密码 &lt;password&gt;repopwd&lt;/password&gt; &lt;/server&gt; --&gt; &lt;!-- Another sample, using keys to authenticate. | 方式二：使用私钥 &lt;server&gt; &lt;id&gt;siteServer&lt;/id&gt; | 鉴权时使用的私钥位置，默认是/home/hudson/.ssh/id_dsa。 &lt;privateKey&gt;/path/to/private/key&lt;/privateKey&gt; | 鉴权时使用的私钥密码，非必要，非必要时留空。 &lt;passphrase&gt;optional; leave empty if not used.&lt;/passphrase&gt; &lt;/server&gt; --&gt; &lt;!-- 实例：对应pom.xml文件中配置的id为ChemAxon Public Repository的仓库。 --&gt; &lt;server&gt; &lt;id&gt;ChemAxon Public Repository&lt;/id&gt; &lt;username&gt;huxiongfeng95@gmail.com&lt;/username&gt; &lt;password&gt;AKCp5dL3HsJftZjXR4wLS7UMnJvQL7oarx8sad8Wh21UV7xQUMmNcZ7TMEHaBVoSrM8jAv48Q&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;!-- | 从远程仓库下载artifacts时，用于替代指定远程仓库的镜像服务器配置； | 例如当无法连接上国外的仓库时，可以指定连接到国内的镜像服务器； | 私服的配置推荐用profile配置而不是mirror。 |--&gt; &lt;!-- mirrors | This is a list of mirrors to be used in downloading artifacts from remote repositories. | | It works like this: a POM may declare a repository to use in resolving certain artifacts. | However, this repository may have problems with heavy traffic at times, so people have mirrored | it to several places. | | That repository definition will have a unique id, so we can create a mirror reference for that | repository, to be used as an alternate download site. The mirror site will be the preferred | server for that repository. |--&gt; &lt;mirrors&gt; &lt;!-- | mirrors匹配顺序： | 多个mirror优先级：按照id字母顺序进行排列，即与编写的顺序无关。 | 在第一个mirror找不到artifact，不会继续查找下一个镜像。 | 只有当前一个mirror无法链接的时候，才会尝试链接下一个镜像，类似容灾备份。 |--&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;!-- maven中央仓库的aliyun镜像，maven中央仓库的id为central。 --&gt; &lt;mirror&gt; &lt;!-- 当前镜像的唯一标识符，id用来区分不同的mirror元素，同时会套用使用server中id相同授权配置链接到镜像。 --&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;!-- 镜像名称，无特殊作用，可视为简述。 --&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;!-- 镜像地址 --&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;!-- 被镜像的服务器的id，必须与repository节点设置的id一致。但是&quot;This must not match the mirror id&quot;。 | mirrorOf 的配置语法: | * = 匹配所有远程仓库。这样所有pom中定义的仓库都不生效。 | external:* = 匹配除localhost、使用file://协议外的所有远程仓库。 | repo1,repo2 = 匹配仓库repo1和repo2。 | *,!repo1 = 匹配所有远程仓库，repo1除外。 |--&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;!-- profiles | This is a list of profiles which can be activated in a variety of ways, and which can modify | the build process. Profiles provided in the settings.xml are intended to provide local machine- | specific paths and repository locations which allow the build to work in the local environment. | | For example, if you have an integration testing plugin - like cactus - that needs to know where | your Tomcat instance is installed, you can provide a variable here such that the variable is | dereferenced during the build process to configure the cactus plugin. | | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles | section of this document (settings.xml) - will be discussed later. Another way essentially | relies on the detection of a system property, either matching a particular value for the property, | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a | value of &#x27;1.4&#x27; might activate a profile when the build is executed on a JDK version of &#x27;1.4.2_07&#x27;. | Finally, the list of active profiles can be specified directly from the command line. | | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact | repositories, plugin repositories, and free-form properties to be used as configuration | variables for plugins in the POM. | |--&gt; &lt;profiles&gt; &lt;!-- profile | Specifies a set of introductions to the build process, to be activated using one or more of the | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles/&gt; | or the command line, profiles have to have an ID that is unique. | | An encouraged best practice for profile identification is to use a consistent naming convention | for profiles, such as &#x27;env-dev&#x27;, &#x27;env-test&#x27;, &#x27;env-production&#x27;, &#x27;user-jdcasey&#x27;, &#x27;user-brett&#x27;, etc. | This will make it more intuitive to understand what the set of introduced profiles is attempting | to accomplish, particularly when you only have a list of profile id&#x27;s for debug. | | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo. &lt;profile&gt; &lt;id&gt;jdk-1.4&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.4&lt;/jdk&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jdk14&lt;/id&gt; &lt;name&gt;Repository for JDK 1.4 builds&lt;/name&gt; &lt;url&gt;http://www.myhost.com/maven/jdk14&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshotPolicy&gt;always&lt;/snapshotPolicy&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; --&gt; &lt;!-- | Here is another profile, activated by the system property &#x27;target-env&#x27; with a value of &#x27;dev&#x27;, | which provides a specific path to the Tomcat instance. To use this, your plugin configuration | might hypothetically look like: | | ... | &lt;plugin&gt; | &lt;groupId&gt;org.myco.myplugins&lt;/groupId&gt; | &lt;artifactId&gt;myplugin&lt;/artifactId&gt; | | &lt;configuration&gt; | &lt;tomcatLocation&gt;$&#123;tomcatPath&#125;&lt;/tomcatLocation&gt; | &lt;/configuration&gt; | &lt;/plugin&gt; | ... | | NOTE: If you just wanted to inject this configuration whenever someone set &#x27;target-env&#x27; to | anything, you could just leave off the &lt;value/&gt; inside the activation-property. | &lt;profile&gt; &lt;id&gt;env-dev&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;target-env&lt;/name&gt; &lt;value&gt;dev&lt;/value&gt; &lt;/property&gt; &lt;/activation&gt; &lt;properties&gt; &lt;tomcatPath&gt;/path/to/tomcat/instance&lt;/tomcatPath&gt; &lt;/properties&gt; &lt;/profile&gt; --&gt; &lt;/profiles&gt; &lt;!-- | 手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 | 任何activeProfile，不论环境设置如何，其对应的profile都会被激活，maven会忽略无效(找不到)的profile。 |--&gt; &lt;!-- activeProfiles | List of profiles that are active for all builds. | &lt;activeProfiles&gt; &lt;activeProfile&gt;alwaysActiveProfile&lt;/activeProfile&gt; &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;/activeProfile&gt; &lt;/activeProfiles&gt; --&gt;&lt;/settings&gt; 关于 profiles 节点的详解： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184&lt;!-- | 构建方法的配置清单，maven将根据不同环境参数来使用这些构建配置。 | settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。 | settings.xml负责的是整体的构建过程，pom.xml负责单独的项目对象构建过程。 | settings.xml只包含了id，activation，repositories，pluginRepositories和properties元素。 | | 如果settings.xml中的profile被激活，它的值会覆盖任何其它定义在pom.xml中或profile.xml中的相同id的profile。 | | 查看当前激活的profile: | mvn help:active-profiles |--&gt;&lt;profiles&gt; &lt;profile&gt; &lt;!-- 该配置的唯一标识符 --&gt; &lt;id&gt;profile_id&lt;/id&gt; &lt;!-- | profile的激活条件配置。 | 除此之外的其他激活方式： | 1. 通过settings.xml文件中的activeProfile元素进行指定激活。 | 2. 在命令行，使用-P标记和逗号分隔的列表来显式的激活，如：mvn clean package -P myProfile |--&gt; &lt;activation&gt; &lt;!-- 是否默认激活 --&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!-- 内建的java版本检测，匹配规则：https://maven.apache.org/enforcer/enforcer-rules/versionRanges.html --&gt; &lt;jdk&gt;9.9&lt;/jdk&gt; &lt;!-- 内建操作系统属性检测， 配置规则：https://maven.apache.org/enforcer/enforcer-rules/requireOS.html --&gt; &lt;os&gt; &lt;!-- 操作系统 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!-- 操作系统家族 --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!-- 操作系统 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!-- 操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!-- | 如果maven检测到某一个属性(其值可以在POM中通过$&#123;名称&#125;引用)，并且其拥有对应的名称和值，Profile就会被激活。 | 如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段。 |--&gt; &lt;property&gt; &lt;!-- 属性名 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!-- 属性值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!-- 根据文件存在/不存在激活profile --&gt; &lt;file&gt; &lt;!-- 如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/path/to/active_on_exists&lt;/exists&gt; &lt;!-- 如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/path/to/active_on_missing&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!-- 扩展属性设置。扩展属性可以在POM中的任何地方通过$&#123;扩展属性名&#125;进行引用。 | | 属性引用方式(包括扩展属性，共5种属性可以引用)： | | env.x：引用shell环境变量，例如，&quot;env.PATH&quot;指代了$path环境变量(在Linux/Windows上是%PATH%)。 | project.x：引用pom.xml(根元素是project)中xml元素内容。例如$&#123;project.artifactId&#125;可以获取pom.xml中设置的&lt;artifactId /&gt;元素的内容。 | settings.x：引用setting.xml(根元素是setting)中xml元素内容，例如$&#123;settings.offline&#125;。 | Java System Properties：所有可通过java.lang.System.getProperties()访问的属性都能在通过$&#123;property_name&#125;访问，例如$&#123;java.home&#125;。 | x：在&lt;properties/&gt;或者外部文件中设置的属性，都可以$&#123;someVar&#125;的形式使用。 | |--&gt; &lt;properties&gt; &lt;!-- 在当前profile被激活时，$&#123;profile.property&#125;就可以被访问到了。 --&gt; &lt;profile.property&gt;this.property.is.accessible.when.current.profile.actived&lt;/profile.property&gt; &lt;/properties&gt; &lt;!-- 远程仓库列表，settings.xml中的repositories不被直接支持，需要在profiles中配置。 --&gt; &lt;repositories&gt; &lt;!-- | releases vs snapshots | maven针对releases和snapshots有不同的处理策略，POM可以在每个单独的仓库中，为每种类型的artifact采取不同的策略。 | 例如： | 开发环境使用snapshots模式实时获取最新的快照版本进行构建 | 生成环境使用releases模式获取稳定版本进行构建 | 参见repositories/repository/releases元素。 |--&gt; &lt;!-- | 依赖包不更新问题： | 1. maven在下载依赖失败后会生成一个.lastUpdated为后缀的文件。如果这个文件存在，那么即使换一个有资源的仓库后， | maven依然不会去下载新资源。可以通过-U参数进行强制更新、手动删除.lastUpdated 文件： | find . -type f -name &quot;*.lastUpdated&quot; -exec echo &#123;&#125;&quot; found and deleted&quot; \\; -exec rm -f &#123;&#125; \\; | | 2. updatePolicy设置更新频率不对，导致没有触发maven检查本地artifact与远程artifact是否一致。 |--&gt; &lt;repository&gt; &lt;!-- 远程仓库唯一标识 --&gt; &lt;id&gt;maven_repository_id&lt;/id&gt; &lt;!-- 远程仓库名称 --&gt; &lt;name&gt;maven_repository_name&lt;/name&gt; &lt;!-- 远程仓库URL，按protocol://hostname/path形式。 --&gt; &lt;url&gt;http://host/maven&lt;/url&gt; &lt;!-- | 用于定位和排序artifact的仓库布局类型-可以是default(默认)或者legacy(遗留)。 | Maven2为其仓库提供了一个默认的布局；然而，Maven1.x有一种不同的布局。 | 我们可以使用该元素指定布局是default(默认)还是legacy(遗留)。 | --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;!-- 如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!-- 是否允许该仓库为artifact提供releases下载功能 --&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;!-- | 每次执行构建命令时，Maven会比较本地POM和远程POM的时间戳，该元素指定比较的频率。 | 有效选项是： | always ：每次构建都检查 | daily ：默认，距上次构建检查时间超过一天 | interval: x ：距上次构建检查超过x分钟 | never从不 ：从不 | | 重要： | 设置为daily时，如果artifact一天更新了几次，在一天之内进行构建，也不会从仓库中重新获取最新版本。 |--&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!-- 当maven验证artifact校验文件失败时该怎么做：ignore(忽略)，fail(失败)，或者warn(警告)。 --&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载 --&gt; &lt;snapshots&gt; &lt;!-- 是否允许该仓库为artifact提供snapshots下载功能 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;!-- | 国内可用的maven仓库地址(updated @ 2019-02-08)： | http://maven.aliyun.com/nexus/content/groups/public | http://maven.wso2.org/nexus/content/groups/public/ | http://jcenter.bintray.com/ | http://maven.springframework.org/release/ | http://repository.jboss.com/maven2/ | http://uk.maven.org/maven2/ | http://repo1.maven.org/maven2/ | http://maven.springframework.org/milestone | http://maven.jeecg.org/nexus/content/repositories/ | http://repo.maven.apache.org/maven2 | http://repo.spring.io/release/ | http://repo.spring.io/snapshot/ | http://mavensync.zkoss.org/maven2/ | https://repository.apache.org/content/groups/public/ | https://repository.jboss.org/nexus/content/repositories/releases/ |--&gt; &lt;/repositories&gt; &lt;!-- | maven插件的远程仓库配置。maven插件实际上是一种特殊类型的artifact。 | 插件仓库独立于artifact仓库。pluginRepositories元素的结构和repositories元素的结构类似。 |--&gt; &lt;!-- &lt;pluginRepositories&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; --&gt; &lt;/profile&gt;&lt;/profiles&gt; pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;!-- 父项目的坐标。如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值。 坐标包括groupID，artifactID和version。 --&gt; &lt;parent&gt; &lt;!-- 被继承的父项目的构件标识符 --&gt; &lt;artifactId /&gt; &lt;!-- 被继承的父项目的全球唯一标识符 --&gt; &lt;groupId /&gt; &lt;!-- 被继承的父项目的版本 --&gt; &lt;version /&gt; &lt;!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是：../pom.xml。 Maven首先在构建当前项目的地方寻找父项目的pom，其次在文件系统的这个位置(relativePath位置)， 然后在本地仓库，最后在远程仓库寻找父项目的pom。 --&gt; &lt;relativePath /&gt; &lt;/parent&gt; &lt;!-- 声明项目描述符遵循哪一个POM模型版本。模型本身的版本很少改变，虽然如此，但它仍然是必不可少的， 这是为了当Maven引入了新的特性或者其他模型变更的时候，确保稳定性。 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 项目的全球唯一标识符，通常使用全限定的包名区分该项目和其他项目。 并且构建时生成的路径也是由此生成，如com.mycompany.app生成的相对路径为：/com/mycompany/app --&gt; &lt;groupId&gt;asia.banseon&lt;/groupId&gt; &lt;!-- 构件的标识符，它和groupID一起唯一标识一个构件。换句话说，你不能有两个不同的项目拥有同样的artifactID和groupID； 在某个特定的groupID下，artifactID也必须是唯一的。 构件是项目产生的或使用的一个东西，Maven为项目产生的构件包括：JARs，源码，二进制发布和WARs等。 --&gt; &lt;artifactId&gt;banseon-maven2&lt;/artifactId&gt; &lt;!-- 项目产生的构件类型，例如jar、war、ear、pom。插件可以创建他们自己的构件类型，所以前面列的不是全部构件类型 --&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;!-- 项目当前版本，格式为：主版本.次版本.增量版本-限定版本号 --&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!-- 项目的名称，Maven产生的文档用 --&gt; &lt;name&gt;banseon-maven&lt;/name&gt; &lt;!-- 项目主页的URL，Maven产生的文档用 --&gt; &lt;url&gt;http://www.baidu.com/banseon&lt;/url&gt; &lt;!-- 项目的详细描述，Maven产生的文档用。当这个元素能够用HTML格式描述时(例如，CDATA中的文本会被解析器忽略， 就可以包含HTML标签)，不鼓励使用纯文本描述。如果你需要修改产生的web站点的索引页面， 你应该修改你自己的索引页文件，而不是调整这里的文档。 --&gt; &lt;description&gt;A maven project to study maven.&lt;/description&gt; &lt;!-- 描述了这个项目构建环境中的前提条件。 --&gt; &lt;prerequisites&gt; &lt;!-- 构建该项目或使用该插件所需要的Maven的最低版本 --&gt; &lt;maven /&gt; &lt;/prerequisites&gt; &lt;!-- 项目的问题管理系统(Bugzilla，Jira，Scarab，或任何你喜欢的问题管理系统)的名称和URL，本例为jira --&gt; &lt;issueManagement&gt; &lt;!-- 问题管理系统(例如jira)的名字 --&gt; &lt;system&gt;jira&lt;/system&gt; &lt;!-- 该项目使用的问题管理系统的URL --&gt; &lt;url&gt;http://jira.xxxx.com/xxxx&lt;/url&gt; &lt;/issueManagement&gt; &lt;!-- 项目持续集成信息 --&gt; &lt;ciManagement&gt; &lt;!-- 持续集成系统的名字，例如continuum --&gt; &lt;system /&gt; &lt;!-- 该项目使用的持续集成系统的URL(如果持续集成系统有web接口的话) --&gt; &lt;url /&gt; &lt;!-- 构建完成时，需要通知的开发者/用户的配置项。包括被通知者信息和通知条件(错误，失败，成功，警告) --&gt; &lt;notifiers&gt; &lt;!-- 配置一种方式，当构建中断时，以该方式通知用户/开发者 --&gt; &lt;notifier&gt; &lt;!-- 传送通知的途径 --&gt; &lt;type /&gt; &lt;!-- 发生错误时是否通知 --&gt; &lt;sendOnError /&gt; &lt;!-- 构建失败时是否通知 --&gt; &lt;sendOnFailure /&gt; &lt;!-- 构建成功时是否通知 --&gt; &lt;sendOnSuccess /&gt; &lt;!-- 发生警告时是否通知 --&gt; &lt;sendOnWarning /&gt; &lt;!-- 不赞成使用。通知发送到哪里 --&gt; &lt;address /&gt; &lt;!-- 扩展配置项 --&gt; &lt;configuration /&gt; &lt;/notifier&gt; &lt;/notifiers&gt; &lt;/ciManagement&gt; &lt;!-- 项目创建年份，4位数字。当产生版权信息时需要使用这个值。 --&gt; &lt;inceptionYear /&gt; &lt;!-- 项目相关邮件列表信息 --&gt; &lt;mailingLists&gt; &lt;!-- 该元素描述了项目相关的所有邮件列表。自动产生的网站引用这些信息。 --&gt; &lt;mailingList&gt; &lt;!-- 邮件的名称 --&gt; &lt;name&gt;Demo&lt;/name&gt; &lt;!-- 发送邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;post&gt;Demo@126.com&lt;/post&gt; &lt;!-- 订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;subscribe&gt;Demo@126.com&lt;/subscribe&gt; &lt;!-- 取消订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;unsubscribe&gt;Demo@126.com&lt;/unsubscribe&gt; &lt;!-- 你可以浏览邮件信息的URL --&gt; &lt;archive&gt;http://localhost:8080/demo/dev/&lt;/archive&gt; &lt;/mailingList&gt; &lt;/mailingLists&gt; &lt;!-- 项目开发者列表 --&gt; &lt;developers&gt; &lt;!-- 某个项目开发者的信息 --&gt; &lt;developer&gt; &lt;!-- SCM里项目开发者的唯一标识符 --&gt; &lt;id&gt;HELLO WORLD&lt;/id&gt; &lt;!-- 项目开发者的全名 --&gt; &lt;name&gt;youname&lt;/name&gt; &lt;!-- 项目开发者的email --&gt; &lt;email&gt;youname@qq.com&lt;/email&gt; &lt;!-- 项目开发者的主页的URL --&gt; &lt;url /&gt; &lt;!-- 项目开发者在项目中扮演的角色，角色元素描述了各种角色 --&gt; &lt;roles&gt; &lt;role&gt;Project Manager&lt;/role&gt; &lt;role&gt;Architect&lt;/role&gt; &lt;/roles&gt; &lt;!-- 项目开发者所属组织 --&gt; &lt;organization&gt;demo&lt;/organization&gt; &lt;!-- 项目开发者所属组织的URL --&gt; &lt;organizationUrl&gt;http://www.xxx.com/&lt;/organizationUrl&gt; &lt;!-- 项目开发者属性，如即时消息如何处理等 --&gt; &lt;properties&gt; &lt;dept&gt;No&lt;/dept&gt; &lt;/properties&gt; &lt;!-- 项目开发者所在时区， -11到12范围内的整数。 --&gt; &lt;timezone&gt;+8&lt;/timezone&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;!-- 项目的其他贡献者列表 --&gt; &lt;contributors&gt; &lt;!-- 项目的其他贡献者。参见developers/developer元素 --&gt; &lt;contributor&gt; &lt;name /&gt; &lt;email /&gt; &lt;url /&gt; &lt;organization /&gt; &lt;organizationUrl /&gt; &lt;roles /&gt; &lt;timezone /&gt; &lt;properties /&gt; &lt;/contributor&gt; &lt;/contributors&gt; &lt;!-- 该元素描述了项目所有license列表。应该只列出该项目的license列表，不要列出依赖项目的license列表。 如果列出多个license，用户可以选择它们中的一个而不是接受所有license。 --&gt; &lt;licenses&gt; &lt;!-- 描述了项目的license，用于生成项目的web站点的license页面，其他一些报表和validation也会用到该元素。 --&gt; &lt;license&gt; &lt;!-- license用于法律上的名称 --&gt; &lt;name&gt;Apache 2&lt;/name&gt; &lt;!-- 官方的license正文页面的URL --&gt; &lt;url&gt;http://www.xxxx.com/LICENSE-2.0.txt&lt;/url&gt; &lt;!-- 项目分发的主要方式：repo，可以从Maven库下载manual，用户必须手动下载和安装依赖 --&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;!-- 关于license的补充信息 --&gt; &lt;comments&gt;A business-friendly OSS license&lt;/comments&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;!-- SCM(Source Control Management)标签允许你配置你的代码库，供Maven web站点和其它插件使用。 --&gt; &lt;scm&gt; &lt;!-- SCM的URL，该URL描述了版本库和如何连接到版本库。欲知详情，请看SCMs提供的URL格式和列表。该连接只读。 --&gt; &lt;connection&gt; scm:svn:http://svn.xxxx.com/maven/xxxxx-maven2-trunk(dao-trunk) &lt;/connection&gt; &lt;!-- 给开发者使用的，类似connection元素。即该连接不仅仅只读。 --&gt; &lt;developerConnection&gt; scm:svn:http://svn.xxxx.com/maven/dao-trunk &lt;/developerConnection&gt; &lt;!-- 当前代码的标签，在开发阶段默认为HEAD --&gt; &lt;tag /&gt; &lt;!-- 指向项目的可浏览SCM库(例如ViewVC或者Fisheye)的URL。 --&gt; &lt;url&gt;http://svn.xxxxx.com/&lt;/url&gt; &lt;/scm&gt; &lt;!-- 描述项目所属组织的各种属性。Maven产生的文档用。 --&gt; &lt;organization&gt; &lt;!-- 组织的全名 --&gt; &lt;name&gt;demo&lt;/name&gt; &lt;!-- 组织主页的URL --&gt; &lt;url&gt;http://www.xxxxxx.com/&lt;/url&gt; &lt;/organization&gt; &lt;!-- 构建项目需要的信息 --&gt; &lt;build&gt; &lt;!-- 该元素设置了项目源码目录 当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;sourceDirectory /&gt; &lt;!-- 该元素设置了项目脚本源码目录 该目录和源码目录不同：绝大多数情况下，该目录下的内容会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。 --&gt; &lt;scriptSourceDirectory /&gt; &lt;!-- 该元素设置了项目单元测试使用的源码目录 当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;testSourceDirectory /&gt; &lt;!-- 被编译过的应用程序class文件存放的目录。 --&gt; &lt;outputDirectory /&gt; &lt;!-- 被编译过的测试class文件存放的目录。 --&gt; &lt;testOutputDirectory /&gt; &lt;!-- 使用来自该项目的一系列构建扩展 --&gt; &lt;extensions&gt; &lt;!-- 描述使用到的构建扩展。 --&gt; &lt;extension&gt; &lt;!-- 构建扩展的groupId --&gt; &lt;groupId /&gt; &lt;!-- 构建扩展的artifactId --&gt; &lt;artifactId /&gt; &lt;!-- 构建扩展的版本 --&gt; &lt;version /&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;!-- 当项目没有规定目标(Maven2叫做阶段)时的默认值 --&gt; &lt;defaultGoal /&gt; &lt;!-- 这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。 --&gt; &lt;resources&gt; &lt;!-- 这个元素描述了项目相关或测试相关的所有资源路径 --&gt; &lt;resource&gt; &lt;!-- 描述了资源的目标路径。该路径相对target/classes目录(例如$&#123;project.build.outputDirectory&#125;)。 举个例子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为: org/apache/maven/messages。然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。 --&gt; &lt;targetPath /&gt; &lt;!-- 是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。 --&gt; &lt;filtering /&gt; &lt;!-- 描述存放资源的目录，该路径相对POM路径 --&gt; &lt;directory /&gt; &lt;!-- 包含的模式列表，例如：**/*.xml --&gt; &lt;includes /&gt; &lt;!-- 排除的模式列表，例如：**/*.xml --&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;!-- 这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。 --&gt; &lt;testResources&gt; &lt;!-- 这个元素描述了测试相关的所有资源路径，参见build/resources/resource元素的说明 --&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;!-- 构建产生的所有文件存放的目录 --&gt; &lt;directory /&gt; &lt;!-- 产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。 --&gt; &lt;finalName /&gt; &lt;!-- 当filtering开关打开时，使用到的过滤器属性文件列表。 --&gt; &lt;filters /&gt; &lt;!-- 子项目可以引用的默认插件信息。该插件配置项直到被引用时才会被解析或绑定到生命周期。 给定插件的任何本地配置都会覆盖这里的配置。 --&gt; &lt;pluginManagement&gt; &lt;!-- 使用的插件列表 --&gt; &lt;plugins&gt; &lt;!-- plugin元素包含描述插件所需要的信息。 --&gt; &lt;plugin&gt; &lt;!-- 插件在仓库里的groupID --&gt; &lt;groupId /&gt; &lt;!-- 插件在仓库里的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 被使用的插件的版本(或版本范围) --&gt; &lt;version /&gt; &lt;!-- 是否从该插件下载Maven扩展，例如打包和类型处理器。 由于性能原因，只有在真需要下载时，该元素才被设置成enabled。 --&gt; &lt;extensions /&gt; &lt;!-- 在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --&gt; &lt;executions&gt; &lt;!-- execution元素包含了插件执行需要的信息 --&gt; &lt;execution&gt; &lt;!-- 执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --&gt; &lt;id /&gt; &lt;!-- 绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --&gt; &lt;phase /&gt; &lt;!-- 配置的执行目标 --&gt; &lt;goals /&gt; &lt;!-- 配置是否被传播到子POM --&gt; &lt;inherited /&gt; &lt;!-- 作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!-- 项目引入插件所需要的额外依赖 --&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!-- 作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!-- 使用的插件列表 --&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!-- 在列的项目构建profile，如果被激活，会修改构建处理。 --&gt; &lt;profiles&gt; &lt;!-- 根据环境参数或命令行参数激活某个构建处理 --&gt; &lt;profile&gt; &lt;!-- 构建配置的唯一标识符。即用于命令行激活，也用于在继承时合并具有相同标识符的profile。 --&gt; &lt;id /&gt; &lt;!-- 自动触发profile的条件逻辑。Activation是profile的开启钥匙。profile的力量来自于它， 能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。 activation元素并不是激活profile的唯一方式。 --&gt; &lt;activation&gt; &lt;!-- profile默认是否激活的标志 --&gt; &lt;activeByDefault /&gt; &lt;!-- 当匹配的jdk被检测到，profile被激活。 例如，&quot;1.4&quot;激活JDK1.4，1.4.0_2，而&quot;!1.4&quot;激活所有版本不是以1.4开头的JDK。 --&gt; &lt;jdk /&gt; &lt;!-- 当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!-- 激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!-- 激活profile的操作系统所属家族(如&quot;windows&quot;) --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!-- 激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x64&lt;/arch&gt; &lt;!-- 激活profile的操作系统版本 --&gt; &lt;version&gt;6.1.7100&lt;/version&gt; &lt;/os&gt; &lt;!-- 如果Maven检测到某一个属性(其值可以在POM中通过$&#123;名称&#125;引用)，其拥有对应的名称和值，Profile就会被激活。 如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段。 --&gt; &lt;property&gt; &lt;!-- 激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!-- 激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!-- 提供一个文件名，通过检测该文件的存在或不存在来激活profile。 exists：检查文件是否存在，如果存在则激活profile。 missing：检查文件是否存在，如果不存在则激活profile。 --&gt; &lt;file&gt; &lt;!-- 如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/usr/local/xxxx/xxxx-home/tomcat/maven-guide-zh-to-production/workspace/&lt;/exists&gt; &lt;!-- 如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/usr/local/xxxx/xxxx-home/tomcat/maven-guide-zh-to-production/workspace/&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!-- 构建项目所需要的信息。参见build元素。 --&gt; &lt;build&gt; &lt;defaultGoal /&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;directory /&gt; &lt;finalName /&gt; &lt;filters /&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!-- 模块(有时称作子项目)被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径。 --&gt; &lt;modules /&gt; &lt;!-- 发现依赖和扩展的远程仓库列表 --&gt; &lt;repositories&gt; &lt;!-- 参见repositories/repository元素 --&gt; &lt;repository&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!-- 发现插件的远程仓库列表，这些插件用于构建和报表 --&gt; &lt;pluginRepositories&gt; &lt;!-- 包含需要连接到远程插件仓库的信息。参见repositories/repository元素。 --&gt; &lt;pluginRepository&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!-- 该元素描述了项目相关的所有依赖。这些依赖组成了项目构建过程中的一个个环节。 它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 不赞成使用。现在Maven忽略该元素。 --&gt; &lt;reports /&gt; &lt;!-- 该元素包括使用报表插件产生报表的规范。当用户执行&quot;mvn site&quot;，这些报表就会运行。 在页面导航栏能看到所有报表的链接。参见reporting 元素。 --&gt; &lt;reporting&gt;......&lt;/reporting&gt; &lt;!-- 参见dependencyManagement元素 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 参见distributionManagement元素 --&gt; &lt;distributionManagement&gt;......&lt;/distributionManagement&gt; &lt;!-- 参见properties元素 --&gt; &lt;properties /&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!-- 模块(有时称作子项目)被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径。 --&gt; &lt;modules /&gt; &lt;!-- 发现依赖和扩展的远程仓库列表，配置多个repository时，按顺序依次查找。 --&gt; &lt;repositories&gt; &lt;!-- 包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!-- 远程仓库唯一标识符。可以用来匹配在settings.xml文件里配置的远程仓库。 --&gt; &lt;id&gt;banseon-repository-proxy&lt;/id&gt; &lt;!-- 远程仓库名称 --&gt; &lt;name&gt;banseon-repository-proxy&lt;/name&gt; &lt;!-- 远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://10.10.10.123:8080/repository/&lt;/url&gt; &lt;!-- 用于定位和排序构件的仓库布局类型-可以是default(默认)或者legacy(遗留)。 Maven2为其仓库提供了一个默认的布局；然而，Maven1.x有一种不同的布局。 我们可以使用该元素指定布局是default(默认)还是legacy(遗留)。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;!-- 如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!-- true或者false表示该仓库是否为下载某种类型构件(发布版，快照版)开启。 --&gt; &lt;enabled /&gt; &lt;!-- 该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。 选项：always(一直)，daily(默认，每日)，interval：X(这里X是以分钟为单位的时间间隔)，或者never(从不)。 --&gt; &lt;updatePolicy /&gt; &lt;!-- 当Maven验证构件校验文件失败时该怎么做：ignore(忽略)，fail(失败)，或者warn(警告)。 --&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载。 有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。 例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素。 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!-- 发现插件的远程仓库列表，这些插件用于构建和报表。 --&gt; &lt;pluginRepositories&gt; &lt;!-- 包含需要连接到远程插件仓库的信息。参见repositories/repository元素。 --&gt; &lt;pluginRepository&gt;......&lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!-- 该元素描述了项目相关的所有依赖。这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。 要获取更多信息，请看项目依赖机制。 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!-- 依赖的groupID --&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;!-- 依赖的artifactID --&gt; &lt;artifactId&gt;maven-artifact&lt;/artifactId&gt; &lt;!-- 依赖的版本号。在Maven2里，也可以配置成版本号的范围。 --&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;!-- 依赖类型，默认类型是jar。它通常表示依赖的文件的扩展名，但也有例外。 一个类型可以被映射成另外一个扩展名或分类器。类型经常和使用的打包方式对应，尽管这也有例外。 一些类型的例子：jar，war，ejb-client和test-jar。 如果设置extensions为true，就可以在plugin里定义新的类型。所以前面的类型的例子不完整。 --&gt; &lt;type&gt;jar&lt;/type&gt; &lt;!-- 依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面。 例如，如果你想要构建两个单独的构件成JAR，一个使用Java 1.4编译器，另一个使用Java 6编译器， 你就可以使用分类器来生成两个单独的JAR构件。 --&gt; &lt;classifier&gt;&lt;/classifier&gt; &lt;!-- 依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 - compile： 默认范围，用于编译 - provided： 类似于编译，但支持你期待jdk或者容器提供，类似于classpath - runtime： 在执行时需要使用 - test： 用于test任务时使用 - system： 需要外在提供相应的元素。通过systemPath来取得 - systemPath： 仅用于范围为system。提供相应的路径 - optional： 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 --&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;!-- 仅供system范围使用。注意，不鼓励使用这个元素，并且在新的版本中该元素可能被覆盖掉。 该元素为依赖规定了文件系统上的路径。需要绝对路径而不是相对路径。 推荐使用属性匹配绝对路径，例如$&#123;java.home&#125;。 --&gt; &lt;systemPath&gt;&lt;/systemPath&gt; &lt;!-- 当计算传递依赖时，从依赖构件列表里，列出被排除的依赖构件集。 即告诉maven你只依赖指定的项目，不依赖项目的依赖。此元素主要用于解决版本冲突问题。 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;!-- 可选依赖，如果你在项目B中把C依赖声明为可选，则要在依赖于B的项目(例如项目A)中显式的引用对C的依赖。 可选依赖阻断依赖的传递性。 --&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 不赞成使用，现在Maven忽略该元素 --&gt; &lt;reports&gt;&lt;/reports&gt; &lt;!-- 该元素描述使用报表插件产生报表的规范。当用户执行&quot;mvn site&quot;，这些报表就会运行。在页面导航栏能看到所有报表的链接。 --&gt; &lt;reporting&gt; &lt;!-- true，则网站不包括默认的报表。这包括&quot;项目信息&quot;菜单中的报表。 --&gt; &lt;excludeDefaults /&gt; &lt;!-- 所有产生的报表存放到哪里。默认值是$&#123;project.build.directory&#125;/site。 --&gt; &lt;outputDirectory /&gt; &lt;!-- 使用的报表插件和他们的配置 --&gt; &lt;plugins&gt; &lt;!-- plugin元素包含描述报表插件需要的信息 --&gt; &lt;plugin&gt; &lt;!-- 报表插件在仓库里的groupID --&gt; &lt;groupId /&gt; &lt;!-- 报表插件在仓库里的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 被使用的报表插件的版本(或版本范围) --&gt; &lt;version /&gt; &lt;!-- 任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!-- 报表插件的配置 --&gt; &lt;configuration /&gt; &lt;!-- 一组报表的多重规范，每个规范可能有不同的配置。一个规范(报表集)对应一个执行目标。 例如，有 1，2，3，4，5，6，7，8，9 个报表， 1，2，5 构成A报表集，对应一个执行目标， 2，5，8 构成B报表集，对应另一个执行目标。 --&gt; &lt;reportSets&gt; &lt;!-- 表示报表的一个集合，以及产生该集合的配置。 --&gt; &lt;reportSet&gt; &lt;!-- 报表集合的唯一标识符，POM继承时用到。 --&gt; &lt;id /&gt; &lt;!-- 产生报表集合时，被使用的报表的配置。 --&gt; &lt;configuration /&gt; &lt;!-- 配置是否被继承到子POMs --&gt; &lt;inherited /&gt; &lt;!-- 这个集合里使用到哪些报表 --&gt; &lt;reports /&gt; &lt;/reportSet&gt; &lt;/reportSets&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt; &lt;!-- 继承自该项目的所有子项目的默认依赖信息。 这部分的依赖信息不会被立即解析，而是当子项目声明一个依赖(必须描述groupID和artifactID信息)时，如果groupID 和artifactID以外的一些信息没有描述，则通过groupID和artifactID匹配到这里的依赖，并使用这里的依赖信息。 比如锁定子项目的一些依赖的版本时，即可在父项目中定义。 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 项目分发信息，在执行&quot;mvn deploy&quot;后表示要发布的位置。 有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。 --&gt; &lt;distributionManagement&gt; &lt;!-- 部署项目产生的构件到远程仓库需要的信息 --&gt; &lt;repository&gt; &lt;!-- 是分配给快照一个唯一的版本号(由时间戳和构建流水号)？还是每次都使用相同的版本号？ 参见repositories/repository元素 --&gt; &lt;uniqueVersion /&gt; &lt;id&gt;xxx-maven2&lt;/id&gt; &lt;name&gt;xxx maven2&lt;/name&gt; &lt;url&gt;file://$&#123;basedir&#125;/target/deploy&lt;/url&gt; &lt;layout /&gt; &lt;/repository&gt; &lt;!-- 构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库。 参见distributionManagement/repository元素 --&gt; &lt;snapshotRepository&gt; &lt;uniqueVersion /&gt; &lt;id&gt;xxx-maven2&lt;/id&gt; &lt;name&gt;xxx-maven2 Snapshot Repository&lt;/name&gt; &lt;url&gt;scp://svn.xxxx.com/xxx:/usr/local/maven-snapshot&lt;/url&gt; &lt;layout /&gt; &lt;/snapshotRepository&gt; &lt;!-- 部署项目的网站需要的信息 --&gt; &lt;site&gt; &lt;!-- 部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置 --&gt; &lt;id&gt;banseon-site&lt;/id&gt; &lt;!-- 部署位置的名称 --&gt; &lt;name&gt;business api website&lt;/name&gt; &lt;!-- 部署位置的URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;scp://svn.baidu.com/xxx:/var/www/localhost/web&lt;/url&gt; &lt;/site&gt; &lt;!-- 项目下载页面的URL。如果没有该元素，用户应该参考主页。 使用该元素的原因是：帮助定位那些不在仓库里的构件(由于license限制)。 --&gt; &lt;downloadUrl /&gt; &lt;!-- 如果构件有了新的groupID和artifactID(构件移到了新的位置)，这里列出构件的重定位信息。 --&gt; &lt;relocation&gt; &lt;!-- 构件新的groupID --&gt; &lt;groupId /&gt; &lt;!-- 构件新的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 构件新的版本号 --&gt; &lt;version /&gt; &lt;!-- 显示给用户的，关于移动的额外信息，例如原因。 --&gt; &lt;message /&gt; &lt;/relocation&gt; &lt;!-- 给出该构件在远程仓库的状态。不得在本地项目中设置该元素，因为这是工具自动更新的。有效的值有： - none： 默认 - converted： 仓库管理员从Maven1 POM转换过来 - partner： 直接从伙伴Maven 2仓库同步过来 - deployed： 从Maven 2实例部署 - verified： 被核实时正确的和最终的 --&gt; &lt;status /&gt; &lt;/distributionManagement&gt; &lt;!-- 以值替代名称，Properties可以在整个POM中使用，也可以作为触发条件(见settings.xml配置文件里activation元素的说明)。 格式是：&lt;name&gt;value&lt;/name&gt;。 --&gt; &lt;properties /&gt;&lt;/project&gt; 本文参考https://www.cnblogs.com/iceJava/p/10356309.html https://www.cnblogs.com/hongmoshui/p/10762272.html https://www.cnblogs.com/cxzdy/p/5126087.html 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"maven 仓库的配置方式以及依赖的下载顺序","slug":"maven-repository","date":"2021-01-19T08:17:18.000Z","updated":"2021-04-09T07:51:24.008Z","comments":true,"path":"2021/01/19/maven-repository/","link":"","permalink":"http://example.com/2021/01/19/maven-repository/","excerpt":"","text":"maven 仓库分为本地仓库和远程仓库，而远程仓库又分为 maven 中央仓库、其他远程仓库和私服 (私有服务器)。 maven 项目使用的仓库一般有如下几种方式： maven 中央仓库，这是默认的仓库。 镜像仓库，通过 sttings.xml 中的 settings.mirrors.mirror 配置。 全局 profile 仓库，通过 settings.xml 中的 settings.repositories.repository 配置。 项目仓库，通过 pom.xml 中的 project.repositories.repository 配置。 项目 profile 仓库，通过 pom.xml 中的 project.profiles.profile.repositories.repository 配置。 本地仓库。 如果所有仓库的配置都存在，那么依赖的搜索顺序也会变得异常复杂。 仓库的配置方式本地仓库maven 缺省的本地仓库地址为 ${user.home}/.m2/repository，也就是说，一个用户会对应的拥有一个本地仓库。 可以通过修改 ${user.home}/.m2/settings.xml，在 节点下添加配置： 1&lt;localRepository&gt;D:\\java\\maven-repo&lt;/localRepository&gt; 如果想让所有的用户使用统一的配置，那么可以修改 maven 主目录下的 setting.xml：${M2_HOME}/conf/setting.xml。 maven 中央仓库在 maven 安装目录的 lib 目录下，有一个 maven-model-builder-3.6.1.jar，里面的 org/apache/maven/model/pom-4.0.0.xml 文件定义了 maven 默认中央仓库的地址：https://repo.maven.apache.org/maven2，如下图所示： 1234567891011121314151617181920212223242526&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;updatePolicy&gt;never&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 一般使用阿里云镜像仓库代替默认的 maven 中央仓库，配置方式有两种： 第一种，全局配置 修改 ${M2_HOME}/conf/setting.xml 文件，在 节点下添加配置： 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;mirrors&gt; 修改全局配置后，所有使用此 maven 的工程都会生效。 &lt; mirrorOf&gt; 可以设置为哪个中央仓库做镜像，为名为 “central” 的中央仓库做镜像，写作 &lt; mirrorOf&gt;central&lt; /mirrorOf&gt;；为所有中央仓库做镜像，写作 &lt; mirrorOf&gt;*&lt; /mirrorOf&gt; (不建议)。maven 默认中央仓库的 id 为 central。id是唯一的。 第二种，局部配置 在需要使用阿里云镜像仓库的 maven 工程的 pom.xml 文件中添加： 123456789101112131415161718192021222324&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;aliyun&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun-plugin&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 修改局部配置后，只对当前工程有效。 私服 第一种，全局配置 修改 ${M2_HOME}/conf/setting.xml 文件，在 节点下添加配置： 1234567891011121314151617181920212223242526272829&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;matgene-nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-plugin&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 然后，在 节点下添加激活配置 (通过配置的 profile 的 id 标识进行激活)： 123&lt;activeProfiles&gt; &lt;activeProfile&gt;matgene-nexus&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 第二种，局部配置 在需要使用私服的 maven 工程的 pom.xml 文件中添加。 上传： settings.xml： 12345678910&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; pom.xml： 12345678910&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 下载： pom.xml： 123456789101112131415161718192021222324&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-plugin&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 在项目中，将私服地址更改为自己公司的实际地址。 依赖的下载顺序准备测试环境安装 jdk 和 maven。 使用如下命令创建测试项目： 1yes | mvn archetype:generate -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-webapp -DinteractiveMode=true -DgroupId=com.pollyduan -DartifactId=myweb -Dversion=1.0 -Dpackage=com.pollyduan 创建完成后，为了避免后续测试干扰，先执行一次 compile。 12cd mywebmvn compile 最后，修改 pom.xml 文件，将 junit 版本号改为 4.12 。我们要使用这个 jar 来测试依赖的搜索顺序。 默认情况首先确保 junit 4.12 不存在： 1rm -rf ~/.m2/repository/junit/junit/4.12 默认情况下没有配置任何仓库，也就是说，既没更改 $M2_HOME/conf/settings.xml，也没有添加 ~/.m2/settings.xml。 执行编译，查看日志中拉取 junit 的仓库。 1234mvn compile...Downloaded from central: https://repo.maven.apache.org/maven2/junit/junit/4.12/junit-4.12.pom (24 kB at 11 kB/s) 从显示的仓库 id 可以看出：默认是从 maven 中央仓库拉取的 jar。 配置镜像仓库 settings_mirror创建 ~/.m2/setttings.xml，配置 maven 中央仓库的镜像，如下： 123456789&lt;settings&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;settings_mirror&lt;/id&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt; 重新测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile 在日志中查看下载依赖的仓库： 1Downloaded from settings_mirror: https://maven.aliyun.com/repository/public/junit/junit/4.12/junit-4.12.pom (24 kB at 35 kB/s) 从显示的仓库 id 可以看出：是从 settings_mirror 中下载的 jar。 结论：settings_mirror 的优先级高于 central。 配置项目仓库 pom_repositories在 project 中的 pom.xml 文件中，增加如下配置： 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;pom_repositories&lt;/id&gt; &lt;name&gt;local&lt;/name&gt; &lt;url&gt;http://10.18.29.128/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;sapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 由于改变了 id 的名字，所以仓库地址无所谓，使用相同的地址也不影响测试。 执行测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile 在日志中查看下载依赖的仓库： 1Downloaded from pom_repositories: http://10.18.29.128/nexus/content/groups/public/junit/junit/4.12/junit-4.12.pom (24 kB at 95 kB/s) 从显示的仓库 id 可以看出：jar 是从 pom_repositories 中下载的。 结论：pom_repositories 优先级高于 settings_mirror。 配置全局 profile 仓库 settings_profile_repo在 ~/.m2/settings.xml 中 settings 的节点内增加： 123456789101112131415161718&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;s_profile&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;settings_profile_repo&lt;/id&gt; &lt;name&gt;netease&lt;/name&gt; &lt;url&gt;http://mirrors.163.com/maven/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 执行测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Ps_profile 在日志中查看下载依赖的仓库： 1Downloaded from settings_profile_repo: http://mirrors.163.com/maven/repository/maven-public/junit/junit/4.12/junit-4.12.pom (24 kB at 63 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：settings_profile_repo 优先级高于 pom_repositories 和 settings_mirror。 配置项目 profile 仓库 pom_profile_repo在 project 中的 pom.xml 文件中，增加如下配置： 123456789101112131415161718&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;p_profile&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;pom_profile_repo&lt;/id&gt; &lt;name&gt;local&lt;/name&gt; &lt;url&gt;http://10.18.29.128/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 执行测试： 123rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Ps_profile,p_profilemvn compile -Pp_profile,s_profile 在日志中查看下载依赖的仓库： 1Downloaded from settings_profile_repo: http://mirrors.163.com/maven/repository/maven-public/junit/junit/4.12/junit-4.12.pom (24 kB at 68 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：settings_profile_repo 优先级高于 pom_profile_repo。 进一步测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Pp_profile 在日志中查看下载依赖的仓库： 1Downloaded from pom_profile_repo: http://10.18.29.128/nexus/content/groups/public/junit/junit/4.12/junit-4.12.pom (24 kB at 106 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：pom_profile_repo 优先级高于 pom_repositories。 本地仓库 local_repo这不算测试了，只是一个结论，可以任意测试：只要 ~/.m2/repository 中包含依赖，无论怎么配置，都会优先使用 local 本地仓库中的 jar。 最终结论 settings_mirror 的优先级高于 central settings_profile_repo 优先级高于 settings_mirror settings_profile_repo 优先级高于 pom_repositories settings_profile_repo 优先级高于 pom_profile_repo pom_repositories 优先级高于 settings_mirror pom_profile_repo 优先级高于 pom_repositories 通过上面的比较，可以得出各种仓库完整的搜索顺序链： local_repo &gt; settings_profile_repo &gt; pom_profile_repo &gt; pom_repositories &gt; settings_mirror &gt; central 简单来说，查找依赖的顺序大致如下： 在本地仓库中寻找，如果没有则进入下一步。 在全局配置的私服仓库 (settings.xml 中配置的并被激活) 中寻找，如果没有则进入下一步。 在项目自身配置的私服仓库 (pom.xml) 中寻找，如果没有则进入下一步。 在中央仓库中寻找，如果没有则终止寻找。 说明： 如果在找寻的过程中，发现该仓库有镜像设置，则用镜像的地址代替，即假设现在进行到要在 respository A 仓库中查找某个依赖，但 A 仓库配置了 mirror，则会转到从 A 的 mirror 中查找该依赖，不会再从 A 中查找。 settings.xml 中配置的 profile (激活的) 下的 respository 优先级高于项目中 pom.xml 文件配置的 respository。 如果仓库的 id 设置成 “central”，则该仓库会覆盖 maven 默认的中央仓库配置。 本文参考https://blog.csdn.net/asdfsfsdgdfgh/article/details/96576665 https://www.cnblogs.com/default/p/11856188.html https://my.oschina.net/polly/blog/2120650 https://blog.csdn.net/fengdayuan/article/details/93089136 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"maven 构建分模块项目","slug":"maven-modules","date":"2021-01-12T09:17:19.000Z","updated":"2021-01-20T07:37:52.805Z","comments":true,"path":"2021/01/12/maven-modules/","link":"","permalink":"http://example.com/2021/01/12/maven-modules/","excerpt":"","text":"分模块构建 maven 工程分析在现实生活中，汽车厂家进行汽车生产时，由于整个生产过程非常复杂和繁琐，工作量非常大，所以车场都会将整个汽车的部件分开生产，最终再将生产好的部件进行组装，形成一台完整的汽车： 类似的，随着项目功能的增加，项目本身会变得越来越庞大，这个时候，代码的良好管理和规划就会变得很重要。为了提高效率，根据业务的不同将揉作一团的业务代码分离出来，业务划分上分割清晰，提高代码复用率，例如： 上述功能的实现就是代码这一层级的变动，可以采用多项目模式和多模块模式： 多项目：每个业务单独新建项目并编写相应逻辑 多模块：业务聚合在一个项目中的不同模块中，然后通过依赖调用实现业务逻辑 maven 工程的继承在 java 语言中，类之间是可以继承的，通过继承，子类就可以引用父类中非 private 的属性和方法。同样，在 maven 工程之间也可以继承，子工程继承父工程后，就可以使用在父工程中引入的依赖。继承的目的是为了消除重复代码。 maven 工程的聚合在 maven 工程的 pom.xml 文件中，可以使用 标签将其他 maven 工程聚合到一起，聚合的目的是为了进行统一操作。 例如，拆分后的 maven 工程有多个，如果要进行打包，就需要针对每个工程分别执行打包命令，操作起来非常繁琐。这时就可以使用 标签将这些工程统一聚合到 maven 工程中，需要打包的时候，只需要在此工程中执行一次打包命令，其下被聚合的工程就都会被打包了。 分模块构建 maven 工程构建父模块新建 maven 项目： 保留 pom.xml 文件，删除 src 目录： 构建子模块新建 module 1： 新建 module 2： 如果需要更多的模块，重复上述步骤。 父模块 pom 配置公用的 pom 配置，可以放在父模块的 pom.xml 文件中： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.matgene.reaction-extractor-assistant&lt;/groupId&gt; &lt;artifactId&gt;reaction-extractor-assistant&lt;/artifactId&gt; &lt;!-- parent必须使用pom格式打包并上传到仓库 --&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;patent-loader&lt;/module&gt; &lt;module&gt;consumer-log&lt;/module&gt; &lt;module&gt;consumer-reaction&lt;/module&gt; &lt;module&gt;consumer-timeout&lt;/module&gt; &lt;/modules&gt; &lt;!-- 全局版本管理 --&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.version&gt;3.8.1&lt;/maven.compiler.version&gt; &lt;maven.compiler.source&gt;$&#123;java.version&#125;&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;$&#123;java.version&#125;&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;!-- 全局依赖管理 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.danielwegener&lt;/groupId&gt; &lt;artifactId&gt;logback-kafka-appender&lt;/artifactId&gt; &lt;version&gt;0.2.0-RC1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.json-lib&lt;/groupId&gt; &lt;artifactId&gt;json-lib&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;classifier&gt;jdk15&lt;/classifier&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt; &lt;version&gt;5.6.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven.compiler.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;maven.compiler.source&#125;&lt;/source&gt; &lt;target&gt;$&#123;maven.compiler.target&#125;&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 子模块 pom 配置子模块单独使用的 pom 配置，放在子模块自己的 pom.xml 文件中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;reaction-extractor-assistant&lt;/artifactId&gt; &lt;groupId&gt;cn.matgene.reaction-extractor-assistant&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;patent-loader&lt;/artifactId&gt; &lt;properties&gt; &lt;app.main.class&gt;cn.matgene.patent.cn.matgene.patent.loader.PatentLoaderJob&lt;/app.main.class&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;configuration&gt; &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;manifestEntries&gt; &lt;Main-Class&gt;$&#123;app.main.class&#125;&lt;/Main-Class&gt; &lt;X-Compile-Source-JDK&gt;$&#123;maven.compiler.source&#125;&lt;/X-Compile-Source-JDK&gt; &lt;X-Compile-Target-JDK&gt;$&#123;maven.compiler.target&#125;&lt;/X-Compile-Target-JDK&gt; &lt;/manifestEntries&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 上面构建的项目比较简单，各模块之间不存在依赖关系，同时，因为每个模块都需要打包，因此把打包的插件放在每一个子模块的 pom.xml 文件中。 一个 spring web 项目的实例 父工程 maven_parent 构建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394 &lt;properties&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt; &lt;springmvc.version&gt;5.0.5.RELEASE&lt;/springmvc.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt;&lt;/properties&gt;&lt;!--锁定jar版本--&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- springMVC --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;springmvc.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 子工程 maven_pojo 构建 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_dao 构建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 &lt;dependencies&gt; &lt;!-- maven_pojo的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_pojo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis和mybatis与spring的整合 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- druid数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- junit测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_service 构建 12345678&lt;dependencies&gt; &lt;!-- maven_dao的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_dao&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_web 构建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- maven_service的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;finalName&gt;maven_web&lt;/finalName&gt; &lt;pluginManagement&gt;&lt;!-- lock down plugins versions to avoid using Maven defaults (may be moved to parent pom) --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/plugin&gt; &lt;!-- see http://maven.apache.org/ref/current/maven-core/default-bindings.html#Plugin_bindings_for_war_packaging --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.22.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 项目整体结构如下： maven_parent 为父工程，其余工程为子工程，都继承父工程 maven_parent； maven_parent 工程将其子工程都进行了聚合 ； 子工程之间存在依赖关系，比如 maven_dao 依赖 maven_pojo，maven_service 依赖 maven_dao，maven_web 依赖 maven_service。 本文参考https://juejin.cn/post/6844903970024980488 https://www.cnblogs.com/tianlong/p/10552848.html 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"linux 常见错误","slug":"linux-error","date":"2021-01-11T06:49:11.000Z","updated":"2021-01-11T08:00:48.503Z","comments":true,"path":"2021/01/11/linux-error/","link":"","permalink":"http://example.com/2021/01/11/linux-error/","excerpt":"","text":"No space left on device有时候，在创建新文件，或者往磁盘写内容时，会提示 No space left on device 异常。 一般来说，linux 空间占满有如两种情况： 空间占满通过 df -h 命令，查看空间的使用情况： 1234567891011121314$ df -hFilesystem Size Used Avail Use% Mounted onudev 3.9G 0 3.9G 0% /devtmpfs 799M 82M 718M 11% /run/dev/mapper/lin--vg-root 491G 195G 272G 42% /tmpfs 3.9G 8.0K 3.9G 1% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 472M 58M 390M 13% /boottmpfs 799M 0 799M 0% /run/user/1000192.168.1.152:/mnt/chenlei 10T 5.0T 4.6T 53% /home/lin/share/storage_server_1192.168.1.236:/mnt 10T 9.0T 520G 95% /home/lin/share/storage_server_2192.168.1.106:/mnt 40T 24T 15T 63% /home/lin/share/storage_server_3192.168.1.102:/home/lin/share 491G 195G 272G 42% /tmp/share 可以看出，各分区仍有较大的空间能够使用。如果某个分区的使用率达到了 100%，那也就无法再创建新文件，也无法再写入内容，需要删除一些文件。 inode 占满通过 df -i 命令，查看 inode 的使用情况。 1234567891011121314$ df -ihFilesystem Inodes IUsed IFree IUse% Mounted onudev 993K 421 993K 1% /devtmpfs 998K 749 998K 1% /run/dev/mapper/lin--vg-root 32M 1.6M 30M 6% /tmpfs 998K 2 998K 1% /dev/shmtmpfs 998K 3 998K 1% /run/locktmpfs 998K 16 998K 1% /sys/fs/cgroup/dev/sda1 122K 303 122K 1% /boottmpfs 998K 4 998K 1% /run/user/1000192.168.1.152:/mnt/chenlei 320M 42M 279M 13% /home/lin/share/storage_server_1192.168.1.236:/mnt 320M 69M 252M 22% /home/lin/share/storage_server_2192.168.1.106:/mnt 640M 641M 0M 100% /home/lin/share/storage_server_3192.168.1.102:/home/lin/share 32M 1.6M 30M 6% /tmp/share 可以看出，每个分区都有一定大小的 inode 空间，但 /home/lin/share/storage_server_3 分区的 inode 空间使用率达到 100%。因此，再往此分区创建新文件或写入内容时，会提示 No space left on device 异常。 解决方法：将 /home/lin/share/storage_server_3 分区上一些不必要的文件删除。 理解 inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做 “扇区” (Sector)，每个扇区储存 512 字节 (相当于 0.5KB)。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个 “块” (Block)。这种由多个扇区组成的 “块”，是文件存取的最小单位。”块” 的大小，最常见的是 4 KB，即连续八个 Sector 组成一个 Block。 文件数据都储存在 “块” 中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做 inode，中文译名为 “索引节点”。 每一个文件都有对应的 inode，里面包含了与该文件有关的一些信息。 某些时候，尽管一个分区的磁盘占用率未满，但是 inode 已经用完，可能是因为该分区的目录下存在大量小文件导致。尽管小文件占用的磁盘空间并不大，但是数量太多，也会导致 inode 用尽。本例中就是因为 /home/lin/share/storage_server_3 分区存在大量的小文件。","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"Java 的日志处理","slug":"java-log","date":"2021-01-06T09:14:19.000Z","updated":"2021-04-09T08:01:06.092Z","comments":true,"path":"2021/01/06/java-log/","link":"","permalink":"http://example.com/2021/01/06/java-log/","excerpt":"","text":"常用日志处理工具常见的 log 日志处理工具有：log4j、Logging、commons-logging、slf4j、logback。其中，commons-loggin、slf4j 是一种日志抽象门面，不是具体的日志框架；log4j、logback 是具体的日志实现框架。 一般使用 slf4j + logback 处理日志，也可以使用 slf4j + log4j、commons-logging + log4j 这两种日志组合框架。 日志级别日志的输出都是分级别的，不同的场合设置不同的级别，以打印不同的日志。下面拿最普遍用的 log4j 日志框架来做个日志级别的说明，这个比较奇全，其他的日志框架也都大同小异。 log4j 的级别类 org.apache.log4j.Level 里面定义了日志级别，日志输出优先级由高到底分别为以下 8 种： 日志级别 描述 OFF 关闭：最高级别，不输出日志。 FATAL 致命：输出非常严重的可能会导致应用程序终止的错误。 ERROR 错误：输出错误，但应用还能继续运行。 WARN 警告：输出可能潜在的危险状况。 INFO 信息：输出应用运行过程的详细信息。 DEBUG 调试：输出更细致的对调试应用有用的信息。 TRACE 跟踪：输出更细致的程序运行轨迹。 ALL 所有：输出所有级别信息。 所以，日志优先级别标准顺序为： ALL &lt; TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL &lt; OFF 如果日志设置为 L ，一个级别为 P 的输出日志只有当 P &gt;= L 时日志才会输出。 即如果日志级别 L 设置 INFO，只有 P 的输出级别为 INFO、WARN，后面的日志才会正常输出。 具体的输出关系可以参考下图： LombokLombok 是一种 java 实用工具，可用来帮助开发人员消除 java 的冗长代码，尤其是对于简单的 java 对象 (POJO)。它通过注释实现这一目的。 引入IntelliJ 安装： Lombok 是侵入性很高的一个 library。 maven 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt;&lt;/dependency&gt; 注解说明常用注解： @Getter 和 @Setter 自动生成 getter 和 setter 方法。 @ToString 自动重写 toString() 方法，打印所有变量。也可以加其他参数，例如 @ToString(exclude=”id”) 排除 id 属性，或者 @ToString(callSuper=true, includeFieldNames=true) 调用父类的 toString() 方法，包含所有属性。 @EqualsAndHashCode 自动生成 equals(Object other) 和 hashcode() 方法，包括所有非静态变量和非 transient 的变量。 如果某些变量不想要加进判断，可以通过 exclude 排除，也可以使用 of 指定某些字段： java 中规定，当两个 object equals 时，它们的 hashcode 一定要相同，反之，当 hashcode 相同时，object 不一定 equals。所以 equals 和 hashcode 要一起 implement，免得出现违反 java 规定的情形。 @NoArgsConstructor，@AllArgsConstructor，@RequiredArgsConstructor 这三个很像，都是自动生成该类的 constructor，差別只在生成的 constructor 的参数不一样而已。 @NoArgsConstructor：生成一个沒有参数的 constructor。 在 java 中，如果沒有指定类的 constructor，java compiler 会自动生成一个无参构造器，但是如果自己写了 constructor 之后，java 就不会再自动生成无参构造器。但是，很多时候，无参构造器是必须的，因此，为避免不必要的麻烦，应在类上至少加上 @NoArgsConstrcutor。 @AllArgsConstructor ：生成一个包含所有参数的 constructor。 @RequiredArgsConstructor：生成一个包含 “特定参数” 的 constructor，特定参数指的是那些有加上 final 修饰词的变量。 如果所有的变量都沒有用 final 修饰，@RequiredArgsConstructor 会生成一个沒有参数的 constructor。 @Data 等于同时添加了以下注解：@Getter，@Setter，@ToString，@EqualsAndHashCode 和 @RequiredArgsConstructor。 @Value 把所有的变量都设成 final，其他的就跟 @Data 类似，等于同时添加了以下注解：@Getter，@ToString，@EqualsAndHashCode 和 @RequiredArgsConstructor。 @Builder 自动生成流式 set 值写法。 注意，虽然只要加上 @Builder 注解，我们就能用流式写法快速设定 Object 的值，但是 setter 还是不应该舍弃的，因为 Spring 或是其他框架，有很多地方都会用到 Object 的 getter/setter 方法来对属性取值/赋值。 所以，通常是 @Data 和 @Builder 会一起用在同个类上，既方便流式写 code，也方便框架做事。比如： 123456@Data@Builderpublic class User &#123; private Integer id; private String name;&#125; @Slf4j 自动生成该类的 log 静态常量，要打日志就可以直接打，不用再手动 new log 静态常量了。 除了 @Slf4j 之外，Lombok 也提供其他日志框架的几种注解，像是 @Log，@Log4j 等，他们都可以创建一个静态常量 log，只是使用的 library 不一样而已。 12345@Log // 对应的log语句如下private static final java.util.logging.Logger log = java.util.logging.Logger.getLogger(LogExample.class.getName());@Log4j // 对应的log语句如下private static final org.apache.log4j.Logger log = org.apache.log4j.Logger.getLogger(LogExample.class); 更多的参考：https://juejin.cn/post/6844903557016076302 Logback引入12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; logback 依赖中，含有对 slf4j 的依赖。 节点configuration 为主节点，其主要字节点如下： property定义变量值的标签，有两个属性，name 和 value，定义变量后，可以使 “${name}” 来使用变量。 1&lt;property name=&quot;logging.level&quot; value=&quot;info&quot;/&gt; appender日志打印的组件，定义打印过滤的条件、打印输出方式、滚动策略、编码方式、打印格式等。 种类： ConsoleAppender：把日志添加到控制台。 12345&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder charset=&quot;utf-8&quot;&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-6level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt; FileAppender：把日志添加到文件。 12345678910111213&lt;appender name=&quot;ReactionExtractorAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/reaction-extractor.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; RollingFileAppender：FileAppender 的子类，滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。 1234567891011121314&lt;appender name=&quot;ReactionExtractorRollingAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/reaction-extractork-%d&#123;yyyy-MM-dd&#125;.log&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; 属性： name：指定 appender 的名称。 class：指定 appender 的全限定名。 子节点： append：默认为 true，表示日志被追加到文件结尾，如果是 false，清空现存文件。 filter：过滤器，执行完一个过滤器后返回 DENY，NEUTRAL，ACCEPT 三个枚举值中的一个。 filter 的返回值含义： DENY：日志将立即被抛弃不再经过其他过滤器。 NEUTRAL：有序列表里的下个过滤器过接着处理日志。 ACCEPT：日志会被立即处理，不再经过剩余过滤器。 filter 的两种类型： ThresholdFilter：临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器返回 NEUTRAL，当日志级别低于临界值时，日志会被拒绝。 123&lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;INFO&lt;/level&gt;&lt;/filter&gt; LevelFilter：级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据 onMath (用于配置符合过滤条件的操作) 和 onMismatch (用于配置不符合过滤条件的操作) 接收或拒绝日志。 12345&lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; file：指定被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 rollingPolicy：滚动策略，只有 appender 的 class 是 RollingFileAppender 时才需要配置。 TimeBasedRollingPolicy：根据时间来制定滚动策略，既负责滚动也负责触发滚动。 12345678&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!-- 日志文件输出的文件名：按天回滚 daily --&gt; &lt;FileNamePattern&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log.%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; &lt;/FileNamePattern&gt; &lt;!-- 日志文件保留天数 --&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt;&lt;/rollingPolicy&gt; 每天生成一个日志文件，日志文件保存 30 天。 FixedWindowRollingPolicy：根据固定窗口算法重命名文件的滚动策略。 encoder：对记录事件进行格式化。主要作用是：把日志信息转换成字节数组，以及把字节数组写入到输出流。 12345&lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;!-- 格式化输出：%d表示日期；%thread表示线程名；%-5level：级别从左显示5个字符宽度；%logger&#123;50&#125; 表示logger名字最长50个字符，否则按照句点分割；%msg：日志消息；%n是换行符 --&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt;&lt;/encoder&gt; logger用来设置某一个包或者具体的某一个类的日志打印级别以及指定 appender。 属性： name：指定受此 logger 约束的某一个包或者具体的某一个类。 level：设置打印级别 (TRACE，DEBUG，INFO，WARN，ERROR，ALL 和 OFF)，还有一个值 INHERITED 或者同义词 NULL，代表强制执行上级的级别。如果没有设置此属性，那么当前 logger 将会继承上级的级别。 addtivity：设置是否向上级 logger 传递打印信息，默认为 true。 123&lt;logger name=&quot;com.glmapper.spring.boot.controller&quot; level=&quot;$&#123;logging.level&#125;&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;GLMAPPER-LOGGERONE&quot; /&gt;&lt;/logger&gt; com.glmapper.spring.boot.controller 这个包下的 ${logging.level} 级别的日志将会使用 GLMAPPER-LOGGERONE 来打印。 root根 logger，也是一种 logger，但只有一个 level 属性。 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187&lt;!-- 使用说明： 1. logback核心jar包：logback-core-1.2.3.jar，logback-classic-1.2.3.jar，slf4j-api-1.7.25.jar 1) logback官方建议配合slf4j使用 2) logback手动下载地址：https://repo1.maven.org/maven2/ch/qos/logback/ 3) slf4j手动下载地址：https://www.mvnjar.com/org.slf4j/slf4j-api/1.7.25/detail.html 4) jar包可以从maven仓库快速获取 2. logback分为3个组件：logback-core，logback-classic和logback-access 1) 其中logback-core提供了logback的核心功能，是另外两个组件的基础 2) logback-classic实现了slf4j的API，所以当想配合slf4j使用时，需要将logback-classic加入classpath 3) logback-access是为了集成servlet环境而准备的，可提供HTTP-access的日志接口 3. 配置中KafkaAppender的jar包：logback-kafka-appender-0.2.0-RC1.jar--&gt;&lt;!-- 参考： https://juejin.im/post/5b51f85c5188251af91a7525 https://my.oschina.net/Declan/blog/1793444--&gt;&lt;!-- 说明：logback.xml配置文件，需放置在项目的resources路径下 --&gt;&lt;!-- configuration属性： scan：热加载，当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true scanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟 debug：当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false packagingData：是否打印包的信息。默认值为false--&gt;&lt;configuration debug=&quot;false&quot; xmlns=&quot;http://ch.qos.logback/xml/ns/logback&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://ch.qos.logback/xml/ns/logback https://raw.githubusercontent.com/enricopulatzo/logback-XSD/master/src/main/xsd/logback.xsd&quot;&gt; &lt;!-- property：定义变量值，两个属性，name和value --&gt; &lt;property name=&quot;logging.path&quot; value=&quot;./&quot;/&gt; &lt;property name=&quot;logging.level&quot; value=&quot;INFO&quot;/&gt; &lt;!-- 日志格式化： %d：日期 %thread：线程名 %-5level：日志级别，从左显示5个字符宽度 %logger&#123;50&#125;：logger名字最长50个字符，超过的按照句点分割 %msg：日志消息 %n：换行符 %ex&#123;full, DISPLAY_EX_EVAL&#125;：异常信息，full表示全输出，可以替换为异常信息指定输出的行数 --&gt; &lt;property name=&quot;message.format&quot; value=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n%ex&#123;full, DISPLAY_EX_EVAL&#125;&quot;/&gt; &lt;!-- kafka topic --&gt; &lt;property name=&quot;topic.name&quot; value=&quot;log-collect&quot;/&gt; &lt;!-- 本地地址 --&gt; &lt;property name=&quot;bootstrap.servers&quot; value=&quot;192.168.1.71:9092&quot;/&gt; &lt;!-- 集群地址 --&gt; &lt;!-- &lt;property name=&quot;bootstrap.servers&quot; value=&quot;hadoopdatanode1:9092,hadoopdatanode2:9092,hadoopdatanode3:9092&quot;/&gt; --&gt; &lt;!-- appender种类： ConsoleAppender：把日志添加到控制台 FileAppender：把日志添加到文件 RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。FileAppender的子类 --&gt; &lt;!-- 控制台输出日志 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义输出日志到文件 --&gt; &lt;appender name=&quot;FileAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;!-- append：true，日志被追加到文件结尾；false，清空现存文件；默认是true --&gt; &lt;append&gt;true&lt;/append&gt; &lt;!-- 级别过滤器： ThresholdFilter：临界值过滤器，过滤掉低于指定临界值的日志 LevelFilter：级别过滤器，需配置onMatch和onMismatch --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/base.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义异常输出日志文件 --&gt; &lt;appender name=&quot;ErrorFileAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/error-file.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义输出日志：滚动记录日志 --&gt; &lt;appender name=&quot;RollingFileAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;!-- 滚动策略：每天生成一个日志文件，保存365天的日志文件 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!-- 日志文件输出的文件名：按天回滚 daily --&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/reaction-log-%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;.log&lt;/FileNamePattern&gt; &lt;!-- 日志文件保留天数 --&gt; &lt;MaxHistory&gt;365&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 日志文件最大的大小 --&gt; &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;MaxFileSize&gt;50MB&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 输出日志到kafka，参考：https://github.com/danielwegener/logback-kafka-appender --&gt; &lt;appender name=&quot;KafkaAppender&quot; class=&quot;com.github.danielwegener.logback.kafka.KafkaAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;topic&gt;$&#123;topic.name&#125;&lt;/topic&gt; &lt;keyingStrategy class=&quot;com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy&quot;/&gt; &lt;deliveryStrategy class=&quot;com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy&quot;/&gt; &lt;!-- Optional parameter to use a fixed partition --&gt; &lt;!-- &lt;partition&gt;0&lt;/partition&gt; --&gt; &lt;!-- Optional parameter to include log timestamps into the kafka message --&gt; &lt;!-- &lt;appendTimestamp&gt;true&lt;/appendTimestamp&gt; --&gt; &lt;!-- each &lt;producerConfig&gt; translates to regular kafka-client config (format: key=value) --&gt; &lt;!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs --&gt; &lt;!-- bootstrap.servers is the only mandatory producerConfig --&gt; &lt;producerConfig&gt;bootstrap.servers=$&#123;bootstrap.servers&#125;&lt;/producerConfig&gt; &lt;!-- this is the fallback appender if kafka is not available. --&gt; &lt;appender-ref ref=&quot;FileAppender&quot;/&gt; &lt;/appender&gt; &lt;!-- 异步输出日志 步骤：异步输出日志就是Logger.info负责往Queue(BlockingQueue)中放日志，然后再起个线程把Queue中的日志写到磁盘上 参考：https://blog.csdn.net/lkforce/article/details/76637071 --&gt; &lt;appender name=&quot;ASYNC&quot; class=&quot;ch.qos.logback.classic.AsyncAppender&quot;&gt; &lt;!-- 不丢失日志。默认的，如果队列的80%已满，则会丢弃TRACT、DEBUG、INFO级别的日志 --&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;!-- 更改默认的队列的深度，该值会影响性能。默认值为256 --&gt; &lt;queueSize&gt;100&lt;/queueSize&gt; &lt;!-- 添加附加的appender，最多只能添加一个，此处指定后，在root下不要再指定该appender，否则会输出两次 --&gt; &lt;appender-ref ref=&quot;KafkaAppender&quot;/&gt; &lt;/appender&gt; &lt;!--日志异步到数据库：未做测试，配置正确与否未知，先记录于此 --&gt; &lt;!--&lt;appender name=&quot;DB&quot; class=&quot;ch.qos.logback.classic.db.DBAppender&quot;&gt; &lt;connectionSource class=&quot;ch.qos.logback.core.db.DriverManagerConnectionSource&quot;&gt; &lt;dataSource class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;driverClass&gt;com.mysql.jdbc.Driver&lt;/driverClass&gt; &lt;url&gt;jdbc:mysql://127.0.0.1:3306/databaseName&lt;/url&gt; &lt;user&gt;root&lt;/user&gt; &lt;password&gt;root&lt;/password&gt; &lt;/dataSource&gt; &lt;/connectionSource&gt; &lt;/appender&gt;--&gt; &lt;!-- 关闭指定包下的日志输出，name里面的内容可以是包路径，或者具体要忽略的文件名称 --&gt; &lt;logger name=&quot;org.apache.flink&quot; level=&quot;OFF&quot;/&gt; &lt;!-- 将指定包下指定级别的日志，输出到指定的appender中 addtivity：是否向上级logger传递打印信息。默认是true。若此包下的日志单独输出到文件中，应设置为false，否则在root日志也会记录一遍 --&gt; &lt;logger name=&quot;org.apache.kafka&quot; level=&quot;ERROR&quot; addtivity=&quot;false&quot;&gt; &lt;!-- 指定此包下的error级别信息，输出到指定的收集文件 --&gt; &lt;appender-ref ref=&quot;ErrorFileAppender&quot;/&gt; &lt;/logger&gt; &lt;root level=&quot;$&#123;logging.level&#125;&quot;&gt; &lt;!--&lt;appender-ref ref=&quot;STDOUT&quot;/&gt;--&gt; &lt;!--&lt;appender-ref ref=&quot;FileAppender&quot;/&gt;--&gt; &lt;appender-ref ref=&quot;ASYNC&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt; 根据实际情况，对 appender 进行取舍，实际使用时不要所有的都添加到 logback.xml 配置文件中。 本文参考https://kucw.github.io/blog/2020/3/java-lombok/ https://juejin.cn/post/6844903641535479821 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"hadoop-hdfs","slug":"hadoop-hdfs","date":"2021-01-03T13:59:31.000Z","updated":"2022-01-05T06:59:29.025Z","comments":true,"path":"2021/01/03/hadoop-hdfs/","link":"","permalink":"http://example.com/2021/01/03/hadoop-hdfs/","excerpt":"","text":"HDFS 常用 shell 命令操作 HDFS 的 shell命令有三种： hadoop fs：适用于任何不同的文件系统，比如本地文件系统和 HDFS 文件系统。 hadoop dfs：只适用于 HDFS 文件系统。 hdfs dfs：只适用于 HDFS 文件系统。 官方不推荐使用第二种命令 hadoop dfs，有些 Hadoop 版本中已将这种命令弃用。 语法1hadoop fs [genericOptions] [commandOptions] 参数说明 HDFS 常用命令 说明 hadoop fs -ls 显示指定文件的详细信息 hadoop fs -cat 将指定文件的内容输出到标准输出 hadoop fs touchz 创建一个指定的空文件 hadoop fs -mkdir [-p] 创建指定的一个或多个文件夹，-p 选项用于递归创建 hadoop fs -cp 将文件从源路径复制到目标路径 hadoop fs -mv 将文件从源路径移动到目标路径 hadoop fs -rm 删除指定的文件，只删除非空目录和文件 hadoop fs -rm -r 删除指定的文件夹及其下的所有文件，-r 表示递归删除子目录 hadoop fs -chown 改变指定文件的所有者，该命令仅适用于超级用户 hadoop fs -chmod 将指定的文件权限更改为可执行文件，该命令仅适用于超级用户和文件所有者 hadoop fs -get 复制指定的文件到本地文件系统指定的文件或文件夹 hadoop fs -put 从本地文件系统中复制指定的单个或多个源文件到指定的目标文件系统 hadoop fs -moveFromLocal 与 -put 命令功能相同，但是文件上传结束后会删除源文件 hadoop fs -copyFromLocal 与 -put 命令功能相同，将本地源文件复制到路径指定的文件或文件夹中 hadoop fs -copyToLocal 与 -get命令功能相同，将目标文件复制到本地文件或文件夹中 hadoop网站： https://xiaoxiaogua.github.io/2019/03/24/YARN-Scheduler/ https://blog.csdn.net/qq_26442553/article/details/117284107 https://blog.csdn.net/shudaqi2010/article/details/114528809 https://www.cnblogs.com/yinzhengjie/p/13383344.html https://bbs.huaweicloud.com/blogs/218022 https://cloud.tencent.com/developer/article/1195056 kafka网站： https://stackoverflow.com/questions/34188574/is-the-group-option-deprecated-from-kafka-console-consumer-tool-if-so-how-ca https://blog.csdn.net/qq_29116427/article/details/80206125 flink网站： https://www.jianshu.com/p/27fa3d590a62 https://zhuanlan.zhihu.com/p/50845911 https://blog.csdn.net/chentangdan2377/article/details/101000408 https://blog.csdn.net/L13763338360/article/details/110873662 https://blog.51cto.com/u_15080019/2653853 https://blog.csdn.net/weixin_33648811/article/details/112103174 https://cloud.tencent.com/developer/article/1500184 https://www.jianshu.com/p/aa00be723f23 https://www.cnblogs.com/gentlescholar/p/15044085.html https://dragonlsl.blog.csdn.net/article/details/105823127 https://www.sohu.com/a/363674737_120342237 redis网站： https://www.cnblogs.com/wei-zw/p/9163687.html https://www.liaoxuefeng.com/wiki/1252599548343744/1282386499207201?luicode=10000011&amp;lfid=1076031658384301&amp;featurecode=newtitl&amp;u=https%3A%2F%2Fwww.liaoxuefeng.com%2Fwiki%2F1252599548343744%2F1282386499207201 https://cloud.tencent.com/developer/article/1683498 线程池网站： https://blog.csdn.net/u010235716/article/details/90059966 https://www.cnblogs.com/meijsuger/p/11492388.html https://www.cnblogs.com/warehouse/p/10732965.html SpringBoot： https://ashiamd.github.io/docsify-notes/#/study/SpringBoot/README","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://example.com/tags/hadoop/"}]},{"title":"Java 线程池","slug":"java-threadpool","date":"2021-01-01T11:44:47.000Z","updated":"2021-04-09T08:01:39.739Z","comments":true,"path":"2021/01/01/java-threadpool/","link":"","permalink":"http://example.com/2021/01/01/java-threadpool/","excerpt":"","text":"线程池的理解线程池是预先创建线程的一种技术，线程池在还没有任务到来之前，事先创建一定数量的线程，放入空闲队列中，然后对这些资源进行复用，从而减少频繁的创建和销毁对象。 系统启动一个新线程的成本是比较高的，因为它涉及与操作系统交互。在这种情形下，使用线程池可以很好地提高性能，尤其是当程序中需要创建大量生存期很短暂的线程时，更应该考虑使用线程池。 与数据库连接池类似的是，线程池在系统启动时即创建大量空闲的线程，程序将一个 Runnable 对象或 Callable 对象传给线程池，线程池就会启动一个线程来执行它们的 run() 或 call() 方法， 当 run() 或 call() 方法执行结束后， 该线程并不会死亡，而是再次返回线程池中成为空闲状态，等待执行下一个 Runnable 对象的 run() 或 call() 方法。 总结：由于系统创建和销毁线程都是需要时间和系统资源开销，为了提高性能，才考虑使用线程池。线程池会在系统启动时就创建大量的空闲线程，然后等待新的线程调用，线程执行结束并不会销毁，而是重新进入线程池，等待再次被调用。这样子就可以减少系统创建启动和销毁线程的时间，提高系统的性能。 线程池的使用使用 Executors 创建线程池 Executor 是线程池的顶级接口，接口中只定义了一个方法 void execute(Runnable command);，线程池的操作方法都是定义在 ExecutorService 子接口中的，所以说 ExecutorService 是线程池真正的接口。 newSingleThreadExecutor创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; newFixedThreadPool创建固定大小的线程池，每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到是大值就会保持不变。如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 123456public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);&#125; newCachedThreadPool创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲 (60 秒不执行任务) 的线程。当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对钱程池大小做限制，线程池大小完全依赖于操作系统 (或者说 JVM) 能够创建的最大线程大小。 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 使用 ThreadPoolExecutor 创建线程池123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 构造函数参数说明corePoolSize：核心线程数大小，当线程数小于 corePoolSize 的时候，会创建线程执行 runnable。 maximumPoolSize：最大线程数， 当线程数大于等于 corePoolSize 的时候，会把 runnable 放入 workQueue 中。 keepAliveTime：保持存活时间，当线程数大于 corePoolSize 的时候，空闲线程能保持的最大时间。 unit：时间单位。 workQueue：保存任务的阻塞队列。 threadFactory：创建线程的工厂。 handler：拒绝策略。 任务执行顺序 当线程数小于 corePoolSize 时，创建线程执行新任务。 当线程数大于等于 corePoolSize，并且 workQueue 没有满时，新任务放入 workQueue 中。 当线程数大于等于 corePoolSize，并且 workQueue 满时，新任务创建新线程运行，但线程总数要小于 maximumPoolSize。 当线程总数等于 maximumPoolSize，并且 workQueue 满时，执行 handler 的 rejectedExecution，也就是拒绝策略。 阻塞队列阻塞队列是一个在队列基础上又支持了两个附加操作的队列： 支持阻塞的插入方法：队列满时，队列会阻塞插入元素的线程，直到队列不满。 支持阻塞的移除方法：队列空时，获取元素的线程会等待队列变为非空。 阻塞队列的应用场景阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。简而言之，阻塞队列是生产者用来存放元素、消费者获取元素的容器。 阻塞队列的方法在阻塞队列不可用的时候，上述两个附加操作提供了四种处理方法： 方法处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e,time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() 不可用 不可用 阻塞队列的类型jdk 7 提供了 7 个阻塞队列，如下： ArrayBlockingQueue：数组结构组成的有界阻塞队列。 此队列按照先进先出 (FIFO) 的原则对元素进行排序，但是默认情况下不保证线程公平的访问队列，即如果队列满了，那么被阻塞在外面的线程对队列访问的顺序是不能保证线程公平 (即先阻塞，先插入) 的。 LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。 此队列按照先出先进的原则对元素进行排序。 PriorityBlockingQueue：支持优先级的无界阻塞队列。 DelayQueue：支持延时获取元素的无界阻塞队列，即可以指定多久才能从队列中获取当前元素。 SynchronousQueue：不存储元素的阻塞队列，每一个 put 必须等待一个 take 操作，否则不能继续添加元素。并且他支持公平访问队列。 LinkedTransferQueue：由链表结构组成的无界阻塞 TransferQueue 队列。 相对于其他阻塞队列，多了 tryTransfer 和 transfer 方法： transfer方法：如果当前有消费者正在等待接收元素 (take 或者待时间限制的 poll 方法)，transfer 可以把生产者传入的元素立刻传给消费者。如果没有消费者等待接收元素，则将元素放在队列的 tail 节点，并等到该元素被消费者消费了才返回。 tryTransfer方法：用来试探生产者传入的元素能否直接传给消费者。如果没有消费者在等待，则返回 false。和上述方法的区别是该方法无论消费者是否接收，方法立即返回，而 transfer 方法是必须等到消费者消费了才返回。 LinkedBlockingDeque：链表结构的双向阻塞队列，优势在于多线程入队时，减少一半的竞争。 拒绝策略当队列和线程池都满了，说明线程池处于饱和的状态，那么必须采取一种策略处理提交的新任务。ThreadPoolExecutor 默认有四个拒绝策略： ThreadPoolExecutor.AbortPolicy()：默认策略，直接抛出异常 RejectedExecutionException。 java.util.concurrent.RejectedExecutionException： 当线程池 ThreadPoolExecutor 执行方法 shutdown () 之后，再向线程池提交任务的时候，如果配置的拒绝策略是 AbortPolicy ，这个异常就会抛出来。 当设置的任务缓存队列过小的时候，或者说，线程池里面所有的线程都在干活（线程数等于 maxPoolSize)，并且任务缓存队列也已经充满了等待的队列， 这个时候，再向它提交任务，也会抛出这个异常。 ThreadPoolExecutor.CallerRunsPolicy()：直接调用 run () 方法并且阻塞执行。 ThreadPoolExecutor.DiscardPolicy()：不处理，直接丢弃后来的任务。 ThreadPoolExecutor.DiscardOldestPolicy()：丢弃在队列中队首的任务，并执行当前任务。 当然可以继承 RejectedExecutionHandler 来自定义拒绝策略。 线程池参数选择CPU 密集型：线程池的大小推荐为 CPU 数量 +1。CPU 数量可以根据 Runtime.getRuntime().availableProcessors() 方法获取。 IO 密集型：CPU 数量 * CPU 利用率 * (1 + 线程等待时间 / 线程 CPU 时间)。 混合型：将任务分为 CPU 密集型和 IO 密集型，然后分别使用不同的线程池去处理，从而使每个线程池可以根据各自的工作负载来调整。 阻塞队列：推荐使用有界队列，有界队列有助于避免资源耗尽的情况发生。 拒绝策略：默认采用的是 AbortPolicy 拒绝策略，直接在程序中抛出 RejectedExecutionException 异常，因为是运行时异常，不强制 catch，但这种处理方式不够优雅。处理拒绝策略有以下几种比较推荐： 在程序中捕获 RejectedExecutionException 异常，在捕获异常中对任务进行处理。针对默认拒绝策略。 使用 CallerRunsPolicy 拒绝策略，该策略会将任务交给调用 execute 的线程执行 (一般为主线程)，此时主线程将在一段时间内不能提交任何任务，从而使工作线程处理正在执行的任务。此时提交的线程将被保存在 TCP 队列中，TCP 队列满将会影响客户端，这是一种平缓的性能降低。 自定义拒绝策略，只需要实现 RejectedExecutionHandler 接口即可。 如果任务不是特别重要，使用 DiscardPolicy 和 DiscardOldestPolicy 拒绝策略将任务丢弃也是可以的。 如果使用 Executors 的静态方法创建 ThreadPoolExecutor 对象，可以通过使用 Semaphore 对任务的执行进行限流也可以避免出现 OOM 异常。 示例1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class TestThreadPoolExecutor &#123; public static void main(String[] args) &#123; long startTimeMillis = System.currentTimeMillis(); // 构造一个线程池 ThreadPoolExecutor threadPool = new ThreadPoolExecutor(5, 6, 3, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3) ); for (int i = 1; i &lt;= 10; i++) &#123; try &#123; String task = &quot;task = &quot; + i; System.out.println(&quot;创建任务并提交到线程池中：&quot; + task); threadPool.execute(new ThreadPoolTask(task)); Thread.sleep(100); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; try &#123; // 等待所有线程执行完毕当前任务 threadPool.shutdown(); boolean loop = true; do &#123; // 等待所有线程执行完毕，当前任务结束 loop = !threadPool.awaitTermination(2, TimeUnit.SECONDS);// 等待2秒 &#125; while (loop); if (!loop) &#123; System.out.println(&quot;所有线程执行完毕&quot;); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(&quot;耗时：&quot; + (System.currentTimeMillis() - startTimeMillis)); &#125; &#125;&#125; 12345678910111213141516171819import java.io.Serializable;public class ThreadPoolTask implements Runnable, Serializable &#123; private String attachData; public ThreadPoolTask(String tasks) &#123; this.attachData = tasks; &#125; public void run() &#123; try &#123; System.out.println(&quot;开始执行：&quot; + attachData + &quot;任务，使用的线程池，线程名称：&quot; + Thread.currentThread().getName() + &quot;\\r\\n&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; attachData = null; &#125;&#125; 运行结果，可以看到线程 pool-1-thread-1 到 pool-1-thread-5 循环使用： 1234567891011121314151617181920212223242526272829303132创建任务并提交到线程池中：task = 1开始执行：task = 1任务，使用的线程池，线程名称：pool-1-thread-1创建任务并提交到线程池中：task = 2开始执行：task = 2任务，使用的线程池，线程名称：pool-1-thread-2创建任务并提交到线程池中：task = 3开始执行：task = 3任务，使用的线程池，线程名称：pool-1-thread-3创建任务并提交到线程池中：task = 4开始执行：task = 4任务，使用的线程池，线程名称：pool-1-thread-4创建任务并提交到线程池中：task = 5开始执行：task = 5任务，使用的线程池，线程名称：pool-1-thread-5创建任务并提交到线程池中：task = 6开始执行：task = 6任务，使用的线程池，线程名称：pool-1-thread-1创建任务并提交到线程池中：task = 7开始执行：task = 7任务，使用的线程池，线程名称：pool-1-thread-2创建任务并提交到线程池中：task = 8开始执行：task = 8任务，使用的线程池，线程名称：pool-1-thread-3创建任务并提交到线程池中：task = 9开始执行：task = 9任务，使用的线程池，线程名称：pool-1-thread-4创建任务并提交到线程池中：task = 10开始执行：task = 10任务，使用的线程池，线程名称：pool-1-thread-5所有线程执行完毕耗时：1014 本文参考https://segmentfault.com/a/1190000011527245 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java String 类","slug":"java-string","date":"2021-01-01T11:31:54.000Z","updated":"2021-04-09T08:01:30.594Z","comments":true,"path":"2021/01/01/java-string/","link":"","permalink":"http://example.com/2021/01/01/java-string/","excerpt":"","text":"String 的特性String：字符串，使用双引号引起来表示。 String 是一个 final 类，不可被继承。 String 继承了 Serializable、Comparable 和 CharSequence 接口。 1public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123;&#125; 实现 Serializable 接口：表示字符串是支持可序列化的。 实现 Comparable 接口 ：表示 String 可以比较大小。 String 内部定义了 final char value[] 用于存储字符串数据。 String 代表不可变的字符序列 — 不可变性。 体现在： 当对字符串重新赋值时，需要重新指定内存区域赋值，不能使用原有的 value 进行赋值。 当对现有的字符串进行连接操作时，也需要重新指定内存区域赋值。 当调用 String 的 replace() 修改原字符串中指定的字符或字符串时，也需要重新指定内存区域赋值。 通过字面量的定义 (区别于 new) 方式给一个字符串赋值，此时的字符串值声明在字符串常量池中。 字符串常量池中不会存储相同内容的字符串。 实例： 1234567891011121314151617181920212223public class Test &#123; public static void main(String[] args) &#123; String s1 = &quot;abc&quot;;// 字面量的定义方式 String s2 = &quot;abc&quot;; System.out.println(s1 == s2);// true s1 = &quot;hello&quot;; System.out.println(s1 == s2);// false System.out.println(s1);// hello System.out.println(s2);// abc System.out.println(&quot;**************************&quot;); String s3 = &quot;abc&quot;; s3 += &quot;def&quot;; System.out.println(s3);// abcdef System.out.println(s2);// abc ---&gt; 原abc没变 System.out.println(&quot;**************************&quot;); String s4 = &quot;abc&quot;; String s5 = s4.replace(&quot;a&quot;, &quot;m&quot;); System.out.println(s4);// abc ---&gt; 原abc没变 System.out.println(s5);// mbc &#125;&#125; String 对象的创建 方式一：通过字面量定义的方式，此时的字符串数据声明在方法区中的字符串常量池中。 方式二：通过 “new + 构造器” 的方式，此时变量保存的地址值，是字符串数据在堆空间中开辟空间以后所对应的地址值。 常用的几种创建方式： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Test &#123; public static void main(String[] args) &#123; // 通过字面量定义的方式：此时的s1和s2的数据javaEE，声明在方法区中的字符串常量池中 String s1 = &quot;javaEE&quot;; String s2 = &quot;javaEE&quot;; // 通过&quot;new+构造器&quot;的方式：此时的s3和s4保存的地址值，是数据在堆空间中开辟空间以后所对应的地址值 String s3 = new String(&quot;javaEE&quot;); String s4 = new String(&quot;javaEE&quot;); System.out.println(s1 == s2);// true System.out.println(s1 == s3);// false System.out.println(s1 == s4);// false System.out.println(s3 == s4);// false System.out.println(&quot;*************************&quot;); Person p1 = new Person(&quot;Tom&quot;, 12);// 通过字面量定义的方式定义的name Person p2 = new Person(&quot;Tom&quot;, 12); System.out.println(p1.name.equals(p2.name));// true System.out.println(p1.name == p2.name);// true p1.name = &quot;Jerry&quot;; System.out.println(p2.name);// Tom System.out.println(&quot;*************************&quot;); Person p3 = new Person(new String(&quot;Tom&quot;), 12);// 通过&quot;new+构造器&quot;的方式定义的name Person p4 = new Person(new String(&quot;Tom&quot;), 12); System.out.println(p3.name.equals(p4.name));// true System.out.println(p3.name == p4.name);// false &#125;&#125;class Person &#123; String name; int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125;&#125; 内存解析说明： 字符串初始化的过程： 字符串拼接： 如果是常量与常量的拼接，则结果在常量池，且常量池中不会存在相同内容的常量。 如果其中有一个是变量，则结果在堆中。 如果拼接的结果调用 intern() 方法，则返回值在常量池中。 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; String s1 = &quot;javaEE&quot;; String s2 = &quot;hadoop&quot;; String s3 = &quot;javaEEhadoop&quot;; String s4 = &quot;javaEE&quot; + &quot;hadoop&quot;; String s5 = s1 + &quot;hadoop&quot;; String s6 = &quot;javaEE&quot; + s2; String s7 = s1 + s2; String s8 = (s1 + s2).intern(); System.out.println(s3 == s4);// true System.out.println(s3 == s5);// false System.out.println(s3 == s6);// false System.out.println(s3 == s7);// false System.out.println(s3 == s8);// true System.out.println(s5 == s6);// false System.out.println(s5 == s7);// false System.out.println(s5 == s8);// false System.out.println(s6 == s7);// false System.out.println(s6 == s8);// false System.out.println(s7 == s8);// false &#125;&#125; 内存解析说明： 特殊的情况： 12345678910111213public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;javaEEhadoop&quot;; String s2 = &quot;javaEE&quot;; String s3 = s2 + &quot;hadoop&quot;; System.out.println(s1 == s3);// false final String s4 = &quot;javaEE&quot;; String s5 = s4 + &quot;hadoop&quot;; System.out.println(s1 == s5);// true System.out.println(s2 == s4);// true &#125;&#125; s4 变量被 final 修饰，实际上也就是常量，等同于 s2。 面试题 String s = new String(&quot;abc&quot;); 方式创建对象，在内存中创建了几个对象？ 两个：一个是堆空间中 new 的结构，另一个是 char[] 对应的常量池中的数据 “abc”。 String str1 = &quot;abc&quot;; 与 String str2 = new String(&quot;abc&quot;); 的区别？ 字符串常量存储在字符串常量池，目的是共享。 字符串非常量对象存储在堆中。 下面程序的运行结果是： 12345678910111213141516public class StringTest &#123; String str = new String(&quot;good&quot;); char[] ch = &#123;&#x27;t&#x27;, &#x27;e&#x27;, &#x27;s&#x27;, &#x27;t&#x27;&#125;; public void change(String str, char ch[]) &#123; str = &quot;test ok&quot;; ch[0] = &#x27;b&#x27;; &#125; public static void main(String[] args) &#123; StringTest ex = new StringTest(); ex.change(ex.str, ex.ch); System.out.println(ex.str);// good System.out.println(ex.ch);// best &#125;&#125; 值传递机制和 String 的不可变性。 String 的常用方法 int length()：返回字符串的长度，return value.length。 char charAt(int index)：返回某索引处的字符，return value[index]。 boolean isEmpty()：判断是否是空字符串，return value.length == 0。 String toLowerCase()：使用默认语言环境，将 String 中的所有字符转换为小写。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; String s2 = s1.toLowerCase();// s1不可变，仍未原来的字符串 System.out.println(s2);// helloworld &#125;&#125; String toUpperCase()：使用默认语言环境，将 String 中的所有字符转换为大写。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; String s2 = s1.toUpperCase();// s1不可变，仍未原来的字符串 System.out.println(s2);// HELLOWORLD &#125;&#125; String trim()：返回字符串的副本，忽略前导空白和尾部空白。 12345678public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot; he llo world &quot;; String s2 = s1.trim(); System.out.println(&quot;---&quot; + s1 + &quot;---&quot;);// --- he llo world --- System.out.println(&quot;---&quot; + s2 + &quot;---&quot;);// ---he llo world--- &#125;&#125; boolean equals(Object obj)：比较字符串的内容是否相同。 boolean equalsIgnoreCase(String anotherString)：比较字符串的内容是否相同，忽略大小写。 String concat(String str)：将指定字符串连接到此字符串的结尾，等价于用 “+”。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;abc&quot;; String s2 = s1.concat(&quot;def&quot;); System.out.println(s2);// abcdef &#125;&#125; int compareTo(String anotherString)：比较两个字符串的大小。 String substring(int beginIndex)：返回一个新的字符串，截取当前字符串从 beginIndex 开始到最后的一个子字符串。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; String s2 = s1.substring(2); System.out.println(s2);// lloWorld &#125;&#125; String substring(int beginIndex, int endIndex)：返回一个新字符串，截取当前字符串从 beginIndex 开始到 endIndex (不包含) 结束的一个子字符串 — 左闭右开，[beginIndex, endIndex)。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; String s2 = s1.substring(2, 6); System.out.println(s2);// lloW &#125;&#125; boolean endsWith(String suffix)：测试此字符串是否以指定的后缀结束。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.endsWith(&quot;ld&quot;));// true &#125;&#125; boolean startsWith(String prefix)：测试此字符串是否以指定的前缀开始。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.startsWith(&quot;ll&quot;));// false &#125;&#125; boolean startsWith(String prefix, int toffset)：测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.startsWith(&quot;ll&quot;, 2));// true &#125;&#125; boolean contains(CharSequence s)：当且仅当此字符串包含指定的 char 值序列时，返回 true。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.contains(&quot;wo&quot;));// false &#125;&#125; int indexOf(String str)：返回指定子字符串在此字符串中第一次出现处的索引，未找到返回 -1。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.indexOf(&quot;lo&quot;));// 3 &#125;&#125; int indexOf(String str, int fromIndex)：返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始，未找到返回 -1。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.indexOf(&quot;lo&quot;, 5));// -1 &#125;&#125; int lastIndexOf(String str)：返回指定子字符串在此字符串中最右边出现处的索引，未找到返回 -1。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;hellorworld&quot;; System.out.println(s1.lastIndexOf(&quot;or&quot;));// 7 &#125;&#125; 面试题：什么情况下，indexOf(str) 和 lastIndexOf(str) 返回值相同？ 情况一：存在唯一的一个 str。情况二：不存在 str。 int lastIndexOf(String str, int fromIndex)：返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索，未找到返回 -1。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;hellorworld&quot;; System.out.println(s1.lastIndexOf(&quot;or&quot;, 6));// 4 &#125;&#125; String replace(char oldChar, char newChar)：返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;hellorworld&quot;; String s2 = s1.replace(&quot;o&quot;, &quot;D&quot;); System.out.println(s2);// hellDrwDrld &#125;&#125; String replace(CharSequence target, CharSequence replacement)：使用指定的字面值替换序列替换此字符串所有匹配字面值目标序列的子字符串。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;hellorworld&quot;; String s2 = s1.replace(&quot;or&quot;, &quot;DE&quot;); System.out.println(s2);// hellDEwDEld &#125;&#125; String replaceAll(String regex, String replacement)：使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串。 12345678910public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;12hello34world5java7891mysql456&quot;; // 把字符串中的数字替换成,，如果结果中开头和结尾有,的话去掉 String s1 = str.replaceAll(&quot;\\\\d+&quot;, &quot;,&quot;); System.out.println(s1);// ,hello,world,java,mysql, String s2 = s1.replaceAll(&quot;^,|,$&quot;, &quot;&quot;); System.out.println(s2);// hello,world,java,mysql &#125;&#125; String replaceFirst(String regex, String replacement)：使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串。 12345678public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;12hello34world5java7891mysql456&quot;; // 把字符串中的数字替换成,，如果结果中开头和结尾有，的话去掉 String s1 = str.replaceFirst(&quot;\\\\d+&quot;, &quot;,&quot;); System.out.println(s1);// ,hello34world5java7891mysql456 &#125;&#125; boolean matches(String regex)：告知此字符串是否匹配给定的正则表达式。 123456789101112public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;1234d5&quot;; // 判断str字符串中是否全部由数字组成，即有1-n个数字组成 boolean matches = str.matches(&quot;\\\\d+&quot;); System.out.println(matches);// false String tel = &quot;0571-4534289&quot;; // 判断这是否是一个杭州的固定电话 boolean result = tel.matches(&quot;0571-\\\\d&#123;7,8&#125;&quot;); System.out.println(result);// true &#125;&#125; String[] split(String regex)：根据匹配给定的正则表达式来拆分此字符串。 123456789101112131415161718192021222324252627282930313233public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;hello|world|java&quot;; String[] strs = str.split(&quot;\\\\|&quot;); for (String value : strs) &#123; System.out.println(value); &#125; System.out.println(); String str2 = &quot;hello.world.java&quot;; String[] strs2 = str2.split(&quot;\\\\.&quot;); for (String s : strs2) &#123; System.out.println(s); &#125; System.out.println(); String str3 = &quot;hello-world-java&quot;; String[] strs3 = str3.split(&quot;-&quot;); for (String s : strs3) &#123; System.out.println(s); &#125; &#125;&#125;输出结果：helloworldjavahelloworldjavahelloworldjava String[] split(String regex, int limit)：根据匹配给定的正则表达式来拆分此字符串，最多不超过 limit 个，如果超过了，剩下的全部都放到最后一个元素中。 123456789101112public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;hello|world|java&quot;; String[] strs = str.split(&quot;\\\\|&quot;,2); for (String value : strs) &#123; System.out.println(value); &#125; &#125;&#125;输出结果：helloworld|java substring() 与 indexOf() 结合使用： 截取第二个 “-“ 之前的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(0, s.indexOf(&quot;-&quot;, s.indexOf(&quot;-&quot;) + 1)); System.out.println(r); &#125;&#125;输出结果：application-2005 截取第二个 “-“ 之后的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(s.indexOf(&quot;-&quot;, s.indexOf(&quot;-&quot;) + 1) + 1); System.out.println(r); &#125;&#125;输出结果：US20050154023A1-20050714 截取倒数第二个 “-“ 之前的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(0, s.lastIndexOf(&quot;-&quot;, s.lastIndexOf(&quot;-&quot;) - 1)); System.out.println(r); &#125;&#125;输出结果：application-2005 截取倒数第二个 “-“ 之后的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(s.lastIndexOf(&quot;-&quot;, s.lastIndexOf(&quot;-&quot;) - 1) + 1); System.out.println(r); &#125;&#125;输出结果：US20050154023A1-20050714 String 与其他结构之间的转换String 与基本数据类型/包装类之间的转换 字符串转换为基本数据类型/包装类 Integer 包装类的 public static int parseInt(String s)：可以将由 “数字” 字符组成的字符串，转换为整型。 类似地，使用 java.lang 包中的 Byte、Short、Long、Float、Double 类调相应的类方法可以将由 “数字” 字符组成的字符串，转换为相应的基本数据类型。 基本数据类型/包装类转换为字符串 调用 String 类的 public String valueOf(int n) 可将 int 型转换为字符串。 相应的 valueOf(byte b)、valueOf(long l)、valueOf(float f)、valueOf(doubled)、valueOf(boolean b) 可将参数的相应类型转换为字符串。 String 与字符数组 (char[]) 之间的转换 字符串转换为字符数组 public char[] toCharArray()：将字符串中的全部字符存放在一个字符数组中的方法。 123456789public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;hello|world|java&quot;; char[] chars = str.toCharArray(); for (char c : chars) &#123; System.out.println(c); &#125; &#125;&#125; public void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin)：提供了将指定索引范围内的字符串存放到数组中的方法。 字符数组转换为字符串 String 类的构造器：String(char[]) 和 String(char[], int offset, intlength) 分别用字符数组中的全部字符和部分字符创建字符串对象。 1234567public class StringTest &#123; public static void main(String[] args) &#123; char[] arr = new char[]&#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;; String str = new String(arr); System.out.println(str); &#125;&#125; String 与字节数组 (byte[]) 之间的转换 字符串转换为字节数组 编码：String —&gt; byte[]，字符串 —&gt; 字节，看得懂的 —&gt; 看不懂的二进制数据。 public byte[] getBytes()：使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到新的 byte 数组中。 public byte[] getBytes(String charsetName)：使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到新的 byte 数组中。 字节数组转换为字符串 解码：byte[] —&gt; String，字节 —&gt; 字符串，看不懂的二进制数据 —&gt; 看得懂的。编码的逆过程。 String(byte[])：通过使用平台的默认字符集解码指定的 byte 数组，构造一个新的 String。 String(byte[] ，int offset ，int length)： ：用指定的字节数组的一部分，即从数组起始位置 offset 开始，取 length 个字节构造一个字符串对象。 实例： 1234567891011121314151617181920212223242526272829public class StringTest &#123; public static void main(String[] args) &#123; String str1 = &quot;abc123ABC中国&quot;; byte[] bytes = str1.getBytes();// 使用默认的字符集进行编码，此处是UTF-8 System.out.println(Arrays.toString(bytes));// [97, 98, 99, 49, 50, 51, 65, 66, 67, -28, -72, -83, -27, -101, -67] byte[] gbks = null; try &#123; gbks = str1.getBytes(&quot;GBK&quot;);// 使用GBK进行编码 System.out.println(Arrays.toString(gbks));// [97, 98, 99, 49, 50, 51, 65, 66, 67, -42, -48, -71, -6] &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;*********************************&quot;); String str2 = new String(bytes);// 使用默认的字符集进行解码，此处是UTF-8 System.out.println(str2);// abc123ABC中国 String str4 = new String(gbks); System.out.println(str4);// abc123ABC�й�，出现乱码，原因：编码集和解码集不一致 try &#123; String gbk = new String(gbks, &quot;GBK&quot;); System.out.println(gbk);// abc123ABC中国，因为编码集和解码集一致，所以不会出现乱码 &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 解码时，要求解码使用的字符集必须与编码时使用的字符集一致，否则会出现乱码。 String 相关的算法题目 模拟一个 trim 方法，去除字符串两端的空格。 1234567891011121314151617181920212223242526272829public class Test &#123; public static String myTrim(String str) &#123; if (str != null) &#123; // 用于记录从前往后首次索引位置不是空格的位置的索引 int start = 0; // 用于记录从后往前首次索引位置不是空格的位置的索引 int end = str.length() - 1; while (start &lt; end &amp;&amp; str.charAt(start) == &#x27; &#x27;) &#123; start++; &#125; while (start &lt; end &amp;&amp; str.charAt(end) == &#x27; &#x27;) &#123; end--; &#125; if (str.charAt(start) == &#x27; &#x27;) &#123; return &quot;&quot;; &#125; return str.substring(start, end + 1); &#125; return null; &#125; public static void main(String[] args) &#123; String s = myTrim(&quot; abc 123 d &quot;); System.out.println(s); &#125;&#125; 将一个字符串中指定部分进行反转。比如 “abcdefg” 反转为 “abfedcg”。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Test &#123; // 方式一： public static String reverse1(String str, int start, int end) &#123; if (str != null) &#123; char[] charArray = str.toCharArray(); for (int i = start, j = end; i &lt; j; i++, j--) &#123; char temp = charArray[i]; charArray[i] = charArray[j]; charArray[j] = temp; &#125; return new String(charArray); &#125; return null; &#125; // 方式二： public static String reverse2(String str, int start, int end) &#123; String newStr = str.substring(0, start); for (int i = end; i &gt;= start; i--) &#123; newStr += str.charAt(i); &#125; newStr += str.substring(end + 1); return newStr; &#125; // 方式三：推荐(相较于方式二做的改进) public static String reverse3(String str, int start, int end) &#123; StringBuilder newStr = new StringBuilder(str.length()); newStr.append(str, 0, start); for (int i = end; i &gt;= start; i--) &#123; newStr.append(str.charAt(i)); &#125; newStr.append(str.substring(end + 1)); return newStr.toString(); &#125; public static void main(String[] args) &#123; String str = &quot;abcdefg&quot;; String str1 = reverse3(str, 2, 5); System.out.println(str1);// abfedcg &#125;&#125; 获取一个字符串在另一个字符串中出现的次数。比如：获取 “ab” 在 “abkkcadkabkebfkabkskab” 中出现的次数。 123456789101112131415161718192021222324252627public class Test &#123; public static int getCount(String mainStr, String subStr) &#123; if (mainStr.length() &gt;= subStr.length()) &#123; int count = 0; int index = 0; /*// 方式一： while ((index = mainStr.indexOf(subStr)) != -1) &#123; count++; mainStr = mainStr.substring(index + subStr.length()); &#125;*/ // 改进： while ((index = mainStr.indexOf(subStr, index)) != -1) &#123; index += subStr.length(); count++; &#125; return count; &#125; return 0; &#125; public static void main(String[] args) &#123; String str1 = &quot;cdabkkcadkabkebfkabkskab&quot;; String str2 = &quot;ab&quot;; int count = getCount(str1, str2); System.out.println(count); &#125;&#125; 获取两个字符串中最大相同子串。比如：str1 = &quot;abcwerthelloyuiodef&quot;; str2 = &quot;cvhellobnm&quot;;。提示：将短的那个串进行长度依次递减的子串与较长的串比较。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class Test &#123; // 如果只存在一个最大长度的相同子串 public static String getMaxSameSubString(String str1, String str2) &#123; if (str1 != null &amp;&amp; str2 != null) &#123; String maxStr = (str1.length() &gt; str2.length()) ? str1 : str2; String minStr = (str1.length() &gt; str2.length()) ? str2 : str1; int len = minStr.length(); for (int i = 0; i &lt; len; i++) &#123;// 此层循环决定要去几个字符 for (int x = 0, y = len - i; y &lt;= len; x++, y++) &#123; if (maxStr.contains(minStr.substring(x, y))) &#123; return minStr.substring(x, y); &#125; &#125; &#125; &#125; return null; &#125; // 如果存在多个长度相同的最大相同子串 // 此时先返回String[]，后面可以用集合中的ArrayList替换，较方便 public static String[] getMaxSameSubString1(String str1, String str2) &#123; if (str1 != null &amp;&amp; str2 != null) &#123; StringBuilder strs = new StringBuilder(); String maxString = (str1.length() &gt; str2.length()) ? str1 : str2; String minString = (str1.length() &gt; str2.length()) ? str2 : str1; int len = minString.length(); for (int i = 0; i &lt; len; i++) &#123; for (int x = 0, y = len - i; y &lt;= len; x++, y++) &#123; String subString = minString.substring(x, y); if (maxString.contains(subString)) &#123; strs.append(subString).append(&quot;,&quot;); &#125; &#125; if (strs.length() != 0) &#123; break; &#125; &#125; return strs.toString().replaceAll(&quot;,$&quot;, &quot;&quot;).split(&quot;,&quot;); &#125; return null; &#125; // 如果存在多个长度相同的最大相同子串：使用ArrayList public static List&lt;String&gt; getMaxSameSubString2(String str1, String str2) &#123; if (str1 != null &amp;&amp; str2 != null) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); String maxString = (str1.length() &gt; str2.length()) ? str1 : str2; String minString = (str1.length() &gt; str2.length()) ? str2 : str1; int len = minString.length(); for (int i = 0; i &lt; len; i++) &#123; for (int x = 0, y = len - i; y &lt;= len; x++, y++) &#123; String subString = minString.substring(x, y); if (maxString.contains(subString)) &#123; list.add(subString); &#125; &#125; if (list.size() != 0) &#123; break; &#125; &#125; return list; &#125; return null; &#125; public static void main(String[] args) &#123; String str1 = &quot;abcwerthelloyuiodef&quot;; String str2 = &quot;cvhellobnmiodef&quot;; String[] strs = getMaxSameSubString1(str1, str2); System.out.println(Arrays.toString(strs)); &#125;&#125; 对字符串中的字符进行自然顺序排序。提示：① 字符串变成字符数组；② 对数组排序，选择，冒泡，Arrays.sort();；③ 将排序后的数组变成字符串。 123456789public class Test &#123; public static void main(String[] args) &#123; String str = &quot;abcwerthelloyuiodef&quot;; char[] arr = str.toCharArray(); Arrays.sort(arr); String newStr = new String(arr); System.out.println(newStr); &#125;&#125; String、StringBuffer 和 StringBuilder java.lang.StringBuffer 代表可变的字符序列，jdk 1.0 中声明，可以对字符串内容进行增删，此时不会产生新的对象。作为参数传递时，方法内部可以改变值。 1public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence &#123;&#125; java.lang.StringBuilder 和 java.lang.StringBuffer 非常类似，也代表可变的字符序列，二者提供相关功能的方法也比较类似。 1public final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence &#123;&#125; StringBuffer 类和 StringBuilder 类不同于 String，其对象必须使用构造器生成。常用以下三个构造器： StringBuffer()/StringBuilder()：初始容量为 16 的字符串缓冲区。 1234567/** * Constructs a string buffer with no characters in it and an * initial capacity of 16 characters. */public StringBuffer() &#123; super(16);&#125; 1234567/** * Constructs a string builder with no characters in it and an * initial capacity of 16 characters. */public StringBuilder() &#123; super(16);&#125; StringBuffer(int capacity)/StringBuilder(int capacity)：构造指定容量的字符串缓冲区。 1234567891011/** * Constructs a string buffer with no characters in it and * the specified initial capacity. * * @param capacity the initial capacity. * @exception NegativeArraySizeException if the &#123;@code capacity&#125; * argument is less than &#123;@code 0&#125;. */public StringBuffer(int capacity) &#123; super(capacity);&#125; 1234567891011/** * Constructs a string builder with no characters in it and an * initial capacity specified by the &#123;@code capacity&#125; argument. * * @param capacity the initial capacity. * @throws NegativeArraySizeException if the &#123;@code capacity&#125; * argument is less than &#123;@code 0&#125;. */public StringBuilder(int capacity) &#123; super(capacity);&#125; StringBuffer(String str)/StringBuilder(String str)：将内容初始化为指定字符串内容。 1234567891011/** * Constructs a string buffer initialized to the contents of the * specified string. The initial capacity of the string buffer is * &#123;@code 16&#125; plus the length of the string argument. * * @param str the initial contents of the buffer. */public StringBuffer(String str) &#123; super(str.length() + 16); append(str);&#125; 1234567891011/** * Constructs a string builder initialized to the contents of the * specified string. The initial capacity of the string builder is * &#123;@code 16&#125; plus the length of the string argument. * * @param str the initial contents of the buffer. */public StringBuilder(String str) &#123; super(str.length() + 16); append(str);&#125; StringBuffer 类的常用方法，很多方法与 String 相同，StringBuilder 类的常用方法参考 StringBuffer。 StringBuffer 类和 StringBuilder 类的方法的主要区别，举例如下： StringBuffer 类 — 同步方法： 123456@Overridepublic synchronized StringBuffer append(String str) &#123; toStringCache = null; super.append(str); return this;&#125; StringBuilder 类 — 非同步方法： 12345@Overridepublic StringBuilder append(String str) &#123; super.append(str); return this;&#125; StringBuffer append(xxx)：提供了参数可为多种类型的 append() 方法，用于进行字符串拼接。 面试题，输出结果： 12345678910111213141516171819public class ExceptionTest &#123; public static void main(String[] args) &#123; String str = null; StringBuilder sb = new StringBuilder(); System.out.println(sb); sb.append(str); System.out.println(sb.length()); System.out.println(sb); StringBuilder sb1 = new StringBuilder(str); System.out.println(sb1); &#125;&#125;输出结果：4nullException in thread &quot;main&quot; java.lang.NullPointerException at java.lang.StringBuilder.&lt;init&gt;(StringBuilder.java:112) at cn.xisun.java.base.ExceptionTest.main(ExceptionTest.java:17) 原因： ① java 里面，null 不占字节。如果一个引用指向 null，该应用就不再指向堆内存中的任何对象。并且，这个对象引用的大小是 4 个字节。 ② append() 方法如果传入 null 参数，最终执行以下方法，因此上面第 7 行和第 8 行输出结果为 4 和 null (字符串 null)。 1234567891011private AbstractStringBuilder appendNull() &#123; int c = count; ensureCapacityInternal(c + 4); final char[] value = this.value; value[c++] = &#x27;n&#x27;; value[c++] = &#x27;u&#x27;; value[c++] = &#x27;l&#x27;; value[c++] = &#x27;l&#x27;; count = c; return this;&#125; ③ new StringBuilder(str) 的代码中：super(str.length() + 16);，调用 null 的 length() 方法，会发生空指针异常。 StringBuffer delete(int start,int end)：删除指定位置的内容。 StringBuffer replace(int start, int end, String str)：把 [start,end) 位置替换为 str。 StringBuffer insert(int offset, xxx)：在指定位置插入多种类型的参数。 1234567public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer(&quot;abc123&quot;); sb.insert(3, &quot;ABC&quot;); System.out.println(sb);// abcABC123 &#125;&#125; StringBuffer reverse()：把当前字符序列逆转。 1234567public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer(&quot;abc123&quot;); sb.reverse(); System.out.println(sb);// 321cba &#125;&#125; int indexOf(String str)：返回指定子字符串在此字符串中第一次出现处的索引，未找到返回 -1。 String substring(int start)：返回一个新的字符串，截取当前字符串从 start 开始到最后的一个子字符串。 String substring(int start,int end)：返回一个新字符串，截取当前字符串从 start 开始到 end (不包含) 结束的一个子字符串 — 左闭右开，[start, end)。 1234567public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer(&quot;abc123abc&quot;); String sb2 = sb.substring(1, 5); System.out.println(sb2);// bc12 &#125;&#125; int length()：返回字符串的长度。 1234@Overridepublic int length() &#123; return count;&#125; char charAt(int n )：返回某索引处的字符。 void setCharAt(int n ,char ch)：在指定索引处插入字符。 1234567public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer(&quot;abc123abc&quot;); sb.setCharAt(1, &#x27;中&#x27;); System.out.println(sb);// a中c123abc &#125;&#125; 方法总结： 增：append，删：delete，改：setCharAt/replace，查：charAt，插：insert，长度：length，遍历：for + charAt/toString。 append、delete、replace、insert 和 reverse 这些方法，支持方法链操作，方法链的原理为： String、StringBuffer 和 StringBuilder 的异同？ String：Since jdk 1.0，不可变的字符序列；底层使用 final char[] 存储。 StringBuffer：Since jdk 1.0，可变的字符序列；线程安全的，效率低；底层使用 char[] 存储。 StringBuilder：Since jdk 1.5，可变的字符序列；线程不安全的，效率高；底层使用 char[] 存储。 StringBuffer 和 StringBuilder 的扩容问题： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; String str = new String();// final char[] value = new char[0]; String str1 = new String(&quot;abc&quot;);// final char[] value = new char[]&#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;; StringBuffer sb = new StringBuffer();// char[] value = new char[16]; 底层创建了一个长度是16的char数组 System.out.println(sb.length());// 0 sb.append(&#x27;a&#x27;);// value[0] = &#x27;a&#x27;; sb.append(&#x27;b&#x27;);// value[1] = &#x27;b&#x27;; sb.append(&#x27;c&#x27;);// value[1] = &#x27;c&#x27;; System.out.println(sb.length());// 3 StringBuffer sb1 = new StringBuffer(&quot;abc&quot;);// char[] value = new char[&quot;abc&quot;.length() + 16]; System.out.println(sb1.length());// 3 &#125;&#125; 扩容问题：如果要添加的数据底层数组盛不下了，那就需要扩容底层的数组。 扩容方法： 1234567private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) &#123; value = Arrays.copyOf(value, newCapacity(minimumCapacity)); &#125;&#125; 123456789101112131415161718192021/** * Returns a capacity at least as large as the given minimum capacity. * Returns the current capacity increased by the same amount + 2 if * that suffices. * Will not return a capacity greater than &#123;@code MAX_ARRAY_SIZE&#125; * unless the given minimum capacity is greater than that. * * @param minCapacity the desired minimum capacity * @throws OutOfMemoryError if minCapacity is less than zero or * greater than Integer.MAX_VALUE */private int newCapacity(int minCapacity) &#123; // overflow-conscious code int newCapacity = (value.length &lt;&lt; 1) + 2; if (newCapacity - minCapacity &lt; 0) &#123; newCapacity = minCapacity; &#125; return (newCapacity &lt;= 0 || MAX_ARRAY_SIZE - newCapacity &lt; 0) ? hugeCapacity(minCapacity) : newCapacity;&#125; 默认情况下，扩容为原来容量的 2 倍 + 2，同时将原来数组中的元素复制到新的数组中。 指导意义：开发中，如果知道创建的字符串的长度，建议使用 StringBuffer(int capacity) 或 StringBuilder(int capacity)，即可能得避免扩容的发生，这样可以提高效率。 String、StringBuffer 和 StringBuilder 三者的效率测试： 123456789101112131415161718192021222324252627282930313233343536public class Test &#123; public static void main(String[] args) &#123; // 初始设置 String text = &quot;&quot;; StringBuffer buffer = new StringBuffer(&quot;&quot;); StringBuilder builder = new StringBuilder(&quot;&quot;); long startTime = 0L; long endTime = 0L; // 开始对比 startTime = System.currentTimeMillis(); for (int i = 0; i &lt; 20000; i++) &#123; text = text + i; &#125; endTime = System.currentTimeMillis(); System.out.println(&quot;String的执行时间：&quot; + (endTime - startTime)); startTime = System.currentTimeMillis(); for (int i = 0; i &lt; 20000; i++) &#123; buffer.append(String.valueOf(i)); &#125; endTime = System.currentTimeMillis(); System.out.println(&quot;StringBuffer的执行时间：&quot; + (endTime - startTime)); startTime = System.currentTimeMillis(); for (int i = 0; i &lt; 20000; i++) &#123; builder.append(String.valueOf(i)); &#125; endTime = System.currentTimeMillis(); System.out.println(&quot;StringBuilder的执行时间：&quot; + (endTime - startTime)); &#125;&#125;输出结果：StringBuffer的执行时间：6StringBuilder的执行时间：3String的执行时间：1713 效率从高到低排列：StringBuilder &gt; StringBuffer &gt; String。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"redis 命令","slug":"redis-command","date":"2020-12-31T09:10:36.000Z","updated":"2021-07-07T06:58:42.830Z","comments":true,"path":"2020/12/31/redis-command/","link":"","permalink":"http://example.com/2020/12/31/redis-command/","excerpt":"","text":"查询当前库 key 的个数info可以看到所有库的key数量 dbsize则是当前库key的数量 keys *这种数据量小还可以，大的时候可以直接搞死生产环境。 dbsize和keys *统计的key数可能是不一样的，如果没记错的话，keys *统计的是当前db有效的key，而dbsize统计的是所有未被销毁的key（有效和未被销毁是不一样的，具体可以了解redis的过期策略）","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}]},{"title":"Kafka 命令行工具","slug":"kafka-command","date":"2020-12-30T02:19:35.000Z","updated":"2021-02-26T02:17:31.813Z","comments":true,"path":"2020/12/30/kafka-command/","link":"","permalink":"http://example.com/2020/12/30/kafka-command/","excerpt":"","text":"查看 Kafka topic 列表命令，返回 topic 名字列表 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --list 创建 Kafka topic 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181,hadoopdatanode2:2181,hadoopdatanode3:2181 --create --partitions 6 --replication-factor 2 --topic patent-grant 查看 Kafka 指定 topic 的详情命令，返回该 topic 的 parition 数量、replica 因子以及每个 partition 的 leader、replica 信息 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --describe --topic patent-grant 查看 Kafka 指定 topic 各 partition 的 offset 信息命令，–time 参数为 -1 时，表示各分区最大的 offset，为 -2 时，表示各分区最小的 offset 1$ ~/kafka_2.12-2.6.0/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list hadoopdatanode1:9092 --time -1 --topic patent-grant 删除 Kafka topic 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --delete -topic patent-grant 只有 topic 不再被使用时，才能被删除。 修改 kafka topic 的数据保存时间： 1$ ~/kafka_2.12-2.6.0/bin/kafka-configs.sh --bootstrap-server hadoopdatanode1:9092 --alter --entity-type topics --entity-name extractor-patent --add-config retention.ms=2592000000 kafka 中默认消息的保留时间是 7 天，若想更改，需在配置文件 server.properties 里更改选项：log.retention.hours=168。 如果需要对某一个主题的消息存留的时间进行变更，但不影响其他主题，并且 kafka 集群不用重启，则使用上面的命令修改，该命令设置的是 30 天。 查看 kafka topic 配置信息： 1$ ~/kafka_2.12-2.6.0/bin/kafka-configs.sh --bootstrap-server hadoopdatanode1:9092 --describe --entity-type topics --entity-name extractor-patent 如果使用的是默认配置，显示： 1Dynamic configs for topic extractor-patent are: 如果更改了配置，显示： 12Dynamic configs for topic extractor-patent are: retention.ms=2592000000 sensitive=false synonyms=&#123;DYNAMIC_TOPIC_CONFIG:retention.ms=2592000000&#125; 查看 kafka consumer group 命令，返回 consumer group 名字列表 (新版信息保存在 broker 中，老版信息保存在 zookeeper 中，二者命令不同) 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --list 老版命令：~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --zookeeper hadoopdatanode1:2181 --list 查看 Kafka 指定 consumer group 的详情命令，返回 consumer group 对应的 topic 信息、当前消费的 offset、总 offset、剩余待消费 offset 等信息 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --describe --group log-consumer 重置 Kafka 指定 consumer group 消费的 topic 的 offset 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --reset-offsets -to-offset 0 --execute --topic patent-app --group log-consumer 删除 Kafka 指定 consumer group 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --delete --group log-consumer 消费 Kafka 指定 topic 的内容命令 kafka-console-consumer.sh 脚本是一个简易的消费者控制台。该 shell 脚本的功能通过调用 kafka.tools 包下的 ConsoleConsumer 类，并将提供的命令行参数全部传给该类实现。 参数说明： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.shThis tool helps to read data from Kafka topics and outputs it to standard output.Option Description ------ ----------- --bootstrap-server &lt;String: server to REQUIRED: The server(s) to connect to. connect to&gt; --consumer-property &lt;String: A mechanism to pass user-defined consumer_prop&gt; properties in the form key=value to the consumer. --consumer.config &lt;String: config file&gt; Consumer config properties file. Note that [consumer-property] takes precedence over this config. --enable-systest-events Log lifecycle events of the consumer in addition to logging consumed messages. (This is specific for system tests.) --formatter &lt;String: class&gt; The name of a class to use for formatting kafka messages for display. (default: kafka.tools. DefaultMessageFormatter) --from-beginning If the consumer does not already have an established offset to consume from, start with the earliest message present in the log rather than the latest message. --group &lt;String: consumer group id&gt; The consumer group id of the consumer. --help Print usage information. --isolation-level &lt;String&gt; Set to read_committed in order to filter out transactional messages which are not committed. Set to read_uncommitted to read all messages. (default: read_uncommitted)--key-deserializer &lt;String: deserializer for key&gt; --max-messages &lt;Integer: num_messages&gt; The maximum number of messages to consume before exiting. If not set, consumption is continual. --offset &lt;String: consume offset&gt; The offset id to consume from (a non- negative number), or &#x27;earliest&#x27; which means from beginning, or &#x27;latest&#x27; which means from end (default: latest) --partition &lt;Integer: partition&gt; The partition to consume from. Consumption starts from the end of the partition unless &#x27;--offset&#x27; is specified. --property &lt;String: prop&gt; The properties to initialize the message formatter. Default properties include: print.timestamp=true|false print.key=true|false print.value=true|false key.separator=&lt;key.separator&gt; line.separator=&lt;line.separator&gt; key.deserializer=&lt;key.deserializer&gt; value.deserializer=&lt;value. deserializer&gt; Users can also pass in customized properties for their formatter; more specifically, users can pass in properties keyed with &#x27;key. deserializer.&#x27; and &#x27;value. deserializer.&#x27; prefixes to configure their deserializers. --skip-message-on-error If there is an error when processing a message, skip it instead of halt. --timeout-ms &lt;Integer: timeout_ms&gt; If specified, exit if no message is available for consumption for the specified interval. --topic &lt;String: topic&gt; The topic id to consume on. --value-deserializer &lt;String: deserializer for values&gt; --version Display Kafka version. --whitelist &lt;String: whitelist&gt; Regular expression specifying whitelist of topics to include for consumption. 参数说明参考：https://blog.csdn.net/qq_29116427/article/details/80206125 从头开始消费： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --from-beginning --topic log-collect 从头开始消费前 10 条消息，并显示 key： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --from-beginning --max-messages 10 --property print.key=true --topic log-collect 从指定分区、指定 offset 开始消费： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --partition 0 --offset 219000 --topic log-collect 从尾开始消费，必须指定分区： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --partition 0 --offset latest --topic log-collect","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"shell 脚本中的一些方法","slug":"shell","date":"2020-12-22T09:16:35.000Z","updated":"2021-01-01T08:13:35.647Z","comments":true,"path":"2020/12/22/shell/","link":"","permalink":"http://example.com/2020/12/22/shell/","excerpt":"","text":"字符串截取shell 截取字符串通常有两种方式：从指定位置开始截取和从指定字符 (子字符串) 开始截取。 从指定位置开始截取两个参数：起始位置，截取长度。 从字符串左边开始计数1$&#123;string: start :length&#125; 其中，string 是要截取的字符串，start 是起始位置 (从左边开始，从 0 开始计数)，length 是要截取的长度 (如果省略表示直到字符串的末尾)。 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 2: 9&#125;biancheng 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 2&#125;biancheng.net 从字符串右边开始计数1$&#123;string: 0-start :length&#125; 0- 是固定的写法，专门用来表示从字符串右边开始计数。 从左边开始计数时，起始数字是 0；从右边开始计数时，起始数字是 1。计数方向不同，起始数字也不同。 不管从哪边开始计数，截取方向都是从左到右。 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 0-13: 9&#125;biancheng 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 0-13&#125;biancheng.net 从右边数，b 是第 13 个字符。 从指定字符 (子字符串) 开始截取这种截取方式无法指定字符串长度，只能从指定字符 (子字符串) 截取到字符串末尾。shell 可以截取指定字符 (子字符串) 右边的所有字符，也可以截取左边的所有字符。 使用 # 截取指定字符 (子字符串)右边字符1$&#123;string#*chars&#125; 其中，string 表示要截取的字符，chars 是指定的字符 (子字符串)，* 是通配符的一种，表示任意长度的字符串。*chars 连起来使用的意思是：忽略左边的所有字符，直到遇见 chars (chars 不会被截取)。 从左往右看。 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*:&#125;//c.biancheng.net/index.html 以下写法也可以得到同样的结果： 12echo $&#123;url#*p:&#125;echo $&#123;url#*ttp:&#125; 如果不需要忽略 chars 左边的字符，那么也可以不写 *，例如： 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#http://&#125;c.biancheng.net/index.html 注意，以上写法遇到第一个匹配的字符 (子字符串) 就结束了。例如： 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*/&#125;/c.biancheng.net/index.html url 字符串中有三个 /，输出结果表明，shell 遇到第一个 / 就匹配结束了。 如果希望直到最后一个指定字符 (子字符串) 再匹配结束，那么可以使用 ##，具体格式为： 1$&#123;string##*chars&#125; 123456789#!/bin/bashurl=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*/&#125; # 结果为 /c.biancheng.net/index.htmlecho $&#123;url##*/&#125; # 结果为 index.htmlstr=&quot;---aa+++aa@@@&quot;echo $&#123;str#*aa&#125; # 结果为 +++aa@@@echo $&#123;str##*aa&#125; # 结果为 @@@ 使用 % 截取指定字符 (子字符串)左边字符1$&#123;string%chars*&#125; 注意 * 的位置，因为要截取 chars 左边的字符，而忽略 chars 右边的字符，所以 * 应该位于 chars 的右侧。其他方面 % 和 # 的用法相同。 从右往左看。 123456789#!/bin/bashurl=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url%/*&#125; # 结果为 http://c.biancheng.netecho $&#123;url%%/*&#125; # 结果为 http:str=&quot;---aa+++aa@@@&quot;echo $&#123;str%aa*&#125; # 结果为 ---aa+++echo $&#123;str%%aa*&#125; # 结果为 --- 汇总 格式 说明 ${string: start :length} 从 string 字符串的左边第 start 个字符开始，向右截取 length 个字符。 ${string: start} 从 string 字符串的左边第 start 个字符开始截取，直到最后。 ${string: 0-start :length} 从 string 字符串的右边第 start 个字符开始，向右截取 length 个字符。 ${string: 0-start} 从 string 字符串的右边第 start 个字符开始截取，直到最后。 ${string#*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string##*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string%*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 ${string%%*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 参考： http://c.biancheng.net/view/1120.html 字符串拼接12345678910111213141516171819202122#!/bin/bashname=&quot;Shell&quot;url=&quot;http://c.biancheng.net/shell/&quot;str1=$name$url # 中间不能有空格str2=&quot;$name $url&quot; # 如果被双引号包围，那么中间可以有空格str3=$name&quot;: &quot;$url # 中间可以出现别的字符串str4=&quot;$name: $url&quot; # 这样写也可以str5=&quot;$&#123;name&#125;Script: $&#123;url&#125;index.html&quot; # 这个时候需要给变量名加上大括号echo $str1echo $str2echo $str3echo $str4echo $str5Shellhttp://c.biancheng.net/shell/Shell http://c.biancheng.net/shell/Shell: http://c.biancheng.net/shell/Shell: http://c.biancheng.net/shell/ShellScript: http://c.biancheng.net/shell/index.html 对于第 7 行代码，$name 和 $url 之间之所以不能出现空格，是因为当字符串不被任何一种引号包围时，遇到空格就认为字符串结束了，空格后边的内容会作为其他变量或者命令解析。 对于第 10 行代码，加 { } 是为了帮助解释器识别变量的边界。 字符串分割以空格为分隔符比如有一个变量 “123 456 789”，要求以空格为分隔符把这个变量分隔，并把分隔后的字段分别赋值给变量，即 a=123；b=456；c=789。共有3中方法：方法一：先定义一个数组，然后把分隔出来的字段赋值给数组中的每一个元素方法二：通过 eval+ 赋值的方式方法三：通过多次 awk 把每个字段赋值 1234567891011121314151617181920212223242526272829#!/bin/basha=&quot;123 456 789&quot;# 方法一：通过数组的方式declare -a arrindex=0for i in $(echo $a | awk &#x27;&#123;print $1,$3&#125;&#x27;)do arr[$index]=$i let &quot;index+=1&quot;doneecho $&#123;arr[0]&#125; # 结果为 123echo $&#123;arr[1]&#125; # 结果为 789# 方法二：通过eval+赋值的方式b=&quot;&quot;c=&quot;&quot;eval $(echo $a | awk &#x27;&#123; printf(&quot;b=%s;c=%s&quot;,$2,$1)&#125;&#x27;)echo $b # 结果为 456echo $c # 结果为 123# 方法三：通过多次awk赋值的方式m=&quot;&quot;n=&quot;&quot;m=`echo $a | awk &#x27;&#123;print $1&#125;&#x27;`n=`echo $a | awk &#x27;&#123;print $2&#125;&#x27;`echo $m # 结果为 123echo $n # 结果为 456 指定分隔符123456789101112131415161718192021222324252627#!/bin/bashstring=&quot;hello,shell,haha&quot;# 方法一 array=($&#123;string//,/ &#125;) for var in $&#123;array[@]&#125;do echo $vardone# 方法二IFS=&quot;,&quot;OLD_IFS=&quot;$IFS&quot;IFS=&quot;$OLD_IFS&quot;array2=($string)for var2 in $&#123;array2[@]&#125;do echo $var2donehelloshellhahahelloshellhaha 变量赋值反引号： 1var=`command` $()： 1var=$(command) 例如： 1234567$ A=`date`$ echo $AFri Dec 25 20:02:30 CST 2020 # 变量A存放了date命令的执行结果$ B=$(date)$ echo $BFri Dec 25 20:03:12 CST 2020 # 变量B存放了date命令的执行结果 注意：= 号前后不要有空格。 参考： https://book.51cto.com/art/201411/457601.htm 判断文件夹是否存在1234567#!/bin/bashif [ ! -d testgrid ];then mkdir testgridelse echo dir existfi 外部传参： 12345678910111213141516#!/bin/bash# 判断传入的参数的个数是不是一个if [ ! $# -eq 1 ];then echo param error! exit 1fi# 判断目录是不是已经存在，如果不存在则创建，存在则输出&quot;dir exist&quot; dirname=$1echo &quot;the dir name is $dirname&quot;if [ ! -d $dirname ];then mkdir $dirnameelse echo dir existfi 循环类 C 语言： 123456# !/bin/shfor ((i=1; i&lt;=100; i ++))do echo $idone in 使用： 123456# !/bin/shfor i in &#123;1..100&#125;do echo $idone seq 使用： 123456# !/bin/shfor i in `seq 1 100`do echo $idone 发送微信消息https://blog.csdn.net/whatday/article/details/105781861","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"linux 命令","slug":"linux-command","date":"2020-12-15T09:31:47.000Z","updated":"2021-12-29T01:04:33.361Z","comments":true,"path":"2020/12/15/linux-command/","link":"","permalink":"http://example.com/2020/12/15/linux-command/","excerpt":"","text":"find用于在指定目录下查找文件。默认列出当前目录及子目录下所有的文件和文件夹。 语法1find path -option [ -print ] [ -exec -ok command ] &#123;&#125; \\; -print： find 命令将匹配到的文件输出到标准输出。 -exec： find 命令对匹配的文件执行该参数所给出的 Shell 命令。 -ok： 和 -exec 的作用相同，只是更安全，在执行每个命令之前，都会给出提示，让用户来确定是否执行。 参数说明1234567891011121314151617181920212223242526272829303132333435363738394041424344-mount, -xdev 只检查和指定目录在同一个文件系统下的文件，避免列出其它文件系统中的文件。-amin n 在过去n分钟内被读取过。-anewer file 比文件file更晚被读取过的文件。-atime n 在过去n天内被读取过的文件。 -cmin n 在过去n分钟内被修改过。 -cnewer file 比文件file更新的文件。 -ctime n 在过去n天内被修改过的文件。 -empty 空的文件。 -gid n gid是n。 -group name group名称是name。 -path p | -ipath p 路径名称符合p的文件。ipath忽略大小写。 -name name | -iname name 文件名称符合name的文件。iname忽略大小写。 -size n 文件大小是n单位，b代表512位元组的区块，c表示字元数，k表示kilo bytes，w是二个位元组。 -type d|f 文件类型是d|f的文件。 -pid n process id是n的文件。 文件类型：d — 目录，f — 一般文件，c — 字型装置文件，b — 区块装置文件，p — 具名贮列，l — 符号连结，s — socket。 实例 查询当前路径下的所有目录|普通文件 12$ find ./ -type d$ find ./ -type f 查询权限为 777 的普通文件 1$ find ./ -type f -perm 777 查询 .XML 文件，且权限不为 777 1$ find ./ -type f -name &quot;*.XML&quot; ! -perm 777 查询 .XML 文件，并统计查询结果的条数 1$ find ./ -name &quot;*.XML&quot; | wc -l 查询 .XML 文件，并复制查询结果到指定路径 12$ find ./ -name &quot;*.XML&quot; | xargs -i cp &#123;&#125; ../111$ find ./ -name &quot;*.XML&quot; -exec cp &#123;&#125; ../111 \\; 此命令不同于 cp，cp *.XML ../111 命令复制的是当前路径下符合条件的所有文件，子路径的不会被复制。 查询 .XML 文件，并删除 12$ find ./ -name &quot;*.XML&quot; | xargs -i rm &#123;&#125;$ find ./ -name &quot;*.XML&quot; -exec rm &#123;&#125; \\; 此命令不同于 rm，rm *.XML 命令删除的是当前路径下符合条件的所有文件，子路径的不会被删除。 查询 .XML 文件，并将查询结果以 “File: 文件名” 的形式打印出来 12$ find ./ -name &quot;*.XML&quot; | xargs -i printf &quot;File: %s\\n&quot; &#123;&#125;$ find ./ -name &quot;*.XML&quot; -exec printf &quot;File: %s\\n&quot; &#123;&#125; \\; 将当前路径及子路径下所有 3 天前的 .XML 格式的文件复制一份到指定路径 1$ find ./ -name &quot;*.XML&quot; -mtime +3 -exec cp &#123;&#125; ../111 \\; 查询多个文件后缀类型的文件 12$ find ./ -regextype posix-extended -regex &quot;.*\\.(java|xml|XML)&quot; # 查找所有的.java、.xml和.XML文件$ find ./ -name &quot;*.java&quot; -o -name &quot;*.xml&quot; -o -name &quot;*.XML&quot; # -o选项，适用于查询少量文件后缀类型 组合查询，可以多次拼接查询条件 1234$ find ./ -name &quot;file1*&quot; -a -name &quot;*.xml&quot; # -a：与，查找以file1开头，且以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; -o -name &quot;*.xml&quot; # -o：或，查找以file1开头，或以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; -not -name &quot;*.xml&quot; # -not：非，查找以file1开头，且不以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; ! -name &quot;file2*&quot; # !：同-not 查询当前目录下文件类型为 d 的文件，不包含子目录 1$ find ./ -maxdepth 1 -type d 和正则表达式的结合使用 12$ find ./ –name &quot;[^abc]*&quot; # 在当前路径中搜索不以a、b、c开头的所有文件$ find ./ -name &quot;[A-Z0-9]*&quot; # 在当前路径中搜索以大写字母或数字开头的所有文件 正则表达式符号含义： * 代表任意字符 (可以没有字符) ? 代表任意单个字符 [] 代表括号内的任意字符，如 [abc] 可以匹配 a\\b\\c 某个字符 [a-z] 可以匹配 a-z 的某个字母 [A-Z] 可以匹配 A-Z 的某个字符 [0-9] 可以匹配 0-9 的某个数字 ^ 用在 [] 内的前缀表示不匹配 [] 中的字符 [^a-z] 表示不匹配a-z的某个字符 参考： https://www.jianshu.com/p/b30a8aa4d1f1 https://www.oracle.com/cn/technical-resources/articles/linux-calish-find.html https://www.cnblogs.com/qmfsun/p/3811142.html https://www.cnblogs.com/ay-a/p/8017419.html cat用于连接文件或标准输入并打印。这个命令常用来显示文件内容，或者将几个文件连接起来显示，或者从标准输入读取内容并显示，它常与重定向符号配合使用。 语法1cat [-AbeEnstTuv] [--help] [--version] fileName 参数说明1234567891011121314151617181920212223242526-n | --number 由1开始对所有输出的行数编号。-b | --number-nonblank 和-n相似，只不过对于空白行不编号。-s | --squeeze-blank 当遇到有连续两行以上的空白行，就代换为一行的空白行。-v | --show-nonprinting 使用^和M-符号，除了LFD和TAB之外。-E | --show-ends 在每行结束处显示$。 -T | --show-tabs 将TAB字符显示为^I。 -A | --show-all 等价于&quot;-vET&quot;。 -e 等价于&quot;-vE&quot;。 -t 等价于&quot;-vT&quot;。 实例 把 textfile1 的文档内容加上行号后输入 textfile2 这个文档里 1$ cat -n textfile1 &gt; textfile2 清空 /etc/test.txt 文档内容 1$ cat /dev/null &gt; /etc/test.txt /dev/null：在类 Unix 系统中，/dev/null 称空设备，是一个特殊的设备文件，它丢弃一切写入其中的数据 (但报告写入操作成功)，读取它则会立即得到一个 EOF。 而使用 cat $filename &gt; /dev/null 则不会得到任何信息，因为我们将本来该通过标准输出显示的文件信息重定向到了 /dev/null 中。 使用 cat $filename 1 &gt; /dev/null 也会得到同样的效果，因为默认重定向的 1 就是标准输出。 如果你对 shell 脚本或者重定向比较熟悉的话，应该会联想到 2 ，也即标准错误输出。 如果我们不想看到错误输出呢？我们可以禁止标准错误 cat $badname 2 &gt; /dev/null。 合并多个文件内容 12$ cat b1.sql b2.sql b3.sql &gt; b_all.sql$ cat *.sql &gt; merge.sql headhttps://blog.csdn.net/zmx19951103/article/details/78575265 tailhttps://blog.csdn.net/luo200618/article/details/52510638 sedhttps://www.cnblogs.com/qmfsun/p/6626361.html jqhttps://www.jianshu.com/p/dde911234761 https://blog.csdn.net/whatday/article/details/105781861 https://blog.csdn.net/qq_26502245/article/details/100191694 https://blog.csdn.net/u011641885/article/details/45559031 basename用于去掉文件名的目录和后缀。 语法12 basename NAME [SUFFIX]or: basename OPTION... NAME... 去掉 NAME 中的目录部分和后缀 SUFFIX，如果输出结果没有，则输出 SUFFIX。 参数说明1234567891011121314-a | --multiple support multiple arguments and treat each as a NAME -s | --suffix=SUFFIX remove a trailing SUFFIX; implies -a -z | --zero end each output line with NUL, not newline(默认情况下，每条输出行以换行符结尾) --help display this help and exit --version output version information and exit 实例 去除目录 12$basename /usr/bin/sortsort 12$ basename /usr/include/stdio.h stdio.h 去除目录和后缀 12$ basename /usr/include/stdio.h .hstdio 12$ basename -s .h /usr/include/stdio.hstdio 12$ basename /usr/include/stdio.h stdio.h stdio.h 去除多个目录 1234$ basename -a any1/str1 any2/str2 any3/str3str1str2str3 dirname用于去除文件名中的非目录部分，删除最后一个 “\\“ 后面的路径，显示父目录。 语法1dirname [OPTION] NAME... 如果 NAME 中不包含 /，则输出 .，即当前目录。 参数说明12345678-z | --zero end each output line with NUL, not newline --help display this help and exit --version output version information and exit 实例1234567891011$ dirname /usr/bin//usr$ dirname /usr/bin/usr$ dirname /etc//$ dirname /etc/httpd/conf/httpd.conf/etc/httpd/conf 123$ dirname dir1/str dir2/strdir1dir2 12$ dirname stdio.h. xargsxargs 是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。 xargs 可以将管道或标准输入 (stdin) 数据转换成命令行参数，也能够从文件的输出中读取数据。 xargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。 xargs 默认的命令是 echo，这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。 xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。 之所以能用到这个命令，关键是由于很多命令不支持管道来传递参数，而日常工作中有有这个必要，所以就有了 xargs 命令，例如： 12find /sbin -perm +700 | ls -l #这个命令是错误的find /sbin -perm +700 | xargs ls -l #这样才是正确的 xargs 一般是和管道一起使用。 语法1some command | xargs -item command 参数说明1234567891011121314151617181920212223242526272829303132333435363738-a file 从文件中读入作为sdtin -e flag 注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。 -p 当每次执行一个argument的时候询问一次用户。 -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。-t 表示先打印命令，然后再执行。-i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给&#123;&#125;，可以用&#123;&#125;代替。-r no-run-if-empty，当xargs的输入为空的时候则停止xargs，不用再去执行了。-s num命令行的最大字符数，指的是xargs后面那个命令的最大命令行字符数。-L num 从标准输入一次读取num行送给command命令。-l 同 -L。-d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符。-x exit的意思，主要是配合-s使用。-P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。 实例 读取输入数据重新格式化后输出 定义一个测试文件，内有多行文本数据： 123456$ cat test.txta b c d e f gh i j k l m no p qr s tu v w x y z 单行输出： 12$ cat test.txt | xargsa b c d e f g h i j k l m n o p q r s t u v w x y z -n 选项自定义多行输出： 12345678910$ cat test.txt | xargs -n3a b cd e fg h ij k lm n op q rs t uv w xy z -d 选项自定义一个定界符： 12$ echo &quot;nameXnameXnameXname&quot; | xargs -dXname name name name 结合 -n 选项使用： 123$ echo &quot;nameXnameXnameXname&quot; | xargs -dX -n2name namename name 读取 stdin，将格式化后的参数传递给命令 假设一个命令为 sk.sh 和一个保存参数的文件 arg.txt： sk.sh 命令内容： 1234#!/bin/bash# 打印出所有参数。echo $* arg.txt 文件内容： 1234$ cat arg.txtaaabbbccc xargs 的一个选项 -I，使用 -I 指定一个替换字符串 {}，这个字符串在 xargs 扩展时会被替换掉，当 -I 与 xargs 结合使用，每一个参数命令都会被执行一次： 1234$ cat arg.txt | xargs -I &#123;&#125; ./sk.sh -p &#123;&#125; -l-p aaa -l-p bbb -l-p ccc -l 复制所有图片文件到 /data/images 目录下： 1ls *.jpg | xargs -n1 -I &#123;&#125; cp &#123;&#125; /data/images 结合 find 使用 用 rm 删除太多的文件时候，可能得到一个错误信息：**/bin/rm Argument list too long.**， 用 xargs 去避免这个问题： 1$ find . -type f -name &quot;*.log&quot; -print0 | xargs -0 rm -f xargs -0 将 \\0 作为定界符。 统计一个源代码目录中所有 php 文件的行数： 1$ find . -type f -name &quot;*.php&quot; -print0 | xargs -0 wc -l 查找所有的 jpg 文件，并且压缩它们： 1$ find . -type f -name &quot;*.jpg&quot; -print | xargs tar -zcvf images.tar.gz xargs 其他应用 假如你有一个文件包含了很多你希望下载的 URL，你能够使用 xargs 下载所有链接： 1$ cat url-list.txt | xargs wget -c 参考：？？？ https://www.cnblogs.com/wangqiguo/p/6464234.html crontablinux 内置的 cron 进程能实现定时任务需求。 crontab 命令是 cron table 的简写，它是 cron 的配置文件，也可以叫它作业列表。我们可以在以下文件夹内找到相关配置文件： **/var/spool/cron/**：该目录下存放的是每个用户包括 root 的 crontab 任务，每个任务以创建者的名字命名。 /etc/crontab：该文件负责调度各种管理和维护任务。 **/etc/cron.d/**：该目录用来存放任何要执行的crontab文件或脚本。 另外，还可以把脚本放在 /etc/cron.hourly、/etc/cron.daily、/etc/cron.weekly、/etc/cron.monthly 目录中，让它每小时/天/星期/月执行一次。 语法1crontab [-u user] [ -e | -l | -r ] 参数说明1234567891011121314151617-u user 用来设定某个用户的crontab服务，省略此参数表示操作当前用户的crontab。file file是命令文件的名字，表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入(键盘)上键入的命令，并将它们载入crontab。-e 编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。-l 显示某个用户的crontab文件内容。如果不指定用户，则表示显示当前用户的crontab文件内容。-r 从/var/spool/cron目录中删除某个用户的crontab文件。如果不指定用户，则默认删除当前用户的crontab文件。-i 在删除用户的crontab文件时给确认提示。 实例设置定时任务时，输入 crontab -e 命令，进入当前用户的工作表编辑，是常见的 vim 界面。每行是一条命令。 crontab 的命令格式： crontab 的命令构成： 时间 + 命令，其时间有分、时、日、月、周五种，时间的操作符有： *****：取值范围内的所有数字 /：每过多少个数字，间隔频率，例如：用在小时段的”*/2”表示每隔两小时 -：从X到Z，例如：”2-6”表示”2,3,4,5,6” ,：散列数字，例如：”1,2,5,7” crontab 的命令实例： 每 2 小时执行一次：0 */2 * * * command 上述命令的含义：能被2整除的整点的0分，执行命令，即 0、2、4、6、…、20、22、24。 crontab 的日志查看： 12$ tail -f /var/log/cron.log$ tail -f /var/spool/mail/[username] 注意事项： 环境变量问题 有时我们创建了一个 crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在 crontab 文件中没有配置环境变量引起的。 在 crontab 文件中定义多个调度任务时，需要特别注环境变量的设置，因为我们手动执行某个任务时，是在当前 shell 环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在 crontab 文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定 cron 知道所需要的特殊环境，它其实并不知道。所以你要保证在 shell 脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下 3 点： 脚本中涉及文件路径时写全局路径； 脚本执行要用到 java 或其他环境变量时，通过 source 命令引入环境变量，如： 12345$ cat start_cbp.sh!/bin/shsource /etc/profileexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; 或者通过在 crontab 中直接引入环境变量解决问题。如： 10 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 及时清理系统用户的邮件日志 每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。 例如，可以在 crontab 文件中设置如下形式，忽略日志输出： 10 */3 * * * /usr/local/apache2/apachectl restart &gt; /dev/null 2&gt;&amp;1 “&gt;/dev/null 2&gt;&amp;1” 表示先将标准输出重定向到 /dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了 /dev/null，因此标准错误也会重定向到 /dev/null，这样日志输出问题就解决了。 其他注意事项 新创建的 cron job，不会马上执行，至少要过 2 分钟才执行。如果重启 cron 则马上执行。 当 crontab 失效时，可以尝试 /etc/init.d/crond restart 解决问题。或者查看日志看某个 job 有没有执行/报错 tail -f /var/log/cron。 **千万别乱运行 crontab -r**。它从 crontab 目录 (/var/spool/cron) 中删除用户的 crontab 文件，删除了该文件，则用户的所有 crontab 都没了。 在 crontab 中 % 是有特殊含义的，表示换行的意思。如果要用的话必须进行转义 %，如经常用的 date ‘+%Y%m%d’ 在 crontab 里是不会执行的，应该换成 date ‘+%Y%m%d’。 参考： https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html https://segmentfault.com/a/1190000021815907 mv用于为文件或目录改名、或将文件或目录移入其它位置。 语法123 mv [OPTION]... [-T] SOURCE DESTor: mv [OPTION]... SOURCE... DIRECTORYor: mv [OPTION]... -t DIRECTORY SOURCE... 参数说明1234567891011121314-b 当目标文件或目录sh存在时，在执行覆盖前，会为其创建一个备份。-i 如果指定移动的源目录或文件与目标的目录或文件同名，则会先询问是否覆盖旧文件，输入y表示直接覆盖，输入n表示取消该操作。-f 如果指定移动的源目录或文件与目标的目录或文件同名，不会询问，直接覆盖旧文件。-n 不要覆盖任何已存在的文件或目录。-u 当源文件比目标文件新或者目标文件不存在时，才执行移动操作。 实例 将源文件名 source_file 改为目标文件名 dest_file 1$ mv source_file(文件) dest_file(文件) 将文件 source_file 移动到目标目录 dest_directory 中 1$ mv source_file(文件) dest_directory(目录) 将源目录 source_directory 移动到 目标目录 dest_directory中 1$ mv source_directory(目录) dest_directory(目录) 若目录名 dest_directory 已存在，则 source_directory 移动到目录名 dest_directory 中； 若目录名 dest_directory 不存在，则 source_directory 改名为目录名 dest_directory。 rename用于实现文件或批量文件重命名。在不同的 linux 版本，命令的语法格式可能不同。 语法1rename [ -h|-m|-V ] [ -v ] [ -n ] [ -f ] [ -e|-E *perlexpr*]*|*perlexpr* [ *files* ] linux 版本： Linux version 4.4.0-116-generic (buildd@lgw01-amd64-021) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) ) #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 或者 1rename [options] expression replacement file... linux 版本： Linux version 3.10.0-1062.el7.x86_64 (&#x6d;&#x6f;&#99;&#x6b;&#x62;&#117;&#105;&#108;&#100;&#64;&#x6b;&#x62;&#117;&#105;&#x6c;&#x64;&#x65;&#x72;&#46;&#x62;&#115;&#x79;&#x73;&#46;&#x63;&#101;&#110;&#x74;&#x6f;&#x73;&#46;&#111;&#x72;&#x67;) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) ) #1 SMP Wed Aug 7 18:08:02 UTC 2019 参数说明123456789101112131415161718192021222324-v, -verbose Verbose: print names of files successfully renamed.-n, -nono No action: print names of files to be renamed, but don&#x27;t rename.-f, -force Over write: allow existing files to be over-written.-h, -help Help: print SYNOPSIS and OPTIONS.-m, -man Manual: print manual page.-V, -version Version: show version number.-e Expression: code to act on files name. May be repeated to build up code (like &quot;perl -e&quot;). If no -e, the first argument is used as code.-E Statement: code to act on files name, as -e but terminated by &#x27;;&#x27;. 或者 12345-v, --verbose explain what is being done-s, --symlink act on symlink target-h, --help display this help and exit-V, --version output version information and exit 实例 替换文件名中特定字段 1$ rename -v &quot;s/20/patent-application/&quot; *.tar.gz 1234567891011121314151617181920212223242526# lin @ lin in ~/share/storage_server_3/Download/test [14:52:08] $ lltotal 76G-rw-rw-r-- 1 lin lin 4.3G Dec 4 16:54 2005.tar.gz-rw-rw-r-- 1 lin lin 4.3G Dec 5 21:50 2006.tar.gz-rw-rw-r-- 1 lin lin 4.4G Dec 5 21:52 2007.tar.gz-rw-rw-r-- 1 lin lin 4.7G Dec 5 21:53 2008.tar.gz-rw-rw-r-- 1 lin lin 5.0G Dec 7 22:10 2009.tar.gz# lin @ lin in ~/share/storage_server_3/Download/test [14:52:08] $ rename -v &quot;s/20/patent-application/&quot; *.tar.gz2005.tar.gz renamed as patent-application05.tar.gz2006.tar.gz renamed as patent-application06.tar.gz2007.tar.gz renamed as patent-application07.tar.gz2008.tar.gz renamed as patent-application08.tar.gz2009.tar.gz renamed as patent-application09.tar.gz# lin @ lin in ~/share/storage_server_3/Download/test [14:53:55] $ lltotal 76G-rw-rw-r-- 1 lin lin 4.3G Dec 4 16:54 patent-application05.tar.gz-rw-rw-r-- 1 lin lin 4.3G Dec 5 21:50 patent-application06.tar.gz-rw-rw-r-- 1 lin lin 4.4G Dec 5 21:52 patent-application07.tar.gz-rw-rw-r-- 1 lin lin 4.7G Dec 5 21:53 patent-application08.tar.gz-rw-rw-r-- 1 lin lin 5.0G Dec 7 22:10 patent-application09.tar.gz 或者 1$ rename 20 patent-application-20 *.tar.gz 12345678910111213141516(base) [hadoop@client version-1.0]$ lltotal 79555796-rw-rw-r-- 1 hadoop hadoop 4527645498 Dec 4 16:54 2005.tar.gz-rw-rw-r-- 1 hadoop hadoop 4550889304 Dec 5 21:50 2006.tar.gz-rw-rw-r-- 1 hadoop hadoop 4712276001 Dec 5 21:52 2007.tar.gz-rw-rw-r-- 1 hadoop hadoop 4986740725 Dec 5 21:53 2008.tar.gz-rw-rw-r-- 1 hadoop hadoop 5311490484 Dec 7 22:10 2009.tar.gz(base) [hadoop@client version-1.0]$ rename 20 patent-application-20 *.tar.gz(base) [hadoop@client version-1.0]$ lltotal 79555796-rw-rw-r-- 1 hadoop hadoop 1372 Dec 16 09:15 hash_calculate.txt-rw-rw-r-- 1 hadoop hadoop 4527645498 Dec 4 16:54 patent-application-2005.tar.gz-rw-rw-r-- 1 hadoop hadoop 4550889304 Dec 5 21:50 patent-application-2006.tar.gz-rw-rw-r-- 1 hadoop hadoop 4712276001 Dec 5 21:52 patent-application-2007.tar.gz-rw-rw-r-- 1 hadoop hadoop 4986740725 Dec 5 21:53 patent-application-2008.tar.gz-rw-rw-r-- 1 hadoop hadoop 5311490484 Dec 7 22:10 patent-application-2009.tar.gz 参考： http://einverne.github.io/post/2018/01/rename-files-batch.html unzip用于解压缩 .zip 文件。 语法12 unzip [-cflptuvz][-agCjLMnoqsVX][-P &lt;密码&gt;][.zip文件][文件][-d &lt;目录&gt;][-x &lt;文件&gt;]or: unzip [-Z] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778-c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。-f 更新现有的文件。-l 显示压缩文件内所包含的文件。-p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。-t 检查压缩文件是否正确。-u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。-v 查看压缩文件目录信息，但是不解压该文件。-z 仅显示压缩文件的备注文字。-a 对文本文件进行必要的字符转换。-b 不要对文本文件进行字符转换。-C 压缩文件中的文件名称区分大小写。-j 不处理压缩文件中原有的目录路径。-L 将压缩文件中的全部文件名改为小写。-M 将输出结果送到more程序处理。-n 解压缩时不要覆盖原有的文件。-o 不必先询问用户，unzip执行后覆盖原有文件。-q 执行时不显示任何信息。-s 将文件名中的空白字符转换为底线字符。-V 保留VMS的文件版本信息。-X 解压缩时同时回存文件原来的UID/GID。-P &lt;密码&gt; 使用zip的密码选项。[.zip文件] 指定.zip压缩文件。[文件] 指定要处理.zip压缩文件中的哪些文件。-d &lt;目录&gt; 指定文件解压缩后所要存储的目录。-x &lt;文件&gt; 指定不要处理.zip压缩文件中的哪些文件。-Z &#x27;unzip -Z&#x27;等于执行zipinfo指令。 实例 查看压缩文件目录信息，但不解压 123456789101112131415$ unzip -v I20090212-SUPP.ZIP Archive: I20090212-SUPP.ZIP Length Method Size Cmpr Date Time CRC-32 Name-------- ------ ------- ---- ---------- ----- -------- ---- 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/ 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/DTDS/ 88199 Stored 88199 0% 2007-01-22 00:07 d5e3060f project/pdds/ICEApplication/I20090212-SUPP/DTDS/DTDS.zip 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/ 15664 Stored 15664 0% 2009-01-28 22:45 3dfa6c1c project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/US20090041797A1-20090212-SUPP.ZIP 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/ 901714 Stored 901714 0% 2009-01-28 22:45 75ce3ca6 project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044288A1-20090212-SUPP.ZIP 1911858 Stored 1911858 0% 2009-01-28 22:45 cbc1d0bd project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044297A1-20090212-SUPP.ZIP-------- ------- --- ------- 2917435 2917435 0% 8 files 解压 .zip 文件 12345678910$ unzip I20090212-SUPP.ZIP Archive: I20090212-SUPP.ZIP creating: project/pdds/ICEApplication/I20090212-SUPP/ creating: project/pdds/ICEApplication/I20090212-SUPP/DTDS/ extracting: project/pdds/ICEApplication/I20090212-SUPP/DTDS/DTDS.zip creating: project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/ extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/US20090041797A1-20090212-SUPP.ZIP creating: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/ extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044288A1-20090212-SUPP.ZIP extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044297A1-20090212-SUPP.ZIP 解压 .zip 文件，但不显示信息 1$ unzip -q I20090212-SUPP.ZIP 注意：如果压缩文件 .zip 是大于 2G 的，那 unzip 就无法使用，此时可以使用 7zip 解压。 参考： https://www.bbsmax.com/A/lk5aMEAP51/ zip用于压缩文件，压缩后的文件后缀名为 .zip。 语法1zip [-AcdDfFghjJKlLmoqrSTuvVwXyz$] [-b &lt;工作目录&gt;] [-ll] [-n &lt;字尾字符串&gt;] [-t &lt;日期时间&gt;] [-&lt;压缩效率&gt;] [压缩文件] [文件...] [-i &lt;范本样式&gt;] [-x &lt;范本样式&gt;] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101-A 调整可执行的自动解压缩文件。 -c 替每个被压缩的文件加上注释。-d 从压缩文件内删除指定的文件。-D 压缩文件内不建立目录名称。-f 更新现有的文件。-F 尝试修复已损坏的压缩文件。-g 将文件压缩后附加在既有的压缩文件之后，而非另行建立新的压缩文件。-h 在线帮助。-j 只保存文件名称及其内容，而不存放任何目录名称。-J 删除压缩文件前面不必要的数据。 -k 使用MS-DOS兼容格式的文件名称。 -l 压缩文件时，把LF字符置换成LF+CR字符。-L 显示版权信息。-m 将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中。-o 以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同。-q 不显示指令执行过程。-r 递归处理，将指定目录下的所有文件和子目录一并处理。-S 包含系统和隐藏文件。-T 检查备份文件内的每个文件是否正确无误。-u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。-v 显示指令执行过程或显示版本信息。-V 保存VMS操作系统的文件属性。-w 在文件名称里假如版本编号，本参数仅在VMS操作系统下有效。-X 不保存额外的文件属性。-y 直接保存符号连接，而非该连接所指向的文件，本参数仅在UNIX之类的系统下有效。-z 替压缩文件加上注释。-$ 保存第一个被压缩文件所在磁盘的卷册名称。 -b &lt;工作目录&gt; 指定暂时存放文件的目录。 -ll 压缩文件时，把LF+CR字符置换成LF字符。-n &lt;字尾字符串&gt; 不压缩具有特定字尾字符串的文件。-t &lt;日期时间&gt; 把压缩文件的日期设成指定的日期。-&lt;压缩效率&gt; 压缩效率是一个介于1-9的数值。-i &lt;范本样式&gt; 只压缩符合条件的文件。-x &lt;范本样式&gt; 压缩时排除符合条件的文件。 实例 将 /home/html/ 目录下所有文件和文件夹打包为当前目录下的 html.zip 1$ zip -q -r html.zip /home/html 如果当前在 /home/html 目录下，可以执行以下命令 1$ zip -q -r html.zip * 从压缩文件 cp.zip 中删除文件 a.c 1zip -dv cp.zip a.c tar用于打包、解包文件。 tar 本身不具有压缩功能，可以通过参数调用其他压缩工具实现压缩功能。 语法1tar [-ABcdgGhiklmMoOpPrRsStuUvwWxzZ] [-b &lt;区块数目&gt;] [-C &lt;目的目录&gt;] [-f &lt;备份文件&gt;] [-F &lt;Script文件&gt;] [-K &lt;文件&gt;] [-L &lt;媒体容量&gt;] [-N &lt;日期时间&gt;] [-T &lt;范本文件&gt;] [-V &lt;卷册名称&gt;] [-X &lt;范本文件&gt;] [-&lt;设备编号&gt;&lt;存储密度&gt;] [--after-date=&lt;日期时间&gt;] [--atime-preserve] [--backuup=&lt;备份方式&gt;] [--checkpoint] [--concatenate] [--confirmation] [--delete] [--exclude=&lt;范本样式&gt;] [--force-local] [--group=&lt;群组名称&gt;] [--help] [--ignore-failed-read] [--new-volume-script=&lt;Script文件&gt;] [--newer-mtime] [--no-recursion] [--null] [--numeric-owner] [--owner=&lt;用户名称&gt;] [--posix] [--erve] [--preserve-order] [--preserve-permissions] [--record-size=&lt;区块数目&gt;] [--recursive-unlink] [--remove-files] [--rsh-command=&lt;执行指令&gt;] [--same-owner] [--suffix=&lt;备份字尾字符串&gt;] [--totals] [--use-compress-program=&lt;执行指令&gt;] [--version] [--volno-file=&lt;编号文件&gt;] [文件或目录...] 语法结构：tar [必要参数] [可选参数] [文件] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212-A | --catenate 新增文件到已存在的备份文件。 -B | --read-full-records 读取数据时重设区块大小。 -c | --create 建立新的备份文件。 -d | --diff | --compare 对比备份文件内和文件系统上的文件的差异。 -g | --listed-incremental 处理GNU格式的大量备份。 -G | --incremental 处理旧的GNU格式的大量备份。 -h | --dereference 不建立符号连接，直接复制该连接所指向的原始文件。 -i | --ignore-zeros 忽略备份文件中的0 Byte区块，也就是EOF。 -k | --keep-old-files 解开备份文件时，不覆盖已有的文件。 -l | --one-file-system 复制的文件或目录存放的文件系统，必须与tar指令执行时所处的文件系统相同，否则不予复制。 -m | --modification-time 还原文件时，不变更文件的更改时间。 -M | --multi-volume 在建立，还原备份文件或列出其中的内容时，采用多卷册模式。 -o | --old-archive | --portability 将资料写入备份文件时使用V7格式。 -O | --stdout 把从备份文件里还原的文件输出到标准输出设备。 -p | --same-permissions 用原来的文件权限还原文件。 -P | --absolute-names 文件名使用绝对名称，不移除文件名称前的&quot;/&quot;号。 -r | --append 新增文件到已存在的备份文件的结尾部分。 -R | --block-number 列出每个信息在备份文件中的区块编号。 -s | --same-order 还原文件的顺序和备份文件内的存放顺序相同。 -S | --sparse 倘若一个文件内含大量的连续0字节，则将此文件存成稀疏文件。 -t | --list 列出备份文件的内容。 -u | --update 仅置换较备份文件内的文件更新的文件。 -U | --unlink-first 解开压缩文件还原文件之前，先解除文件的连接。 -v | --verbose 显示指令执行过程。 -w | --interactive 遭遇问题时先询问用户。 -W | --verify 写入备份文件后，确认文件正确无误。 -x | --extract | --get 从备份文件中还原文件。 -z | --gzip | --ungzip 通过gzip指令处理备份文件。 -Z | --compress | --uncompress 通过compress指令处理备份文件。-b &lt;区块数目&gt; | --blocking-factor=&lt;区块数目&gt; 设置每笔记录的区块数目，每个区块大小为12Bytes。-C &lt;目的目录&gt; | --directory=&lt;目的目录&gt; 切换到指定的目录。-f &lt;备份文件&gt; | --file=&lt;备份文件&gt; 指定备份文件。多个命令时需要放在最后面。 -F &lt;Script文件&gt; | --info-script=&lt;Script文件&gt; 每次更换磁带时，就执行指定的Script文件。-K &lt;文件&gt; | --starting-file=&lt;文件&gt; 从指定的文件开始还原。-L &lt;媒体容量&gt; | -tape-length=&lt;媒体容量&gt; 设置存放每体的容量，单位以1024Bytes计算。-N &lt;日期格式&gt; | --newer=&lt;日期时间&gt; 只将较指定日期更新的文件保存到备份文件里。-T &lt;范本文件&gt; | --files-from=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让tar解开或建立符合设置条件的文件。-V&lt;卷册名称&gt; | --label=&lt;卷册名称&gt; 建立使用指定的卷册名称的备份文件。-X &lt;范本文件&gt; | --exclude-from=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让tar排除符合设置条件的文件。-&lt;设备编号&gt;&lt;存储密度&gt; 设置备份用的外围设备编号及存放数据的密度。 --after-date=&lt;日期时间&gt; 此参数的效果和指定&quot;-N&quot;参数相同。--atime-preserve 不变更文件的存取时间。--backup=&lt;备份方式&gt; | --backup 移除文件前先进行备份。--checkpoint 读取备份文件时列出目录名称。--concatenate 此参数的效果和指定&quot;-A&quot;参数相同。--confirmation 此参数的效果和指定&quot;-w&quot;参数相同。--delete 从备份文件中删除指定的文件。--exclude=&lt;范本样式&gt; 排除符合范本样式的文件。--group=&lt;群组名称&gt; 把加入设备文件中的文件的所属群组设成指定的群组。--help 在线帮助。--ignore-failed-read 忽略数据读取错误，不中断程序的执行。--new-volume-script=&lt;Script文件&gt; 此参数的效果和指定&quot;-F&quot;参数相同。--newer-mtime 只保存更改过的文件。--no-recursion 不做递归处理，也就是指定目录下的所有文件及子目录不予处理。--null 从null设备读取文件名称。--numeric-owner 以用户识别码及群组识别码取代用户名称和群组名称。--owner=&lt;用户名称&gt; 把加入备份文件中的文件的拥有者设成指定的用户。--posix 将数据写入备份文件时使用POSIX格式。--preserve 此参数的效果和指定&quot;-ps&quot;参数相同。--preserve-order 此参数的效果和指定&quot;-A&quot;参数相同。--preserve-permissions 此参数的效果和指定&quot;-p&quot;参数相同。--record-size=&lt;区块数目&gt; 此参数的效果和指定&quot;-b&quot;参数相同。--recursive-unlink 解开压缩文件还原目录之前，先解除整个目录下所有文件的连接。--remove-files 文件加入备份文件后，就将其删除。--rsh-command=&lt;执行指令&gt; 设置要在远端主机上执行的指令，以取代rsh指令。--same-owner 尝试以相同的文件拥有者还原文件。--suffix=&lt;备份字尾字符串&gt; 移除文件前先行备份。--totals 备份文件建立后，列出文件大小。--use-compress-program=&lt;执行指令&gt; 通过指定的指令处理备份文件。--version 显示版本信息。--volno-file=&lt;编号文件&gt; 使用指定文件内的编号取代预设的卷册编号。 实例 打包，不压缩 1234567$ tar -cvf test.tar testtest/test/3test/1test/2test/5test/4 解包 1234567$ tar -xvf test.tar test/test/3test/1test/2test/5test/4 打包，并以 gzip 压缩 1234567$ tar -zcvf test.tar.gz testtest/test/3test/1test/2test/5test/4 在参数 f 之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加 z 参数，则以 .tar.gz 或 .tgz 来代表 gzip 压缩过的 tar 包。 解压 .tar.gz 1234567$ tar -zxvf test.tar.gz test/test/3test/1test/2test/5test/4 打包，以 bzip2 压缩 1234567$ tar -zcvf test.tar.bz2 testtest/test/3test/1test/2test/5test/4 解压 .tar.bz2 1234567$ tar -zxvf test.tar.bz2 test/test/3test/1test/2test/5test/4 查看 .tar.gz 或 .tar.bz2 压缩包内的文件，但不解压 123456789101112131415$ tar -ztvf test.tar.gz drwxrwxr-x lin/lin 0 2020-12-21 11:38 test/-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/3-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/1-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/2-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/5-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/4$ tar -ztvf test.tar.bz2 drwxrwxr-x lin/lin 0 2020-12-21 11:38 test/-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/3-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/1-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/2-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/5-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/4 解压 .tar.gz 压缩包内的部分文件 123$ tar -zxvf test.tar.gz test/2 test/3test/3test/2 解压 .tar.gz 到指定目录 1234567$ tar -zxvf test.tar.gz -C ../Download test/test/3test/1test/2test/5test/4 使用绝对路径打包压缩和解压 12345# 压缩$ tar -zcPf /home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0/patent-grant-2019.tar.gz /home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0/2019# 解压$ tar -zxPf patent-grant-2019.tar.gz tar 对文件打包时，一般不建议使用绝对路径。 如果使用绝对路径，需要加 -P 参数。如果不添加，会发出警告：tar: Removing leading &#39;/&#39; from member names。 对于使用绝对路径打包压缩的文件，解压时 tar 会在当前目录下创建压缩时的绝对路径所对应的目录，在上面例子中，即为在当前目录下创建一个子目录 home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0。 如果在解压时使用 -P 参数，需要保证系统存在压缩时的绝对路径。 使用 pigz 并发压缩和解压 安装 pigz： 1$ sudo apt install pigz 打包： 1$ tar --use-compress-program=pigz -cvpf package.tgz ./package 解包： 1$ tar --use-compress-program=pigz -xvpf package.tgz -C ./package pigz 是支持并行的 gzip，默认用当前逻辑 cpu 个数来并发压缩，无法检测个数的话，则并发 8 个线程。 另一种方式： 12345# 语法$ tar -cvpf - $Dir | pigz -9 -p 6 $target-name# 实例$ tar -cvpf - /usr/bin | pigz -9 -p 6 bin.tgz -9：代表压缩率-p ：代表 cpu 数量 time用于检测特定指令执行时所需消耗的时间及系统资源 (内存和 I/O) 等资讯。 语法1time [options] COMMAND [arguments] 参数说明12345678-o | --output=FILE 设定结果输出档。这个选项会将time的输出写入所指定的档案中。如果档案已经存在，系统将覆写其内容。 -a | --append 配合-o使用，会将结果写到档案的末端，而不会覆盖掉原来的内容。 -f FORMAT | --format=FORMAT 以FORMAT字串设定显示方式。当这个选项没有被设定的时候，会用系统预设的格式。不过你可以用环境变数time来设定这个格式，如此一来就不必每次登入系统都要设定一次。 实例 date 命令的运行时间 123$ time dateTue Dec 22 12:01:50 CST 2020date 0.00s user 0.01s system 8% cpu 0.092 total 查找文件并复制的运行时间 123$ time find /home/lin/share/storage_server_3/patent/application/unzip_version-1.0/2019 -iname &quot;*.xml&quot; | xargs -P 6 -i cp &#123;&#125; /home/lin/share/storage_server_3/patent/application-patent/patent_version-1.0/2019 find -iname &quot;*.xml&quot; 24.00s user 114.39s system 1% cpu 2:08:02.95 totalxargs -P 6 -i cp &#123;&#125; 4.35s user 28.35s system 0% cpu 2:08:02.99 total 参考： http://c.biancheng.net/linux/time.html mkdir语法参数说明实例 创建多级目录 1$ mkdir -p Project/a/src 创建多层次、多维度的目录树 1$ mkdir -p Project/&#123;a,b,c,d&#125;/src sh -csh -c 命令，可以让 bash 将一个字串作为完整的命令来执行。 比如，向 test.asc 文件中随便写入点内容，可以： 1$ echo &quot;信息&quot; &gt; test.asc 或者 1$ echo &quot;信息&quot; &gt;&gt; test.asc 下面，如果将 test.asc 权限设置为只有 root 用户才有权限进行写操作： 1$ sudo chown root.root test.asc 然后，我们使用 sudo 并配合 echo 命令再次向修改权限之后的 test.asc 文件中写入信息： 12$ sudo echo &quot;又一行信息&quot; &gt;&gt; test.asc-bash: test.asc: Permission denied 这时，可以看到 bash 拒绝这么做，说是权限不够。这是因为重定向符号 &gt; 和 &gt;&gt; 也是 bash 的命令。我们使用 sudo 只是让 echo 命令具有了 root 权限，但是没有让 &gt; 和 &gt;&gt; 命令也具有 root 权限，所以 bash 会认为这两个命令都没有向 test.asc 文件写入信息的权限。解决这一问题的途径有两种。 第一种是利用 sh -c 命令，它可以让 bash 将一个字串作为完整的命令来执行，这样就可以将 sudo 的影响范围扩展到整条命令。具体用法如下： 1$ sudo sh -c &#x27;echo &quot;又一行信息&quot; &gt;&gt; test.asc&#x27; 另一种方法是利用管道和 tee 命令，该命令可以从标准输入中读入信息并将其写入标准输出或文件中，具体用法如下： 1$ echo &quot;第三条信息&quot; | sudo tee -a test.asc 注意，tee 命令的 -a 选项的作用等同于 &gt;&gt; 命令，如果去除该选项，那么 tee 命令的作用就等同于 &gt; 命令。 1&gt;/dev/null 2&gt;&amp;1https://blog.csdn.net/ithomer/article/details/9288353 tophttps://www.jianshu.com/p/e9e0ce23a152 PID：进程的ID USER：进程所有者 ​ PR：进程的优先级别，越小越优先被执行 ​ NI：进程Nice值，代表这个进程的优先值 ​ VIRT：进程占用的虚拟内存 ​ RES：进程占用的物理内存 ​ SHR：进程使用的共享内存 S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态 ​ %CPU：进程占用CPU的使用 ​ %MEM：进程使用的物理内存和总内存的百分 ​ TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值 ​ COMMAND：启动该进程的命令名称 freefree 用KB为单位展示数据 free -m 用MB为单位展示数据 free -h 用GB为单位展示数据 total : 总计屋里内存的大小 used : 已使用内存的大小 free : 可用内存的大小 shared : 多个进程共享的内存总额 buff/cache : 磁盘缓存大小 available : 可用内存大小 ， 从应用程序的角度来说：available = free + buff/cache . psmd5sumsha1sum用来为给定的文件或文件夹计算单个哈希，以校验文件或文件夹的完整性。 给文件： 12$ sha1sum patent-grant-2005.tar.gz 77b6416501d34b904bd25f9aa32ca60d3e14659a patent-grant-2005.tar.gz https://www.itranslater.com/qa/details/2326085750774825984 parallelhttps://linux.cn/article-9718-1.html https://www.myfreax.com/gnu-parallel/ https://www.hi-linux.com/posts/32794.html https://www.jianshu.com/p/c5a2369fa613 https://www.aqee.net/post/use-multiple-cpu-cores-with-your-linux-commands.html https://blog.csdn.net/orangefly0214/article/details/103701600","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"Java 关闭 IO 流","slug":"java-io-close","date":"2020-12-14T13:54:17.000Z","updated":"2021-04-09T08:00:50.216Z","comments":true,"path":"2020/12/14/java-io-close/","link":"","permalink":"http://example.com/2020/12/14/java-io-close/","excerpt":"","text":"在操作 java 流对象后要将流关闭，但实际编写代码时，可能会出现一些误区，导致不能正确关闭流。 在 try 中关流，而没在 finally 中关流错误： 1234567try &#123; OutputStream out = new FileOutputStream(&quot;&quot;); // ...操作流代码 out.close();&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 修正： 12345678910111213141516OutputStream out = null;try &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 在一个 try 中关闭多个流错误： 1234567891011121314151617181920OutputStream out = null;OutputStream out2 = null;try &#123; out = new FileOutputStream(&quot;&quot;); out2 = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close();// 如果此处出现异常，则out2流没有被关闭 &#125; if (out2 != null) &#123; out2.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 修正： 123456789101112131415161718192021222324OutputStream out = null;OutputStream out2 = null;try &#123; out = new FileOutputStream(&quot;&quot;); out2 = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close();// 如果此处出现异常，则out2流也会被关闭 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; try &#123; if (out2 != null) &#123; out2.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 在循环中创建流，在循环外关闭，导致关闭的是最后一个流错误： 1234567891011121314151617OutputStream out = null;try &#123; for (int i = 0; i &lt; 10; i++) &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码 &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 修正： 1234567891011121314151617for (int i = 0; i &lt; 10; i++) &#123; OutputStream out = null; try &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 在 java 7 中，关闭流的方式得到很大的简化12345try (OutputStream out = new FileOutputStream(&quot;&quot;))&#123; // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 只要实现的自动关闭接口 (Closeable) 的类都可以在 try 结构体上定义，java 会自动帮我们关闭，即使在发生异常的情况下也会。 可以在 try 结构体上定义多个，用分号隔开即可，如： 123456try (OutputStream out = new FileOutputStream(&quot;&quot;); OutputStream out2 = new FileOutputStream(&quot;&quot;))&#123; // ...操作流代码&#125; catch (Exception e) &#123; throw e;&#125; Android SDK 20 版本对应 java 7，低于 20 版本无法使用 try-catch-resources 自动关流。 内存流的关闭内存流可以不用关闭。 ByteArrayOutputStream 和 ByteArrayInputStream 其实是伪装成流的字节数组 (把它们当成字节数据来看就好了)，他们不会锁定任何文件句柄和端口，如果不再被使用，字节数组会被垃圾回收掉，所以不需要关闭。 装饰流的关闭装饰流是指通过装饰模式实现的 java 流，又称为包装流，装饰流只是为原生流附加额外的功能或效果，java 中的缓冲流、桥接流也是属于装饰流。 例如： 123456789101112InputStream is = new FileInputStream(&quot;C:\\\\Users\\\\tang\\\\Desktop\\\\test.txt&quot;);InputStreamReader isr = new InputStreamReader(is);BufferedReader br = new BufferedReader(isr);String string = br.readLine();System.out.println(string);// 只需要关闭最后的br即可try &#123; br.close();&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 装饰流关闭时会调用原生流关闭。 BufferedReader.java 源码如下： 12345678910111213public void close() throws IOException &#123; synchronized (lock) &#123; if (in == null) return; try &#123; // 这里的in就是原生流 in.close(); &#125; finally &#123; in = null; cb = null; &#125; &#125;&#125; InputStreamReader.java 源码如下： 1234public void close() throws IOException &#123; // 这里的sd就是原生流的解码器(StreamDecoder)，解码器的close会调用原生流的close sd.close();&#125; 如上所示，有这样层层关闭的机制，我们就只需要关闭最外层的流就行了。 关闭流的顺序问题两个不相干的流的关闭顺序没有任何影响，如： 123// 这里的out1和out2谁先关谁后关都一样，没有任何影响out1 = new FileOutputStream(&quot;&quot;);out2 = new FileOutputStream(&quot;&quot;); 如果两个流有依赖关系，那么可以像上面说的，只关闭最外层的即可。 如果不嫌麻烦，非得一个个关闭，那么需要先关闭最里层，从里往外一层层进行关闭。 为什么不能从外层往里层逐步关闭？原因上面讲装饰流已经讲的很清楚了，关闭外层时，内层的流其实已经同时关闭了，你再去关内层的流，就会报错。 至于网上说的先声明先关闭，就是这个道理，先声明的是内层，最先申明的是最内层，最后声明的是最外层。 一定要关闭流的原因一个流绑定了一个文件句柄 (或网络端口)，如果流不关闭，该文件 (或端口) 将始终处于被锁定 (不能读取、写入、删除和重命名) 状态，占用大量系统资源却没有释放。 本文参考https://blog.csdn.net/u012643122/article/details/38540721 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 的 IO 流","slug":"java-io","date":"2020-11-27T03:33:09.000Z","updated":"2021-04-13T07:21:31.070Z","comments":true,"path":"2020/11/27/java-io/","link":"","permalink":"http://example.com/2020/11/27/java-io/","excerpt":"","text":"File 类 java.io.File 类：文件和文件目录路径的抽象表示形式，与平台无关。 File 主要表示类似 D:\\\\文件目录1 与 D:\\\\文件目录1\\\\文件.txt，前者是文件夹 (directory)，后者则是文件 (file)，而 File 类就是操作这两者的类。 File 能新建、删除、重命名文件和目录，但 File 不能访问文件内容本身。如果需要访问文件内容本身，则需要使用输入/输出流。 File 跟流无关，File 类不能对文件进行读和写也就是输入和输出。 想要在 Java 程序中表示一个真实存在的文件或目录，那么必须有一个 File 对象，但是 Java 程序中的一个 File 对象，可能不对应一个真实存在的文件或目录。 File 对象可以作为参数传递给流的构造器，指明读取或写入的 “终点”。 在 Java 中，一切皆是对象，File 类也不例外，不论是哪个对象都应该从该对象的构造方法说起： public File(String pathname) ：以 pathname 为路径创建 File 对象，可以是绝对路径或者相对路径，如果 pathname 是相对路径，则默认的当前路径在系统属性 user.dir 中存储。 绝对路径：是一个固定的路径，从盘符开始。 相对路径：是相对于某个位置开始。 IDEA 中的路径说明，main() 和 Test 中，相对路径不一样： 123456789101112public class Test &#123; public static void main(String[] args) &#123; File file = new File(&quot;hello.txt&quot;);// 相较于当前工程 System.out.println(file.getAbsolutePath());// D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-projects\\hello.txt &#125; @Test public void testFileReader() &#123; File file = new File(&quot;hello.txt&quot;);// 相较于当前Module System.out.println(file.getAbsolutePath());// D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-projects\\xisun-java_base\\hello.txt &#125;&#125; public File(String parent, String child) ：以 parent 为父路径，child 为子路径创建 File 对象。 public File(File parent, String child) ：根据一个父 File 对象和子文件路径创建 File 对象。 实例： 1234567891011121314151617// 通过文件路径名 String path1 = &quot;D:\\\\123.txt&quot;;File file1 = new File(path1); // 通过文件路径名String path2 = &quot;D:\\\\1\\\\2.txt&quot;;File file2 = new File(path2); -------------相当于d:\\\\1\\\\2.txt// 通过父路径和子路径字符串 String parent = &quot;F:\\\\aaa&quot;; String child = &quot;bbb.txt&quot;; File file3 = new File(parent, child); --------相当于f:\\\\aaa\\\\bbb.txt// 通过父级File对象和子路径字符串File parentDir = new File(&quot;F:\\\\aaa&quot;);String child = &quot;bbb.txt&quot;;File file4 = new File(parentDir, child); --------相当于f:\\\\aaa\\\\bbb.txt 路径分隔符： 路径中的每级目录之间用一个路径分隔符隔开。 路径分隔符和系统有关： windows 和 DOS 系统默认使用 “\\“ 来表示。 UNIX 和 URL 使用 “/“ 来表示。 Java 程序支持跨平台运行，因此路径分隔符要慎用。为了解决这个隐患，File 类提供了一个常量 public static final String separator，能够根据操作系统，动态的提供分隔符。 实例： 123File file1 = new File(&quot;d:\\\\test\\\\info.txt&quot;);File file2 = new File(&quot;d:/test/info.txt&quot;);File file3 = new File(&quot;d:&quot; + File.separator + &quot;test&quot; + File.separator + &quot;info.txt&quot;); 获取功能的方法： public String getAbsolutePath()：获取绝对路径。 public String getPath()：获取路径。 public String getName()：获取名称。 public String getParent()：获取上层文件目录路径。若无，返回 null。 public long length()：获取文件长度，即：字节数。不能获取目录的长度。 public long lastModified()：获取最后一次的修改时间，毫秒值。 实例： 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; File file1 = new File(&quot;hello.txt&quot;); File file2 = new File(&quot;d:\\\\io\\\\hi.txt&quot;); System.out.println(file1.getAbsolutePath()); System.out.println(file1.getPath()); System.out.println(file1.getName()); System.out.println(file1.getParent()); System.out.println(file1.length()); System.out.println(new Date(file1.lastModified())); System.out.println(); System.out.println(file2.getAbsolutePath()); System.out.println(file2.getPath()); System.out.println(file2.getName()); System.out.println(file2.getParent()); System.out.println(file2.length()); System.out.println(file2.lastModified()); &#125;&#125; public String[] list()：获取指定目录下的所有文件或者文件目录的名称数组，如果指定目录不存在，返回 null。 public File[] listFiles()：获取指定目录下的所有文件或者文件目录的 File 数组，如果指定目录不存在，返回 null。 实例： 1234567891011121314151617181920212223public class Test &#123; public static void main(String[] args) &#123; File file = new File(&quot;D:\\\\workspace_idea1\\\\JavaSenior&quot;); String[] list = file.list(); System.out.println(list); if (list != null) &#123; for (String s : list) &#123; System.out.println(s); &#125; &#125; System.out.println(); File[] files = file.listFiles(); System.out.println(files); if (files != null) &#123; for (File f : files) &#123; System.out.println(f); &#125; &#125; &#125;&#125; public String[] list(FilenameFilter filter)：指定文件过滤器。 public File[] listFiles(FilenameFilter filter)：指定文件过滤器。 public File[] listFiles(FileFilter filter)：指定文件过滤器。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041public class Test &#123; public static void main(String[] args) &#123; File srcFile = new File(&quot;d:\\\\code&quot;); String[] subFiles1 = srcFile.list(new FilenameFilter() &#123; @Override public boolean accept(File dir, String name) &#123; return name.endsWith(&quot;.jpg&quot;); &#125; &#125;); if (subFiles1 != null) &#123; for (String fileName : subFiles1) &#123; System.out.println(fileName); &#125; &#125; File[] subFiles2 = srcFile.listFiles(new FilenameFilter() &#123; @Override public boolean accept(File dir, String name) &#123; return name.endsWith(&quot;.jpg&quot;); &#125; &#125;); if (subFiles2 != null) &#123; for (File file : subFiles2) &#123; System.out.println(file.getAbsolutePath()); &#125; &#125; File[] subFiles3 = srcFile.listFiles(new FileFilter() &#123; @Override public boolean accept(File pathname) &#123; return pathname.getName().endsWith(&quot;.jpg&quot;); &#125; &#125;); if (subFiles3 != null) &#123; for (File file : subFiles3) &#123; System.out.println(file.getAbsolutePath()); &#125; &#125; &#125;&#125; 重命名功能的方法 public boolean renameTo(File dest)：把文件重命名为指定的文件路径。以 file1.renameTo(file2) 为例：要想保证返回 true，需要 file1 在硬盘中是存在的，且 file2 在硬盘中不能存在。 实例： 123456789public class Test &#123; public static void main(String[] args) &#123; File file1 = new File(&quot;hello.txt&quot;); File file2 = new File(&quot;D:\\\\io\\\\hi.txt&quot;); boolean renameTo = file2.renameTo(file1); System.out.println(renameTo); &#125;&#125; 判断功能的方法 public boolean exists()：判断是否存在。 public boolean isDirectory()：判断是否是文件目录。 public boolean isFile()：判断是否是文件。 public boolean canRead()：判断是否可读。 public boolean canWrite()：判断是否可写。 public boolean isHidden()：判断是否隐藏。 实例： 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; File file1 = new File(&quot;hello.txt&quot;); file1 = new File(&quot;hello1.txt&quot;); System.out.println(file1.isDirectory()); System.out.println(file1.isFile()); System.out.println(file1.exists()); System.out.println(file1.canRead()); System.out.println(file1.canWrite()); System.out.println(file1.isHidden()); System.out.println(); File file2 = new File(&quot;d:\\\\io&quot;); file2 = new File(&quot;d:\\\\io1&quot;); System.out.println(file2.isDirectory()); System.out.println(file2.isFile()); System.out.println(file2.exists()); System.out.println(file2.canRead()); System.out.println(file2.canWrite()); System.out.println(file2.isHidden()); &#125;&#125; 创建功能的方法 public boolean createNewFile()：创建文件。若文件不存在，则创建一个新的空文件并返回 true；若文件存在，则不创建文件并返回 false。 public boolean mkdir()：创建文件目录。如果此文件目录存在，则不创建；如果此文件目录的上层目录不存在，也不创建。 public boolean mkdirs()：创建文件目录。如果上层文件目录不存在，也一并创建。 如果创建文件或者文件目录时，没有写盘符路径，那么，默认在项目路径下。 实例： 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; File file1 = new File(&quot;hi.txt&quot;); if (!file1.exists()) &#123; // 文件不存在 try &#123; boolean newFile = file1.createNewFile(); System.out.println(&quot;创建成功？&quot; + newFile); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; else &#123; // 文件存在 boolean delete = file1.delete(); System.out.println(&quot;删除成功？&quot; + delete); &#125; &#125;&#125; 删除功能的方法 public boolean delete()：删除文件或者文件夹。 Java 中的删除不走回收站。要删除一个文件目录，请注意该文件目录内不能包含文件或者文件目录，即只能删除空的文件目录。 实例： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) &#123; // 文件目录的创建 File file1 = new File(&quot;d:\\\\io\\\\io1\\\\io3&quot;); boolean mkdir = file1.mkdir(); if (mkdir) &#123; System.out.println(&quot;创建成功1&quot;); &#125; File file2 = new File(&quot;d:\\\\io\\\\io1\\\\io4&quot;); boolean mkdir1 = file2.mkdirs(); if (mkdir1) &#123; System.out.println(&quot;创建成功2&quot;); &#125; // 要想删除成功，io4文件目录下不能有子目录或文件 File file3 = new File(&quot;D:\\\\io\\\\io1\\\\io4&quot;); file3 = new File(&quot;D:\\\\io\\\\io1&quot;); System.out.println(file3.delete()); &#125;&#125; 递归遍历文件夹下所有文件以及子文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class Test &#123; public static void main(String[] args) &#123; // 递归:文件目录 /** 打印出指定目录所有文件名称，包括子文件目录中的文件 */ // 1.创建目录对象 File dir = new File(&quot;E:\\\\teach\\\\01_javaSE\\\\_尚硅谷Java编程语言\\\\3_软件&quot;); // 2.打印目录的子文件 printSubFile(dir); &#125; // 方式一： public static void printSubFile(File dir) &#123; // 判断传入的是否是目录 if (!dir.isDirectory()) &#123; // 不是目录直接退出 return; &#125; // 打印目录的子文件 File[] subfiles = dir.listFiles(); if (subfiles != null) &#123; for (File f : subfiles) &#123; if (f.isDirectory()) &#123; // 文件目录 printSubFile(f); &#125; else &#123; // 文件 System.out.println(f.getAbsolutePath()); &#125; &#125; &#125; &#125; // 方式二：循环实现 // 列出file目录的下级内容，仅列出一级的话，使用File类的String[] list()比较简单 public void listSubFiles(File file) &#123; if (file.isDirectory()) &#123; String[] all = file.list(); if (all != null) &#123; for (String s : all) &#123; System.out.println(s); &#125; &#125; &#125; else &#123; System.out.println(file + &quot;是文件！&quot;); &#125; &#125; // 方式三：列出file目录的下级，如果它的下级还是目录，接着列出下级的下级，依次类推 // 建议使用File类的File[] listFiles() public void listAllSubFiles(File file) &#123; if (file.isFile()) &#123; System.out.println(file); &#125; else &#123; File[] all = file.listFiles(); // 如果all[i]是文件，直接打印 // 如果all[i]是目录，接着再获取它的下一级 if (all != null) &#123; for (File f : all) &#123; // 递归调用：自己调用自己就叫递归 listAllSubFiles(f); &#125; &#125; &#125; &#125; // 拓展1：计算指定目录所在空间的大小 // 求任意一个目录的总大小 public long getDirectorySize(File file) &#123; // file是文件，那么直接返回file.length() // file是目录，把它的下一级的所有大小加起来就是它的总大小 long size = 0; if (file.isFile()) &#123; size += file.length(); &#125; else &#123; // 获取file的下一级 File[] all = file.listFiles(); if (all != null) &#123; // 累加all[i]的大小 for (File f : all) &#123; // f的大小 size += getDirectorySize(f); &#125; &#125; &#125; return size; &#125; // 拓展2：删除指定文件目录及其下的所有文件 public void deleteDirectory(File file) &#123; // 如果file是文件，直接delete // 如果file是目录，先把它的下一级干掉，然后删除自己 if (file.isDirectory()) &#123; File[] all = file.listFiles(); // 循环删除的是file的下一级 if (all != null) &#123; // f代表file的每一个下级 for (File f : all) &#123; deleteDirectory(f); &#125; &#125; &#125; // 删除自己 file.delete(); &#125;&#125; 字符编码 字符集 Charset：也叫编码表。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等。 编码表的由来：计算机只能识别二进制数据，早期由来是电信号。为了方便应用计算机，让它可以识别各个国家的文字。就将各个国家的文字用数字来表示，并一一对应，形成一张表。这就是编码表。 常见的编码表： ASCII：美国标准信息交换码。用一个字节的 7 位可以表示。 ISO8859-1：拉丁码表，欧洲码表。用一个字节的 8 位表示。 GB2312：中国的中文编码表。最多两个字节编码所有字符。 GBK：中国的中文编码表升级，融合了更多的中文文字符号。最多两个字节编码。 Unicode：国际标准码，融合了目前人类使用的所有字符。为每个字符分配唯一的字符码。所有的文字都用两个字节来表示。 UTF-8：变长的编码方式，可用 1 ~ 4 个字节来表示一个字符。 在 Unicode 出现之前，所有的字符集都是和具体编码方案绑定在一起的，即字符集 ≈ 编码方式，都是直接将字符和最终字节流绑定死了。 GBK 等双字节编码方式，用最高位是 1 或 0 表示两个字节和一个字节。 Unicode 不完美，这里就有三个问题，一个是，我们已经知道，英文字母只用一个字节表示就够了，第二个问题是如何才能区别 Unicode 和 ASCII，计算机怎么知道是两个字节表示一个符号，而不是分别表示两个符号呢？第三个，如果和 GBK 等双字节编码方式一样，用最高位是 1 或 0 表示两个字节和一个字节，就少了很多值无法用于表示字符，不够表示所有字符。Unicode 在很长一段时间内无法推广，直到互联网的出现。 面向传输的众多 UTF (UCS Transfer Format) 标准出现了，顾名思义，UTF-8 就是每次 8 个位传输数据，而 UTF-16 就是每次 16 个位。这是为传输而设计的编码，并使编码无国界，这样就可以显示全世界上所有文化的字符了。 Unicode 只是定义了一个庞大的、全球通用的字符集，并为每个字符规定了唯一确定的编号，具体存储成什么样的字节流，取决于字符编码方案。推荐的 Unicode 编码是 UTF-8 和 UTF-16。 计算机中储存的信息都是用二进制数表示的，而能在屏幕上看到的数字、英文、标点符号、汉字等字符是二进制数转换之后的结果。按照某种规则，将字符存储到计算机中，称为编码 。反之，将存储在计算机中的二进制数按照某种规则解析显示出来，称为解码 。 编码规则和解码规则要对应，否则会导致乱码。比如说，按照 A 规则存储，同样按照 A 规则解析，那么就能显示正确的文本符号。反之，按照 A 规则存储，再按照 B 规则解析，就会导致乱码现象。 编码： 字符串 —&gt; 字节数组。(能看懂的 —&gt; 看不懂的) 解码： 字节数组 —&gt; 字符串。(看不懂的 —&gt; 能看懂的) 启示：客户端/浏览器端 &lt;——&gt; 后台 (Java，GO，Python，Node.js，php…) &lt;——&gt; 数据库，要求前前后后使用的字符集要统一，都使用 UTF-8，这样才不会乱码。 IO 流原理 I/O 是 Input/Output 的缩写， I/O 技术是非常实用的技术，用于处理设备之间的数据传输。如读/写文件，网络通讯等。 Java 程序中，对于数据的输入/输出操作以 “流 (stream)” 的方式进行。 java.io 包下提供了各种 “流” 类和接口，用以获取不同种类的数据，并通过标准的方法输入或输出数据。 输入 input：读取外部数据 (磁盘、光盘等存储设备的数据) 到程序 (内存) 中。 输出 output：将程序 (内存) 数据输出到磁盘、光盘等外部存储设备中。 IO 流的分类 按操作数据单位不同分为：字节流 (8 bit)，字符流 (16 bit)。 字节流：以字节为单位，读写数据的流。 字符流：以字符为单位，读写数据的流。 按数据流的流向不同分为：输入流，输出流。 输入流：把数据从其他设备上读取到内存中的流。 输出流：把数据从内存中写出到其他设备上的流。 按流的角色的不同分为：节点流，处理流。 节点流：直接从数据源或目的地读写数据。也叫文件流。 处理流：不直接连接到数据源或目的地，而是连接在已存在的流 (节点流或处理流) 之上，通过对数据的处理为程序提供更为强大的读写功能。 Java 的 IO 流共涉及 40 多个类，实际上非常规则，都是从如下四个抽象基类派生的。同时，由这四个类派生出来的子类名称都是以其父类名作为子类名后缀： IO 流体系： 四个抽象基类InputStream &amp; Reader： InputStream 和 Reader 是所有输入流的基类。 InputStream 的典型实现：FileInputStream。 int read() int read(byte[] b) int read(byte[] b, int off, int len) Reader 的典型实现：FileReader。 int read() int read(char [] c) int read(char [] c, int off, int len) 程序中打开的文件 IO 资源不属于内存里的资源，垃圾回收机制无法回收该资源，所以应该显式关闭文件 IO 资源。 FileInputStream 从文件系统中的某个文件中获得输入字节。FileInputStream 用于读取非文本数据之类的原始字节流。如果要读取文本数据的字符流，需要使用 FileReader。 InputStream： int read()：从输入流中读取数据的下一个字节。返回 0 到 255 范围内的 int 字节值。如果因为已经到达流末尾而没有可用的字节，则返回值 -1。 int read(byte[] b)：从输入流中将最多 b.length() 个字节的数据读入一个 byte 数组中。以整数形式返回实际读取的字节数。如果因为已经到达流末尾而没有可用的字节，则返回值 -1。 int read(byte[] b, int off,int len)：将输入流中最多 len 个数据字节读入 byte 数组。尝试读取 len 个字节，但读取的字节也可能小于该值。以整数形式返回实际读取的字节数。如果因为已经到达流末尾而没有可用的字节，则返回值 -1。 public void close() throws IOException：关闭输入流并释放与该流关联的所有系统资源。 Reader： int read()：读取单个字符。作为整数读取的字符，范围在 0 到 65535 之间 (0x00-0xffff) (2 个字节的 Unicode 码)，如果已到达流的末尾，则返回 -1。 int read(char[] cbuf)：将字符读入数组。如果已到达流的末尾，则返回 -1。否则返回本次读取的字符数。 int read(char[] cbuf,int off,int len)：将字符读入数组的某一部分。存到数组 cbuf 中，从 off 处开始存储，最多读 len 个字符。如果已到达流的末尾，则返回 -1。否则返回本次读取的字符数。 public void close() throws IOException：关闭此输入流并释放与该流关联的所有系统资源。 OutputStream &amp; Writer： OutputStream 和 Writer 是所有输入流的基类。 OutputStream 的典型实现：FileOutStream。 void write(int b) void write(byte[] b) void write(byte[] b, int off, int len) public void flush() throws IOException public void close() throws IOException Writer 的典型实现：FileWriter。 void write(int c) void write(char[] cbuf) void write(char[] buff, int off, int len) public void flush() throws IOException public void close() throws IOException 因为字符流直接以字符作为操作单位，所以 Writer 还可以用字符串来替换字符数组，即以 String 对象作为参数。 void write(String str) void write(String str, int off, int len) FileOutputStream 从文件系统中的某个文件中获得输出字节。FileOutputStream 用于写出非文本数据之类的原始字节流。如果要要写出文本数据的字符流，需要使用 FileWriter。 OutputStream： void write(int b)：将指定的字节写入此输出流。write 的常规协定是：向输出流写入一个字节。要写入的字节是参数 b 的八个低位。b 的 24 个高位将被忽略，即写入 0 ~ 255 范围的。 void write(byte[] b)：将 b.length() 个字节从指定的 byte 数组写入此输出流。write(b) 的常规协定是：应该与调用 write(b, 0, b.length) 的效果完全相同。 void write(byte[] b,int off,int len)：将指定 byte 数组中从偏移量 off 开始的 len 个字节写入此输出流。 public void flush() throws IOException：刷新此输出流并强制写出所有缓冲的输出字节，调用此方法指示应将这些字节立即写入它们预期的目标。 public void close() throws IOException：关闭此输出流并释放与该流关联的所有系统资源。 Writer： void write(int c)：写入单个字符。要写入的字符包含在给定整数值的 16 个低位中，16 高位被忽略。 即写入0 到 65535 之间的 Unicode 码。 void write(char[] cbuf)：写入字符数组。 void write(char[] cbuf,int off,int len)：写入字符数组的某一部分。从 off 开始，写入 len 个字符。 void write(String str)：写入字符串。 void write(String str,int off,int len)：写入字符串的某一部分。 public void flush() throws IOException：刷新该流的缓冲，则立即将它们写入预期目标。 public void close() throws IOException：关闭此输出流并释放与该流关联的所有系统资源。 节点流 (或文件流) 读取文件流程： 实例化 File 类的对象，指明要操作的文件。 提供具体的流对象。 数据的读入。 流的关闭操作。 写入文件流程： 实例化 File 类的对象，指明写出到的文件。 提供具体的流对象。 数据的写入。 流的关闭操作。 定义文件路径时，可以用 / 或者 \\。 在写入一个文件时，如果使用构造器 FileOutputStream(file)，则目录下有同名文件将被覆盖。 如果使用构造器 FileOutputStream(file,true)，则目录下的同名文件不会被覆盖，而是在文件内容末尾追加内容。 在读取文件时，必须保证该文件已存在，否则报异常。 对于非文本文件 (.jpg，.mp3，.mp4，.avi，.rmvb，.doc，.ppt 等)，使用字节流处理。如果使用字节流操作文本文件，在输出到控制台时，可能会出现乱码。 如果只是将文本文件复制到其他地方，也可以使用字节流。 对于文本文件 (.txt，.java，.c，.cpp 等)，使用字符流处理。 FileReader 和 FileWriter 操作的实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216/** * 一、流的分类： * 1.操作数据单位：字节流、字符流 * 2.数据的流向：输入流、输出流 * 3.流的角色：节点流、处理流 * * 二、流的体系结构 * 抽象基类 节点流（或文件流） 缓冲流（处理流的一种） * InputStream FileInputStream (read(byte[] buffer)) BufferedInputStream (read(byte[] buffer)) * OutputStream FileOutputStream (write(byte[] buffer,0,len) BufferedOutputStream (write(byte[] buffer,0,len)/flush() * Reader FileReader (read(char[] cbuf)) BufferedReader (read(char[] cbuf)/readLine()) * Writer FileWriter (write(char[] cbuf,0,len) BufferedWriter (write(char[] cbuf,0,len)/flush() */public class FileReaderWriterTest &#123; /* 将当前Module下的hello.txt文件内容读入程序中，并输出到控制台 说明点： 1. read()的理解：返回读入的一个字符。如果达到文件末尾，返回-1 2. 异常的处理：为了保证流资源一定可以执行关闭操作。需要使用try-catch-finally处理 3. 读入的文件一定要存在，否则就会报FileNotFoundException。 */ // read(): 返回读入的一个字符。如果达到文件末尾，返回-1 @Test public void testFileReader() &#123; FileReader fr = null; try &#123; // 1.实例化File类的对象，指明要操作的文件 File file = new File(&quot;hello.txt&quot;);// 相较于当前Module // 2.提供具体的流 fr = new FileReader(file); // 3.数据的读入 // 方式一： /*int data = fr.read(); while (data != -1) &#123; System.out.print((char) data); data = fr.read(); &#125;*/ // 方式二：语法上针对于方式一的修改 int data; while ((data = fr.read()) != -1) &#123; System.out.print((char) data); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.流的关闭操作 // 方式一： /*try &#123; if (fr != null) fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;*/ // 方式二： if (fr != null) &#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; // 对read()操作升级：使用read的重载方法read(char[] cbuf) @Test public void testFileReader1() &#123; FileReader fr = null; try &#123; // 1.File类的实例化 File file = new File(&quot;hello.txt&quot;); // 2.FileReader流的实例化 fr = new FileReader(file); // 3.读入的操作 // read(char[] cbuf)：返回每次读入cbuf数组中的字符的个数。如果达到文件末尾，返回-1 char[] cbuf = new char[5]; int len; while ((len = fr.read(cbuf)) != -1) &#123; // 方式一： // 错误的写法，如果以cubf的length为基准，可能会造成多输出内容 /*for (int i = 0; i &lt; cbuf.length; i++) &#123; System.out.print(cbuf[i]); &#125;*/ // 正确的写法 /*for (int i = 0; i &lt; len; i++) &#123; System.out.print(cbuf[i]); &#125;*/ //方式二： // 错误的写法，对应着方式一的错误的写法 /*String str = new String(cbuf); System.out.print(str);*/ // 正确的写法，对应着方式一的正确的写法 String str = new String(cbuf, 0, len); System.out.print(str); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fr != null) &#123; // 4.资源的关闭 try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 从内存中写出数据到硬盘的文件里。 说明： 1. 输出操作，对应的File可以不存在的。并不会报异常 2. File对应的硬盘中的文件如果不存在，在输出的过程中，会自动创建此文件。 File对应的硬盘中的文件如果存在： 如果流使用的构造器是：FileWriter(file,false) / FileWriter(file)---&gt;对原有文件的覆盖 如果流使用的构造器是：FileWriter(file,true)---&gt;不会对原有文件覆盖，而是在原有文件基础上追加内容 */ @Test public void testFileWriter() &#123; FileWriter fw = null; try &#123; // 1.提供File类的对象，指明写出到的文件 File file = new File(&quot;hello1.txt&quot;); // 2.提供FileWriter的对象，用于数据的写出 fw = new FileWriter(file, false); // 3.写出的操作 fw.write(&quot;I have a dream!\\n&quot;); fw.write(&quot;you need to have a dream!&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.流资源的关闭 if (fw != null) &#123; try &#123; fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 实现对已存在文件的复制 */ @Test public void testFileReaderFileWriter() &#123; FileReader fr = null; FileWriter fw = null; try &#123; // 1.创建File类的对象，指明读入和写出的文件 File srcFile = new File(&quot;hello.txt&quot;); File destFile = new File(&quot;hello2.txt&quot;); // 不能使用字符流来处理图片等字节数据 /*File srcFile = new File(&quot;爱情与友情.jpg&quot;); File destFile = new File(&quot;爱情与友情1.jpg&quot;);*/ // 2.创建输入流和输出流的对象 fr = new FileReader(srcFile); fw = new FileWriter(destFile); // 3.数据的读入和写出操作 char[] cbuf = new char[5]; // 记录每次读入到cbuf数组中的字符的个数 int len; while ((len = fr.read(cbuf)) != -1) &#123; // 每次写出len个字符 fw.write(cbuf, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭流资源 // 方式一： /*try &#123; if (fw != null) fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fr != null) fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;*/ // 方式二： try &#123; if (fw != null) fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; if (fr != null) fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; FileInputStream 和 FileOutputStream 操作的实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147/** * 测试FileInputStream和FileOutputStream的使用 * * 结论： * 1. 对于文本文件(.txt,.java,.c,.cpp)，使用字符流处理 * 2. 对于非文本文件(.jpg,.mp3,.mp4,.avi,.doc,.ppt,...)，使用字节流处理 */public class FileInputOutputStreamTest &#123; /* 使用字节流FileInputStream处理文本文件，可能出现乱码。 */ @Test public void testFileInputStream() &#123; FileInputStream fis = null; try &#123; // 1. 造文件 File file = new File(&quot;hello.txt&quot;); // 2.造流 fis = new FileInputStream(file); // 3.读数据 byte[] buffer = new byte[5]; // 记录每次读取的字节的个数 int len; while ((len = fis.read(buffer)) != -1) &#123; String str = new String(buffer, 0, len); System.out.print(str); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fis != null) &#123; // 4.关闭资源 try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 实现对图片的复制操作 */ @Test public void testFileInputOutputStream() &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; // 1.获取文件 File srcFile = new File(&quot;爱情与友情.jpg&quot;); File destFile = new File(&quot;爱情与友情2.jpg&quot;); // 2.获取流 fis = new FileInputStream(srcFile); fos = new FileOutputStream(destFile); // 3.复制的过程 byte[] buffer = new byte[5]; int len; while ((len = fis.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭流 if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 指定路径下文件的复制 */ public void copyFile(String srcPath, String destPath) &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; // 1.获取文件 File srcFile = new File(srcPath); File destFile = new File(destPath); // 2.获取流 fis = new FileInputStream(srcFile); fos = new FileOutputStream(destFile); // 3.复制的过程 byte[] buffer = new byte[1024]; int len; while ((len = fis.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭流 if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; @Test public void testCopyFile() &#123; long start = System.currentTimeMillis(); String srcPath = &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\01-视频.avi&quot;; String destPath = &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\02-视频.avi&quot;; /*String srcPath = &quot;hello.txt&quot;; String destPath = &quot;hello3.txt&quot;;*/ copyFile(srcPath, destPath); long end = System.currentTimeMillis(); System.out.println(&quot;复制操作花费的时间为：&quot; + (end - start));// 618 &#125;&#125; 处理流之一：缓冲流 为了提高数据读写的速度，Java API 提供了带缓冲功能的流类，在使用这些流类时，会创建一个内部缓冲区数组，缺省使用 8192 个字节 (8Kb) 的缓冲区。 123public class BufferedInputStream extends FilterInputStream &#123; private static int DEFAULT_BUFFER_SIZE = 8192;&#125; 123public class BufferedReader extends Reader &#123; private static int defaultCharBufferSize = 8192;&#125; 123public class BufferedWriter extends Writer &#123; private static int defaultCharBufferSize = 8192;&#125; 缓冲流要 “套接” 在相应的节点流之上，根据数据操作单位可以把缓冲流分为： BufferedInputStream 和 和 BufferedOutputStream public BufferedInputStream(InputStream in) ：创建一个新的缓冲输入流，注意参数类型为 InputStream。 public BufferedOutputStream(OutputStream out)： 创建一个新的缓冲输出流，注意参数类型为 OutputStream。 BufferedReader 和 BufferedWriter public BufferedReader(Reader in) ：创建一个新的缓冲输入流，注意参数类型为 Reader。 public BufferedWriter(Writer out)： 创建一个新的缓冲输出流，注意参数类型为 Writer。 当读取数据时，数据按块读入缓冲区，其后的读操作则直接访问缓冲区。 当使用 BufferedInputStream 读取字节文件时，BufferedInputStream 会一次性从文件中读取 8192 个字节 (8Kb) 存在缓冲区中，直到缓冲区装满了，才重新从文件中读取下一个 8192 个字节数组。 向流中写入字节时，不会直接写到文件，先写到缓冲区中直到缓冲区写满，BufferedOutputStream 才会把缓冲区中的数据一次性写到文件里。使用 flush() 可以强制将缓冲区的内容全部写入输出流。 flush() 的使用：手动将 buffer 中内容写入文件。 如果使用带缓冲区的流对象的 close()，不但会关闭流，还会在关闭流之前刷新缓冲区，但关闭流后不能再写出。 关闭流的顺序和打开流的顺序相反。一般只需关闭最外层流即可，关闭最外层流也会相应关闭内层节点流。 流程示意图： 实现非文本文件及文本文件的复制： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183/** * 处理流之一：缓冲流的使用 * * 1.缓冲流： * BufferedInputStream * BufferedOutputStream * BufferedReader * BufferedWriter * * 2.作用：提高流的读取、写入的速度 * 提高读写速度的原因：内部提供了一个缓冲区 * * 3. 处理流，就是&quot;套接&quot;在已有的流的基础上。(不一定必须是套接在节点流之上) */public class BufferedStreamTest &#123; /* 使用BufferedInputStream和BufferedOutputStream实现非文本文件的复制 */ @Test public void BufferedStreamTest() throws FileNotFoundException &#123; BufferedInputStream bis = null; BufferedOutputStream bos = null; try &#123; // 1.造文件 File srcFile = new File(&quot;爱情与友情.jpg&quot;); File destFile = new File(&quot;爱情与友情3.jpg&quot;); // 2.造流 // 2.1 造节点流 FileInputStream fis = new FileInputStream((srcFile)); FileOutputStream fos = new FileOutputStream(destFile); // 2.2 造缓冲流 bis = new BufferedInputStream(fis); bos = new BufferedOutputStream(fos); // 3.复制的细节：读取、写入 byte[] buffer = new byte[10]; int len; while ((len = bis.read(buffer)) != -1) &#123; bos.write(buffer, 0, len); // bos.flush();// 显示的刷新缓冲区，一般不需要 &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.资源关闭 // 要求：先关闭外层的流，再关闭内层的流 if (bos != null) &#123; try &#123; bos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (bis != null) &#123; try &#123; bis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 说明：关闭外层流的同时，内层流也会自动的进行关闭。关于内层流的关闭，我们可以省略. // fos.close(); // fis.close(); &#125; &#125; /* 使用BufferedInputStream和BufferedOutputStream实现文件复制的方法 */ public void copyFileWithBuffered(String srcPath, String destPath) &#123; BufferedInputStream bis = null; BufferedOutputStream bos = null; try &#123; // 1.造文件 File srcFile = new File(srcPath); File destFile = new File(destPath); // 2.造流 // 2.1 造节点流 FileInputStream fis = new FileInputStream((srcFile)); FileOutputStream fos = new FileOutputStream(destFile); // 2.2 造缓冲流 bis = new BufferedInputStream(fis); bos = new BufferedOutputStream(fos); // 3.复制的细节：读取、写入 byte[] buffer = new byte[1024]; int len; while ((len = bis.read(buffer)) != -1) &#123; bos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.资源关闭 // 要求：先关闭外层的流，再关闭内层的流 if (bos != null) &#123; try &#123; bos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (bis != null) &#123; try &#123; bis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 说明：关闭外层流的同时，内层流也会自动的进行关闭。关于内层流的关闭，我们可以省略. // fos.close(); // fis.close(); &#125; &#125; @Test public void testCopyFileWithBuffered() &#123; long start = System.currentTimeMillis(); String srcPath = &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\01-视频.avi&quot;; String destPath = &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\03-视频.avi&quot;; copyFileWithBuffered(srcPath, destPath); long end = System.currentTimeMillis(); System.out.println(&quot;复制操作花费的时间为：&quot; + (end - start));//618 - 176 &#125; /* 使用BufferedReader和BufferedWriter实现文本文件的复制 */ @Test public void testBufferedReaderBufferedWriter() &#123; BufferedReader br = null; BufferedWriter bw = null; try &#123; // 1.创建文件和相应的流 br = new BufferedReader(new FileReader(new File(&quot;dbcp.txt&quot;))); bw = new BufferedWriter(new FileWriter(new File(&quot;dbcp1.txt&quot;))); // 2.读写操作 // 方式一：使用char[]数组 /*char[] cbuf = new char[1024]; int len; while ((len = br.read(cbuf)) != -1) &#123;// 读到文件末尾时返回-1 bw.write(cbuf, 0, len); // bw.flush(); &#125;*/ // 方式二：使用String String data; while ((data = br.readLine()) != null) &#123;// 读到文件末尾时返回null // 方法一： // bw.write(data + &quot;\\n&quot;);// data中不包含换行符 // 方法二： bw.write(data);// data中不包含换行符 bw.newLine();// 提供换行的操作 &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 3.关闭资源 if (bw != null) &#123; try &#123; bw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (br != null) &#123; try &#123; br.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 实现图片加密： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class ImageEncryption &#123; /* 图片的加密 */ @Test public void test1() &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; fis = new FileInputStream(&quot;爱情与友情.jpg&quot;); fos = new FileOutputStream(&quot;爱情与友情secret.jpg&quot;); byte[] buffer = new byte[20]; int len; while ((len = fis.read(buffer)) != -1) &#123; // 加密：对字节数组进行修改，异或操作 // 错误的写法，buffer数组中的数据没有改变，只是重新复制给了变量b /*for (byte b : buffer) &#123; b = (byte) (b ^ 5); &#125;*/ // 正确的写法 for (int i = 0; i &lt; len; i++) &#123; buffer[i] = (byte) (buffer[i] ^ 5); &#125; fos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 图片的解密 */ @Test public void test2() &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; fis = new FileInputStream(&quot;爱情与友情secret.jpg&quot;); fos = new FileOutputStream(&quot;爱情与友情4.jpg&quot;); byte[] buffer = new byte[20]; int len; while ((len = fis.read(buffer)) != -1) &#123; // 解密：对字节数组进行修改，异或操作之后再异或，返回的是自己本身 // 错误的写法 /*for (byte b : buffer) &#123; b = (byte) (b ^ 5); &#125;*/ // 正确的写法 for (int i = 0; i &lt; len; i++) &#123; buffer[i] = (byte) (buffer[i] ^ 5); &#125; fos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 获取文本上每个字符出现的次数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class WordCount &#123; /* 说明：如果使用单元测试，文件相对路径为当前module 如果使用main()测试，文件相对路径为当前工程 */ @Test public void testWordCount() &#123; FileReader fr = null; BufferedWriter bw = null; try &#123; // 1.创建Map集合 Map&lt;Character, Integer&gt; map = new HashMap&lt;Character, Integer&gt;(); // 2.遍历每一个字符，每一个字符出现的次数放到map中 fr = new FileReader(&quot;dbcp.txt&quot;); int c; while ((c = fr.read()) != -1) &#123; // int 还原 char char ch = (char) c; // 判断char是否在map中第一次出现 if (map.get(ch) == null) &#123; map.put(ch, 1); &#125; else &#123; map.put(ch, map.get(ch) + 1); &#125; &#125; // 3.把map中数据存在文件count.txt // 3.1 创建Writer bw = new BufferedWriter(new FileWriter(&quot;wordcount.txt&quot;)); // 3.2 遍历map，再写入数据 Set&lt;Map.Entry&lt;Character, Integer&gt;&gt; entrySet = map.entrySet(); for (Map.Entry&lt;Character, Integer&gt; entry : entrySet) &#123; switch (entry.getKey()) &#123; case &#x27; &#x27;: bw.write(&quot;空格 = &quot; + entry.getValue()); break; case &#x27;\\t&#x27;://\\t表示tab 键字符 bw.write(&quot;tab键 = &quot; + entry.getValue()); break; case &#x27;\\r&#x27;:// bw.write(&quot;回车 = &quot; + entry.getValue()); break; case &#x27;\\n&#x27;:// bw.write(&quot;换行 = &quot; + entry.getValue()); break; default: bw.write(entry.getKey() + &quot; = &quot; + entry.getValue()); break; &#125; bw.newLine(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭流 if (fr != null) &#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (bw != null) &#123; try &#123; bw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 处理流之二：转换流 转换流提供了在字节流和字符流之间的转换。 Java API 提供了两个转换流： InputStreamReader：将 InputStream 转换为 Reader。 InputStreamReader(InputStream in)：创建一个使用默认字符集的字符流。 InputStreamReader(InputStream in, String charsetName)：创建一个指定字符集的字符流。 OutputStreamWriter：将 Writer 转换为 OutputStream。 OutputStreamWriter(OutputStream in)：创建一个使用默认字符集的字符流。 OutputStreamWriter(OutputStream in, String charsetName)：创建一个指定字符集的字符流。 字节流中的数据都是字符时，转成字符流操作更高效。 很多时候我们使用转换流来处理文件乱码问题，实现编码和解码的功能。 InputStreamReader： 实现将字节的输入流按指定字符集转换为字符的输入流。 需要和 InputStream 套接。 构造器 public InputStreamReader(InputStream in) public InputSreamReader(InputStream in,String charsetName) 比如：Reader isr = new InputStreamReader(System.in,&quot;gbk&quot;);，指定字符集为 gbk。 OutputStreamWriter： 实现将字符的输出流按指定字符集转换为字节的输出流。 需要和 OutputStream 套接。 构造器 public OutputStreamWriter(OutputStream out) public OutputSreamWriter(OutputStream out,String charsetName) 使用 InputStreamReader 解码时，使用的字符集取决于 OutputStreamWriter 编码时使用的字符集。 流程示意图： 转换流的编码应用： 可以将字符按指定编码格式存储。 可以对文本数据按指定编码格式来解读。 指定编码表的动作由构造器完成。 为了达到最高效率，可以考虑在 BufferedReader 内包装 InputStreamReader： 1BufferedReader in = new BufferedReader(new InputStreamReader(System.in))； 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101/** * 处理流之二：转换流的使用 * 1.转换流：属于字符流 * InputStreamReader：将一个字节的输入流转换为字符的输入流 * OutputStreamWriter：将一个字符的输出流转换为字节的输出流 * * 2.作用：提供字节流与字符流之间的转换 * * 3. 解码：字节、字节数组 ---&gt;字符数组、字符串 * 编码：字符数组、字符串 ---&gt; 字节、字节数组 * * * 4.字符集 * ASCII：美国标准信息交换码。 * 用一个字节的7位可以表示。 * ISO8859-1：拉丁码表。欧洲码表 * 用一个字节的8位表示。 * GB2312：中国的中文编码表。最多两个字节编码所有字符 * GBK：中国的中文编码表升级，融合了更多的中文文字符号。最多两个字节编码 * Unicode：国际标准码，融合了目前人类使用的所有字符。为每个字符分配唯一的字符码。所有的文字都用两个字节来表示。 * UTF-8：变长的编码方式，可用1-4个字节来表示一个字符。 */public class InputStreamReaderTest &#123; /* 此时处理异常的话，仍然应该使用try-catch-finally InputStreamReader的使用，实现字节的输入流到字符的输入流的转换 */ @Test public void test1() &#123; InputStreamReader isr = null; try &#123; FileInputStream fis = new FileInputStream(&quot;dbcp.txt&quot;); // InputStreamReader isr = new InputStreamReader(fis);// 使用系统默认的字符集，如果在IDEA中，就是看IDEA设置的默认字符集 // 参数2指明了字符集，具体使用哪个字符集，取决于文件dbcp.txt保存时使用的字符集 isr = new InputStreamReader(fis, StandardCharsets.UTF_8);// 指定字符集 char[] cbuf = new char[20]; int len; while ((len = isr.read(cbuf)) != -1) &#123; String str = new String(cbuf, 0, len); System.out.print(str); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (isr != null) &#123; try &#123; isr.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125; /* 此时处理异常的话，仍然应该使用try-catch-finally 综合使用InputStreamReader和OutputStreamWriter */ @Test public void test2() &#123; InputStreamReader isr = null; OutputStreamWriter osw = null; try &#123; // 1.造文件、造流 File file1 = new File(&quot;dbcp.txt&quot;); File file2 = new File(&quot;dbcp_gbk.txt&quot;); FileInputStream fis = new FileInputStream(file1); FileOutputStream fos = new FileOutputStream(file2); isr = new InputStreamReader(fis, StandardCharsets.UTF_8); osw = new OutputStreamWriter(fos, &quot;gbk&quot;); // 2.读写过程 char[] cbuf = new char[20]; int len; while ((len = isr.read(cbuf)) != -1) &#123; osw.write(cbuf, 0, len); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; // 3.关闭资源 if (isr != null) &#123; try &#123; isr.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; if (osw != null) &#123; try &#123; osw.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 处理流之三：标准输入、输出流 System.in 和 System.out 分别代表了系统标准的输入和输出设备。 默认输入设备是：键盘，输出设备是：显示器。 System.in 的类型是 InputStream。 System.out 的类型是 PrintStream，其是 OutputStream 的子类 FilterOutputStream 的子类。 重定向：通过 System 类的 setIn() 和 setOut() 对默认设备进行改变。 public static void setIn(InputStream in) public static void setOut(PrintStream out) 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class OtherStreamTest &#123; /* 1.标准的输入、输出流 1.1 System.in: 标准的输入流，默认从键盘输入 System.out: 标准的输出流，默认从控制台输出 1.2 System类的setIn(InputStream is) / setOut(PrintStream ps)方式重新指定输入和输出的流。 1.3练习： 从键盘输入字符串，要求将读取到的整行字符串转成大写输出。然后继续进行输入操作， 直至当输入“e”或者“exit”时，退出程序。 方法一：使用Scanner实现，调用next()返回一个字符串 方法二：使用System.in实现。System.in ---&gt; 转换流 ---&gt; BufferedReader的readLine() */ // IDEA的单元测试不支持从键盘输入，更改为main() public static void main(String[] args) &#123; BufferedReader br = null; try &#123; InputStreamReader isr = new InputStreamReader(System.in); br = new BufferedReader(isr); while (true) &#123; System.out.println(&quot;请输入字符串：&quot;); String data = br.readLine(); if (&quot;e&quot;.equalsIgnoreCase(data) || &quot;exit&quot;.equalsIgnoreCase(data)) &#123; System.out.println(&quot;程序结束&quot;); break; &#125; String upperCase = data.toUpperCase(); System.out.println(upperCase); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (br != null) &#123; try &#123; br.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 模拟 Scanner： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * MyInput.java: Contain the methods for reading int, double, float, boolean, short, byte and * string values from the keyboard */public class MyInput &#123; // Read a string from the keyboard public static String readString() &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); // Declare and initialize the string String string = &quot;&quot;; // Get the string from the keyboard try &#123; string = br.readLine(); &#125; catch (IOException ex) &#123; System.out.println(ex); &#125; // Return the string obtained from the keyboard return string; &#125; // Read an int value from the keyboard public static int readInt() &#123; return Integer.parseInt(readString()); &#125; // Read a double value from the keyboard public static double readDouble() &#123; return Double.parseDouble(readString()); &#125; // Read a byte value from the keyboard public static double readByte() &#123; return Byte.parseByte(readString()); &#125; // Read a short value from the keyboard public static double readShort() &#123; return Short.parseShort(readString()); &#125; // Read a long value from the keyboard public static double readLong() &#123; return Long.parseLong(readString()); &#125; // Read a float value from the keyboard public static double readFloat() &#123; return Float.parseFloat(readString()); &#125; public static void main(String[] args) &#123; int i = readInt(); System.out.println(&quot;输出的数为：&quot; + i); &#125;&#125; 处理流之四：打印流 实现将基本数据类型的数据格式转化为字符串输出。 打印流：PrintStream 和 PrintWriter。 提供了一系列重载的 print() 和 println()，用于多种数据类型的输出。 PrintStream 和 PrintWriter 的输出不会抛出 IOException 异常。 PrintStream 和 PrintWriter 有自动 flush 功能。 PrintStream 打印的所有字符都使用平台的默认字符编码转换为字节。在需要写入字符而不是写入字节的情况下，应该使用 PrintWriter 类。 System.out 返回的是 PrintStream 的实例。 把标准输出流 (控制台输出) 改成文件： 123456789101112131415161718192021222324252627282930313233343536public class OtherStreamTest &#123; /* 2. 打印流：PrintStream 和PrintWriter 2.1 提供了一系列重载的print()和println() 2.2 练习：将ASCII字符输出到自定义的外部文件 */ @Test public void test2() &#123; PrintStream ps = null; try &#123; FileOutputStream fos = new FileOutputStream(new File(&quot;D:\\\\text.txt&quot;)); // 创建打印输出流，设置为自动刷新模式(写入换行符或字节 &#x27;\\n&#x27; 时都会刷新输出缓冲区) ps = new PrintStream(fos, true); // 把标准输出流(控制台输出)改成输出到本地文件 if (ps != null) &#123; // 如果不设置，下面的循环输出是在控制台 // 设置之后，控制台不再输出，而是输出到D:\\text.txt System.setOut(ps); &#125; // 开始输出ASCII字符 for (int i = 0; i &lt;= 255; i++) &#123; System.out.print((char) i); if (i % 50 == 0) &#123;// 每50个数据一行 System.out.println();// 换行 &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (ps != null) &#123; ps.close(); &#125; &#125; &#125;&#125; 处理流之五：数据流 为了方便地操作 Java 语言的基本数据类型和 String 类型的数据，可以使用数据流。(不能操作内存中的对象) 数据流有两个类：分别用于读取和写出基本数据类型、String类的数据。 DataInputStream 和 DataOutputStream。 分别套接在 InputStream 和 和 OutputStream 子类的流上。 用 DataOutputStream 输出的文件需要用 DataInputStream 来读取。 DataInputStream 读取不同类型的数据的顺序，要与当初 DataOutputStream 写入文件时，保存的数据的顺序一致。 DataInputStream 中的方法： boolean readBoolean()，byte readByte() char readChar()，float readFloat() double readDouble()，short readShort() long readLong()，int readInt() String readUTF()，void readFully(byte[] b) DataOutputStream 中的方法： 将上述的方法的 read 改为相应的 write 即可。 将内存中的字符串、基本数据类型的变量写出到文件中，再读取到内存中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class OtherStreamTest &#123; /* 3. 数据流 3.1 DataInputStream 和 DataOutputStream 3.2 作用：用于读取或写出基本数据类型的变量或字符串 练习：将内存中的字符串、基本数据类型的变量写出到文件中。 注意：处理异常的话，仍然应该使用try-catch-finally。 */ @Test public void test3() &#123; DataOutputStream dos = null; try &#123; // 1.造流 dos = new DataOutputStream(new FileOutputStream(&quot;data.txt&quot;)); // 2.写入操作 dos.writeUTF(&quot;刘建辰&quot;);// 写入String dos.flush();// 刷新操作，将内存中的数据立即写入文件，也可以在关闭流时自动刷新 dos.writeInt(23);// 写入int dos.flush(); dos.writeBoolean(true);// 写入boolean dos.flush(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; // 3.关闭流 if (dos != null) &#123; try &#123; dos.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125; /* 将文件中存储的基本数据类型变量和字符串读取到内存中，保存在变量中。 注意点：读取不同类型的数据的顺序要与当初写入文件时，保存的数据的顺序一致！ */ @Test public void test4() &#123; DataInputStream dis = null; try &#123; // 1.造流 dis = new DataInputStream(new FileInputStream(&quot;data.txt&quot;)); // 2.读取操作 String name = dis.readUTF();// 读取String int age = dis.readInt();// 读取int boolean isMale = dis.readBoolean();// 读取boolean System.out.println(&quot;name = &quot; + name); System.out.println(&quot;age = &quot; + age); System.out.println(&quot;isMale = &quot; + isMale); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; // 3.关闭流 if (dis!=null) &#123; try &#123; dis.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 处理流之六：对象流 ObjectInputStream 和 OjbectOutputSteam：用于存储和读取基本数据类型数据或对象的处理流。它的强大之处就是可以把 Java 中的对象写入到数据源中，也能把对象从数据源中还原回来。 一般情况下，会把对象转换为 Json 字符串，然后进行序列化和反序列化操作，而不是直接操作对象。 序列化：用 ObjectOutputStream 类保存基本类型数据或对象的机制。 反序列化：用 ObjectInputStream 类读取基本类型数据或对象的机制。 ObjectOutputStream 和 ObjectInputStream 不能序列化 static 和 transient 修饰的成员变量。 在序列化一个类的对象时，如果类中含有 static 和 transient 修饰的成员变量，则在反序列化时，这些成员变量的值会变成默认值，而不是序列化时这个对象赋予的值。比如，Person 类含有一个 static 修饰的 String name 属性，序列化时，对象把 name 赋值为张三，在反序列化时，name 会变为 null 对象序列化机制允许把内存中的 Java 对象转换成平台无关的二进制流 (序列化操作)，从而允许把这种二进制流持久地保存在磁盘上，或通过网络将这种二进制流传输到另一个网络节点。当其它程序获取了这种二进制流，就可以恢复成原来的 Java 对象 (反序列化操作)。 序列化的好处在于可将任何实现了 Serializable 接口的对象转化为字节数据，使其在保存和传输时可被还原。 序列化是 RMI (Remote Method Invoke – 远程方法调用) 过程的参数和返回值都必须实现的机制，而 RMI 是 JavaEE 的基础，因此序列化机制是 JavaEE 平台的基础。 如果需要让某个对象支持序列化机制，则必须让对象所属的类及其属性是可序列化的，为了让某个类是可序列化的，该类必须实现如下两个接口之一。否则，会抛出 NotSerializableException 异常。 Serializable Externalizable 凡是实现 Serializable 接口的类都有一个表示序列化版本标识符的静态变量： private static final long serialVersionUID; serialVersionUID 用来表明类的不同版本间的兼容性。 简言之，其目的是以序列化对象进行版本控制，有关各版本反序列化时是否兼容。 如果类没有显示定义这个静态常量，它的值是 Java 运行时环境根据类的内部细节自动生成的。此时，若类的实例变量做了修改，serialVersionUID 可能发生变化，则再对修改之前被序列化的类进行反序列化操作时，会操作失败。因此，建议显式声明 serialVersionUID。 在某些场合，希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有相同的 serialVersionUID；在某些场合，不希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有不同的 serialVersionUID。 当序列化了一个类实例后，后续可能更改一个字段或添加一个字段。如果不设置 serialVersionUID，所做的任何更改都将导致无法反序化旧有实例，并在反序列化时抛出一个异常；如果你添加了 serialVersionUID，在反序列旧有实例时，新添加或更改的字段值将设为初始化值 (对象为 null，基本类型为相应的初始默认值)，字段被删除将不设置。 简单来说，Java 的序列化机制是通过在运行时判断类的 serialVersionUID 来验证版本一致性的。在进行反序列化时，JVM 会把传来的字节流中的 serialVersionUID 与本地相应实体类的 serialVersionUID 进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常，即 InvalidCastException。 若某个类实现了 Serializable 接口，该类的对象就是可序列化的： 创建一个 ObjectOutputStream。 public ObjectOutputStream(OutputStream out)： 创建一个指定 OutputStream 的 ObjectOutputStream。 调用 ObjectOutputStream 对象的 writeObject(Object obj) 输出可序列化对象。 注意写出一次，操作 flush() 一次。 反序列化： 创建一个 ObjectInputStream。 public ObjectInputStream(InputStream in)： 创建一个指定 InputStream 的 ObjectInputStream。 调用 readObject() 读取流中的对象。 强调：如果某个类的属性不是基本数据类型或 String 类型，而是另一个引用类型，那么这个引用类型必须是可序列化的，否则拥有该类型的 Field 的类也不能序列化。 默认情况下，基本数据类型是可序列化的。String 实现了 Serializable 接口。 流程示意图： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * Person需要满足如下的要求，方可序列化 * 1.需要实现接口：Serializable * 2.当前类提供一个全局常量：serialVersionUID * 3.除了当前Person类需要实现Serializable接口之外，还必须保证其内部所有属性 * 也必须是可序列化的。（默认情况下，基本数据类型可序列化） * * 补充：ObjectOutputStream和ObjectInputStream不能序列化static和transient修饰的成员变量 */public class Person implements Serializable &#123; public static final long serialVersionUID = 475463534532L; private String name; private int age; private int id; private Account acct; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public Person(String name, int age, int id) &#123; this.name = name; this.age = age; this.id = id; &#125; public Person(String name, int age, int id, Account acct) &#123; this.name = name; this.age = age; this.id = id; this.acct = acct; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, id=&quot; + id + &quot;, acct=&quot; + acct + &#x27;&#125;&#x27;; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125;class Account implements Serializable &#123; public static final long serialVersionUID = 4754534532L; private double balance; public Account(double balance) &#123; this.balance = balance; &#125; @Override public String toString() &#123; return &quot;Account&#123;&quot; + &quot;balance=&quot; + balance + &#x27;&#125;&#x27;; &#125; public double getBalance() &#123; return balance; &#125; public void setBalance(double balance) &#123; this.balance = balance; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * 对象流的使用 * 1.ObjectInputStream 和 ObjectOutputStream * 2.作用：用于存储和读取基本数据类型数据或对象的处理流。它的强大之处就是可以把Java中的对象写入到数据源中，也能把对象从数据源中还原回来。 * * 3.要想一个java对象是可序列化的，需要满足相应的要求。见Person.java * * 4.序列化机制： * 对象序列化机制允许把内存中的Java对象转换成平台无关的二进制流，从而允许把这种 * 二进制流持久地保存在磁盘上，或通过网络将这种二进制流传输到另一个网络节点。 * 当其它程序获取了这种二进制流，就可以恢复成原来的Java对象。 */public class ObjectInputOutputStreamTest &#123; /* 序列化过程：将内存中的java对象保存到磁盘中或通过网络传输出去 使用ObjectOutputStream实现 */ @Test public void testObjectOutputStream() &#123; ObjectOutputStream oos = null; try &#123; // 1.造流 oos = new ObjectOutputStream(new FileOutputStream(&quot;object.dat&quot;)); // 2.序列化：写操作 oos.writeObject(new String(&quot;我爱北京天安门&quot;)); oos.flush();// 刷新操作 oos.writeObject(new Person(&quot;王铭&quot;, 23)); oos.flush(); oos.writeObject(new Person(&quot;张学&quot;, 23, 1001, new Account(5000))); oos.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (oos != null) &#123; // 3.关闭流 try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 反序列化：将磁盘文件中的对象还原为内存中的一个java对象 使用ObjectInputStream来实现 */ @Test public void testObjectInputStream() &#123; ObjectInputStream ois = null; try &#123; // 1.造流 ois = new ObjectInputStream(new FileInputStream(&quot;object.dat&quot;)); // 2.反序列化：读操作 // 文件中保存的是不同类型的对象，反序列化时，需要与序列化时的顺序一致 Object obj = ois.readObject(); String str = (String) obj; System.out.println(str); Person p = (Person) ois.readObject(); System.out.println(p); Person p1 = (Person) ois.readObject(); System.out.println(p1); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (ois != null) &#123; try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 面试题：谈谈你对 java.io.Serializable 接口的理解，我们知道它用于序列化，是空方法接口，还有其它认识吗？ 实现了 Serializable 接口的对象，可将它们转换成一系列字节，并可在以后完全恢复回原来的样子。 这一过程亦可通过网络进行。这意味着序列化机制能自动补偿操作系统间的差异。换句话说，可以先在 Windows 机器上创台 建一个对象，对其序列化，然后通过网络发给一台 Unix 机器，然后在那里准确无误地重新“装配”。不必关心数据在不同机器上如何表示，也不必关心字节的顺序或者其他任何细节。 由于大部分作为参数的类如 String 、Integer 等都实现了 java.io.Serializable 接口，也可以利用多态的性质，作为参数使接口更灵活。 随机存取文件流 RandomAccessFile 声明在 java.io 包下，但直接继承于 java.lang.Object 类。并且它实现了 DataInput、DataOutput 这两个接口，也就意味着这个类既可以读也可以写。 RandomAccessFile 类支持 “随机访问” 的方式，程序可以直接跳到文件的任意地方来读、写文件。 支持只访问文件的部分内容。 可以向已存在的文件后追加内容。 RandomAccessFile 对象包含一个记录指针，用以标示当前读写处的位置。RandomAccessFile 类对象可以自由移动记录指针： long getFilePointer()：获取文件记录指针的当前位置。 void seek(long pos)：将文件记录指针定位到 pos 位置。 构造器 public RandomAccessFile(File file, String mode) public RandomAccessFile(String name, String mode) 创建 RandomAccessFile 类实例需要指定一个 mode 参数，该参数指定 RandomAccessFile 的访问模式： r：以只读方式打开。 rw：打开以便读取和写入。 rwd：打开以便读取和写入；同步文件内容的更新。 rws：打开以便读取和写入；同步文件内容和元数据的更新。 JDK 1.6 上面写的每次 write 数据时，rw 模式，数据不会立即写到硬盘中，而 rwd 模式，数据会被立即写入硬盘。如果写数据过程发生异常，rwd 模式中已被 write 的数据会被保存到硬盘，而 rw 模式的数据会全部丢失。 如果模式为只读 r，则不会创建文件，而是会去读取一个已经存在的文件，如果读取的文件不存在则会出现异常。 如果模式为读写 rw，如果文件不存在则会去创建文件，如果存在则不会创建。 RandomAccessFile 的应用：我们可以用 RandomAccessFile 这个类，来实现一个多线程断点下载的功能，用过下载工具的朋友们都知道，下载前都会建立两个临时文件，一个是与被下载文件大小相同的空文件，另一个是记录文件指针的位置文件，每次暂停的时候，都会保存上一次的指针，然后断点下载的时候，会继续从上一次的地方下载，从而实现断点下载或上传的功能，有兴趣的朋友们可以自己实现下。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132/** * RandomAccessFile的使用 * 1.RandomAccessFile直接继承于java.lang.Object类，实现了DataInput和DataOutput接口 * 2.RandomAccessFile既可以作为一个输入流，又可以作为一个输出流 * * 3.如果RandomAccessFile作为输出流时，写出到的文件如果不存在，则在执行过程中自动创建。 * 如果写出到的文件存在，则会对原有文件内容进行覆盖。（默认情况下，从头覆盖） * * 4. 可以通过相关的操作，实现RandomAccessFile“插入”数据的效果 */public class RandomAccessFileTest &#123; /* 使用RandomAccessFile实现文件的复制 */ @Test public void test1() &#123; RandomAccessFile raf1 = null; RandomAccessFile raf2 = null; try &#123; // 1.造流 raf1 = new RandomAccessFile(new File(&quot;爱情与友情.jpg&quot;), &quot;r&quot;); raf2 = new RandomAccessFile(new File(&quot;爱情与友情1.jpg&quot;), &quot;rw&quot;); // 2.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = raf1.read(buffer)) != -1) &#123; raf2.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 3.关闭流 if (raf1 != null) &#123; try &#123; raf1.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (raf2 != null) &#123; try &#123; raf2.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 使用RandomAccessFile实现文件内容的覆盖和追加 */ @Test public void test2() &#123; RandomAccessFile raf1 = null; try &#123; // hello.txt内容为：abcdefghijklmn File file = new File(&quot;hello.txt&quot;); raf1 = new RandomAccessFile(file, &quot;rw&quot;); raf1.write(&quot;123&quot;.getBytes());// 从头开始覆盖：123defghijklmn raf1.seek(5);// 将指针调到角标为5的位置，角标从0开始 raf1.write(&quot;456&quot;.getBytes());// 从角标为5处开始覆盖：123de456ijklmn raf1.seek(file.length());// 将指针调到文件末尾 raf1.write(&quot;789&quot;.getBytes());// 在文件末尾追加：123de456ijklmn789 &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (raf1 != null) &#123; try &#123; raf1.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125; /* 使用RandomAccessFile实现数据的插入效果 */ @Test public void test3() &#123; RandomAccessFile raf1 = null; try &#123; // hello.txt内容为：abcdefghijklmn File file = new File(&quot;hello.txt&quot;); raf1 = new RandomAccessFile(file, &quot;rw&quot;); // 将指针调到角标为3的位置，从此处开始读入文件的数据 raf1.seek(3); // 方法一：保存指针3后面的所有数据到StringBuilder中 /*StringBuilder builder = new StringBuilder((int) file.length()); byte[] buffer = new byte[20]; int len; while ((len = raf1.read(buffer)) != -1) &#123; builder.append(new String(buffer, 0, len)); &#125;*/ // 方法二：保存指针3后面的所有数据到ByteArrayOutputStream中 ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte[] buffer = new byte[10]; int len; while ((len = raf1.read(buffer)) != -1) &#123; baos.write(buffer, 0, len); &#125; // 经过上面的读操作后，指针位置移到了文件的末尾处 // 调回指针，写入&quot;123&quot;，实际上是覆盖原文件内容 raf1.seek(3); raf1.write(&quot;123&quot;.getBytes());// abc123ghijklmn // 经过上面的写入操作，指针位置已到了123后，紧接着： // 方法一：将StringBuilder中的数据写入到文件中，实际上是覆盖123后的内容 // raf1.write(builder.toString().getBytes());// abc123defghijklmn // 方法二：将ByteArrayOutputStream中的数据写入到文件中 raf1.write(baos.toString().getBytes());// abc123defghijklmn &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (raf1 != null) &#123; try &#123; raf1.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 流的基本应用小结 流是用来处理数据的。 处理数据时，一定要先明确数据源，与数据目的地： 数据源可以是文件，可以是键盘。 数据目的地可以是文件、显示器或者其他设备。 流只是在帮助数据进行传输，并对传输的数据进行处理，比如过滤处理、转换处理等。 NIO.2 中 Path 、Paths 、Files Java NIO (New IO 或 Non-Blocking IO) 是从 Java 1.4 版本开始引入的一套新的 IO API，可以替代标准的 Java IO API。NIO 与原来的 IO 有同样的作用和目的，但是使用的方式完全不同，NIO 支持面向缓冲区的 (IO是面向流的)、基于通道的 IO 操作，NIO 也会以更加高效的方式进行文件的读写操作。 Java API 中提供了两套 NIO，一套是针对标准输入输出 NIO，另一套就是网络编程 NIO。 |—– java.nio.channels.Channel |—– FileChannel：处理本地文件。 |—– SocketChannel：TCP 网络编程的客户端的 Channel。 |—– ServerSocketChannel：TCP 网络编程的服务器端的 Channel。 |—– DatagramChannel：UDP 网络编程中发送端和接收端的 Channel。 随着 JDK 7 的发布，Java 对 NIO 进行了极大的扩展，增强了对文件处理和文件系统特性的支持，以至于我们称他们为 NIO.2。因为 NIO 提供的一些功能，NIO 已经成为文件处理中越来越重要的部分。 早期的 Java 只提供了一个 File 类来访问文件系统，但 File 类的功能比较有限，所提供的方法性能也不高。而且，大多数方法在出错时仅返回失败，并不会提供异常信息。 NIO. 2 为了弥补这种不足，引入了 Path 接口，代表一个平台无关的平台路径，描述了目录结构中文件的位置。Path 可以看成是 File 类的升级版本，实际引用的资源也可以不存在。 在以前 IO 操作是类似如下写法的： 123import java.io.File;File file = new File(&quot;index.html&quot;); 但在 Java 7 中，我们可以这样写： 1234import java.nio.file.Path;import java.nio.file.Paths;Path path = Paths.get(&quot;index.html&quot;); 同时，NIO.2 在 java.nio.file 包下还提供了 Files、Paths 工具类，Files 包含了大量静态的工具方法来操作文件；Paths 则包含了两个返回 Path 的静态工厂方法。 Paths 类提供的获取 Path 对象的方法： static Path get(String first, String … more)：用于将多个字符串串连成路径。 static Path get(URI uri)：返回指定 uri 对应的 Path 路径。 12345678910111213141516public class PathTest &#123; /* 如何使用Paths实例化Path */ @Test public void test1() &#123; Path path1 = Paths.get(&quot;d:\\\\nio\\\\hello.txt&quot;);// = new File(String filepath) System.out.println(path1); Path path2 = Paths.get(&quot;d:\\\\&quot;, &quot;nio\\\\hello.txt&quot;);// = new File(String parent,String filename); System.out.println(path2); Path path3 = Paths.get(&quot;d:\\\\&quot;, &quot;nio&quot;); System.out.println(path3); &#125;&#125; Path 类常用方法： String toString()：返回调用 Path 对象的字符串表示形式。 boolean startsWith(String path)：判断是否以 path 路径开始。 boolean endsWith(String path)：判断是否以 path 路径结束。 boolean isAbsolute()：判断是否是绝对路径。 Path getParent()：返回 Path 对象包含整个路径，不包含 Path 对象指定的文件路径。 Path getRoot()：返回调用 Path 对象的根路径。 Path getFileName()：返回与调用 Path 对象关联的文件名。 int getNameCount()：返回 Path 根目录后面元素的数量。 Path getName(int idx)：返回指定索引位置 idx 的路径名称。 Path toAbsolutePath()：作为绝对路径返回调用 Path 对象。 Path resolve(Path p)：合并两个路径，返回合并后的路径对应的 Path 对象。 File toFile()：将 Path 转化为 File 类的对象。File 类转化为 Path 对象的方法是：Path toPath()。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class PathTest &#123; /* Path中的常用方法 */ @Test public void test2() &#123; Path path1 = Paths.get(&quot;d:\\\\&quot;, &quot;nio\\\\nio1\\\\nio2\\\\hello.txt&quot;); Path path2 = Paths.get(&quot;hello1.txt&quot;);// 相对当前Module的路径 // String toString()：返回调用Path对象的字符串表示形式 System.out.println(path1);// d:\\nio\\nio1\\nio2\\hello.txt // boolean startsWith(String path): 判断是否以path路径开始 System.out.println(path1.startsWith(&quot;d:\\\\nio&quot;));// true // boolean endsWith(String path): 判断是否以path路径结束 System.out.println(path1.endsWith(&quot;hello.txt&quot;));// true // boolean isAbsolute(): 判断是否是绝对路径 System.out.println(path1.isAbsolute() + &quot;~&quot;);// true~ System.out.println(path2.isAbsolute() + &quot;~&quot;);// false~ // Path getParent()：返回Path对象包含整个路径，不包含Path对象指定的文件路径 System.out.println(path1.getParent());// d:\\nio\\nio1\\nio2 System.out.println(path2.getParent());// null // Path getRoot()：返回调用Path对象的根路径 System.out.println(path1.getRoot());// d:\\ System.out.println(path2.getRoot());// null // Path getFileName(): 返回与调用Path对象关联的文件名 System.out.println(path1.getFileName() + &quot;~&quot;);// hello.txt~ System.out.println(path2.getFileName() + &quot;~&quot;);// hello1.txt~ // int getNameCount(): 返回Path根目录后面元素的数量 // Path getName(int idx): 返回指定索引位置idx的路径名称 for (int i = 0; i &lt; path1.getNameCount(); i++) &#123; // nio*****nio1*****nio2*****hello.txt***** System.out.print(path1.getName(i) + &quot;*****&quot;); &#125; System.out.println(); // Path toAbsolutePath(): 作为绝对路径返回调用Path对象 System.out.println(path1.toAbsolutePath());// d:\\nio\\nio1\\nio2\\hello.txt System.out.println(path2.toAbsolutePath());// D:\\xisun-projects\\java_base\\hello1.txt // Path resolve(Path p): 合并两个路径，返回合并后的路径对应的Path对象 Path path3 = Paths.get(&quot;d:\\\\&quot;, &quot;nio&quot;); Path path4 = Paths.get(&quot;nioo\\\\hi.txt&quot;); path3 = path3.resolve(path4); System.out.println(path3);// d:\\nio\\nioo\\hi.txt // File toFile(): 将Path转化为File类的对象 File file = path1.toFile();// Path---&gt;File的转换 // Path toPath(): 将File转化为Path类的对象 Path newPath = file.toPath();// File---&gt;Path的转换 &#125;&#125; java.nio.file.Files：用于操作文件或目录的工具类。 Files 常用方法： Path copy(Path src, Path dest, CopyOption … how)：文件的复制。 Path createDirectory(Path path, FileAttribute&lt;?&gt; … attr)：创建一个目录。 Path createFile(Path path, FileAttribute&lt;?&gt; … arr)：创建一个文件。 void delete(Path path)：删除一个文件/目录，如果不存在，执行报错。 void deleteIfExists(Path path)：Path 对应的文件/目录如果存在，执行删除。 Path move(Path src, Path dest, CopyOption…how)：将 src 移动到 dest 位置。 long size(Path path)：返回 path 指定文件的大小。 boolean exists(Path path, LinkOption … opts)：判断文件是否存在。 boolean isDirectory(Path path, LinkOption … opts)：判断是否是目录。 boolean isRegularFile(Path path, LinkOption … opts)：判断是否是文件。 boolean isHidden(Path path)：判断是否是隐藏文件。 boolean isReadable(Path path)：判断文件是否可读。 boolean isWritable(Path path)：判断文件是否可写。 boolean notExists(Path path, LinkOption … opts)：判断文件是否不存在。 SeekableByteChannel newByteChannel(Path path, OpenOption…how)：获取与指定文件的连接，how 指定打开方式。 DirectoryStream\\&lt;Path&gt; newDirectoryStream(Path path)：打开 path 指定的目录。 InputStream newInputStream(Path path, OpenOption…how)：获取 InputStream 对象。 OutputStream newOutputStream(Path path, OpenOption…how)：获取 OutputStream 对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class FilesTest &#123; @Test public void test1() throws IOException &#123; Path path1 = Paths.get(&quot;d:\\\\nio&quot;, &quot;hello.txt&quot;); Path path2 = Paths.get(&quot;atguigu.txt&quot;); // Path copy(Path src, Path dest, CopyOption … how): 文件的复制 // 要想复制成功，要求path1对应的物理上的文件存在。path2 对应的文件没有要求。 // Files.copy(path1, path2, StandardCopyOption.REPLACE_EXISTING); // Path createDirectory(Path path, FileAttribute&lt;?&gt; … attr): 创建一个目录 // 要想执行成功，要求path对应的物理上的文件目录不存在。一旦存在，抛出异常。 Path path3 = Paths.get(&quot;d:\\\\nio\\\\nio1&quot;); // Files.createDirectory(path3); // Path createFile(Path path, FileAttribute&lt;?&gt; … arr): 创建一个文件 // 要想执行成功，要求path对应的物理上的文件不存在。一旦存在，抛出异常。 Path path4 = Paths.get(&quot;d:\\\\nio\\\\hi.txt&quot;); // Files.createFile(path4); // void delete(Path path): 删除一个文件/目录，如果不存在，执行报错 // Files.delete(path4); // void deleteIfExists(Path path): Path对应的文件/目录如果存在，执行删除。如果不存在，正常执行结束 Files.deleteIfExists(path3); // Path move(Path src, Path dest, CopyOption…how): 将src移动到dest位置 // 要想执行成功，src对应的物理上的文件需要存在，dest对应的文件没有要求。 // Files.move(path1, path2, StandardCopyOption.ATOMIC_MOVE); // long size(Path path): 返回path指定文件的大小 long size = Files.size(path2); System.out.println(size); &#125; @Test public void test2() throws IOException &#123; Path path1 = Paths.get(&quot;d:\\\\nio&quot;, &quot;hello.txt&quot;); Path path2 = Paths.get(&quot;atguigu.txt&quot;); // boolean exists(Path path, LinkOption … opts): 判断文件是否存在 System.out.println(Files.exists(path2, LinkOption.NOFOLLOW_LINKS)); // boolean isDirectory(Path path, LinkOption … opts): 判断是否是目录 // 不要求此path对应的物理文件存在。 System.out.println(Files.isDirectory(path1, LinkOption.NOFOLLOW_LINKS)); // boolean isRegularFile(Path path, LinkOption … opts): 判断是否是文件 // /boolean isHidden(Path path): 判断是否是隐藏文件 // 要求此path对应的物理上的文件需要存在。才可判断是否隐藏。否则，抛异常。 System.out.println(Files.isHidden(path1)); // /boolean isReadable(Path path): 判断文件是否可读 System.out.println(Files.isReadable(path1)); // boolean isWritable(Path path): 判断文件是否可写 System.out.println(Files.isWritable(path1)); // boolean notExists(Path path, LinkOption … opts): 判断文件是否不存在 System.out.println(Files.notExists(path1, LinkOption.NOFOLLOW_LINKS)); &#125; /** * StandardOpenOption.READ: 表示对应的Channel是可读的。 * StandardOpenOption.WRITE：表示对应的Channel是可写的。 * StandardOpenOption.CREATE：如果要写出的文件不存在，则创建。如果存在，忽略 * StandardOpenOption.CREATE_NEW：如果要写出的文件不存在，则创建。如果存在，抛异常 * * @throws IOException */ @Test public void test3() throws IOException &#123; Path path1 = Paths.get(&quot;d:\\\\nio&quot;, &quot;hello.txt&quot;); // InputStream newInputStream(Path path, OpenOption…how): 获取InputStream对象 InputStream inputStream = Files.newInputStream(path1, StandardOpenOption.READ); // OutputStream newOutputStream(Path path, OpenOption…how): 获取OutputStream对象 OutputStream outputStream = Files.newOutputStream(path1, StandardOpenOption.WRITE, StandardOpenOption.CREATE); // SeekableByteChannel newByteChannel(Path path, OpenOption…how): 获取与指定文件的连接，how指定打开方式 SeekableByteChannel channel = Files.newByteChannel(path1, StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE); // DirectoryStream&lt;Path&gt; newDirectoryStream(Path path): 打开path指定的目录 Path path2 = Paths.get(&quot;e:\\\\teach&quot;); DirectoryStream&lt;Path&gt; directoryStream = Files.newDirectoryStream(path2); Iterator&lt;Path&gt; iterator = directoryStream.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125; FileUtils 工具类 Maven 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt;&lt;/dependency&gt; 复制功能： 123456789101112public class FileUtilsTest &#123; public static void main(String[] args) &#123; File srcFile = new File(&quot;day10\\\\爱情与友情.jpg&quot;); File destFile = new File(&quot;day10\\\\爱情与友情2.jpg&quot;); try &#123; FileUtils.copyFile(srcFile, destFile); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 遍历文件夹和文件的每一行： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class FileUtilsMethod &#123; /** * 常规方法：若文件路径内的文件比较少，可以采用此方法 * * @param filePath 文件路径 */ public static void common(String filePath) &#123; File file = new File(filePath); if (file.exists()) &#123; // 获取子文件夹内所有文件，放到文件数组里，如果含有大量文件，会创建一个很大的数组，占用空间 File[] fileList = file.listFiles(); for (File currentFile : fileList) &#123; // 当前文件是普通文件（排除文件夹），且不是隐藏文件 if (currentFile.isFile() &amp;&amp; !currentFile.isHidden()) &#123; // 当前文件的完整路径，含文件名 String currentFilePath = currentFile.getPath(); if (currentFilePath.endsWith(&quot;xml&quot;) || currentFilePath.endsWith(&quot;XML&quot;)) &#123; // 当前文件的文件名，含后缀 String fileName = currentFile.getName(); System.out.println(&quot;文件名：&quot; + fileName); &#125; &#125; &#125; System.out.println(&quot;=======================================&quot;); // list方法返回的是文件名的String数组 String[] fileNameList = file.list(); for (String fileName : fileNameList) &#123; System.out.println(&quot;文件名：&quot; + fileName); &#125; &#125; &#125; /** * 根据文件路径，迭代获取该路径下指定文件后缀类型的文件：若文件路径内含有大量文件，建议采用此方法 * * @param filePath 文件路径 */ public static void iterateFiles(String filePath) &#123; File file = FileUtils.getFile(filePath); if (file.isDirectory()) &#123; Iterator&lt;File&gt; fileIterator = FileUtils.iterateFiles(file, new String[]&#123;&quot;xml&quot;, &quot;XML&quot;&#125;, false); while (fileIterator.hasNext()) &#123; File currentFile = fileIterator.next(); if (currentFile.isFile() &amp;&amp; !currentFile.isHidden()) &#123; // 绝对路径 String currentFilePath = currentFile.getAbsolutePath(); System.out.println(&quot;绝对路径：&quot; + currentFilePath); // 文件名，含文件后缀 String fileName = currentFilePath.substring(currentFilePath.lastIndexOf(&quot;\\\\&quot;) + 1); System.out.println(&quot;含后缀文件名：&quot; + fileName); // 文件名，不含文件后缀 fileName = fileName.substring(0, fileName.lastIndexOf(&quot;.&quot;)); System.out.println(&quot;不含后缀文件名：&quot; + fileName); &#125; &#125; &#125; &#125; /** * 读取目标文件每一行数据，返回List：若文件内容较少，可以采用此方法 * * @param filePath 文件路径 * @throws IOException */ public static void readLinesForList(String filePath) throws IOException &#123; List&lt;String&gt; linesList = FileUtils.readLines(new File(filePath), &quot;utf-8&quot;); for (String line : linesList) &#123; System.out.println(line); &#125; &#125; /** * 读取目标文件每一行数据，返回迭代器：若文件内容较多，建议采用此方法 * * @param filePath 文件路径 * @throws IOException */ public static void readLinesForIterator(String filePath) throws IOException &#123; LineIterator lineIterator = FileUtils.lineIterator(new File(filePath), &quot;utf-8&quot;); while (lineIterator.hasNext()) &#123; System.out.println(lineIterator.next()); &#125; &#125;&#125; 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 https://juejin.cn/post/6844903985078337550#heading-55 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"KafkaConsumer 源码之 consumer 的 offset commit 机制和 partition 分配机制","slug":"kafka-consumer-commitandpartition","date":"2020-11-24T07:17:06.000Z","updated":"2021-01-05T07:32:30.026Z","comments":true,"path":"2020/11/24/kafka-consumer-commitandpartition/","link":"","permalink":"http://example.com/2020/11/24/kafka-consumer-commitandpartition/","excerpt":"","text":"紧接着上篇文章，这篇文章讲述 consumer 提供的 offset commit 机制和 partition 分配机制，具体如何使用是需要用户结合具体的场景进行选择，本文讲述一下其底层实现。 自动 offset commit 机制1234// 自动提交，默认trueprops.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// 设置自动每1s提交一次，默认为5sprops.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 通过上面设置，启动自动提交 offset 以及设置自动提交间隔时间。 手动 offset commit 机制先看下两种不同的手动 offset commit 机制，一种是同步 commit，一种是异步 commit，既然其作用都是 offset commit，应该不难猜到它们底层使用接口都是一样的，其调用流程如下图所示： 同步 commit1234567// 对poll()中返回的所有topics和partition列表进行commit// 这个方法只能将offset提交Kafka中，Kafka将会在每次rebalance之后的第一次拉取或启动时使用同步commit// 这是同步commit，它将会阻塞进程，直到commit成功或者遇到一些错误public void commitSync() &#123;&#125;// 只对指定的topic-partition列表进行commitpublic void commitSync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) &#123;&#125; 其实，从上图中，就已经可以看出，同步 commit 的实现方式，client.poll () 方法会阻塞直到这个 request 完成或超时才会返回。 异步 commit12345public void commitAsync() &#123;&#125;public void commitAsync(OffsetCommitCallback callback) &#123;&#125;public void commitAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback) &#123;&#125; 对于异步的 commit，最后调用的都是 doCommitOffsetsAsync () 方法，其具体实现如下： 12345678910111213141516171819202122232425262728private void doCommitOffsetsAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, final OffsetCommitCallback callback) &#123; // 发送offset-commit请求 RequestFuture&lt;Void&gt; future = sendOffsetCommitRequest(offsets); final OffsetCommitCallback cb = callback == null ? defaultOffsetCommitCallback : callback; future.addListener(new RequestFutureListener&lt;Void&gt;() &#123; @Override public void onSuccess(Void value) &#123; if (interceptors != null) interceptors.onCommit(offsets); // 添加成功的请求，以唤醒相应的回调函数 completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, null)); &#125; @Override public void onFailure(RuntimeException e) &#123; Exception commitException = e; if (e instanceof RetriableException) &#123; commitException = new RetriableCommitFailedException(e); &#125; // 添加失败的请求，以唤醒相应的回调函数 completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, commitException)); if (commitException instanceof FencedInstanceIdException) &#123; asyncCommitFenced.set(true); &#125; &#125; &#125;);&#125; 在异步 commit 中，可以添加相应的回调函数，如果 request 处理成功或处理失败，ConsumerCoordinator 会通过 invokeCompletedOffsetCommitCallbacks () 方法唤醒相应的回调函数。 上面简单的介绍了同步 commit 和异步 commit，更详细的分析参考：Kafka consumer 的 offset 的提交方式。 注意：手动 commit 时，提交的是下一次要读取的 offset。举例如下： 1234567891011121314151617181920try &#123; while(running) &#123; // 取得消息 ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE); // 根据分区来遍历数据 for (TopicPartition partition : records.partitions()) &#123; List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition); // 数据处理 for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123; System.out.println(record.offset() + &quot;: &quot; + record.value()); &#125; // 取得当前读取到的最后一条记录的offset long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); // 提交offset，记得要 + 1 consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1))); &#125; &#125;&#125; finally &#123; consumer.close();&#125; commit offset 请求的处理当 Kafka Server 端接收到来自 client 端的 offset commit 请求时，对于提交的 offset，GroupCoordinator 会记录在 GroupMetadata 对象中，至于其实现的逻辑细节，此处不再赘述。 partition 分配机制consumer 提供了三种不同的 partition 分配策略，可以通过 partition.assignment.strategy 参数进行配置，默认情况下使用的是 org.apache.kafka.clients.consumer.RangeAssignor，Kafka 中提供了另外两种 partition 的分配策略 org.apache.kafka.clients.consumer.RoundRobinAssignor 和 org.apache.kafka.clients.consumer.StickyAssignor，它们关系如下图所示： 通过上图可以看出，用户可以自定义相应的 partition 分配机制，只需要继承这个 AbstractPartitionAssignor 抽象类即可。 partition 分配策略，其实也就是 reblance 策略。 AbstractPartitionAssignorAbstractPartitionAssignor 有一个抽象方法，如下所示： 12345678910/** * Perform the group assignment given the partition counts and member subscriptions * @param partitionsPerTopic The number of partitions for each subscribed topic. Topics not in metadata will be excluded * from this map. * @param subscriptions Map from the memberId to their respective topic subscription * @return Map from each member to the list of partitions assigned to them. */// 根据partitionsPerTopic和subscriptions进行分配，具体的实现会在子类中实现(不同的子类，其实现方法不相同)public abstract Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions); assign () 这个方法，有两个参数： partitionsPerTopic：所订阅的每个 topic 与其 partition 数的对应关系，metadata 没有的 topic 将会被移除； subscriptions：每个 consumerId 与其所订阅的 topic 列表的关系。 继承 AbstractPartitionAssignor 的子类，通过实现 assign () 方法，来进行相应的 partition 分配。 RangeAssignor 分配模式assign () 方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; // 1.参数含义:(topic, List&lt;consumerId&gt;)，获取每个topic被多少个consumer订阅了 Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions); // 2.存储最终的分配方案 Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) assignment.put(memberId, new ArrayList&lt;&gt;()); for (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123; String topic = topicEntry.getKey(); List&lt;String&gt; consumersForTopic = topicEntry.getValue(); // 3.每个topic的partition数量 Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic == null) continue; Collections.sort(consumersForTopic); // 4.取商，表示平均每个consumer会分配到多少个partition int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size(); // 5.取余，表示平均分配后还剩下多少个partition未被分配 int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size(); List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic); // 6.这里是关键点，分配原则是将未能被平均分配的partition分配到前consumersWithExtraPartition个consumer for (int i = 0, n = consumersForTopic.size(); i &lt; n; i++) &#123; // 假设partition有7个，consumer有5个，则numPartitionsPerConsumer=1，consumersWithExtraPartition=2 // i=0, start: 0, length: 2, topic-partition: p0, p1 // i=1, start: 2, length: 2, topic-partition: p2, p3 // i=2, start: 4, length: 1, topic-partition: p4 // i=3, start: 5, length: 1, topic-partition: p5 // i=4, start: 6, length: 1, topic-partition: p6 int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition); int length = numPartitionsPerConsumer + (i + 1 &gt; consumersWithExtraPartition ? 0 : 1); assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length)); &#125; &#125; return assignment;&#125; 假设 topic 的 partition 数为 numPartitionsForTopic，group 中订阅这个 topic 的 member 数为 consumersForTopic.size()，首先需要算出两个值： numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size()：表示平均每个 consumer 会分配到几个 partition； consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size()：表示平均分配后还剩下多少个 partition 未分配。 分配的规则是：对于剩下的那些 partition 分配到前 consumersWithExtraPartition 个 consumer 上，也就是前 consumersWithExtraPartition 个 consumer 获得 topic-partition 列表会比后面多一个。 在上述的程序中，举了一个例子，假设有一个 topic 有 7 个 partition，group 有5个 consumer，这个5个 consumer 都订阅这个 topic，那么 range 的分配方式如下： 消费者 分配方案 consumer 0 start: 0, length: 2, topic-partition: p0, p1 consumer 1 start: 2, length: 2, topic-partition: p2, p3 consumer 2 start: 4, length: 1, topic-partition: p4 consumer 3 start: 5, length: 1, topic-partition: p5 consumer 4 start: 6, length: 1, topic-partition: p6 而如果 group 中有 consumer 没有订阅这个 topic，那么这个 consumer 将不会参与分配。下面再举个例子，假设有 2 个 topic，一个有 5 个 partition，另一个有 7 个 partition，group 中有 5 个 consumer，但是只有前 3 个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下： consumer 订阅 topic1 的列表 订阅 topic2 的列表 consumer 0 t1 p0, t1 p1 t2 p0, t2 p1 consumer 1 t1 p2, t1 p3 t2 p2, t2 p3 consumer 2 t1 p4 t2 p4 consumer 3 t2 p5 consumer 4 t2 p6 RoundRobinAssignorassign () 方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536@Overridepublic Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) assignment.put(memberId, new ArrayList&lt;&gt;()); // 环状链表，存储所有的consumer，一次迭代完之后又会回到原点 CircularIterator&lt;String&gt; assigner = new CircularIterator&lt;&gt;(Utils.sorted(subscriptions.keySet())); // 获取所有订阅的topic的partition总数 for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) &#123; final String topic = partition.topic(); while (!subscriptions.get(assigner.peek()).topics().contains(topic)) assigner.next(); assignment.get(assigner.next()).add(partition); &#125; return assignment;&#125;public List&lt;TopicPartition&gt; allPartitionsSorted(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; // 所有的topics(有序) SortedSet&lt;String&gt; topics = new TreeSet&lt;&gt;(); for (Subscription subscription : subscriptions.values()) topics.addAll(subscription.topics()); // 订阅的Topic的所有的TopicPartition集合 List&lt;TopicPartition&gt; allPartitions = new ArrayList&lt;&gt;(); for (String topic : topics) &#123; Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic != null) // topic的所有partition都添加进去 allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic)); &#125; return allPartitions;&#125; Round Robin 的实现原则，简单来说就是：列出所有 topic-partition 和列出所有的 consumer member，然后开始分配，一轮之后继续下一轮，假设有一个 topic，它有7个 partition，group 中有 3 个 consumer 都订阅了这个 topic，那么其分配方式为： 消费者 分配列表 consumer 0 p0, p3, p6 consumer 1 p1, p4 consumer 2 p2, p5 对于多个 topic 的订阅，假设有 2 个 topic，一个有 5 个 partition，一个有 7 个 partition，group 中有 5 个 consumer，但是只有前 3 个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下： 消费者 订阅 topic1 的列表 订阅的 topic2 的列表 consumer 0 t1 p0, t1 p3 t2 p0, t2 p5 consumer 1 t1 p1, t1 p4 t2 p1, t2 p6 consumer 2 t1 p2 t2 p2 consumer 3 t2 p3 consumer 4 t2 p4 StickyAssignorassign () 方法的实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;TopicPartition&gt;&gt; currentAssignment = new HashMap&lt;&gt;(); Map&lt;TopicPartition, ConsumerGenerationPair&gt; prevAssignment = new HashMap&lt;&gt;(); partitionMovements = new PartitionMovements(); prepopulateCurrentAssignments(subscriptions, currentAssignment, prevAssignment); boolean isFreshAssignment = currentAssignment.isEmpty(); // a mapping of all topic partitions to all consumers that can be assigned to them final Map&lt;TopicPartition, List&lt;String&gt;&gt; partition2AllPotentialConsumers = new HashMap&lt;&gt;(); // a mapping of all consumers to all potential topic partitions that can be assigned to them final Map&lt;String, List&lt;TopicPartition&gt;&gt; consumer2AllPotentialPartitions = new HashMap&lt;&gt;(); // initialize partition2AllPotentialConsumers and consumer2AllPotentialPartitions in the following two for loops for (Entry&lt;String, Integer&gt; entry: partitionsPerTopic.entrySet()) &#123; for (int i = 0; i &lt; entry.getValue(); ++i) partition2AllPotentialConsumers.put(new TopicPartition(entry.getKey(), i), new ArrayList&lt;&gt;()); &#125; for (Entry&lt;String, Subscription&gt; entry: subscriptions.entrySet()) &#123; String consumer = entry.getKey(); consumer2AllPotentialPartitions.put(consumer, new ArrayList&lt;&gt;()); entry.getValue().topics().stream().filter(topic -&gt; partitionsPerTopic.get(topic) != null).forEach(topic -&gt; &#123; for (int i = 0; i &lt; partitionsPerTopic.get(topic); ++i) &#123; TopicPartition topicPartition = new TopicPartition(topic, i); consumer2AllPotentialPartitions.get(consumer).add(topicPartition); partition2AllPotentialConsumers.get(topicPartition).add(consumer); &#125; &#125;); // add this consumer to currentAssignment (with an empty topic partition assignment) if it does not already exist if (!currentAssignment.containsKey(consumer)) currentAssignment.put(consumer, new ArrayList&lt;&gt;()); &#125; // a mapping of partition to current consumer Map&lt;TopicPartition, String&gt; currentPartitionConsumer = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry: currentAssignment.entrySet()) for (TopicPartition topicPartition: entry.getValue()) currentPartitionConsumer.put(topicPartition, entry.getKey()); List&lt;TopicPartition&gt; sortedPartitions = sortPartitions( currentAssignment, prevAssignment.keySet(), isFreshAssignment, partition2AllPotentialConsumers, consumer2AllPotentialPartitions); // all partitions that need to be assigned (initially set to all partitions but adjusted in the following loop) List&lt;TopicPartition&gt; unassignedPartitions = new ArrayList&lt;&gt;(sortedPartitions); for (Iterator&lt;Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt;&gt; it = currentAssignment.entrySet().iterator(); it.hasNext();) &#123; Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry = it.next(); if (!subscriptions.containsKey(entry.getKey())) &#123; // if a consumer that existed before (and had some partition assignments) is now removed, remove it from currentAssignment for (TopicPartition topicPartition: entry.getValue()) currentPartitionConsumer.remove(topicPartition); it.remove(); &#125; else &#123; // otherwise (the consumer still exists) for (Iterator&lt;TopicPartition&gt; partitionIter = entry.getValue().iterator(); partitionIter.hasNext();) &#123; TopicPartition partition = partitionIter.next(); if (!partition2AllPotentialConsumers.containsKey(partition)) &#123; // if this topic partition of this consumer no longer exists remove it from currentAssignment of the consumer partitionIter.remove(); currentPartitionConsumer.remove(partition); &#125; else if (!subscriptions.get(entry.getKey()).topics().contains(partition.topic())) &#123; // if this partition cannot remain assigned to its current consumer because the consumer // is no longer subscribed to its topic remove it from currentAssignment of the consumer partitionIter.remove(); &#125; else // otherwise, remove the topic partition from those that need to be assigned only if // its current consumer is still subscribed to its topic (because it is already assigned // and we would want to preserve that assignment as much as possible) unassignedPartitions.remove(partition); &#125; &#125; &#125; // at this point we have preserved all valid topic partition to consumer assignments and removed // all invalid topic partitions and invalid consumers. Now we need to assign unassignedPartitions // to consumers so that the topic partition assignments are as balanced as possible. // an ascending sorted set of consumers based on how many topic partitions are already assigned to them TreeSet&lt;String&gt; sortedCurrentSubscriptions = new TreeSet&lt;&gt;(new SubscriptionComparator(currentAssignment)); sortedCurrentSubscriptions.addAll(currentAssignment.keySet()); balance(currentAssignment, prevAssignment, sortedPartitions, unassignedPartitions, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer); return currentAssignment;&#125; sticky 分区策略是从 0.11 版本才开始引入的，它主要有两个目的： 分区的分配要尽可能均匀 分区的分配要尽可能与上次分配的保持相同 当两者冲突的时候，第一个目标优先于第二个目标。 sticky 的分区方式作用发生分区重分配的时候，尽可能地让前后两次分配相同，进而减少系统资源的损耗及其他异常情况的发生。因为 sticky 分区策略的代码，要比 range 和 roundrobin 复杂很多，此处不做具体的细节分析，只简单举例如下： 假设有 3 个 topic，一个有 2 个 partition，一个有 3 个 partition，另外一个有 4 个 partition，group 中有 3 个 consumer，第一个 consumer 订阅了第一个 topic，第二个 consumer 订阅了前两个 topic，第三个 consumer 订阅了三个 topic，那么它们的分配方案如下： 消费者 订阅 topic1 的列表 订阅 topic2 的列表 订阅 topic3 的列表 consumer1 t1 p0 consumer2 t1 p1, t2 p1 t2 p0, t2 p3 consumer3 t3 p0, t3 p1, t3 p2, t3 p3 上面三个分区策略有着不同的分配方式，在实际使用过程中，需要根据自己的需求选择合适的策略，但是如果你只有一个 consumer，那么选择哪个方式都是一样的，但是如果是多个 consumer 不在同一台设备上进行消费，那么 sticky 方式应该更加合适。 自定义分区策略如之前所说，只需要继承 AbstractPartitionAssignor 并复写其中方法即可 (当然也可以直接实现 PartitionAssignor 接口) 自定义分区策略，其中有两个方法需要复写： 1234public String name();public abstract Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions); 其中 assign () 方法表示的是分区分配方案的实现，而 name () 方法则表示了这个分配策略的唯一名称，比如之前提到的 range，roundrobin 和 sticky, 这个名字会在和 GroupCoordinator 的通信中返回，通过它 consumer leader 来确定整个 group 的分区方案 (分区策略是由 group 中的 consumer 共同投票决定的，谁使用的多，就使用哪个策略)。 本文参考http://generalthink.github.io/2019/06/06/kafka-consumer-partition-assign/ https://matt33.com/2017/11/19/consumer-two-summary/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。 至此，关于 Kafka 的学习暂时告一段落，未来有需要时，会继续学习。更多关于 Kafka 原理等知识的介绍，参考： http://generalthink.github.io/tags/Kafka/ https://matt33.com/tags/kafka/","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaConsumer 源码之 consumer 的两种订阅模式","slug":"kafka-consumer-subscribeandassign","date":"2020-11-24T02:18:47.000Z","updated":"2021-01-05T07:33:01.339Z","comments":true,"path":"2020/11/24/kafka-consumer-subscribeandassign/","link":"","permalink":"http://example.com/2020/11/24/kafka-consumer-subscribeandassign/","excerpt":"","text":"在前面的文章中，有简单的介绍了 KafkaConsumer 的两种订阅模式，本篇文章对此进行扩展说明一下。 KafkaConsumer 的两种订阅模式， subscribe () 模式和 assign () 模式，前者是 topic 粒度 (使用 group 管理)，后者是 topic-partition 粒度 (用户自己去管理)。 订阅模式KafkaConsumer 为订阅模式提供了 4 种 API，如下： 12345678910111213// 订阅指定的topic列表，并且会自动进行动态partition订阅// 当发生以下情况时，会进行rebalance:1.订阅的topic列表改变；2.topic被创建或删除；3.consumer线程die；4.加一个新的consumer线程// 当发生rebalance时，会唤醒ConsumerRebalanceListener线程public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123;&#125;// 同上，但是这里没有设置listenerpublic void subscribe(Collection&lt;String&gt; topics) &#123;&#125;// 订阅那些满足一定规则(pattern)的topicpublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123;&#125;// 同上，但是这里没有设置listenerpublic void subscribe(Pattern pattern) &#123;&#125; 以上 4 种 API 都是按照 topic 级别去订阅，可以动态地获取其分配的 topic-partition，这是使用 Group 动态管理，它不能与手动 partition 管理一起使用。当监控到发生下面的事件时，Group 将会触发 rebalance 操作： 订阅的 topic 列表变化； topic 被创建或删除； consumer group 的某个 consumer 实例挂掉； 一个新的 consumer 实例通过 join 方法加入到一个 group 中。 在这种模式下，当 KafkaConsumer 调用 poll () 方法时，第一步会首先加入到一个 group 中，并获取其分配的 topic-partition 列表，具体细节在前面的文章中已经分析过了。 这里介绍一下当调用 subscribe () 方法之后，consumer 所做的事情，分两种情况介绍，一种按 topic 列表订阅，一种是按 pattern 模式订阅： topic 列表订阅 topic 列表订阅，最终调用如下方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Subscribe to the given list of topics to get dynamically * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * As part of group management, the consumer will keep track of the list of consumers that belong to a particular * group and will trigger a rebalance operation if any one of the following events are triggered: * &lt;ul&gt; * &lt;li&gt;Number of partitions change for any of the subscribed topics * &lt;li&gt;A subscribed topic is created or deleted * &lt;li&gt;An existing member of the consumer group is shutdown or fails * &lt;li&gt;A new member is added to the consumer group * &lt;/ul&gt; * &lt;p&gt; * When any of these events are triggered, the provided listener will be invoked first to indicate that * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received. * Note that rebalances will only occur during an active call to &#123;@link #poll(Duration)&#125;, so callbacks will * also only be invoked during that time. * * The provided listener will immediately override any listener set in a previous call to subscribe. * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics * subscribed in this call. See &#123;@link ConsumerRebalanceListener&#125; for more details. * * @param topics The list of topics to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; acquireAndEnsureOpen(); try &#123; maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot be null&quot;); if (topics.isEmpty()) &#123; // treat subscribing to empty topic list as the same as unsubscribing this.unsubscribe(); &#125; else &#123; for (String topic : topics) &#123; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;); &#125; throwIfNoAssignorsConfigured(); fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(&quot;Subscribed to topic(s): &#123;&#125;&quot;, Utils.join(topics, &quot;, &quot;)); // 核心步骤在此处执行 if (this.subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; 将 SubscriptionType 类型设置为 AUTO_TOPICS，并更新 SubscriptionState 中记录的 subscription 属性 (记录的是订阅的 topic 列表)； 12345678910111213141516171819public synchronized boolean subscribe(Set&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_TOPICS); return changeSubscription(topics);&#125;private boolean changeSubscription(Set&lt;String&gt; topicsToSubscribe) &#123; if (subscription.equals(topicsToSubscribe)) return false; subscription = topicsToSubscribe; if (subscriptionType != SubscriptionType.USER_ASSIGNED) &#123; groupSubscription = new HashSet&lt;&gt;(groupSubscription); groupSubscription.addAll(topicsToSubscribe); &#125; else &#123; groupSubscription = new HashSet&lt;&gt;(topicsToSubscribe); &#125; return true;&#125; 请求更新 metadata。 1234567891011121314public synchronized void requestUpdateForNewTopics() &#123; // Override the timestamp of last refresh to let immediate update. this.lastRefreshMs = 0; this.requestVersion++; requestUpdate();&#125;/** * Request an update of the current cluster metadata info, return the current updateVersion before the update */public synchronized int requestUpdate() &#123; this.needUpdate = true; return this.updateVersion;&#125; pattern 模式订阅 pattern 模式订阅，最终调用如下方法： 123456789101112131415161718192021222324252627282930313233343536/** * Subscribe to all topics matching specified pattern to get dynamically assigned partitions. * The pattern matching will be done periodically against all topics existing at the time of check. * This can be controlled through the &#123;@code metadata.max.age.ms&#125; configuration: by lowering * the max metadata age, the consumer will refresh metadata more often and check for matching topics. * &lt;p&gt; * See &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125; for details on the * use of the &#123;@link ConsumerRebalanceListener&#125;. Generally rebalances are triggered when there * is a change to the topics matching the provided pattern and when consumer group membership changes. * Group rebalances only take place during an active call to &#123;@link #poll(Duration)&#125;. * * @param pattern Pattern to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If pattern or listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with topics, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; maybeThrowInvalidGroupIdException(); if (pattern == null) throw new IllegalArgumentException(&quot;Topic pattern to subscribe to cannot be null&quot;); acquireAndEnsureOpen(); try &#123; throwIfNoAssignorsConfigured(); log.info(&quot;Subscribed to pattern: &#x27;&#123;&#125;&#x27;&quot;, pattern); this.subscriptions.subscribe(pattern, listener); this.coordinator.updatePatternSubscription(metadata.fetch()); this.metadata.requestUpdateForNewTopics(); &#125; finally &#123; release(); &#125;&#125; 将 SubscriptionType 类型设置为 AUTO_PATTERN，并更新 SubscriptionState 中记录的 subscribedPattern 属性，设置为 pattern； 12345public synchronized void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_PATTERN); this.subscribedPattern = pattern;&#125; 调用 coordinator 的 updatePatternSubscription () 方法，遍历所有 topic 的 metadata，找到所有满足 pattern 的 topic 列表，更新到 SubscriptionState 的 subscriptions 属性，并请求更新 Metadata； 1234567891011121314151617181920212223242526272829public void updatePatternSubscription(Cluster cluster) &#123; final Set&lt;String&gt; topicsToSubscribe = cluster.topics().stream() .filter(subscriptions::matchesSubscribedPattern) .collect(Collectors.toSet()); if (subscriptions.subscribeFromPattern(topicsToSubscribe)) metadata.requestUpdateForNewTopics();&#125;public synchronized boolean subscribeFromPattern(Set&lt;String&gt; topics) &#123; if (subscriptionType != SubscriptionType.AUTO_PATTERN) throw new IllegalArgumentException(&quot;Attempt to subscribe from pattern while subscription type set to &quot; + subscriptionType); return changeSubscription(topics);&#125;private boolean changeSubscription(Set&lt;String&gt; topicsToSubscribe) &#123; if (subscription.equals(topicsToSubscribe)) return false; subscription = topicsToSubscribe; if (subscriptionType != SubscriptionType.USER_ASSIGNED) &#123; groupSubscription = new HashSet&lt;&gt;(groupSubscription); groupSubscription.addAll(topicsToSubscribe); &#125; else &#123; groupSubscription = new HashSet&lt;&gt;(topicsToSubscribe); &#125; return true;&#125; 1234567891011121314public synchronized void requestUpdateForNewTopics() &#123; // Override the timestamp of last refresh to let immediate update. this.lastRefreshMs = 0; this.requestVersion++; requestUpdate();&#125;/** * Request an update of the current cluster metadata info, return the current updateVersion before the update */public synchronized int requestUpdate() &#123; this.needUpdate = true; return this.updateVersion;&#125; 其他部分，两者基本一样，只是 pattern 模型在每次更新 topic-metadata 时，获取全局的 topic 列表，如果发现有新加入的符合条件的 topic，就立马去订阅，其他的地方，包括 group 管理、topic-partition 的分配都是一样的。 分配模式当调用 assign () 方法手动分配 topic-partition 列表时，不会使用 consumer 的 Group 管理机制，也即是当 consumer group member 变化或 topic 的 metadata 信息变化时，不会触发 rebalance 操作。比如：当 topic 的 partition 增加时，这里无法感知，需要用户进行相应的处理，Apache Flink 就是使用的这种方式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Manually assign a list of partitions to this consumer. This interface does not allow for incremental assignment * and will replace the previous assignment (if there is one). * &lt;p&gt; * If the given list of topic partitions is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * &lt;p&gt; * Manual topic assignment through this method does not use the consumer&#x27;s group management * functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic * metadata change. Note that it is not possible to use both manual partition assignment with &#123;@link #assign(Collection)&#125; * and group assignment with &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;. * &lt;p&gt; * If auto-commit is enabled, an async commit (based on the old assignment) will be triggered before the new * assignment replaces the old one. * * @param partitions The list of partitions to assign this consumer * @throws IllegalArgumentException If partitions is null or contains null or empty topics * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with topics or pattern * (without a subsequent call to &#123;@link #unsubscribe()&#125;) */@Overridepublic void assign(Collection&lt;TopicPartition&gt; partitions) &#123; acquireAndEnsureOpen(); try &#123; if (partitions == null) &#123; throw new IllegalArgumentException(&quot;Topic partition collection to assign to cannot be null&quot;); &#125; else if (partitions.isEmpty()) &#123; this.unsubscribe(); &#125; else &#123; for (TopicPartition tp : partitions) &#123; String topic = (tp != null) ? tp.topic() : null; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic partitions to assign to cannot have null or empty topic&quot;); &#125; fetcher.clearBufferedDataForUnassignedPartitions(partitions); // make sure the offsets of topic partitions the consumer is unsubscribing from // are committed since there will be no following rebalance if (coordinator != null) this.coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds()); log.info(&quot;Subscribed to partition(s): &#123;&#125;&quot;, Utils.join(partitions, &quot;, &quot;)); if (this.subscriptions.assignFromUser(new HashSet&lt;&gt;(partitions))) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; assign () 方法是手动向 consumer 分配一些 topic-partition 列表，并且这个接口不允许增加分配的 topic-partition 列表，将会覆盖之前分配的 topic-partition 列表，如果给定的 topic-partition 列表为空，它的作用将会与 unsubscribe () 方法一样。 这种手动 topic 分配也不会使用 consumer 的 group 管理，当 group 的 member 变化或 topic 的 metadata 变化时，也不会触发 rebalance 操作。 这里所说的 consumer 的 group 管理，就是前面所说的 consumer 如何加入 group 的管理过程。如果使用的是 assign 模式，也即是非 AUTO_TOPICS 或 AUTO_PATTERN 模式时，consumer 实例在调用 poll () 方法时，不会向 GroupCoordinator 发送 join-group、sync-group、heartbeat 请求，也就是说 GroupCoordinator 拿不到这个 consumer 实例的相关信息，也不会去维护这个 member 是否存活，这种情况下就需要用户自己管理自己的处理程序。但是这种模式可以进行 offset commit，这将在下一篇文章进行分析。 小结根据上面的讲述，这里做一下小结，两种模式对比如下图所示： 简单说明如下： 模式 不同之处 相同之处 subscribe () 使用 Kafka group 管理，自动进行 rebalance 操作 可以在 Kafka 保存 offset assign () 用户自己进行相关的处理 也可以进行 offset commit，但是尽量保证 group.id 唯一性，如果使用一个与上面模式一样的 group，offset commit 请求将会被拒绝 本文参考https://matt33.com/2017/11/18/consumer-subscribe/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"maven 的基础使用","slug":"maven","date":"2020-11-18T07:18:48.000Z","updated":"2021-04-09T07:50:19.362Z","comments":true,"path":"2020/11/18/maven/","link":"","permalink":"http://example.com/2020/11/18/maven/","excerpt":"","text":"maven 的功能maven 是一个项目管理工具，主要作用是在项目开发阶段对项目进行依赖管理和项目构建。 依赖管理：仅仅通过 jar 包的几个属性，就能确定唯一的 jar 包，在指定的文件 pom.xml 中，只要写入这些依赖属性，就会自动下载并管理 jar 包。 项目构建：内置很多的插件与生命周期，支持多种任务，比如校验、编译、测试、打包、部署、发布… 项目的知识管理：管理项目相关的其他内容，比如开发者信息，版本等等。 maven 的安装与配置 下载，地址：http://maven.apache.org/download.cgi 注意：安装 maven 之前，必须先确保你的机器中已经安装了 jdk，如果是 maven 3 则必须 jdk 1.7 以上。 解压，添加环境变量 MAVEN_HOME，值为解压后的 maven 路径。 在 Path 环境变量的变量值末尾添加 %MAVEN_HOME%\\bin; 。 在 cmd 窗口输入 mvn –version，显示 maven 版本信息，说明安装配置成功。 在 IDEA 中使用 maven maven 的仓库maven 仓库分为本地仓库和远程仓库，而远程仓库又分为 maven 中央仓库、其他远程仓库和私服 (私有服务器)。其中，中央仓库是由 maven 官方提供的，而私服就需要我们自己搭建。 本地仓库默认情况下，不管 linux 还是 windows，每个用户在自己的用户目录下都有一个路径名为 .m2\\repository 的仓库目录，如：C:\\Users\\XiSun\\.m2\\repository。如果不自定义本地仓库的地址，则会将下载的构件放到该目录下。 修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，可以自定义本地仓库地址，例如： 1&lt;localRepository&gt;D:\\Program Files\\Maven\\apache-maven-3.6.3-maven-repository&lt;/localRepository&gt; 运行 maven 的时候，maven 所需要的任何构件都是直接从本地仓库获取的。如果本地仓库没有，它会首先尝试从远程仓库下载构件至本地仓库，然后再使用本地仓库的构件。 远程仓库maven 中央仓库maven 中央仓库，是由 maven 社区提供的仓库，其中包含了大量常用的库。一般来说，简单的 java 项目依赖的构件都可以在这里下载到。 在 maven 安装目录的 lib 目录下，有一个 maven-model-builder-3.6.1.jar，里面的 org/apache/maven/model/pom-4.0.0.xml 文件定义了 maven 默认中央仓库的地址：https://repo.maven.apache.org/maven2 因为 maven 中央仓库默认在国外，国内使用难免很慢，推荐将其更换为阿里云的镜像。 全局配置：修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，在 mirrors 节点上，添加如下内容。 1234567891011121314151617181920&lt;mirrors&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 局部配置：修改项目的 pom.xml 文件，在 repositories 上，添加如下内容。 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 更多中央仓库地址参考：https://blog.csdn.net/Hello_World_QWP/article/details/82463799。 私服maven 私服就是公司局域网内的 maven 远程仓库，每个员工的电脑上安装 maven 软件并且连接 maven 私服，程序员可以将自己开发的项目打成 jar 并发布到私服，其它项目组成员就可以从私服下载所依赖的 jar。 私服还充当一个代理服务器的角色，当私服上没有 jar 包时，会从 maven 中央仓库自动下载。 nexus 是一个 maven 仓库管理器 (其实就是一个软件)，nexus 可以充当 maven 私服，同时 nexus 还提供强大的仓库管理、构件搜索等功能。 如果 maven 在中央仓库中也找不到依赖的文件，它会停止构建过程并输出错误信息到控制台。为避免这种情况，maven 提供了远程仓库的概念，它是开发人员自己定制的仓库，包含了所需要的代码库或者其他工程中用到的 jar 文件。 搭建 maven 私服 下载 nexus，地址：https://help.sonatype.com/repomanager2/download/download-archives---repository-manager-oss 安装 nexus 将下载的压缩包进行解压，进入 bin 目录： 打开 cmd 窗口并进入上面 bin 目录下，执行 nexus.bat install 命令安装服务 (注意需要以管理员身份运行 cmd 命令)： 启动 nexus 经过前面命令已经完成 nexus 的安装，可以通过如下两种方式启动 nexus 服务。 在 Windows 系统服务中启动 nexus 在命令行执行 nexus.bat start 命令启动 nexus 访问 nexus 启动 nexus 服务后，访问 http://localhost:8081/nexus，点击右上角 LogIn 按钮，使用默认用户名 admin 和密码 admin123 登录系统。 登录成功后，点击左侧菜单 Repositories，可以看到 nexus 内置的仓库列表，如下图： nexus 仓库类型通过前面的仓库列表可以看到，nexus 默认内置了很多仓库，这些仓库可以划分为 4 种类型，每种类型的仓库用于存放特定的 jar 包，具体说明如下。 hosted：宿主仓库，部署自己的 jar 到这个类型的仓库，包括 Releases 和 Snapshots 两部分，Releases 为公司内部发布版本仓库，Snapshots 为公司内部测试版本仓库。 proxy：代理仓库，用于代理远程的公共仓库，如 maven 中央仓库，用户连接私服，私服自动去中央仓库下载 jar 包或者插件。 group：仓库组，用来合并多个 hosted 或 proxy 仓库，通常我们配置自己的 maven 连接仓库组。 virtual (虚拟)：兼容 Maven1 版本的 jar 或者插件。 nexus 仓库类型与安装目录对应关系 将项目发布到 maven 私服maven 私服是搭建在公司局域网内的 maven 仓库，公司内的所有开发团队都可以使用。例如技术研发团队开发了一个基础组件，就可以将这个基础组件打成 jar 包发布到私服，其他团队成员就可以从私服下载这个 jar 包到本地仓库并在项目中使用。 具体操作步骤如下： 配置 maven 的 settings.xml 文件 12345678910&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 注意：一定要在 idea 工具中引入的 maven 的 settings.xml 文件中配置。 配置项目的 pom.xml 文件 12345678910&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 执行 mvn clean deploy 命令 从私服下载 jar 到本地仓库前面我们已经完成了将本地项目打成 jar 包发布到 maven 私服，下面我们就需要从 maven 私服下载 jar 包到本地仓库。 具体操作步骤如下： 在 maven 的 settings.xml 文件中配置下载模板 1234567891011121314151617181920212223242526&lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;!--仓库地址，即nexus仓库组的地址--&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;!--是否下载releases构件--&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;!--是否下载snapshots构件--&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;!-- 插件仓库，maven的运行依赖插件，也需要从私服下载插件 --&gt; &lt;pluginRepository&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/profile&gt; 在 maven 的 settings.xml 文件中配置激活下载模板 123&lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 将第三方 jar 安装到本地仓库和 maven 私服在 maven 工程的 pom.xml 文件中配置某个 jar 包的坐标后，如果本地的 maven 仓库不存在这个 jar 包，maven 工具会自动到配置的 maven 私服下载，如果私服中也不存在，maven 私服就会从 maven 中央仓库进行下载。 但是并不是所有的 jar 包都可以从中央仓库下载到，比如常用的 Oracle 数据库驱动的 jar 包在中央仓库就不存在。此时需要到 Oracle 的官网下载驱动 jar 包，然后将此 jar 包通过 maven 命令安装到我们本地的 maven 仓库或者 maven 私服中，这样在 maven 项目中就可以使用 maven 坐标引用到此 jar 包了。 将第三方 jar 安装到本地仓库 下载 Oracle 的 jar 包 mvn install 命令进行安装 1mvn install:install-file -Dfile=ojdbc14-10.2.0.4.0.jar -DgroupId=com.oracle -DartifactId=ojdbc14 –Dversion=10.2.0.4.0 -Dpackaging=jar 查看本地 maven 仓库，确认安装是否成功 再比如安装 Classifier4J-0.6.jar，打开 cmd 窗口，切换到 jar 包所在目录，输入 mvn 命令，命令格式如下： 1mvn install:install-file -DgroupId=net.sf(自定义，需要与pom.xml文件中的groupId一致) -DartifactId=classifier4j(自定义，需要与pom.xml文件中的artifaceId一致) -Dversion=0.6(自定义，需要与pom.xml文件中的version一致) -Dpackaging=jar -Dfile=Classifier4J-0.6.jar(本地jar包) -DgroupId、-DartifactId、-Dversion、-Dpackaging、-Dfile 前面均有一个空格。 使用示例如下： 之后，在 maven 的本地仓库，根据 groupId —— artifactId —— version，即可找到打包进来的本地 jar 包，也可以在项目中的 pom.xml 文件引入： 12345&lt;dependency&gt; &lt;groupId&gt;net.sf&lt;/groupId&gt; &lt;artifactId&gt;classifier4j&lt;/artifactId&gt; &lt;version&gt;0.6&lt;/version&gt;&lt;/dependency&gt; 将第三方 jar 安装到 maven 私服 下载 Oracle 的 jar 包 在 maven 的 settings.xml 配置文件中配置第三方仓库的 server 信息 12345&lt;server&gt; &lt;id&gt;thirdparty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 执行 mvn deploy 命令进行安装 1mvn deploy:deploy-file -Dfile=ojdbc14-10.2.0.4.0.jar -DgroupId=com.oracle -DartifactId=ojdbc14 –Dversion=10.2.0.4.0 -Dpackaging=jar –Durl=http://localhost:8081/nexus/content/repositories/thirdparty/ -DrepositoryId=thirdparty maven 的依赖搜索顺序一般情况下，当执行 maven 构建命令时，maven 按照以下顺序查找依赖的库： 步骤 1：在本地仓库中搜索，如果找不到，执行步骤 2，如果找到了则执行其他操作。 步骤 2：在中央仓库中搜索，如果找不到，并且有一个或多个远程仓库已经设置，则执行步骤 4，如果找到了则下载到本地仓库中以备将来引用。 步骤 3：如果远程仓库没有被设置，maven 将简单的停滞处理并抛出错误 (无法找到依赖的文件)。 步骤 4：在一个或多个远程仓库中搜索依赖的文件，如果找到则下载到本地仓库以备将来引用，否则 maven 将停止处理并抛出错误 (无法找到依赖的文件)。 maven 的常用命令 clean： 清理 compile：编译 test： 测试 package：打包 install： 安装 maven 的坐标书写规范12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt;&lt;/dependency&gt; maven 的依赖范围 依赖范围 对于编译 classpath 有效 对于测试 classpath 有效 对于运行 classpath 有效 例子 compile Y Y Y spring-core test - Y - Junit provided Y Y - servlet-api runtime - Y Y JDBC 驱动 system Y Y - 本地的，maven 仓库之外的类库 默认使用 compile 依赖范围。 使用 system 依赖范围的依赖时，必须通过 systemPath 元素显示地指定依赖文件的路径。由于此类依赖不是通过 maven 仓库解析的，而且往往与本机系统绑定，可能构成构建的不可移植，因此应该谨慎使用。systemPath 元素可以引用环境变量，例如： 1234567&lt;dependency&gt; &lt;groupId&gt;javax.sql&lt;/groupId&gt; &lt;artifactId&gt;jdbc-stdext&lt;/artifactId&gt; &lt;Version&gt;2.0&lt;/Version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;java.home&#125;/lib/rt.jar&lt;/systemPath&gt;&lt;/dependency&gt; maven 的依赖传递什么是依赖传递在 maven 中，依赖是可以传递的，假设存在三个项目，分别是项目 A，项目 B 以及项目 C。假设 C 依赖 B，B 依赖 A，那么根据 maven 项目依赖的特征，不难推出项目 C 也依赖 A。如图所示： ​ 通过上面的图可以看到， 在一个 web 项目中，直接依赖了 spring-webmvc，而 spring-webmvc 依赖了 spring-aop、spring-beans 等。最终的结果就是在这个 web 项目中，间接依赖了 spring-aop、spring-beans 等。 什么是依赖冲突由于依赖传递现象的存在，如图所示，spring-webmvc 依赖 spirng-beans-4.2.4，spring-aop 依赖 spring-beans-5.0.2，现在 spirng-beans-4.2.4 已经加入到了工程中，而我们希望 spring-beans-5.0.2 加入工程。这就造成了依赖冲突。 如何解决依赖冲突 使用 maven 提供的依赖调节原则 排除依赖 锁定版本 依赖调节原则路径近者优先原则当依赖声明不在同一个 pom.xml 文件中时，或者说存在依赖传递时，路径最短的 jar 包将被选为最终依赖。 上图中，Jar2.0 将被选为最终依赖。 第一声明者优先原则当依赖声明不在同一个 pom.xml 文件中时，或者说存在依赖传递时，并且依赖传递长度相同时，最先声明的依赖将被选为最终依赖。 上图中，spring-aop 和 spring-webmvc 都依赖了 spring-beans，但是因为 spring-aop 在前面，所以最终使用的 spring-beans 是由 spring-aop 传递过来的，而 spring-webmvc 传递过来的 spring-beans 则被忽略了。 覆盖优先当依赖声明在同一个 pom.xml 文件中时，后面声明的依赖将覆盖前面声明的依赖。 排除依赖使用 exclusions 标签将传递过来的依赖排除出去。 版本锁定采用直接锁定版本的方法确定依赖 jar 包的版本，版本锁定后则不考虑依赖的声明顺序或依赖的路径，以锁定的版本为准添加到工程中，此方法在企业开发中经常使用。 版本锁定的使用方式： 第一步：在 dependencyManagement 标签中锁定依赖的版本 第二步：在 dependencies 标签中声明需要导入的 maven 坐标 备注能查找依赖的网站：https://mvnrepository.com/ 本文参考https://juejin.cn/post/6844903543711907848 https://www.jianshu.com/p/a1d9fd97f568 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"KafkaConsumer 源码之 consumer 如何拉取 offset 和数据","slug":"kafka-consumer-offsetandfetcher","date":"2020-11-10T03:32:06.000Z","updated":"2021-01-05T07:32:53.038Z","comments":true,"path":"2020/11/10/kafka-consumer-offsetandfetcher/","link":"","permalink":"http://example.com/2020/11/10/kafka-consumer-offsetandfetcher/","excerpt":"","text":"上一篇文章讲了 consumer 如何加入 consumer group，现在加入 group 成功之后，就要准备开始消费。 kafkaConsumer.poll () 方法的核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private ConsumerRecords&lt;K, V&gt; poll(final Timer timer, final boolean includeMetadataInTimeout) &#123; // Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭 acquireAndEnsureOpen(); try &#123; if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123; throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;); &#125; // poll for new data until the timeout expires do &#123; client.maybeTriggerWakeup(); if (includeMetadataInTimeout) &#123; // Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息 if (!updateAssignmentMetadataIfNeeded(timer)) &#123; return ConsumerRecords.empty(); &#125; &#125; else &#123; while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123; log.warn(&quot;Still waiting for metadata&quot;); &#125; &#125; // Step3:拉取数据，核心步骤 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer); if (!records.isEmpty()) &#123; // before returning the fetched records, we can send off the next round of fetches // and avoid block waiting for their responses to enable pipelining while the user // is handling the fetched records. // // NOTE: since the consumed position has already been updated, we must not allow // wakeups or any other errors to be triggered prior to returning the fetched records. // 在返回数据之前，发送下次的fetch请求，避免用户在下次获取数据时线程block if (fetcher.sendFetches() &gt; 0 || client.hasPendingRequests()) &#123; client.pollNoWakeup(); &#125; return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records)); &#125; &#125; while (timer.notExpired()); return ConsumerRecords.empty(); &#125; finally &#123; release(); &#125;&#125; 紧跟上一篇文章，我们继续分析 consumer 加入 group 后的行为： 123456789101112/** * Visible for testing */boolean updateAssignmentMetadataIfNeeded(final Timer timer) &#123; // 1.上一篇主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group) if (coordinator != null &amp;&amp; !coordinator.poll(timer)) &#123; return false; &#125; // 2.本篇文章从updateFetchPositions(timer)方法开始继续分析(主要功能是:consumer获得partition的offset) return updateFetchPositions(timer);&#125; KafkaConsumer 的消费策略首先，我们应该知道，KafkaConsumer 关于如何消费的 2 种策略： 手动指定：调用 consumer.seek(TopicPartition, offset)，然后开始 poll ()。 自动指定：poll () 之前给集群发送请求，让集群告知客户端，当前该 TopicPartition 的 offset 是多少，这也是我们此次分析的重点。 在讲如何拉取 offset 之前，先认识下下面这个类 (SubscriptionState 的内部类)： 12345678910111213private static class TopicPartitionState &#123; private FetchState fetchState; private FetchPosition position; // last consumed position private Long highWatermark; // the high watermark from last fetch private Long logStartOffset; // the log start offset private Long lastStableOffset; private boolean paused; // whether this partition has been paused by the user private OffsetResetStrategy resetStrategy; // the strategy to use if the offset needs resetting private Long nextRetryTimeMs; private Integer preferredReadReplica; private Long preferredReadReplicaExpireTimeMs; ...&#125; consumer 实例订阅的每个 topic-partition 都会有一个对应的 TopicPartitionState 对象，在这个对象中会记录上面内容，最需要关注的就是 position 这个属性，它表示上一次消费的位置。通过 consumer.seek () 方式指定消费 offset 的时候，其实设置的就是这个 position 值。 updateFetchPositions - 拉取 offset在 consumer 成功加入 group 并开始消费之前，我们还需要知道 consumer 是从 offset 为多少的位置开始消费。consumer 加入 group 之后，就得去获取 offset 了，下面的方法，就是开始更新 position (offset)： 1234567891011121314151617181920212223242526272829303132333435363738/** * Set the fetch position to the committed position (if there is one) * or reset it using the offset reset policy the user has configured. * * @throws org.apache.kafka.common.errors.AuthenticationException if authentication fails. See the exception for more details * @throws NoOffsetForPartitionException If no offset is stored for a given partition and no offset reset policy is * defined * @return true iff the operation completed without timing out */private boolean updateFetchPositions(final Timer timer) &#123; // If any partitions have been truncated due to a leader change, we need to validate the offsets fetcher.validateOffsetsIfNeeded(); // Step1:查看TopicPartitionState的position是否为空，第一次消费肯定为空 cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions(); if (cachedSubscriptionHashAllFetchPositions) return true; // If there are any partitions which do not have a valid position and are not // awaiting reset, then we need to fetch committed offsets. We will only do a // coordinator lookup if there are partitions which have missing positions, so // a consumer with manually assigned partitions can avoid a coordinator dependence // by always ensuring that assigned partitions have an initial position. // Step2:如果没有有效的offset，那么需要从GroupCoordinator中获取 if (coordinator != null &amp;&amp; !coordinator.refreshCommittedOffsetsIfNeeded(timer)) return false; // If there are partitions still needing a position and a reset policy is defined, // request reset using the default policy. If no reset strategy is defined and there // are partitions with a missing position, then we will raise an exception. // Step3:如果还存在partition不知道position，并且设置了offsetreset策略，那么就等待重置，不然就抛出异常 subscriptions.resetMissingPositions(); // Finally send an asynchronous request to lookup and update the positions of any // partitions which are awaiting reset. // Step4:向PartitionLeader(GroupCoordinator所在机器)发送ListOffsetRequest重置position fetcher.resetOffsetsIfNeeded(); return true;&#125; 上面的代码主要分为 4 个步骤，具体如下： 首先，查看当前 TopicPartition 的 position 是否为空，如果不为空，表示知道下次 fetch position (即拉取数据时从哪个位置开始拉取)，但如果是第一次消费，这个 TopicPartitionState.position 肯定为空。 然后，通过 GroupCoordinator 为缺少 fetch position 的 partition 拉取 position (即 last committed offset)。 继而，仍不知道 partition 的 position (_consumer_offsets 中未保存位移信息)，且设置了 offsetreset 策略，那么就等待重置，如果没有设置重置策略，就抛出 NoOffsetForPartitionException 异常。 最后，为那些需要重置 fetch position 的 partition 发送 ListOffsetRequest 重置 position (consumer.beginningOffsets ()，consumer.endOffsets ()，consumer.offsetsForTimes ()，consumer.seek () 都会发送 ListOffRequest 请求)。 上面说的几个方法相当于都是用户自己自定义消费的 offset，所以可能出现越界 (消费位置无法在实际分区中查到) 的情况，所以也是会发送 ListOffsetRequest 请求的，即触发 auto.offset.reset 参数的执行。比如现在某个 partition 的可拉取 offset 最大值为 100，如果你指定消费 offset=200 的位置，那肯定拉取不到，此时就会根据 auto.offset.reset 策略将拉取位置重置为 100 (默认的 auto.offset.reset 为 latest)。 refreshCommittedOffsetsIfNeeded我们先看下 Setp 2 中 GroupCoordinator 是如何 fetch position 的： 1234567891011121314151617181920212223242526272829/** * Refresh the committed offsets for provided partitions. * * @param timer Timer bounding how long this method can block * @return true iff the operation completed within the timeout */public boolean refreshCommittedOffsetsIfNeeded(Timer timer) &#123; final Set&lt;TopicPartition&gt; missingFetchPositions = subscriptions.missingFetchPositions(); // 1.发送获取offset的请求，核心步骤 final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = fetchCommittedOffsets(missingFetchPositions, timer); if (offsets == null) return false; for (final Map.Entry&lt;TopicPartition, OffsetAndMetadata&gt; entry : offsets.entrySet()) &#123; final TopicPartition tp = entry.getKey(); // 2.获取response中的offset final OffsetAndMetadata offsetAndMetadata = entry.getValue(); final ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.leaderAndEpoch(tp); final SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(), leaderAndEpoch); log.info(&quot;Setting offset for partition &#123;&#125; to the committed offset &#123;&#125;&quot;, tp, position); entry.getValue().leaderEpoch().ifPresent(epoch -&gt; this.metadata.updateLastSeenEpochIfNewer(entry.getKey(), epoch)); // 3.实际就是设置SubscriptionState的position值 this.subscriptions.seekUnvalidated(tp, position); &#125; return true;&#125; fetchCommittedOffsets () 方法的核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Fetch the current committed offsets from the coordinator for a set of partitions. * * @param partitions The partitions to fetch offsets for * @return A map from partition to the committed offset or null if the operation timed out */public Map&lt;TopicPartition, OffsetAndMetadata&gt; fetchCommittedOffsets(final Set&lt;TopicPartition&gt; partitions, final Timer timer) &#123; if (partitions.isEmpty()) return Collections.emptyMap(); final Generation generation = generation(); if (pendingCommittedOffsetRequest != null &amp;&amp; !pendingCommittedOffsetRequest.sameRequest(partitions, generation)) &#123; // if we were waiting for a different request, then just clear it. pendingCommittedOffsetRequest = null; &#125; do &#123; if (!ensureCoordinatorReady(timer)) return null; // contact coordinator to fetch committed offsets final RequestFuture&lt;Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt; future; if (pendingCommittedOffsetRequest != null) &#123; future = pendingCommittedOffsetRequest.response; &#125; else &#123; // 1.封装FetchRequest请求 future = sendOffsetFetchRequest(partitions); pendingCommittedOffsetRequest = new PendingCommittedOffsetRequest(partitions, generation, future); &#125; // 2.通过KafkaClient发送请求 client.poll(future, timer); if (future.isDone()) &#123; pendingCommittedOffsetRequest = null; if (future.succeeded()) &#123; // 3.请求成功，获取请求的响应数据 return future.value(); &#125; else if (!future.isRetriable()) &#123; throw future.exception(); &#125; else &#123; timer.sleep(retryBackoffMs); &#125; &#125; else &#123; return null; &#125; &#125; while (timer.notExpired()); return null;&#125; 上面的步骤和我们之前提到的发送其他请求毫无区别，基本就是这三个套路。 在获取到响应之后，会通过 subscriptions.seekUnvalidated () 方法为每个 TopicPartition 设置 position 值后，就知道从哪里开始消费订阅 topic 下的 partition 了。 resetMissingPositions在 Step 3 中，什么时候发起 FetchRequest 拿不到 position 呢？ 我们知道消费位移 (consume offset) 是保存在 _consumer_offsets 这个 topic 里面的，当我们进行消费的时候需要知道上次消费到了什么位置。那么就会发起请求去看上次消费到了 topic 的 partition 的哪个位置，但是这个消费位移是有保存时长的，默认为 7 天 (broker 端通过 offsets.retention.minutes 设置)。 当隔了一段时间再进行消费，如果这个间隔时间超过了参数的配置值，那么原先的位移信息就会丢失，最后只能通过客户端参数 auto.offset.reset 来确定开始消费的位置。 如果我们第一次消费 topic，那么在 _consumer_offsets 中也是找不到消费位移的，所以就会执行第四个步骤，发起 ListOffsetRequest 请求根据配置的 reset 策略 (即 auto.offset.reset) 来决定开始消费的位置。 resetOffsetsIfNeeded在 Step 4 中，发起 ListOffsetRequest 请求和处理 response 的核心代码如下： 123456789101112131415161718192021222324252627/** * Reset offsets for all assigned partitions that require it. * * @throws org.apache.kafka.clients.consumer.NoOffsetForPartitionException If no offset reset strategy is defined * and one or more partitions aren&#x27;t awaiting a seekToBeginning() or seekToEnd(). */public void resetOffsetsIfNeeded() &#123; // Raise exception from previous offset fetch if there is one RuntimeException exception = cachedListOffsetsException.getAndSet(null); if (exception != null) throw exception; // 1.需要执行reset策略的partition Set&lt;TopicPartition&gt; partitions = subscriptions.partitionsNeedingReset(time.milliseconds()); if (partitions.isEmpty()) return; final Map&lt;TopicPartition, Long&gt; offsetResetTimestamps = new HashMap&lt;&gt;(); for (final TopicPartition partition : partitions) &#123; Long timestamp = offsetResetStrategyTimestamp(partition); if (timestamp != null) offsetResetTimestamps.put(partition, timestamp); &#125; // 2.执行reset策略 resetOffsetsAsync(offsetResetTimestamps);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940private void resetOffsetsAsync(Map&lt;TopicPartition, Long&gt; partitionResetTimestamps) &#123; Map&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; timestampsToSearchByNode = groupListOffsetRequests(partitionResetTimestamps, new HashSet&lt;&gt;()); for (Map.Entry&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; entry : timestampsToSearchByNode.entrySet()) &#123; Node node = entry.getKey(); final Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; resetTimestamps = entry.getValue(); subscriptions.setNextAllowedRetry(resetTimestamps.keySet(), time.milliseconds() + requestTimeoutMs); // 1.发送ListOffsetRequest请求 RequestFuture&lt;ListOffsetResult&gt; future = sendListOffsetRequest(node, resetTimestamps, false); // 2.为ListOffsetRequest请求添加监听器 future.addListener(new RequestFutureListener&lt;ListOffsetResult&gt;() &#123; @Override public void onSuccess(ListOffsetResult result) &#123; if (!result.partitionsToRetry.isEmpty()) &#123; subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs); metadata.requestUpdate(); &#125; for (Map.Entry&lt;TopicPartition, ListOffsetData&gt; fetchedOffset : result.fetchedOffsets.entrySet()) &#123; TopicPartition partition = fetchedOffset.getKey(); ListOffsetData offsetData = fetchedOffset.getValue(); ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition); // 3.发送ListOffsetRequest请求成功，对结果reset，如果reset策略设置的是latest，那么requestedReset.timestamp = -1，如果是earliest，requestedReset.timestamp = -2 resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData); &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; subscriptions.requestFailed(resetTimestamps.keySet(), time.milliseconds() + retryBackoffMs); metadata.requestUpdate(); if (!(e instanceof RetriableException) &amp;&amp; !cachedListOffsetsException.compareAndSet(null, e)) log.error(&quot;Discarding error in ListOffsetResponse because another error is pending&quot;, e); &#125; &#125;); &#125;&#125; sendListOffsetRequest () 方法的核心代码如下： 1234567891011121314151617181920212223242526/** * Send the ListOffsetRequest to a specific broker for the partitions and target timestamps. * * @param node The node to send the ListOffsetRequest to. * @param timestampsToSearch The mapping from partitions to the target timestamps. * @param requireTimestamp True if we require a timestamp in the response. * @return A response which can be polled to obtain the corresponding timestamps and offsets. */private RequestFuture&lt;ListOffsetResult&gt; sendListOffsetRequest(final Node node, final Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; timestampsToSearch, boolean requireTimestamp) &#123; ListOffsetRequest.Builder builder = ListOffsetRequest.Builder .forConsumer(requireTimestamp, isolationLevel) .setTargetTimes(timestampsToSearch); log.debug(&quot;Sending ListOffsetRequest &#123;&#125; to broker &#123;&#125;&quot;, builder, node); return client.send(node, builder) .compose(new RequestFutureAdapter&lt;ClientResponse, ListOffsetResult&gt;() &#123; @Override public void onSuccess(ClientResponse response, RequestFuture&lt;ListOffsetResult&gt; future) &#123; ListOffsetResponse lor = (ListOffsetResponse) response.responseBody(); log.trace(&quot;Received ListOffsetResponse &#123;&#125; from broker &#123;&#125;&quot;, lor, node); handleListOffsetResponse(timestampsToSearch, lor, future); &#125; &#125;);&#125; resetOffsetIfNeeded () 方法的核心代码如下： 1234567private void resetOffsetIfNeeded(TopicPartition partition, OffsetResetStrategy requestedResetStrategy, ListOffsetData offsetData) &#123; SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetData.offset, offsetData.leaderEpoch, metadata.leaderAndEpoch(partition)); offsetData.leaderEpoch.ifPresent(epoch -&gt; metadata.updateLastSeenEpochIfNewer(partition, epoch)); // reset对应的TopicPartition fetch的position subscriptions.maybeSeekUnvalidated(partition, position.offset, requestedResetStrategy);&#125; 这里解释下 auto.offset.reset 的两个值 (latest 和 earliest) 的区别： 假设我们现在要消费 MyConsumerTopic 的数据，它有 3 个分区，生产者往这个 topic 发送了 10 条数据，然后分区数据按照 MyConsumerTopic-0 (3 条数据)，MyConsumerTopic-1 (3 条数据)，MyConsumerTopic-2 (4 条数据) 这样分配。 当设置为 latest 的时候，返回的 offset 具体到每个 partition 就是 HW 值 (partition 0 是 3，partition 1 是 3，partition 2 是 4)。 当设置为 earliest 的时候，就会从起始处 (即 LogStartOffset，注意不是 LSO) 开始消费，这里就是从 0 开始。 Log Start Offset：表示 partition 的起始位置，初始值为 0，由于消息的增加以及日志清除策略影响，这个值会阶段性增大。尤其注意这个不能缩写为 LSO，LSO 代表的是 LastStableOffset，和事务有关。 Consumer Offset：消费位移，表示 partition 的某个消费者消费到的位移位置。 High Watermark：简称 HW，代表消费端能看到的 partition 的最高日志位移，HW 大于等于 ConsumerOffset 的值。 Log End Offset：简称 LEO，代表 partition 的最高日志位移，对消费者不可见，HW 到 LEO 这之间的数据未被 follwer 完全同步。 至此，我们成功的知道 consumer 消费的 partition 的 offset 位置在哪里，下面就开始拉取 partition 里的数据。 pollForFetches - 拉取数据现在万事俱备只欠东风了，consumer 成功加入 group，也确定了需要拉取的 topic partition 的 offset，那么现在就应该去拉取数据了，其核心源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) &#123; long pollTimeout = coordinator == null ? timer.remainingMs() : Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs()); // if data is available already, return it immediately // 1.获取fetcher已经拉取到的数据 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords(); if (!records.isEmpty()) &#123; return records; &#125; // 到此，说明上次fetch到的数据已经全部拉取了，需要再次发送fetch请求，从broker拉取新的数据 // send any new fetches (won&#x27;t resend pending fetches) // 2.发送fetch请求，会从多个topic-partition拉取数据(只要对应的topic-partition没有未完成的请求) fetcher.sendFetches(); // We do not want to be stuck blocking in poll if we are missing some positions // since the offset lookup may be backing off after a failure // NOTE: the use of cachedSubscriptionHashAllFetchPositions means we MUST call // updateAssignmentMetadataIfNeeded before this method. if (!cachedSubscriptionHashAllFetchPositions &amp;&amp; pollTimeout &gt; retryBackoffMs) &#123; pollTimeout = retryBackoffMs; &#125; Timer pollTimer = time.timer(pollTimeout); // 3.真正开始发送，底层同样使用NIO client.poll(pollTimer, () -&gt; &#123; // since a fetch might be completed by the background thread, we need this poll condition // to ensure that we do not block unnecessarily in poll() return !fetcher.hasCompletedFetches(); &#125;); timer.update(pollTimer.currentTimeMs()); // after the long poll, we should check whether the group needs to rebalance // prior to returning data so that the group can stabilize faster // 4.如果group需要rebalance，直接返回空数据，这样更快地让group进入稳定状态 if (coordinator != null &amp;&amp; coordinator.rejoinNeededOrPending()) &#123; return Collections.emptyMap(); &#125; // 5.返回拉取到的新数据 return fetcher.fetchedRecords();&#125; fetcher.sendFetches这里需要注意的是 fetcher.sendFetches () 方法，在发送请求的同时会注册回调函数，当有 response 的时候，会解析 response，将返回的数据放到 Fetcher 的成员变量中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/** * Set-up a fetch request for any node that we have assigned partitions for which doesn&#x27;t already have * an in-flight fetch or pending fetch data. * @return number of fetches sent */public synchronized int sendFetches() &#123; // Update metrics in case there was an assignment change sensors.maybeUpdateAssignment(subscriptions); // 1.创建FetchRequest Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; fetchRequestMap = prepareFetchRequests(); for (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData&gt; entry : fetchRequestMap.entrySet()) &#123; final Node fetchTarget = entry.getKey(); final FetchSessionHandler.FetchRequestData data = entry.getValue(); final FetchRequest.Builder request = FetchRequest.Builder .forConsumer(this.maxWaitMs, this.minBytes, data.toSend()) .isolationLevel(isolationLevel) .setMaxBytes(this.maxBytes) .metadata(data.metadata()) .toForget(data.toForget()) .rackId(clientRackId); if (log.isDebugEnabled()) &#123; log.debug(&quot;Sending &#123;&#125; &#123;&#125; to broker &#123;&#125;&quot;, isolationLevel, data.toString(), fetchTarget); &#125; // 2.发送FetchRequest RequestFuture&lt;ClientResponse&gt; future = client.send(fetchTarget, request); // We add the node to the set of nodes with pending fetch requests before adding the // listener because the future may have been fulfilled on another thread (e.g. during a // disconnection being handled by the heartbeat thread) which will mean the listener // will be invoked synchronously. this.nodesWithPendingFetchRequests.add(entry.getKey().id()); future.addListener(new RequestFutureListener&lt;ClientResponse&gt;() &#123; @Override public void onSuccess(ClientResponse resp) &#123; synchronized (Fetcher.this) &#123; try &#123; @SuppressWarnings(&quot;unchecked&quot;) FetchResponse&lt;Records&gt; response = (FetchResponse&lt;Records&gt;) resp.responseBody(); FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler == null) &#123; log.error(&quot;Unable to find FetchSessionHandler for node &#123;&#125;. Ignoring fetch response.&quot;, fetchTarget.id()); return; &#125; if (!handler.handleResponse(response)) &#123; return; &#125; Set&lt;TopicPartition&gt; partitions = new HashSet&lt;&gt;(response.responseData().keySet()); FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions); for (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&lt;Records&gt;&gt; entry : response.responseData().entrySet()) &#123; TopicPartition partition = entry.getKey(); FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition); if (requestData == null) &#123; String message; if (data.metadata().isFull()) &#123; message = MessageFormatter.arrayFormat( &quot;Response for missing full request partition: partition=&#123;&#125;; metadata=&#123;&#125;&quot;, new Object[]&#123;partition, data.metadata()&#125;).getMessage(); &#125; else &#123; message = MessageFormatter.arrayFormat( &quot;Response for missing session request partition: partition=&#123;&#125;; metadata=&#123;&#125;; toSend=&#123;&#125;; toForget=&#123;&#125;&quot;, new Object[]&#123;partition, data.metadata(), data.toSend(), data.toForget()&#125;).getMessage(); &#125; // Received fetch response for missing session partition throw new IllegalStateException(message); &#125; else &#123; long fetchOffset = requestData.fetchOffset; FetchResponse.PartitionData&lt;Records&gt; fetchData = entry.getValue(); log.debug(&quot;Fetch &#123;&#125; at offset &#123;&#125; for partition &#123;&#125; returned fetch data &#123;&#125;&quot;, isolationLevel, fetchOffset, partition, fetchData); // 3.发送FetchRequest请求成功，将返回的数据放到ConcurrentLinkedQueue&lt;CompletedFetch&gt;中 completedFetches.add(new CompletedFetch(partition, fetchOffset, fetchData, metricAggregator, resp.requestHeader().apiVersion())); &#125; &#125; sensors.fetchLatency.record(resp.requestLatencyMs()); &#125; finally &#123; nodesWithPendingFetchRequests.remove(fetchTarget.id()); &#125; &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; synchronized (Fetcher.this) &#123; try &#123; FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler != null) &#123; handler.handleError(e); &#125; &#125; finally &#123; nodesWithPendingFetchRequests.remove(fetchTarget.id()); &#125; &#125; &#125; &#125;); &#125; return fetchRequestMap.size();&#125; 该方法主要分为以下两步： prepareFetchRequests ()：为订阅的所有 topic-partition list 创建 fetch 请求 (只要该 topic-partition 没有还在处理的请求)，创建的 fetch 请求依然是按照 node 级别创建的； client.send ()：发送 fetch 请求，并设置相应的 Listener，请求处理成功的话，就加入到 completedFetches 中，在加入这个 completedFetches 队列时，是按照 topic-partition 级别去加入，这样也就方便了后续的处理。 从这里可以看出，在每次发送 fetch 请求时，都会向所有可发送的 topic-partition 发送 fetch 请求，调用一次 fetcher.sendFetches，拉取到的数据，可能需要多次 pollForFetches 循环才能处理完，因为 Fetcher 线程是在后台运行，这也保证了尽可能少地阻塞用户的处理线程，因为如果 Fetcher 中没有可处理的数据，用户的线程是会阻塞在 poll 方法中的。 fetcher.fetchedRecords这个方法的作用就是获取已经从 server 拉取到的 Records，其核心源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117/** * Return the fetched records, empty the record buffer and update the consumed position. * * NOTE: returning empty records guarantees the consumed position are NOT updated. * * @return The fetched records per partition * @throws OffsetOutOfRangeException If there is OffsetOutOfRange error in fetchResponse and * the defaultResetPolicy is NONE * @throws TopicAuthorizationException If there is TopicAuthorization error in fetchResponse. */public Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords() &#123; Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetched = new HashMap&lt;&gt;(); // 在max.poll.records中设置单词最大的拉取条数，默认500条 int recordsRemaining = maxPollRecords; try &#123; while (recordsRemaining &gt; 0) &#123; if (nextInLineRecords == null || nextInLineRecords.isFetched) &#123;// nextInLineRecords为空时 // Step1:当一个nextInLineRecords处理完，就从completedFetches处理下一个完成的Fetch请求 CompletedFetch completedFetch = completedFetches.peek(); if (completedFetch == null) break; try &#123; // Step2:获取下一个要处理的nextInLineRecords nextInLineRecords = parseCompletedFetch(completedFetch); &#125; catch (Exception e) &#123; // Remove a completedFetch upon a parse with exception if (1) it contains no records, and // (2) there are no fetched records with actual content preceding this exception. // The first condition ensures that the completedFetches is not stuck with the same completedFetch // in cases such as the TopicAuthorizationException, and the second condition ensures that no // potential data loss due to an exception in a following record. FetchResponse.PartitionData partition = completedFetch.partitionData; if (fetched.isEmpty() &amp;&amp; (partition.records == null || partition.records.sizeInBytes() == 0)) &#123; completedFetches.poll(); &#125; throw e; &#125; completedFetches.poll(); &#125; else &#123; // Step3:拉取records，更新position List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = fetchRecords(nextInLineRecords, recordsRemaining); TopicPartition partition = nextInLineRecords.partition; if (!records.isEmpty()) &#123; List&lt;ConsumerRecord&lt;K, V&gt;&gt; currentRecords = fetched.get(partition); if (currentRecords == null) &#123;// 正常情况下，一个node只会发送一个request，一般只会有一个 fetched.put(partition, records); &#125; else &#123; // this case shouldn&#x27;t usually happen because we only send one fetch at a time per partition, // but it might conceivably happen in some rare cases (such as partition leader changes). // we have to copy to a new list because the old one may be immutable List&lt;ConsumerRecord&lt;K, V&gt;&gt; newRecords = new ArrayList&lt;&gt;(records.size() + currentRecords.size()); newRecords.addAll(currentRecords); newRecords.addAll(records); fetched.put(partition, newRecords); &#125; recordsRemaining -= records.size(); &#125; &#125; &#125; &#125; catch (KafkaException e) &#123; if (fetched.isEmpty()) throw e; &#125; // Step4:返回相应的Records数据 return fetched;&#125;private List&lt;ConsumerRecord&lt;K, V&gt;&gt; fetchRecords(PartitionRecords partitionRecords, int maxRecords) &#123; if (!subscriptions.isAssigned(partitionRecords.partition)) &#123; // this can happen when a rebalance happened before fetched records are returned to the consumer&#x27;s poll call log.debug(&quot;Not returning fetched records for partition &#123;&#125; since it is no longer assigned&quot;, partitionRecords.partition); &#125; else if (!subscriptions.isFetchable(partitionRecords.partition)) &#123; // this can happen when a partition is paused before fetched records are returned to the consumer&#x27;s // poll call or if the offset is being reset // 这个topic-partition不能被消费了，比如调用了pause log.debug(&quot;Not returning fetched records for assigned partition &#123;&#125; since it is no longer fetchable&quot;, partitionRecords.partition); &#125; else &#123; SubscriptionState.FetchPosition position = subscriptions.position(partitionRecords.partition); if (partitionRecords.nextFetchOffset == position.offset) &#123;// offset对的上，也就是拉取是按顺序拉的 // 获取该topic-partition对应的records，并更新partitionRecords的fetchOffset(用于判断是否顺序) List&lt;ConsumerRecord&lt;K, V&gt;&gt; partRecords = partitionRecords.fetchRecords(maxRecords); if (partitionRecords.nextFetchOffset &gt; position.offset) &#123; SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition( partitionRecords.nextFetchOffset, partitionRecords.lastEpoch, position.currentLeader); log.trace(&quot;Returning fetched records at offset &#123;&#125; for assigned partition &#123;&#125; and update &quot; + &quot;position to &#123;&#125;&quot;, position, partitionRecords.partition, nextPosition); // 更新消费到的offset(the fetch position) subscriptions.position(partitionRecords.partition, nextPosition); &#125; // 获取Lag(即position与hw之间差值)，hw为null时，才返回null Long partitionLag = subscriptions.partitionLag(partitionRecords.partition, isolationLevel); if (partitionLag != null) this.sensors.recordPartitionLag(partitionRecords.partition, partitionLag); Long lead = subscriptions.partitionLead(partitionRecords.partition); if (lead != null) &#123; this.sensors.recordPartitionLead(partitionRecords.partition, lead); &#125; return partRecords; &#125; else &#123; // these records aren&#x27;t next in line based on the last consumed position, ignore them // they must be from an obsolete request log.debug(&quot;Ignoring fetched records for &#123;&#125; at offset &#123;&#125; since the current position is &#123;&#125;&quot;, partitionRecords.partition, partitionRecords.nextFetchOffset, position); &#125; &#125; partitionRecords.drain(); return emptyList();&#125; consumer 的 Fetcher 处理从 server 获取的 fetch response 大致分为以下几个过程： 通过 completedFetches.peek() 获取已经成功的 fetch response (在 fetcher.sendFetches () 方法中会把发送FetchRequest请求成功后的结果放在这个集合中，是拆分为 topic-partition 的粒度放进去的)； parseCompletedFetch() 处理上面获取的 completedFetch，构造成 PartitionRecords 类型； 通过 fetchRecords() 方法处理 PartitionRecords 对象，在这个里面会去验证 fetchOffset 是否能对得上，只有 fetchOffset 是一致的情况下才会去处理相应的数据，并更新 the fetch offset 的信息，如果 fetchOffset 不一致，这里就不会处理，the fetch offset 就不会更新，下次 fetch 请求时是会接着 the fetch offset 的位置去请求相应的数据； 返回相应的 Records 数据。 至此，KafkaConsumer 如何拉取消息的整体流程也分析完毕。 本文参考http://generalthink.github.io/2019/05/31/kafka-consumer-offset/ https://matt33.com/2017/11/11/consumer-pollonce/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"欧阳修","slug":"ouyangxiu","date":"2020-11-10T00:51:06.000Z","updated":"2020-11-23T08:48:11.442Z","comments":true,"path":"2020/11/10/ouyangxiu/","link":"","permalink":"http://example.com/2020/11/10/ouyangxiu/","excerpt":"","text":"伐树记署之东园，久茀不治。修至始辟之，粪瘠溉枯，为蔬圃十数畦，又植花果桐竹凡百本。 春阳既浮，萌者将动。园之守启曰：“园有樗焉，其根壮而叶大。根壮则梗地脉，耗阳气，而新植者不得滋；叶大则阴翳蒙碍，而新植者不得畅以茂。又其材拳曲臃肿，疏轻而不坚，不足养，是宜伐。”因尽薪之。明日，圃之守又曰：“圃之南有杏焉，凡其根庇之广可六七尺，其下之地最壤腴，以杏故，特不得蔬，是亦宜薪。”修曰：“噫！今杏方春且华，将待其实，若独不能损数畦之广为杏地邪？“因勿伐。 既而悟且叹曰：“吁！庄周之说曰：樗、栎以不材终其天年，桂、漆以有用而见伤夭。今樗诚不材矣，然一旦悉翦弃；杏之体最坚密，美泽可用，反见存。岂才不才各遭其时之可否邪？” 他日，客有过修者。仆夫曳薪过堂下，因指而语客以所疑。客曰： “是何怪邪？夫以无用处无用，庄周之贵也。以无用而贼有用，乌能免哉！彼杏之有华实也，以有生之具而庇其根，幸矣。若桂、漆之不能逃乎斤斧者，盖有利之者在死，势不得以生也，与乎杏实异矣。今樗之臃肿不材，而以壮大害物，其见伐，诚宜尔。与夫‘才者死、不才者生’之说，又异矣。凡物幸之与不幸，视其处之而已。”客既去，修善其言而记之。 非非堂记权衡之平物，动则轻重差，其于静也，锱铢不失。水之鉴物，动则不能有睹，其于静也，毫发可辨。在乎人，耳司听，目司视，动则乱于聪明，其于静也，闻见必审。处身者不为外物眩晃而动，则其心静，心静则智识明，是是非非，无所施而不中。夫是是近乎谄，非非近乎讪，不幸而过，宁讪无谄。是者，君子之常，是之何加？一以视之，未若非非之为正也。 予居洛之明年，既新厅事，有文纪于壁末。营其西偏作堂，户北向，植丛竹，辟户于其南，纳日月之光。设一几一榻，架书数百卷，朝夕居其中。以其静也，闭目澄心，览今照古，思虑无所不至焉。故其堂以非非为名云。 浪淘沙 · 把酒祝东风把酒祝东风，且共从容。垂杨紫陌洛城东。总是当时携手处，游遍芳丛。 聚散苦匆匆，此恨无穷。今年花胜去年红。可惜明年花更好，知与谁同？ 玉楼春 · 尊前拟把归期说尊前拟把归期说，欲语春容先惨咽。人生自是有情痴，此恨不关风与月。 离歌且莫翻新阕，一曲能教肠寸结。直须看尽洛城花，始共春风容易别。 生查子 · 元夕去年元夜时，花市灯如昼。 月上柳梢头，人约黄昏后。 今年元夜时，月与灯依旧。 不见去年人，泪满春衫袖。 读李翱文予始读翱《复性书》三篇，曰：此《中庸》之义疏尔。智者诚其性，当读《中庸》；愚者虽读此不晓也，不作可焉。又读《与韩侍郎荐贤书》，以谓翱特穷时愤世无荐己者，故丁宁如此；使其得志，亦未必。然以韩为秦汉间好侠行义之一豪俊，亦善论人者也。最后读《幽怀赋》，然后置书而叹，叹已复读，不自休。恨翱不生于今，不得与之交；又恨予不得生翱时，与翱上下其论也。 凡昔翱一时人，有道而能文者，莫若韩愈。愈尝有赋矣，不过羡二鸟之光荣，叹一饱之无时尔。此其心使光荣而饱，则不复云矣。若翱独不然，其赋曰：“众嚣嚣而杂处兮，咸叹老而嗟卑；视予心之不然兮，虑行道之犹非。”又怪神尧以一旅取天下，后世子孙不能以天下取河北，以为忧。呜呼！使当时君子皆易其叹老嗟卑之心为翱所忧之心，则唐之天下岂有乱与亡哉！ 然翱幸不生今时，见今之事，则其忧又甚矣。奈何今之人不忧也？余行天下，见人多矣，脱有一人能如翱忧者，又皆贱远，与翱无异；其余光荣而饱者，一闻忧世之言，不以为狂人，则以为病痴子，不怒则笑之矣。呜呼，在位而不肯自忧，又禁他人使皆不得忧，可叹也夫! 景祐三年十月十七日，欧阳修书。 答吴充秀才书修顿首白，先辈吴君足下。前辱示书及文三篇，发而读之，浩乎若千万言之多，及少定而视焉，才数百言尔。非夫辞丰意雄，沛然有不可御之势，何以至此！然犹自患伥伥莫有开之使前者，此好学之谦言也。 修材不足用于时，仕不足荣于世，其毁誉不足轻重，气力不足动人。世之欲假誉以为重，借力而后进者，奚取于修焉？先辈学精文雄，其施于时，又非待修誉而为重，力而后进者也。然而惠然见临，若有所责，得非急于谋道，不择其人而问焉者欤？ 夫学者未始不为道，而至者鲜焉；非道之于人远也，学者有所溺焉尔。盖文之为言，难工而可喜，易悦而自足。世之学者往往溺之，一有工焉，则曰：“吾学足矣。“甚者至弃百事不关于心，曰：“吾文士也，职于文而已。”此其所以至之鲜也。 昔孔子老而归鲁，六经之作，数年之顷尔。然读《易》者如无《春秋》，读《书》者如无《诗》，何其用功少而至于至也！圣人之文虽不可及，然大抵道胜者，文不难而自至也。故孟子皇皇不暇著书，荀卿盖亦晚而有作。若子云、仲淹，方勉焉以模言语，此道未足而强言者也。后之惑者，徒见前世之文传，以为学者文而已，故愈力愈勤而愈不至。此足下所谓”终日不出于轩序，不能纵横高下皆如意“者，道未足也。若道之充焉，虽行乎天地，入于渊泉，无不之也。 先辈之文浩乎沛然，可谓善矣。而又志于为道，犹自以为未广。若不止焉，孟、荀可至而不难也。修，学道而不至者，然幸不甘于所悦而溺于所止。因吾子之能不自止，又以励修之少进焉。幸甚！幸甚！修白。 答祖择之书修启。秀才人至，蒙示书一通，并诗赋杂文两策，谕之曰：“一览以为如何？”某既陋，不足以辱好学者之问；又其少贱而长穷，其素所为未有足称以取信于人。亦尝有人问者，以不足问之愚，而未尝答人之问。足下卒然及之，是以愧惧不知所言。虽然，不远数百里走使者以及门，意厚礼勤，何敢不报。 某闻古之学者必严其师，师严然后道尊，道尊然后笃敬，笃敬然后能自守，能自守然后果于用，果于用然后不畏而不迁。三代之衰，学校废。至两汉，师道尚存，故其学者各守其经以自用。是以汉之政理文章与其当时之事，后世莫及者，其所从来深矣。后世师，法渐坏，而今世无师，则学者不尊严，故自轻其道。轻之则不能至，不至则不能笃信，信不笃则不知所守，守不固则有所畏而物可移。是故学者惟俯仰徇时，以希禄利为急，至于忘本趋末，流而不返。夫以不信不固之心，守不至之学，虽欲果于自用，而莫知其所以用之之道，又况有禄利之诱、刑祸之惧以迁之哉！此足下所谓志古知道之士世所鲜，而未有合者，由此也。 足下所为文，用意甚高，卓然有不顾世俗之心，直欲自到于古人。今世之人用心如足下者有几？是则乡曲之中能为足下之师者谓谁，交游之间能发足下之议论者谓谁？学不师则守不一，议论不博则无所发明而究其深。足下之言高趣远，甚善，然所守未一而议论未精，此其病也。窃惟足下之交游能为足下称才誉美者不少，今皆舍之，远而见及，乃知足下是欲求其不至。此古君子之用心也，是以言之不敢隐。 夫世无师矣，学者当师经，师经必先求其意，意得则心定，心定则道纯，道纯则充于中者实，中充实则发为文者辉光，施于世者果致。三代、两汉之学，不过此也。足下患世未有合者，而不弃其愚，将某以为合，故敢道此。未知足下之意合否？ 与荆南乐秀才书修顿首白秀才足下。前者舟行往来，屡辱见过。又辱以所业一编，先之启事，及门而贽。田秀才西来，辱书；其后予家奴自府还县，比又辱书。仆有罪之人，人所共弃，而足下见礼如此，何以当之？当之未暇答，宜遂绝，而再辱书；再而未答，益宜绝，而又辱之。何其勤之甚也！如修者，天下穷贱之人尔，安能使足下之切切如是邪？盖足下力学好问，急于自为谋而然也。然蒙索仆所为文字者，此似有所过听也。 仆少从进士举于有司，学为诗赋，以备程试，凡三举而得第。与士君子相识者多，故往往能道仆名字，而又以游从相爱之私，或过称其文字。故使足下闻仆虚名，而欲见其所为者，由此也。 仆少孤贫，贪禄仕以养亲，不暇就师穷经，以学圣人之遗业。而涉猎书史，姑随世俗作所谓时文者，皆穿蠹经传，移此俪彼，以为浮薄，惟恐不悦于时人，非有卓然自立之言如古人者。然有司过采，屡以先多士。及得第已来，自以前所为不足以称有司之举而当长者之知，始大改其为，庶几有立。然言出而罪至，学成而身辱，为彼则获誉，为此则受祸，此明效也。 夫时文虽曰浮巧，然其为功，亦不易也。仆天姿不好而强为之，故比时人之为者尤不工，然已足以取禄仕而窃名誉者，顺时故也。先辈少年志盛，方欲取荣誉于世，则莫若顺时。天圣中，天子下诏书，敕学者去浮华，其后风俗大变。今时之士大夫所为，彬彬有两汉之风矣。先辈往学之，非徒足以顺时取誉而已，如其至之，是直齐肩于两汉之士也。若仆者，其前所为既不足学，其后所为慎不可学，是以徘徊不敢出其所为者，为此也。 在《易》之《困》曰：“有言不信。”谓夫人方困时，其言不为人所信也。今可谓困矣，安足为足下所取信哉？辱书既多且切，不敢不答。幸察。 答李诩第一书修白。人至，辱书及《性诠》三篇，曰以质其果是。夫自信笃者，无所待于人；有质于人者，自疑者也。今吾子自谓“夫子与孟、荀、扬、韩复生，不能夺吾言”，其可谓自信不疑者矣。而返以质于修。使修有过于夫子者，乃可为吾子辩，况修未及孟、荀、扬、韩之一二也。修非知道者，好学而未至者也。世无师久矣，尚赖朋友切磋之益，苟不自满而中止，庶几终身而有成。固常乐与学者论议往来，非敢以益于人，盖求益于人者也。况如吾子之文章论议，岂易得哉？固乐为吾子辩也。苟尚有所疑，敢不尽其所学以告，既吾子自信如是，虽夫子不能夺，使修何所说焉？人还索书，未知所答，惭惕惭惕。修再拜。 答李诩第二书修白。前辱示书及《性诠》三篇，见吾子好学善辩，而文能尽其意之详。令世之言性者多矣，有所不及也，故思与吾子卒其说。 修患世之学者多言性，故常为说曰“夫性，非学者之所急，而圣人之所罕言也。《易》六十四卦不言性，其言者动静得失吉凶之常理也；《春秋》二百四十二年不言性，其言者善恶是非之实录也；《诗》三百五篇不言性，其言者政教兴衰之美刺也；《书》五十九篇不言性，其言者尧、舜、三代之治乱也；《礼》、《乐》之书虽不完，而杂出于诸儒之记，然其大要，治国修身之法也。六经之所载，皆人事之切于世者，是以言之甚详。至于性也，百不一二言之，或因言而及焉，非为性而言也，故虽言而不究。 予之所谓不言者，非谓绝而无言，盖其言者鲜，而又不主于性而言也。《论语》所载七十二子之问于孔子者，问孝、问忠、问仁义、问礼乐、问修身、问为政、问朋友、问鬼神者有矣，未尝有问性者。孔子之告其弟子者，凡数千言，其及于性者一言而已。予故曰：非学者之所急，而圣人之罕言也。 《书》曰“习与性成”，《语》曰“性相近，习相远”者，戒人慎所习而言也。《中庸》曰“天命之谓性，率性之谓道”者，明性无常，必有以率之也。《乐记》亦曰“感物而动，性之欲”者，明物之感人无不至也。然终不言性果善果恶，但戒人慎所习与所感，而勤其所以率之者尔。予故曰“因言以及之，而不究也。 修少好学，知学之难。凡所谓六经之所载，七十二子之所问者，学之终身，有不能达者矣；于其所达，行之终身，有不能至者矣。以予之汲汲于此而不暇乎其他，因以知七十二子亦以是汲汲而不暇也，又以知圣人所以教人垂世，亦皇皇而不暇也。今之学者于古圣贤所皇皇汲汲者，学之行之，或未至其一二，而好为性说，以穷圣贤之所罕言而不究者，执后儒之偏说，事无用之空言，此予之所不暇也。 或有问曰：性果不足学乎？予曰：性者，与身俱生而人之所皆有也。为君子者，修身治人而已，性之善恶不必究也。使性果善邪，身不可以不修，人不可以不治；使性果恶邪，身不可以不修，人不可以不治。不修其身，虽君子而为小人，《书》曰“惟圣罔念作狂”是也；能修其身，虽小人而为君子，《书》曰“惟狂克念作圣”是也。治道备，人斯为善矣，《书》曰“黎民于变时雍”是也；治道失，人斯为恶矣，《书》曰“殷顽民”，又曰“旧染污俗”是也。故为君子者，以修身治人为急，而不穷性以为言。夫七十二子之不问，六经之不主言，或虽言而不究，岂略之哉，盖有意也。 或又问曰：然则三子言性，过欤？曰：不过也。其不同何也？曰：始异而终同也。使孟子曰人性善矣，遂怠而不教，则是过也；使荀子曰人性恶矣，遂弃而不教，则是过也；使扬子曰人性混矣，遂肆而不教，则是过也。然三子者，或身奔走诸侯以行其道，或著书累千万言以告于后世，未尝不区区以仁义礼乐为急。盖其意以谓善者一日不教，则失而入于恶；恶者勤而教之，则可使至于善；混者驱而率之，则可使去恶而就善也。其说与《书》之“习与性成”，《语》之“性近习远”，《中庸》之“有以率之”，《乐记》之“慎物所感”皆合。夫三子者，推其言则殊，察其用心则一，故予以为推其言不过始异而终同也。凡论三子者，以予言而一之，则譊譊者可以息矣。 予之所说如此，吾子其择焉。 醉翁亭记环滁皆山也。其西南诸峰，林壑尤美，望之蔚然而深秀者，琅琊也。山行六七里，渐闻水声潺潺，而泻出于两峰之间者，酿泉也。峰回路转，有亭翼然临于泉上者，醉翁亭也。作亭者谁？山之僧智仙也。名之者谁？太守自谓也。太守与客来饮于此，饮少辄醉，而年又最高，故自号曰醉翁也。醉翁之意不在酒，在乎山水之间也。山水之乐，得之心而寓之酒也。 若夫日出而林霏开，云归而岩穴暝，晦明变化者，山间之朝暮也。野芳发而幽香，佳木秀而繁阴，风霜高洁，水落而石出者，山间之四时也。朝而往，暮而归，四时之景不同，而乐亦无穷也。 至于负者歌于途，行者休于树，前者呼，后者应，伛偻提携，往来而不绝者，滁人游也。临溪而渔，溪深而鱼肥。酿泉为酒，泉香而酒洌；山肴野蔌，杂然而前陈者，太守宴也。宴酣之乐，非丝非竹，射者中，弈者胜，觥筹交错，起坐而喧哗者，众宾欢也。苍颜白发，颓然乎其间者，太守醉也。 已而夕阳在山，人影散乱，太守归而宾客从也。树林阴翳，鸣声上下，游人去而禽鸟乐也。然而禽鸟知山林之乐，而不知人之乐；人知从太守游而乐，而不知太守之乐其乐也。醉能同其乐，醒能述以文者，太守也。太守谓谁？庐陵欧阳修也。 朝中措 · 送刘仲原甫出守维扬平山阑槛倚晴空，山色有无中。手种堂前垂柳，别来几度春风？ 文章太守，挥毫万字，一饮千钟。行乐直须年少，尊前看取衰翁。 夜行船 · 忆昔西都欢纵忆昔西都欢纵。自别后、有谁能共。伊川山水洛川花，细寻思、旧游如梦。 今日相逢情愈重。愁闻唱、画楼钟动。白发天涯逢此景，倒金尊、殢谁相送。 伶官传序呜呼！盛衰之理，虽曰天命，岂非人事哉！原庄宗之所以得天下，与其所以失之者，可以知之矣。 世言晋王之将终也，以三矢赐庄宗而告之曰：“梁，吾仇也；燕王，吾所立，契丹，与吾约为兄弟，而皆背晋以归梁。此三者，吾遗恨也。与尔三矢，尔其无忘乃父之志！”庄宗受而藏之于庙。其后用兵，则遣从事以一少牢告庙，请其矢，盛以锦囊，负而前驱，及凯旋而纳之。 方其系燕父子以组，函梁君臣之首，入于太庙，还矢先王，而告以成功，其意气之盛，可谓壮哉！及仇雠已灭，天下已定，一夫夜呼，乱者四应，仓皇东出，未及见贼而士卒离散，君臣相顾，不知所归。至于誓天断发，泣下沾襟，何其衰也！岂得之难而失之易欤？抑本其成败之迹，而皆自于人欤？ 《书》曰：“满招损，谦得益。”忧劳可以兴国，逸豫可以亡身，自然之理也。故方其盛也，举天下之豪杰莫能与之争；及其衰也，数十伶人困之，而身死国灭，为天下笑。夫祸患常积于忽微，而智勇多困于所溺，岂独伶人也哉！作《伶官传》。","categories":[],"tags":[{"name":"Poetry and Prose","slug":"Poetry-and-Prose","permalink":"http://example.com/tags/Poetry-and-Prose/"}]},{"title":"KafkaConsumer 源码之 consumer 如何加入 consumer group","slug":"kafka-consumer-group","date":"2020-11-04T01:35:04.000Z","updated":"2021-01-05T07:32:45.777Z","comments":true,"path":"2020/11/04/kafka-consumer-group/","link":"","permalink":"http://example.com/2020/11/04/kafka-consumer-group/","excerpt":"","text":"Kafka 的 consumer 比 producer 要复杂许多，producer 没有 group 的概念，也不需要关注 offset，而 consumer 不一样，它有组织 (consumer group)，有纪律 (offset)。这些对 consumer 的要求就会很高，这篇文章就主要介绍 consumer 是如何加入 consumer group 的。 在这之前，我们需要先了解一下什么是 GroupCoordinator。简单地说，GroupCoordinator 是运行在服务器上的一个服务，Kafka 集群上的每一个 broker 节点启动的时候，都会启动一个 GroupCoordinator 服务，其功能是负责进行 consumer 的 group 成员与 offset 管理 (但每个 GroupCoordinator 只是管理一部分的 consumer group member 和 offset 信息)。 consumer group 对应的 GroupCoordinator 节点的确定，会通过如下方式： 将 consumer group 的 group.id 进行 hash，把得到的值的绝对值，对 _consumer_offsets 的 partition 总数取余，然后得到其对应的 partition 值，该 partition 的 leader 所在的 broker 即为该 consumer group 所对应的 GroupCoordinator 节点，GroupCoordinator 会存储与该 consumer group 相关的所有的 Meta 信息。 1._consumer_offsets 这个 topic 是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有 50 个 partition，每个 partition 默认有三个副本。 2.partition 计算方式：abs(GroupId.hashCode()) % NumPartitions，其中，NumPartitions 是 _consumer_offsets 的 partition 数，默认是 50 个。 3.比如，现在通过计算 abs(GroupId.hashCode()) % NumPartitions 的值为 35，然后就找第 35 个 partition 的 leader 在哪个 broker 上 (假设在 192.168.1.12)，那么 GroupCoordinator 节点就在这个 broker 上。 同时，这个 consumer group 所提交的消费 offset 信息也会发送给这个 partition 的 leader 所对应的 broker 节点，因此，这个节点不仅是 GroupCoordinator，而且还保存分区分配方案和组内消费者 offset 信息。 更多关于 GroupCoordinator 的解析，参考：Kafka 源码解析之 GroupCoordinator 详解。 KafkaConsumer 消费消息的主体流程接下来，我们回顾下 KafkaConsumer 消费消息的主体流程： 12345678// 创建消费者KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props);// 订阅主题kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;))// 从服务器拉取数据ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(100)); 创建 KafkaConsumer创建 KafkaConsumer 的时候，会创建一个 ConsumerCoordinator 服务，由它来负责和 GroupCoordinator 通信： 1234567891011121314151617181920// no coordinator will be constructed for the default (null) group idthis.coordinator = groupId == null ? null : new ConsumerCoordinator(logContext, this.client, groupId, this.groupInstanceId, maxPollIntervalMs, sessionTimeoutMs, new Heartbeat(time, sessionTimeoutMs, heartbeatIntervalMs, maxPollIntervalMs, retryBackoffMs), assignors, this.metadata, this.subscriptions, metrics, metricGrpPrefix, this.time, retryBackoffMs, enableAutoCommit, config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG), this.interceptors, config.getBoolean(ConsumerConfig.LEAVE_GROUP_ON_CLOSE_CONFIG)); 订阅 topicKafkaConsumer 订阅 topic 的方式有好几种，这在前面的文章有提到过。订阅的时候，会根据订阅的方式，设置其对应的订阅类型，默认存在四种订阅类型： 12345678910private enum SubscriptionType &#123; // 默认 NONE, // subscribe方式订阅 AUTO_TOPICS, // subscribe方式订阅 AUTO_PATTERN, // assign方式订阅 USER_ASSIGNED&#125; 比如，采用 kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;)) 方式订阅 topic 时，会将订阅类型设置为 SubscriptionType.AUTO_TOPICS，其核心代码如下： 12345678910111213141516171819202122232425/** * Subscribe to the given list of topics to get dynamically assigned partitions. * &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; It is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * This is a short-hand for &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;, which * uses a no-op listener. If you need the ability to seek to particular offsets, you should prefer * &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;, since group rebalances will cause partition offsets * to be reset. You should also provide your own listener if you are doing your own offset * management since the listener gives you an opportunity to commit offsets before a rebalance finishes. * * @param topics The list of topics to subscribe to * @throws IllegalArgumentException If topics is null or contains null or empty elements * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics) &#123; subscribe(topics, new NoOpConsumerRebalanceListener());&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Subscribe to the given list of topics to get dynamically * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * As part of group management, the consumer will keep track of the list of consumers that belong to a particular * group and will trigger a rebalance operation if any one of the following events are triggered: * &lt;ul&gt; * &lt;li&gt;Number of partitions change for any of the subscribed topics * &lt;li&gt;A subscribed topic is created or deleted * &lt;li&gt;An existing member of the consumer group is shutdown or fails * &lt;li&gt;A new member is added to the consumer group * &lt;/ul&gt; * &lt;p&gt; * When any of these events are triggered, the provided listener will be invoked first to indicate that * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received. * Note that rebalances will only occur during an active call to &#123;@link #poll(Duration)&#125;, so callbacks will * also only be invoked during that time. * * The provided listener will immediately override any listener set in a previous call to subscribe. * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics * subscribed in this call. See &#123;@link ConsumerRebalanceListener&#125; for more details. * * @param topics The list of topics to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; acquireAndEnsureOpen(); try &#123; maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot be null&quot;); if (topics.isEmpty()) &#123; // treat subscribing to empty topic list as the same as unsubscribing this.unsubscribe(); &#125; else &#123; for (String topic : topics) &#123; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;); &#125; throwIfNoAssignorsConfigured(); fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(&quot;Subscribed to topic(s): &#123;&#125;&quot;, Utils.join(topics, &quot;, &quot;)); if (this.subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; 12345public synchronized boolean subscribe(Set&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_TOPICS); return changeSubscription(topics);&#125; 123456789101112/** * This method sets the subscription type if it is not already set (i.e. when it is NONE), * or verifies that the subscription type is equal to the give type when it is set (i.e. * when it is not NONE) * @param type The given subscription type */private void setSubscriptionType(SubscriptionType type) &#123; if (this.subscriptionType == SubscriptionType.NONE) this.subscriptionType = type; else if (this.subscriptionType != type) throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);&#125; 从服务器拉取数据订阅完成后，就可以从服务器拉取数据了，应该注意的是，KafkaConsumer 没有后台线程默默的拉取数据，它的所有行为都集中在 poll () 方法中，KafkaConsumer 是线程不安全的，同时只能允许一个线程运行。 kafkaConsumer.poll () 方法的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445private ConsumerRecords&lt;K, V&gt; poll(final Timer timer, final boolean includeMetadataInTimeout) &#123; // Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭 acquireAndEnsureOpen(); try &#123; if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123; throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;); &#125; // poll for new data until the timeout expires do &#123; client.maybeTriggerWakeup(); if (includeMetadataInTimeout) &#123; // Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息 if (!updateAssignmentMetadataIfNeeded(timer)) &#123; return ConsumerRecords.empty(); &#125; &#125; else &#123; while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123; log.warn(&quot;Still waiting for metadata&quot;); &#125; &#125; // Step3:拉取数据 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer); if (!records.isEmpty()) &#123; // before returning the fetched records, we can send off the next round of fetches // and avoid block waiting for their responses to enable pipelining while the user // is handling the fetched records. // // NOTE: since the consumed position has already been updated, we must not allow // wakeups or any other errors to be triggered prior to returning the fetched records. if (fetcher.sendFetches() &gt; 0 || client.hasPendingRequests()) &#123; client.pollNoWakeup(); &#125; return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records)); &#125; &#125; while (timer.notExpired()); return ConsumerRecords.empty(); &#125; finally &#123; release(); &#125;&#125; 可以看出，在 Step 1 阶段， poll () 方法会先进行判定，如果有多个线程同时使用一个 KafkaConsumer 则会抛出异常： 1234567891011/** * Acquire the light lock and ensure that the consumer hasn&#x27;t been closed. * @throws IllegalStateException If the consumer has been closed */private void acquireAndEnsureOpen() &#123; acquire(); if (this.closed) &#123; release(); throw new IllegalStateException(&quot;This consumer has already been closed.&quot;); &#125;&#125; 123456789101112/** * Acquire the light lock protecting this consumer from multi-threaded access. Instead of blocking * when the lock is not available, however, we just throw an exception (since multi-threaded usage is not * supported). * @throws ConcurrentModificationException if another thread already has the lock */private void acquire() &#123; long threadId = Thread.currentThread().getId(); if (threadId != currentThread.get() &amp;&amp; !currentThread.compareAndSet(NO_CURRENT_THREAD, threadId)) throw new ConcurrentModificationException(&quot;KafkaConsumer is not safe for multi-threaded access&quot;); refcount.incrementAndGet();&#125; KafkaConsumer 如何加入 consumer group一个 KafkaConsumer 实例消费数据的前提是能够加入一个 consumer group 成功，并获取其要订阅的 tp（topic-partition）列表，因此首先要做的就是和 GroupCoordinator 建立连接，加入组织。 consumer 加入 group 的过程，也就是 reblance 的过程。如果出现了频繁 reblance 的问题，可能和 max.poll.interval.ms 和 max.poll.records 两个参数有关。 因此，我们先把目光集中在 ConsumerCoordinator 上，这个过程主要发生在 Step 2 阶段： 123456789101112/** * Visible for testing */boolean updateAssignmentMetadataIfNeeded(final Timer timer) &#123; // 1.本篇文章的内容主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group) if (coordinator != null &amp;&amp; !coordinator.poll(timer)) &#123; return false; &#125; // 2.updateFetchPositions(timer)方法留待下一篇文章分析(主要功能是:consumer获得partition的offset) return updateFetchPositions(timer);&#125; 关于对 ConsumerCoordinator 的处理都集中在 coordinator.poll () 方法中。其主要逻辑如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * Poll for coordinator events. This ensures that the coordinator is known and that the consumer * has joined the group (if it is using group management). This also handles periodic offset commits * if they are enabled. * (确保group的coordinator是已知的，并且这个consumer是已经加入到了group中，也用于offset周期性的commit) * &lt;p&gt; * Returns early if the timeout expires * * @param timer Timer bounding how long this method can block * @return true iff the operation succeeded */public boolean poll(Timer timer) &#123; maybeUpdateSubscriptionMetadata(); invokeCompletedOffsetCommitCallbacks(); // 如果是subscribe方式订阅的topic if (subscriptions.partitionsAutoAssigned()) &#123; // Always update the heartbeat last poll time so that the heartbeat thread does not leave the // group proactively due to application inactivity even if (say) the coordinator cannot be found. // 1.检查心跳线程运行是否正常，如果心跳线程失败则抛出异常，反之则更新poll调用的时间 pollHeartbeat(timer.currentTimeMs()); // 2.如果coordinator未知，则初始化ConsumeCoordinator if (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) &#123; return false; &#125; // 判断是否需要重新加入group，如果订阅的partition变化或者分配的partition变化，都可能需要重新加入group if (rejoinNeededOrPending()) &#123; // due to a race condition between the initial metadata fetch and the initial rebalance, // we need to ensure that the metadata is fresh before joining initially. This ensures // that we have matched the pattern against the cluster&#x27;s topics at least once before joining. if (subscriptions.hasPatternSubscription()) &#123; // For consumer group that uses pattern-based subscription, after a topic is created, // any consumer that discovers the topic after metadata refresh can trigger rebalance // across the entire consumer group. Multiple rebalances can be triggered after one topic // creation if consumers refresh metadata at vastly different times. We can significantly // reduce the number of rebalances caused by single topic creation by asking consumer to // refresh metadata before re-joining the group as long as the refresh backoff time has // passed. if (this.metadata.timeToAllowUpdate(timer.currentTimeMs()) == 0) &#123; this.metadata.requestUpdate(); &#125; if (!client.ensureFreshMetadata(timer)) &#123; return false; &#125; maybeUpdateSubscriptionMetadata(); &#125; // 3.确保group是active的，重新加入group，分配订阅的partition if (!ensureActiveGroup(timer)) &#123; return false; &#125; &#125; &#125; else &#123; // For manually assigned partitions, if there are no ready nodes, await metadata. // If connections to all nodes fail, wakeups triggered while attempting to send fetch // requests result in polls returning immediately, causing a tight loop of polls. Without // the wakeup, poll() with no channels would block for the timeout, delaying re-connection. // awaitMetadataUpdate() initiates new connections with configured backoff and avoids the busy loop. // When group management is used, metadata wait is already performed for this scenario as // coordinator is unknown, hence this check is not required. if (metadata.updateRequested() &amp;&amp; !client.hasReadyNodes(timer.currentTimeMs())) &#123; client.awaitMetadataUpdate(timer); &#125; &#125; // 4.如果设置的是自动commit,如果定时达到则自动commit maybeAutoCommitOffsetsAsync(timer.currentTimeMs()); return true;&#125; coordinator.poll () 方法中，具体实现可以分为四个步骤： pollHeartbeat ()：检测心跳线程运行是否正常，需要定时向 GroupCoordinator 发送心跳，如果超时未发送心跳，consumer 会离开 consumer group。 ensureCoordinatorReady ()：当通过 subscribe () 方法订阅 topic 时，如果 coordinator 未知，则初始化 ConsumerCoordinator (在 ensureCoordinatorReady () 中实现，该方法主要的作用是发送 FindCoordinatorRequest 请求，并建立连接)。 ensureActiveGroup ()：判断是否需要重新加入 group，如果订阅的 partition 变化或者分配的 partition 变化时，需要 rejoin，则通过 ensureActiveGroup () 发送 join-group、sync-group 请求，加入 group 并获取其 assign 的 TopicPartition list。 maybeAutoCommitOffsetsAsync ()：如果设置的是自动 commit，并且达到了发送时限则自动 commit offset。 关于 rejoin，下列几种情况会触发再均衡 (reblance) 操作： 订阅的 topic 列表变化 topic 被创建或删除 新的消费者加入消费者组 (第一次进行消费也属于这种情况) 消费者宕机下线 (长时间未发送心跳包) 消费者主动退出消费组，比如调用 unsubscrible () 方法取消对主题的订阅 消费者组对应的 GroupCoorinator 节点发生了变化 消费者组内所订阅的任一主题或者主题的分区数量发生了变化 取消 topic 订阅，consumer 心跳线程超时以及在 Server 端给定的时间内未收到心跳请求，这三个都是触发的 LEAVE_GROUP 请求。 下面重点介绍下第二步中的 ensureCoordinatorReady () 方法和第三步中的 ensureActiveGroup () 方法。 ensureCoordinatorReadyensureCoordinatorReady ()这个方法主要作用：选择一个连接数最少的 broker (还未响应请求最少的 broker)，发送 FindCoordinator 请求，找到 GroupCoordinator 后，建立对应的 TCP 连接。 方法调用流程是 ensureCoordinatorReady () → lookupCoordinator () → sendFindCoordinatorRequest ()。 如果 client 收到 server response，那么就与 GroupCoordinator 建立连接。 1234567891011121314151617181920212223242526272829303132333435363738/** * Visible for testing. * * Ensure that the coordinator is ready to receive requests. * * @param timer Timer bounding how long this method can block * @return true If coordinator discovery and initial connection succeeded, false otherwise */protected synchronized boolean ensureCoordinatorReady(final Timer timer) &#123; if (!coordinatorUnknown()) return true; do &#123; // 找到GroupCoordinator，并建立连接 final RequestFuture&lt;Void&gt; future = lookupCoordinator(); client.poll(future, timer); if (!future.isDone()) &#123; // ran out of time break; &#125; if (future.failed()) &#123; if (future.isRetriable()) &#123; log.debug(&quot;Coordinator discovery failed, refreshing metadata&quot;); client.awaitMetadataUpdate(timer); &#125; else throw future.exception(); &#125; else if (coordinator != null &amp;&amp; client.isUnavailable(coordinator)) &#123; // we found the coordinator, but the connection has failed, so mark // it dead and backoff before retrying discovery markCoordinatorUnknown(); timer.sleep(retryBackoffMs); &#125; &#125; while (coordinatorUnknown() &amp;&amp; timer.notExpired()); return !coordinatorUnknown();&#125; 12345678910111213protected synchronized RequestFuture&lt;Void&gt; lookupCoordinator() &#123; if (findCoordinatorFuture == null) &#123; // find a node to ask about the coordinator(找一个最少连接的broker，此处对应的应该就是文章开头处确定GroupCoordinator节点的发发) Node node = this.client.leastLoadedNode(); if (node == null) &#123; log.debug(&quot;No broker available to send FindCoordinator request&quot;); return RequestFuture.noBrokersAvailable(); &#125; else // 对找到的broker发送FindCoordinator请求，并对response进行处理 findCoordinatorFuture = sendFindCoordinatorRequest(node); &#125; return findCoordinatorFuture;&#125; 12345678910111213141516171819/** * Discover the current coordinator for the group. Sends a GroupMetadata request to * one of the brokers. The returned future should be polled to get the result of the request. * @return A request future which indicates the completion of the metadata request */private RequestFuture&lt;Void&gt; sendFindCoordinatorRequest(Node node) &#123; // initiate the group metadata request log.debug(&quot;Sending FindCoordinator request to broker &#123;&#125;&quot;, node); FindCoordinatorRequest.Builder requestBuilder = new FindCoordinatorRequest.Builder( new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.groupId)); // 发送请求，并将response转换为RequestFuture // compose的作用是将FindCoordinatorResponseHandler类转换为RequestFuture // 实际上就是为返回的Future类重置onSuccess()和onFailure()方法 return client.send(node, requestBuilder) .compose(new FindCoordinatorResponseHandler());&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142// 根据response返回的ip以及端口信息，和该broke上开启的GroupCoordinator建立连接private class FindCoordinatorResponseHandler extends RequestFutureAdapter&lt;ClientResponse, Void&gt; &#123; @Override public void onSuccess(ClientResponse resp, RequestFuture&lt;Void&gt; future) &#123; log.debug(&quot;Received FindCoordinator response &#123;&#125;&quot;, resp); clearFindCoordinatorFuture(); FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) resp.responseBody(); Errors error = findCoordinatorResponse.error(); if (error == Errors.NONE) &#123; // 如果正确获取broker上的GroupCoordinator，建立连接，并更新心跳时间 synchronized (AbstractCoordinator.this) &#123; // use MAX_VALUE - node.id as the coordinator id to allow separate connections // for the coordinator in the underlying network client layer int coordinatorConnectionId = Integer.MAX_VALUE - findCoordinatorResponse.data().nodeId(); AbstractCoordinator.this.coordinator = new Node( coordinatorConnectionId, findCoordinatorResponse.data().host(), findCoordinatorResponse.data().port()); log.info(&quot;Discovered group coordinator &#123;&#125;&quot;, coordinator); // 初始化tcp连接 client.tryConnect(coordinator); // 更新心跳时间 heartbeat.resetSessionTimeout(); &#125; future.complete(null); &#125; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else &#123; log.debug(&quot;Group coordinator lookup failed: &#123;&#125;&quot;, findCoordinatorResponse.data().errorMessage()); future.raise(error); &#125; &#125; @Override public void onFailure(RuntimeException e, RequestFuture&lt;Void&gt; future) &#123; clearFindCoordinatorFuture(); super.onFailure(e, future); &#125;&#125; 上面代码主要作用就是：往一个负载最小的 broker 节点发起 FindCoordinator 请求，Kafka 在走到这个请求后会根据 group_id 查找对应的 GroupCoordinator 节点 (文章开头处介绍的方法)，如果找到对应的 GroupCoordinator 则会返回其对应的 node_id，host 和 port 信息，并建立连接。 这里的 GroupCoordinator 节点的确定在文章开头提到过，是通过 group.id 和 partitionCount 来确定的。 ensureActiveGroup现在已经知道了 GroupCoordinator 节点，并建立了连接。ensureActiveGroup () 这个方法的主要作用：向 GroupCoordinator 发送 join-group、sync-group 请求，获取 assign 的 tp list。 调用过程是 ensureActiveGroup () → ensureCoordinatorReady () → startHeartbeatThreadIfNeeded () → joinGroupIfNeeded ()。 joinGroupIfNeeded () 方法中最重要的方法是 initiateJoinGroup ()，它的的调用流程是 disableHeartbeatThread () → sendJoinGroupRequest () → JoinGroupResponseHandler::handle () → onJoinLeader ()，onJoinFollower () → sendSyncGroupRequest ()。 12345678910111213141516171819/** * Ensure the group is active (i.e., joined and synced) * * @param timer Timer bounding how long this method can block * @return true iff the group is active */boolean ensureActiveGroup(final Timer timer) &#123; // always ensure that the coordinator is ready because we may have been disconnected // when sending heartbeats and does not necessarily require us to rejoin the group. // 1.确保GroupCoordinator已经连接 if (!ensureCoordinatorReady(timer)) &#123; return false; &#125; // 2.启动心跳线程，但是并不一定发送心跳，满足条件后才会发送心跳 startHeartbeatThreadIfNeeded(); // 3.发送joinGroup请求，并对返回的信息进行处理，核心步骤 return joinGroupIfNeeded(timer);&#125; 心跳线程就是在这里启动的，但是并不一定马上发送心跳包，会在满足条件之后才会开始发送。后面最主要的逻辑就集中在 joinGroupIfNeeded () 方法，它的核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Joins the group without starting the heartbeat thread. * * Visible for testing. * * @param timer Timer bounding how long this method can block * @return true iff the operation succeeded */boolean joinGroupIfNeeded(final Timer timer) &#123; while (rejoinNeededOrPending()) &#123; if (!ensureCoordinatorReady(timer)) &#123; return false; &#125; // call onJoinPrepare if needed. We set a flag to make sure that we do not call it a second // time if the client is woken up before a pending rebalance completes. This must be called // on each iteration of the loop because an event requiring a rebalance (such as a metadata // refresh which changes the matched subscription set) can occur while another rebalance is // still in progress. // 触发onJoinPrepare，包括offset commit和rebalance listener if (needsJoinPrepare) &#123; // 如果是自动提交，则要开始提交offset以及在join group之前回调reblance listener接口 onJoinPrepare(generation.generationId, generation.memberId); needsJoinPrepare = false; &#125; // 初始化joinGroup请求，并发送joinGroup请求，核心步骤 final RequestFuture&lt;ByteBuffer&gt; future = initiateJoinGroup(); client.poll(future, timer); if (!future.isDone()) &#123; // we ran out of time return false; &#125; // join succeed，这一步时，时间上sync-group已经成功了 if (future.succeeded()) &#123; // Duplicate the buffer in case `onJoinComplete` does not complete and needs to be retried. ByteBuffer memberAssignment = future.value().duplicate(); // 发送完成，consumer加入group成功，触发onJoinComplete()方法 onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment); // We reset the join group future only after the completion callback returns. This ensures // that if the callback is woken up, we will retry it on the next joinGroupIfNeeded. // 重置joinFuture为空 resetJoinGroupFuture(); needsJoinPrepare = true; &#125; else &#123; resetJoinGroupFuture(); final RuntimeException exception = future.exception(); if (exception instanceof UnknownMemberIdException || exception instanceof RebalanceInProgressException || exception instanceof IllegalGenerationException || exception instanceof MemberIdRequiredException) continue; else if (!future.isRetriable()) throw exception; timer.sleep(retryBackoffMs); &#125; &#125; return true;&#125; initiateJoinGroup () 方法的核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private synchronized RequestFuture&lt;ByteBuffer&gt; initiateJoinGroup() &#123; // we store the join future in case we are woken up by the user after beginning the // rebalance in the call to poll below. This ensures that we do not mistakenly attempt // to rejoin before the pending rebalance has completed. if (joinFuture == null) &#123; // fence off the heartbeat thread explicitly so that it cannot interfere with the join group. // Note that this must come after the call to onJoinPrepare since we must be able to continue // sending heartbeats if that callback takes some time. // Step1:rebalance期间，心跳线程停止运行 disableHeartbeatThread(); // 设置当前状态为rebalance state = MemberState.REBALANCING; // Step2:发送joinGroup请求，核心步骤 joinFuture = sendJoinGroupRequest(); // Step3:为joinGroup请求添加监听器，监听joinGroup请求的结果并做相应的处理 joinFuture.addListener(new RequestFutureListener&lt;ByteBuffer&gt;() &#123; @Override public void onSuccess(ByteBuffer value) &#123; // handle join completion in the callback so that the callback will be invoked // even if the consumer is woken up before finishing the rebalance synchronized (AbstractCoordinator.this) &#123; log.info(&quot;Successfully joined group with generation &#123;&#125;&quot;, generation.generationId); // 如果joinGroup成功，设置状态为stable state = MemberState.STABLE; rejoinNeeded = false; if (heartbeatThread != null) // Step4:允许心跳线程继续运行 heartbeatThread.enable(); &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; // we handle failures below after the request finishes. if the join completes // after having been woken up, the exception is ignored and we will rejoin synchronized (AbstractCoordinator.this) &#123; // 如果joinGroup失败，设置状态为unjoined state = MemberState.UNJOINED; &#125; &#125; &#125;); &#125; return joinFuture;&#125; 可以看到在 joinGroup 之前会让心跳线程暂时停下来，此时会将 ConsumerCoordinator 的状态设置为 rebalance 状态，当 joinGroup 成功之后会将状态设置为 stable 状态，同时让之前停下来的心跳线程继续运行。 sendJoinGroupRequest () 方法的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536/** * Join the group and return the assignment for the next generation. This function handles both * JoinGroup and SyncGroup, delegating to &#123;@link #performAssignment(String, String, List)&#125; if * elected leader by the coordinator. * * NOTE: This is visible only for testing * * @return A request future which wraps the assignment returned from the group leader */// 发送joinGroup请求RequestFuture&lt;ByteBuffer&gt; sendJoinGroupRequest() &#123; if (coordinatorUnknown()) return RequestFuture.coordinatorNotAvailable(); // send a join group request to the coordinator log.info(&quot;(Re-)joining group&quot;); JoinGroupRequest.Builder requestBuilder = new JoinGroupRequest.Builder( new JoinGroupRequestData() .setGroupId(groupId) .setSessionTimeoutMs(this.sessionTimeoutMs) .setMemberId(this.generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setProtocolType(protocolType()) .setProtocols(metadata()) .setRebalanceTimeoutMs(this.rebalanceTimeoutMs) ); log.debug(&quot;Sending JoinGroup (&#123;&#125;) to coordinator &#123;&#125;&quot;, requestBuilder, this.coordinator); // Note that we override the request timeout using the rebalance timeout since that is the // maximum time that it may block on the coordinator. We add an extra 5 seconds for small delays. int joinGroupTimeoutMs = Math.max(rebalanceTimeoutMs, rebalanceTimeoutMs + 5000); return client.send(coordinator, requestBuilder, joinGroupTimeoutMs) .compose(new JoinGroupResponseHandler());// Step5:处理joinGroup请求后的response&#125; 在发送 joinGroup 请求之后，会收到来自服务器的响应，然后针对这个响应再做一些重要的事情： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// 处理发送joinGroup请求后的response的handler(同步group信息)private class JoinGroupResponseHandler extends CoordinatorResponseHandler&lt;JoinGroupResponse, ByteBuffer&gt; &#123; @Override public void handle(JoinGroupResponse joinResponse, RequestFuture&lt;ByteBuffer&gt; future) &#123; Errors error = joinResponse.error(); if (error == Errors.NONE) &#123; log.debug(&quot;Received successful JoinGroup response: &#123;&#125;&quot;, joinResponse); sensors.joinLatency.record(response.requestLatencyMs()); synchronized (AbstractCoordinator.this) &#123; if (state != MemberState.REBALANCING) &#123; // if the consumer was woken up before a rebalance completes, we may have already left // the group. In this case, we do not want to continue with the sync group. future.raise(new UnjoinedGroupException()); &#125; else &#123; AbstractCoordinator.this.generation = new Generation(joinResponse.data().generationId(), joinResponse.data().memberId(), joinResponse.data().protocolName()); // Step6:joinGroup成功，下面需要进行sync-group，获取分配的tp列表 if (joinResponse.isLeader()) &#123; // 当前consumer是leader onJoinLeader(joinResponse).chain(future); &#125; else &#123; // 当前consumer是follower onJoinFollower().chain(future); &#125; &#125; &#125; &#125; else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS) &#123; log.debug(&quot;Attempt to join group rejected since coordinator &#123;&#125; is loading the group.&quot;, coordinator()); // backoff and retry future.raise(error); &#125; else if (error == Errors.UNKNOWN_MEMBER_ID) &#123; // reset the member id and retry immediately resetGeneration(); log.debug(&quot;Attempt to join group failed due to unknown member id.&quot;); future.raise(Errors.UNKNOWN_MEMBER_ID); &#125; else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123; // re-discover the coordinator and retry with backoff markCoordinatorUnknown(); log.debug(&quot;Attempt to join group failed due to obsolete coordinator information: &#123;&#125;&quot;, error.message()); future.raise(error); &#125; else if (error == Errors.FENCED_INSTANCE_ID) &#123; log.error(&quot;Received fatal exception: group.instance.id gets fenced&quot;); future.raise(error); &#125; else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL || error == Errors.INVALID_SESSION_TIMEOUT || error == Errors.INVALID_GROUP_ID || error == Errors.GROUP_AUTHORIZATION_FAILED || error == Errors.GROUP_MAX_SIZE_REACHED) &#123; // log the error and re-throw the exception log.error(&quot;Attempt to join group failed due to fatal error: &#123;&#125;&quot;, error.message()); if (error == Errors.GROUP_MAX_SIZE_REACHED) &#123; future.raise(new GroupMaxSizeReachedException(groupId)); &#125; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else &#123; future.raise(error); &#125; &#125; else if (error == Errors.UNSUPPORTED_VERSION) &#123; log.error(&quot;Attempt to join group failed due to unsupported version error. Please unset field group.instance.id and retry&quot; + &quot;to see if the problem resolves&quot;); future.raise(error); &#125; else if (error == Errors.MEMBER_ID_REQUIRED) &#123; // Broker requires a concrete member id to be allowed to join the group. Update member id // and send another join group request in next cycle. synchronized (AbstractCoordinator.this) &#123; AbstractCoordinator.this.generation = new Generation(OffsetCommitRequest.DEFAULT_GENERATION_ID, joinResponse.data().memberId(), null); AbstractCoordinator.this.rejoinNeeded = true; AbstractCoordinator.this.state = MemberState.UNJOINED; &#125; future.raise(Errors.MEMBER_ID_REQUIRED); &#125; else &#123; // unexpected error, throw the exception log.error(&quot;Attempt to join group failed due to unexpected error: &#123;&#125;&quot;, error.message()); future.raise(new KafkaException(&quot;Unexpected error in join group response: &quot; + error.message())); &#125; &#125;&#125; 上面代码的主要过程如下： 如果 group 是新的 group.id，那么此时 group 初始化的状态为 Empty； 当 GroupCoordinator 接收到 consumer 的 join-group 请求后，由于此时这个 group 的 member 列表还是空 (group 是新建的，每个 consumer 实例被称为这个 group 的一个 member)，第一个加入的 member 将被选为 leader，也就是说，对于一个新的 consumer group 而言，当第一个 consumer 实例加入后将会被选为 leader。如果后面 leader 挂了，会从其他 member 里面随机选择一个 member 成为新的 leader； 如果 GroupCoordinator 接收到 leader 发送 join-group 请求，将会触发 rebalance，group 的状态变为 PreparingRebalance； 此时，GroupCoordinator 将会等待一定的时间，如果在一定时间内，接收到 join-group 请求的 consumer 将被认为是依然存活的，此时 group 会变为 AwaitSync 状态，并且 GroupCoordinator 会向这个 group 的所有 member 返回其 response； consumer 在接收到 GroupCoordinator 的 response 后，如果这个 consumer 是 group 的 leader，那么这个 consumer 将会负责为整个 group assign partition 订阅安排（默认是按 range 的策略，目前也可选 roundrobin），然后 leader 将分配后的信息以 sendSyncGroupRequest () 请求的方式发给 GroupCoordinator，而作为 follower 的 consumer 实例会发送一个空列表； GroupCoordinator 在接收到 leader 发来的请求后，会将 assign 的结果返回给所有已经发送 sync-group 请求的 consumer 实例，并且 group 的状态将会转变为 Stable，如果后续再收到 sync-group 请求，由于 group 的状态已经是 Stable，将会直接返回其分配结果。 sync-group 发送请求核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 当consumer为follower时，从GroupCoordinator拉取分配结果private RequestFuture&lt;ByteBuffer&gt; onJoinFollower() &#123; // send follower&#x27;s sync group with an empty assignment SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder( new SyncGroupRequestData() .setGroupId(groupId) .setMemberId(generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setGenerationId(generation.generationId) .setAssignments(Collections.emptyList()) ); log.debug(&quot;Sending follower SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;, this.coordinator, requestBuilder); // 发送sync-group请求 return sendSyncGroupRequest(requestBuilder);&#125;// 当consumer客户端为leader时，对group下的所有实例进行分配，将assign的结果发送到GroupCoordinatorprivate RequestFuture&lt;ByteBuffer&gt; onJoinLeader(JoinGroupResponse joinResponse) &#123; try &#123; // perform the leader synchronization and send back the assignment for the group(assign 操作) Map&lt;String, ByteBuffer&gt; groupAssignment = performAssignment(joinResponse.data().leader(), joinResponse.data().protocolName(), joinResponse.data().members()); List&lt;SyncGroupRequestData.SyncGroupRequestAssignment&gt; groupAssignmentList = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, ByteBuffer&gt; assignment : groupAssignment.entrySet()) &#123; groupAssignmentList.add(new SyncGroupRequestData.SyncGroupRequestAssignment() .setMemberId(assignment.getKey()) .setAssignment(Utils.toArray(assignment.getValue())) ); &#125; SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder( new SyncGroupRequestData() .setGroupId(groupId) .setMemberId(generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setGenerationId(generation.generationId) .setAssignments(groupAssignmentList) ); log.debug(&quot;Sending leader SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;, this.coordinator, requestBuilder); // 发送sync-group请求 return sendSyncGroupRequest(requestBuilder); &#125; catch (RuntimeException e) &#123; return RequestFuture.failure(e); &#125;&#125;// 发送SyncGroup请求，获取对partition分配的安排private RequestFuture&lt;ByteBuffer&gt; sendSyncGroupRequest(SyncGroupRequest.Builder requestBuilder) &#123; if (coordinatorUnknown()) return RequestFuture.coordinatorNotAvailable(); return client.send(coordinator, requestBuilder) .compose(new SyncGroupResponseHandler());&#125;private class SyncGroupResponseHandler extends CoordinatorResponseHandler&lt;SyncGroupResponse, ByteBuffer&gt; &#123; @Override public void handle(SyncGroupResponse syncResponse, RequestFuture&lt;ByteBuffer&gt; future) &#123; Errors error = syncResponse.error(); if (error == Errors.NONE) &#123; // sync-group成功 sensors.syncLatency.record(response.requestLatencyMs()); future.complete(ByteBuffer.wrap(syncResponse.data.assignment())); &#125; else &#123; // join的标志位设置为true requestRejoin(); if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else if (error == Errors.REBALANCE_IN_PROGRESS) &#123; // group正在进行rebalance，任务失败 log.debug(&quot;SyncGroup failed because the group began another rebalance&quot;); future.raise(error); &#125; else if (error == Errors.FENCED_INSTANCE_ID) &#123; log.error(&quot;Received fatal exception: group.instance.id gets fenced&quot;); future.raise(error); &#125; else if (error == Errors.UNKNOWN_MEMBER_ID || error == Errors.ILLEGAL_GENERATION) &#123; log.debug(&quot;SyncGroup failed: &#123;&#125;&quot;, error.message()); resetGeneration(); future.raise(error); &#125; else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123; log.debug(&quot;SyncGroup failed: &#123;&#125;&quot;, error.message()); markCoordinatorUnknown(); future.raise(error); &#125; else &#123; future.raise(new KafkaException(&quot;Unexpected error from SyncGroup: &quot; + error.message())); &#125; &#125; &#125;&#125; 这个阶段主要是将分区分配方案同步给各个消费者，这个同步仍然是通过 GroupCoordinator 来转发的。 分区策略并非由 leader 消费者来决定，而是各个消费者投票决定的，谁的票多就采用什么分区策略。这里的分区策略是通过 partition.assignment.strategy 参数设置的，可以设置多个。如果选举出了消费者不支持的策略，那么就会抛出异常 IllegalArgumentException: Member does not support protocol。 经过上面的步骤，一个 consumer 实例就已经加入 group 成功了，加入 group 成功后，将会触发 ConsumerCoordinator 的 onJoinComplete () 方法，其作用就是：更新订阅的 tp 列表、更新其对应的 metadata 及触发注册的 listener。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 加入group成功@Overrideprotected void onJoinComplete(int generation, String memberId, String assignmentStrategy, ByteBuffer assignmentBuffer) &#123; // only the leader is responsible for monitoring for metadata changes (i.e. partition changes) if (!isLeader) assignmentSnapshot = null; PartitionAssignor assignor = lookupAssignor(assignmentStrategy); if (assignor == null) throw new IllegalStateException(&quot;Coordinator selected invalid assignment protocol: &quot; + assignmentStrategy); Assignment assignment = ConsumerProtocol.deserializeAssignment(assignmentBuffer); if (!subscriptions.assignFromSubscribed(assignment.partitions())) &#123; handleAssignmentMismatch(assignment); return; &#125; Set&lt;TopicPartition&gt; assignedPartitions = subscriptions.assignedPartitions(); // The leader may have assigned partitions which match our subscription pattern, but which // were not explicitly requested, so we update the joined subscription here. maybeUpdateJoinedSubscription(assignedPartitions); // give the assignor a chance to update internal state based on the received assignment assignor.onAssignment(assignment, generation); // reschedule the auto commit starting from now if (autoCommitEnabled) this.nextAutoCommitTimer.updateAndReset(autoCommitIntervalMs); // execute the user&#x27;s callback after rebalance ConsumerRebalanceListener listener = subscriptions.rebalanceListener(); log.info(&quot;Setting newly assigned partitions: &#123;&#125;&quot;, Utils.join(assignedPartitions, &quot;, &quot;)); try &#123; listener.onPartitionsAssigned(assignedPartitions); &#125; catch (WakeupException | InterruptException e) &#123; throw e; &#125; catch (Exception e) &#123; log.error(&quot;User provided listener &#123;&#125; failed on partition assignment&quot;, listener.getClass().getName(), e); &#125;&#125; 至此，一个 consumer 实例算是真正上意义上加入 group 成功。 然后 consumer 就进入正常工作状态，同时 consumer 也通过向 GroupCoordinator 发送心跳来维持它们与消费者组的从属关系，以及它们对分区的所有权关系。只要以正常的间隔发送心跳，就被认为是活跃的，但是如果 GroupCoordinator 没有响应，那么就会发送 LeaveGroup 请求退出消费者组。 本文参考http://generalthink.github.io/2019/05/15/how-to-join-kafka-consumer-group/ https://matt33.com/2017/10/22/consumer-join-group/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"Kafka 的核心配置参数","slug":"kafka-properties","date":"2020-10-30T07:48:10.000Z","updated":"2021-01-05T07:33:28.544Z","comments":true,"path":"2020/10/30/kafka-properties/","link":"","permalink":"http://example.com/2020/10/30/kafka-properties/","excerpt":"","text":"Kafka Producer 核心配置参数bootstrap.servers broke 服务器地址，多个服务器，用逗号隔开。 acks 发送应答，默认：1。 acks 参数指定了生产者希望 leader 返回的用于确认请求完成的确认数量，即必须要有多少个分区副本收到该消息，生产者才会认为消息写入是成功的。 允许以下设置： acks=0：生产者将完全不等待来自服务器的任何确认。记录将立即添加到 socket 缓冲区，并被认为已发送。在这种情况下，不能保证服务器已经收到记录，重试配置将不会生效 (因为客户机通常不会知道任何失败)。响应里来自服务端的 offset 总是-1。同时，由于不需要等待响应，所以可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量。 acks=1：只需要集群的 leader 收到消息，生产者就会收到一个来自服务器的成功响应。leader 会将记录写到本地日志中，但不会等待所有 follower 的完全确认。在这种情况下，如果 follower 复制数据之前，leader 挂掉，数据就会丢失。 acks=all / -1：当所有参与复制的节点全部收到消息的时候，生产者才会收到一个来自服务器的成功响应，最安全不过延迟比较高。如果需要保证消息不丢失, 需要使用该设置，同时需要设置 broke端 unclean.leader.election.enable 为 true，保证当 ISR 列表为空时，选择其他存活的副本作为新的 leader。 batch.size 批量发送大小，默认：16384，即 16 K。 当有多个消息需要被发送到同一个 partition 的时候，生产者会把他们放到同一个批次里面 (Deque)，该参数指定了一个批次可以使用的内存大小，按照字节数计算，当批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定会等到批次被填满才发送，半满甚至只包含一个消息的批次也有可能被发送。 生产者产生的消息缓存到本地，每次批量发送 batch.size 大小到服务器。太小的 batch 会降低吞吐，太大则会浪费内存。 linger.ms 发送延迟时间，默认：0。 指定了生产者在发送批次之前等待更多消息加入批次的时间。生产者会在批次填满或 linger.ms 达到上限时把批次发送出去。把 linger.ms 设置成比0大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次，虽然这样会增加延迟，但也会提升吞吐量。 说明：batch.size 和 linger.ms 满足任何一个条件都会发送。 buffer.memory 生产者最大可用缓存，默认：33554432，即 32 M。 生产者可以用来缓冲等待发送到服务器的记录的总内存字节。如果应用程序发送消息的速度超过生产者发送消息到服务器的速度，即超出 max.block.ms，将会抛出异常。 该设置应该大致与生产者将使用的总内存相对应，但不是硬绑定，因为生产者使用的内存并非全部都用于缓冲。一些额外的内存将用于压缩 (如果启用了压缩) 以及维护飞行中的请求。 max.block.ms 阻塞时间，默认：60000，即 1 分钟。 指定了在调用 send () 方法或者 partitionsFor () 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 max.block.ms 时，就会抛出 new TimeoutException(“Failed to allocate memory within the configured max blocking time “ + maxTimeToBlockMs + “ ms.”);。 用户提供的序列化器或分区程序中的阻塞将不计入此超时。 client.id 生产者 ID，默认：空。 请求时传递给服务器的 id 字符串，用来标识消息来源，后台线程会根据它命名。这样做的目的是通过允许在服务器端请求日志中包含逻辑应用程序名称，从而能够跟踪 ip/端口之外的请求源。 compression.type 生产者数据被发送到服务器之前被压缩的压缩类型，默认：none，即不压缩。 指定给定主题的最终压缩类型。此配置接受标准压缩编解码器 (“gzip”、“snappy”、“lz4”、“zstd”)。 “gzip”：压缩效率高，适合高内存、CPU。 “snappy”：适合带宽敏感性，压缩力度大。 retries 失败重试次数，默认：2147483647。 异常是 RetriableException 类型，或者 TransactionManager 允许重试 (transactionManager.canRetry () )。 RetriableException 类型异常如下： retry.backoff.ms 失败请求重试的间隔时间，默认：100。 这避免了在某些失败场景下以紧密循环的方式重复发送请求。 max.in.flight.requests.per.connection 单个连接上发送的未确认请求的最大数量，默认：5。 阻塞前客户端在单个连接上发送的未确认请求的最大数量。即指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。 如果设置为 1，可以保证消息是按照发送的顺序写入服务器的，即便发生了重试。 如果设置大于 1，在 retries 不为0的情况下可能会出现消息发送顺序的错误。例如将两个批发送到同一个分区，第一个批处理失败并重试，但是第二个批处理成功，那么第二个批处理中的记录可能会先出现。 delivery.timeout.ms 传输时间，默认：120000，即 2 分钟。 生产者发送完请求接受服务器 ACK 的时间，该时间允许重试 ，该配置应该大于 request.timeout.ms + linger.ms。 request.timeout.ms 请求超时时间，默认：30000，即30秒。 配置控制客户端等待请求响应的最长时间。 如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽，则该请求将失败。 这应该大于replica.lag.time.max.ms (broker 端配置)，以减少由于不必要的生产者重试引起的消息重复的可能性。 connections.max.idle.ms 连接空闲超时时间，默认：540000，即 9 分钟。 在此配置指定的毫秒数之后关闭空闲连接。 enable.idempotence 开启幂等，默认：false。 如果设置为 true ，将开启 exactly-once 模式，生产者将确保在流中准确地写入每个消息的副本。如果设置为 false，则由于代理失败而导致生产者重试，等等，可能会在流中写入重试消息的副本。 注意，启用幂等需要以下条件 ：max.in.flight.requests.per.connection 小于或等于 5，retries 大于 0， acks 必须为 all 或者 -1。如果用户没有显式地设置这些值，将选择合适的值。如果设置了不兼容的值，就会抛出 ConfigException。 key.serializer key 序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Serializer 。Kafka 提供以下几个默认的 key 序列化器： String：org.apache.kafka.common.serialization.StringSerializer。 value.serializer value 序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Serializer。Kafka 提供以下几个默认的 value 序列化器： byte[]：org.apache.kafka.common.serialization.ByteArraySerializer。 String：org.apache.kafka.common.serialization.StringSerializer。 max.request.size 请求的最大字节大小，默认：1048576，即 1 M。 该参数用于控制生产者发送的请求大小，单次发送的消息大小超过 max.request.size 时，会抛出异常 ，如：org.apache.kafka.common.errors.RecordTooLargeException: The message is 70459102 bytes when serialized which is larger than the maximum request size you have configured with the max.request.size configuration.。 注意：broker 对可接收的消息最大值也有自己的限制 (通过 message.max.bytes 参数设置)，所以两边的配置最好可以匹配，避免生产者发送的消息被 broker 拒绝。 metric.reporters 自定义指标报告器，默认：无。 用作指标报告器的类的列表，需要实现接口：org.apache.kafka.common.metrics.MetricsReporter，该接口允许插入将在创建新度量时得到通知的类。JmxReporter 始终包含在注册 JMX 统计信息中。 interceptor.classes 拦截器，默认：无。 用作拦截器的类的列表，需要实现接口：org.apache.kafka.clients.producer.ProducerInterceptor 。允许将生产者接收到的记录发布到 Kafka 集群之前拦截它们 (可能还会发生突变)。 partitioner.class 分区策略，默认：org.apache.kafka.clients.producer.internals.DefaultPartitioner。 如果自定义分区策略，需要实现接口： org.apache.kafka.clients.producer.Partitioner。 receive.buffer.bytes 默认：32768，即 32 K。 指定了 TCP socket 接收数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 send.buffer.bytes 默认：131072，即 128 K。 指定了 TCP socket 发送数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 transaction.timeout.ms 事务协调器等待生产者更新事务状态的最大毫秒数，默认：60000，即 1 分钟。 如果超过该时间，事务协调器会终止进行中的事务。 如果设置的时间大于 broker 端的 max.transaction.timeout.ms，会抛出 InvalidTransactionTimeout 异常。 transactional.id 用于事务传递的 TransactionalId，默认：空，即不使用事务。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的 TransactionalId 的事务已经完成。如果没有提供 TransactionalId，则生产者被限制为幂等传递。 注意：如果配置了 TransactionalId，则必须启用 enable.idempotence。 Kafka Consumer 核心配置参数bootstrap.servers broke 服务器地址，多个服务器，用逗号隔开。 enable.auto.commit 是否开启自动提交 offset，默认：true。 如果为 true，consumer 的偏移量将在后台定期提交，自动提交频率通过 auto.commit.interval.ms 设置。 auto.commit.interval.ms 自动提交频率，默认：5000。 auto.offset.reset 初始偏移量，默认：latest。 如果 Kafka 中没有初始偏移量，或者服务器上不再存在当前偏移量 (例如，该数据已被删除)，该怎么处理： earliest：自动重置偏移到最早的偏移。 latest：自动将偏移量重置为最新偏移量。 none：如果没有为使用者的组找到以前的偏移量，则向使用者抛出 exception。 anything else：向使用者抛出异常。 client.id 客户端 id，默认：空。 便于跟踪日志。 check.crcs 是否开启数据校验，默认：true。 自动检查消耗的记录的 CRC32。这确保不会发生对消息的在线或磁盘损坏。此检查增加了一些开销，因此在寻求极端性能的情况下可能禁用此检查。 group.id 消费者所属的群组，默认：空。 唯一标识用户群组，每个 partition 只会分配给同一个 group 里面的一个 consumer 来消费。 max.poll.records 拉取的最大记录，默认：500。 单次轮询调用 poll () 方法能返回的记录的最大数量。 max.poll.interval.ms 拉取记录间隔，默认：300000，即 5 分钟。 使用消费者组管理时轮询调用之间的最大延迟。这为使用者在获取更多记录之前空闲的时间设置了上限。如果在此超时过期之前没有调用 poll ()，则认为使用者失败，组将重新平衡，以便将分区重新分配给另一个成员。 request.timeout.ms 请求超时时间，默认：30000 。 配置控制客户机等待请求响应的最长时间。如果在超时之前没有收到响应，客户端将在需要时重新发送请求，或者在重试耗尽时失败请求。 session.timeout.ms consumer session 超时时间，默认：10000。 用于检测 worker 程序失败的超时。worker 定期发送心跳，以向代理表明其活性。如果在此会话超时过期之前代理没有接收到心跳，则代理将从组中删除。 注意：该值必须在 broker 端配置的 group.min.session.timeout 和 group.max.session.timeout.ms 范围之间。 heartbeat.interval.ms 心跳时间，默认：3000。 心跳是在 consumer 与 coordinator 之间进行的。心跳是确定 consumer 存活，加入或者退出 group 的有效手段。 这个值必须设置的小于 session.timeout.ms 的1/3，因为： 当 consumer 由于某种原因不能发 Heartbeat 到 coordinator 时，并且时间超过 session.timeout.ms 时，就会认为该 consumer 已退出，它所订阅的 partition 会分配到同一 group 内的其它的 consumer 上。 connections.max.idle.ms 连接空闲超时时间，默认：540000，即 9 分钟。 在此配置指定的毫秒数之后关闭空闲连接。 key.deserializer key 反序列化器，默认：无。 需要实现接口：org.apache.kafka.common.serialize.Deserializer。Kafka 提供以下几个默认的 key 反序列化器： String：org.apache.kafka.common.serialization.StringDeserializer。 value.deserializer value 反序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Deserializer。Kafka 提供以下几个默认的 value 反序列化器： String：org.apache.kafka.common.serialization.StringDeserializer。 partition.assignment.strategy consumer订阅分区策略，默认：org.apache.kafka.clients.consumer.RangeAssignor。 当使用组管理时，客户端将使用分区分配策略的类名在使用者实例之间分配分区所有权。 max.partition.fetch.bytes 一次 fetch 请求，从一个 partition 中取得的 records 的最大值，默认：1048576，即 1 M。 如果在从 topic 中第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。 broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 message.max.bytes 配置和 topic 端的 max.message.bytes 配置。 fetch.max.bytes 一次 fetch 请求，从一个 broker 中取得的 records 的最大值，默认：52428800，即 50 M。 如果在从 topic中 第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。 broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 message.max.bytes 配置 和 topic 端的 max.message.bytes 配置。 fetch.min.bytes 一次 fetch 请求，从一个 broker 中取得的 records 的最小值，默认：1。 如果 broker 中数据量不够的话会 wait，直到积累的数据大小满足这个条件。默认值设置为1的目的是：使得 consumer 的请求能够尽快的返回。将此设置为大于 1 的值将导致服务器等待更大数量的数据累积，这可以稍微提高服务器吞吐量，但代价是增加一些延迟。 fetch.max.wait.ms 拉取阻塞时间，默认：500。 如果没有足够的数据立即满足 fetch.min.bytes 提供的要求，服务器在响应 fetch 请求之前将阻塞的最长时间。 exclude.internal.topics 公开内部 topic，默认：true。 是否应该将来自内部主题 (如偏移量) 的记录公开给使用者，consumer 共享 offset。如果设置为 true，从内部主题接收记录的唯一方法是订阅它。 isolation.level 隔离级别，默认：read_uncommitted。 控制如何以事务方式读取写入的消息。如果设置为 read_committed，poll () 方法将只返回已提交的事务消息。如果设置为 read_uncommitted，poll () 方法将返回所有消息，甚至是已经中止的事务消息。在任何一种模式下，非事务性消息都将无条件返回。 Kafka Broker 核心配置参数zookeeper.connect zookeeper 地址，多个地址用逗号隔开。 broker.id 服务器的 broke id，默认：-1。 每一个 broker 在集群中的唯一表示，要求是正数。 如果未设置，将生成唯一的代理 id。为了避免 zookeeper 生成的 broke id 和用户配置的 broke id 之间的冲突，生成的代理 id 从 reserve.broker.max.id 开始 id + 1。 advertised.host.name 默认：null。 不赞成使用： 在 server.properties 里还有另一个参数是解决这个问题的， advertised.host.name 参数用来配置返回的 host.name值，把这个参数配置为外网 IP 地址即可。 这个参数默认没有启用，默认是返回的 java.net.InetAddress.getCanonicalHostName() 的值，在我的 mac 上这个值并不等于 hostname 的值而是返回 IP，但在 linux 上这个值就是 hostname 的值。 advertised.listeners hostname 和端口注册到 zookeeper 给生产者和消费者使用的，如果没有设置，将会使用 listeners 的配置，如果 listeners 也没有配置，将使用 java.net.InetAddress.getCanonicalHostName() 来获取这个 hostname 和 port，对于 ipv4，基本就是 localhost 了。 auto.create.topics.enable 是否允许自动创建 topic，默认：true。 如果为 true，第一次发动消息时，允许自动创建 topic。否则，只能通过命令创建 topic。 auto.leader.rebalance.enable 自动 rebalance，默认：true。 支持自动 leader balance。如果需要，后台线程定期检查并触发 leader balance。 background.threads 默认：10。 一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改。 compression.type 压缩类型，默认：producer。 对发送的消息采取的压缩编码方式 (‘gzip’，’snappy’，’lz4’)。 ‘uncompressed’：不压缩， ‘producer’：保持 producer 本身设置的压缩编码。 delete.topic.enable 是否允许删除 topic，默认：true。 如果关闭此配置，则通过管理工具删除主题将无效。 leader.imbalance.check.interval.seconds rebalance 检测频率，默认：300。 控制器触发分区 rebalance 检查的频率。 leader.imbalance.per.broker.percentage 触发 rebalance 得比率，默认：10，即 10%。 每个 broke 允许的 leader 不平衡比率。如果控制器超过每个 broke 的这个值，控制器将触发一个 leader balance。该值以百分比指定。 log.dir 保存日志数据的目录，默认：/tmp/kafka-logs。 log.dirs 保存日志数据的目录，默认：null。 可以指定多个存储路径，以逗号分隔。如果未设置，使用 log.dir 中设置的值。 log.flush.interval.messages 默认：9223372036854775807。 在将消息刷新到磁盘之前，日志分区上累积的消息数量。 log 文件 ”sync” 到磁盘之前累积的消息条数。因为磁盘 IO 操作是一个慢操作，但又是一个”数据可靠性”的必要手段。所以此参数的设置，需要在”数据可靠性”与”性能”之间做必要的权衡。 如果此值过大，将会导致每次 ”fsync” 的时间较长 (IO 阻塞)；如果此值过小，将会导致 ”fsync” 的次数较多，这也意味着整体的 client 请求有一定的延迟。 物理 server 故障，将会导致没有 fsync 的消息丢失。 log.flush.interval.ms 默认：null。 任何 topic 中的消息在刷新到磁盘之前保存在内存中的最长时间。如果没有设置，则使用 log.flush.scheduler.interval.ms 中的值。 log.flush.scheduler.interval.ms 日志刷新器检查是否需要将任何日志刷新到磁盘的频率，默认：9223372036854775807。 log.flush.offset.checkpoint.interval.ms 作为日志恢复点的上次刷新的持久记录的更新频率，默认：60000。 log.retention.bytes 删除前日志的最大大小，默认：-1。 topic 每个分区的最大文件大小，一个 topic 的大小限制 = 分区数 * log.retention.bytes。 log.retention.hours 日志文件最大保存时间 (小时)，默认：168，即 7 天。 删除日志文件之前保存它的小时数。 log.retention.minutes 日志文件最大保存时间 (分钟)，默认：null。 在删除日志文件之前保存它的分钟数，如果没有设置，则使用 log.retention.hours 中的值。 log.retention.ms 日志文件最大保存时间 (毫秒)，默认：null。 在删除日志文件之前保存它的毫秒数，如果没有设置，则使用 log.retention.minutes 中的值。如果设置为 -1，则没有时间限制。 log.roll.hours 新 segment 产生时间，默认：168，即 7 天。 即使文件没有到达 log.segment.bytes 设置的大小，只要文件创建时间到达此属性，也会强制创建新 segment。 log.roll.ms 新 segment 产生时间，默认：null。 如果未设置，则使用 log.roll.hours 中的值。 log.segment.bytes 单个 segment 文件的最大值，默认：1073741824，即 1 G。 log.segment.delete.delay.ms segment 删除前等待时间， 默认：60000，即 1 分钟。 message.max.bytes 最大 batch size，默认：1048588，即 1.000011 M。 Kafka 允许的最大 record batch size (如果启用了压缩，则是压缩后的大小)。如果增加了这个值，并且是 0.10.2 版本之前的 consumer，那么也必须增加 consumer 的 fetch 大小，以便他们能够获取这么大的 record batch。在最新的消息格式版本中，记录总是按批进行分组，以提高效率。在以前的消息格式版本中，未压缩记录没有分组成批，这种限制只适用于单个 record。针对每个 topic，可以使用 max.message.bytes 设置。 min.insync.replicas insync中最小副本值，默认：1。 当生产者将 acks 设置为 “all” (或 “-1”)时，min.insync.replicas 指定了必须确认写操作成功的最小副本数量。如果不能满足这个最小值，则生产者将抛出一个异常 (要么是 NotEnoughReplicas，要么是 NotEnoughReplicasAfterAppend)。 当一起使用时，min.insync.replicas 和 ack 允许你执行更大的持久性保证。一个典型的场景是创建一个复制因子为 3 的主题，设置 min.insync.replicas 为 2，生产者设置 acks 为 “all”，这将确保如果大多数副本没有收到写操作，则生产者会抛出异常。 num.io.threads 服务器用于处理请求的线程数，其中可能包括磁盘 I/O，默认：8。 num.network.threads 服务器用于接收来自网络的请求和向网络发送响应的线程数，默认：3。 num.recovery.threads.per.data.dir 每个数据目录在启动时用于日志恢复和在关闭时用于刷新的线程数，默认：1。 num.replica.alter.log.dirs.threads 可以在日志目录 (可能包括磁盘 I/O) 之间移动副本的线程数，默认：null。 num.replica.fetchers 从 leader 复制数据到 follower 的线程数，默认：1。 offset.metadata.max.bytes 与 offset 提交关联的 metadata 的最大大小，默认：4096。 offsets.commit.timeout.ms offset 提交将被延迟，直到偏移量主题的所有副本收到提交或达到此超时。这类似于生产者请求超时。默认：5000。 offsets.topic.num.partitions 偏移量提交主题的分区数量 (部署后不应再更改)，默认：50。 offsets.topic.replication.factor 副本大小，默认：3。 offsets.topic.segment.bytes 默认104857600，即 100 M。 segment 映射文件 (index) 文件大小，应该保持相对较小以便加快日志压缩和缓存负载。 queued.max.requests 阻塞网络线程之前，允许排队的请求数，默认：500。 replica.fetch.min.bytes 每个 fetch 响应所需的最小字节，默认：1。 如果字节不够，则等待 replicaMaxWaitTimeMs。 replica.lag.time.max.ms 默认：30000。 如果 follower 没有发送任何获取请求，或者至少在这段时间没有消耗到 leader 日志的结束偏移量，那么 leader 将从 isr 中删除 follower。 transaction.max.timeout.ms 默认：900000，即15分钟。 事务执行最长时间，超时则抛出异常。 unclean.leader.election.enable 默认：false。 指示是否在最后不得已的情况下启用 ISR 集中以外的副本作为 leader，即使这样做可能导致数据丢失。 zookeeper.connection.timeout.ms 默认：null。 客户端等待与 zookeeper 建立连接的最长时间。如果未设置，则使用 zookeeper.session.timeout.ms 中的值。 zookeeper.max.in.flight.requests 默认：10。 阻塞之前 consumer 将发送给 zookeeper 的未确认请求的最大数量。 group.max.session.timeout.ms 默认：1800000，即 30 分钟。 注册使用者允许的最大会话超时。超时时间越长，消费者在心跳之间处理消息的时间就越多，而检测故障的时间就越长。 group.min.session.timeout.ms 默认：6000。 注册使用者允许的最小会话超时。更短的超时导致更快的故障检测，但代价是更频繁的用户心跳，这可能会耗尽 broker 资源。 num.partitions 每个主题的默认日志分区数量，默认：1。 本文参考https://www.cnblogs.com/wangzhuxing/p/10111831.html#_label0_11 https://atbug.com/kafka-producer-config/ https://blog.csdn.net/jiecxy/article/details/53389892 本文只整理了部分有关 Kafka 的配置，仅作参考，更多的关于 broker，topic，producer 和 consumer 的配置，请参考 Kafka 官网。 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaConsumer 消费消息的基本流程","slug":"kafka-consumer","date":"2020-10-29T07:10:10.000Z","updated":"2021-01-05T07:33:06.237Z","comments":true,"path":"2020/10/29/kafka-consumer/","link":"","permalink":"http://example.com/2020/10/29/kafka-consumer/","excerpt":"","text":"如何消费数据在上一篇文章中，介绍了 KafkaProducer 如何发送数据到 Kafka，既然有数据发送，那么肯定就有数据消费，KafkaConsumer 也是 Kafka 整个体系中不可缺少的一环。 下面是一段创建 KafkaConsumer 的代码： 1234567891011121314151617181920212223242526272829public class KafkaConsumerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // 必须设置的属性 props.put(&quot;bootstrap.servers&quot;, &quot;192.168.239.131:9092&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;group.id&quot;, &quot;group1&quot;); // 可选设置的属性 props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;auto.offset.reset&quot;, &quot;earliest &quot;); props.put(&quot;client.id&quot;, &quot;test_client_id&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // 订阅主题 consumer.subscribe(Collections.singletonList(&quot;test&quot;)); while (true) &#123; // 拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); records.forEach(record -&gt; System.out.printf(&quot;topic = %s, partition = %d, offset = %d, key = %s, value = %s%n&quot;, record.topic(), record.partition(), record.offset(), record.key(), record.value())); &#125; &#125;&#125; 必须设置的属性创建 KafkaConsumer 时，必须设置的属性有 4 个： bootstrap.servers：连接 Kafka 集群的地址，多个地址以逗号分隔。 key.deserializer：消息中 key 反序列化类，需要和 KafkaProducer 中 key 序列化类相对应。 value.deserializer：消息中 value 的反序列化类，需要和 KafkaProducer 中 value 序列化类相对应。 group.id：消费者所属消费者组的唯一标识。 这里着重说一下 group.id 这个属性，KafkaConsumer 和 KafkaProducer 不一样，KafkaConsumer 中有一个 consumer group (消费者组)，由它来决定同一个 consumer group 中的消费者具体拉取哪个 partition 的数据，所以这里必须指定 group.id 属性。 订阅和取消主题 使用 subscribe () 方式订阅主题 1234// 订阅指定列表的topicpublic void subscribe(Collection&lt;String&gt; topics) &#123; subscribe(topics, new NoOpConsumerRebalanceListener());&#125; 1234// 订阅指定列表的topic，同时指定一个监听器public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; ...&#125; 1234// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行public void subscribe(Pattern pattern) &#123; subscribe(pattern, new NoOpConsumerRebalanceListener());&#125; 1234// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行，同时指定一个监听器public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; ...&#125; 使用 assign () 方式订阅主题和分区 1234// 手动将分区列表分配给consumerpublic void assign(Collection&lt;TopicPartition&gt; partitions) &#123; ...&#125; 使用示例 (仅作参考，assign() 方式的用法，应在使用时再做查询)： 123456List&lt;PartitionInfo&gt; partitionInfoList = kafkaConsumer.partitionsFor(&quot;test&quot;);if (partitionInfoList != null) &#123; for (PartitionInfo partitionInfo : partitionInfoList) &#123; kafkaConsumer.assign(Collections.singletonList(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()))); &#125;&#125; 取消主题的三种方式 123kafkaConsumer.unsubscribe();kafkaConsumer.subscribe(new ArrayList&lt;&gt;());kafkaConsumer.assign(new ArrayList&lt;TopicPartition&gt;()); 上面的三行代码作用相同，都是取消订阅，其中 unsubscribe () 方法即可以取消通过 subscribe () 方式实现的订阅，也可以取消通过 assign () 方式实现的订阅。 拉取数据KafkaConsumer 采用的是主动拉取 broker 数据进行消费的。 一般消息中间件存在推送 (push，server 推送数据给 consumer) 和拉取 (poll，consumer 主动去 server 拉取数据) 两种方式，这两种方式各有优劣。 如果是选择推送的方式，最大的阻碍就是 server 不清楚 consumer 的消费速度，如果 consumer 中执行的操作是比较耗时的，那么 consumer 可能会不堪重负，甚至会导致系统挂掉。 而采用拉取的方式则可以解决这种情况，consumer 根据自己的状态来拉取数据，可以对服务器的数据进行延迟处理。但是这种方式也有一个劣势就是 server 没有数据的时候可能会一直轮询，不过还好 KafkaConsumer 的 poll () 方法有参数允许 consumer 请求在”长轮询”中阻塞，以等待数据到达 (并且可选地等待直到给定数量的字节可用以确保传输大小)。 如何更好的消费数据文章开头处的代码展示了我们是如何消费数据的，但是代码未免过于简单，我们测试的时候这样写没有问题，但是实际开发过程中我们并不会这样写，我们会选择更加高效的方式，这里提供两种方式供大家参考。 一个 consumer group，多个 consumer，数量小于等于 partition 的数量 一个 consumer，多线程处理事件 第一种方式每个 consumer 都要维护一个独立的 TCP 连接，如果 partition 数和创建 consumer 线程的数量过多，会造成不小的系统开销。但是如果处理消息足够快速，消费性能也会提升，如果慢的话就会导致消费性能降低。 第二种方式是采用一个 consumer，多个消息处理线程来处理消息，其实在生产中，瓶颈一般是集中在消息处理上 (因为可能会插入数据到数据库，或者请求第三方 API)，所以我们采用多个线程来处理这些消息。 当然可以结合第一和第二两种方式，采用多 consumer + 多个消息处理线程来消费 Kafka 中的数据，核心代码如下： 1234567891011121314151617181920212223for (int i = 0; i &lt; consumerNum; i++) &#123; // 根据属性创建Consumer，并添加到consumer列表中 final Consumer&lt;String, byte[]&gt; consumer = consumerFactory.getConsumer(getServers(), groupId); consumerList.add(consumer); // 订阅主题 consumer.subscribe(Arrays.asList(this.getTopic())); // consumer.poll()拉取数据 BufferedConsumerRecords bufferedConsumerRecords = new BufferedConsumerRecords(consumer); getExecutor().scheduleWithFixedDelay(() -&gt; &#123; long startTime = System.currentTimeMillis(); // 进行消息处理 consumeEvents(bufferedConsumerRecords); long sleepTime = intervalMillis - (System.currentTimeMillis() - startTime); if (sleepTime &gt; 0) &#123; Thread.sleep(sleepTime); &#125; &#125;, 0, 1000, TimeUnit.MILLISECONDS);&#125; 不过这种方式不能顺序处理数据，如果你的业务是顺序处理，那么第一种方式可能更适合你。所以实际生产中请根据业务选择最适合自己的方式。 消费数据时应该考虑的问题什么是 offset？在 Kafka 中无论是 KafkarPoducer 往 topic 中写数据，还是 KafkaConsumer 从 topic 中读数据，都避免不了和 offset 打交道，关于 offset 主要有以下几个概念： Last Committed Offset：consumer group 最新一次 commit 的 offset，表示这个 consumer group 已经把 Last Committed Offset 之前的数据都消费成功了。 Current Position：consumer group 当前消费数据的 offset，也就是说，Last Committed Offset 到 Current Position 之间的数据已经拉取成功，可能正在处理，但是还未 commit。 High Watermark：HW，已经成功备份到其他 replica 中的最新一条数据的 offset，也就是说，High Watermark 与 Log End Offset 之间的数据已经写入到该 partition 的 leader 中，但是还未完全备份到其他的 replica 中，consumer 也无法消费这部分消息。 Log End Offset：LEO，记录底层日志 (log) 中的下一条消息的 offset。对 KafkaProducer 来说，就是即将插入下一条消息的 offset。 每个 Kafka 副本对象都有两个重要的属性：HW 和 LEO。注意是所有的副本，而不只是 leader 副本。关于这两者更详细解释，参考：[Kafka 的 High Watermark 与 leader epoch 的讨论 对于消费者而言，我们更多时候关注的是消费完成之后如何和服务器进行消费确认，告诉服务器这部分数据我已经消费过了。 这里就涉及到了 2 个 offset，一个是 Current Position，一个是处理完毕向服务器确认的 Last Committed Offset。显然，异步模式下 Last Committed Offset 是落后于 Current Position 的。如果 consumer 挂掉了，那么下一次消费数据又只会从 Last Committed Offset 的位置拉取数据，就会导致数据被重复消费。 如何选择 offset 的提交策略？Kafka 提供了三种提交 offset 的方式。 1. 自动提交 1234// 自动提交，默认trueprops.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// 设置自动每1s提交一次props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 2.手动同步提交 1kafkaConsumer.commitSync(); 3.手动异步提交 1kafkaConsumer.commitAsync(); 上面说了，既然异步提交 offset 可能会重复消费，那么我使用同步提交是否就可以解决数据重复消费的问题呢？我只能说 too young, too sample。且看如下代码： 1234567while (true) &#123; ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(100)); records.forEach(record -&gt; &#123; insertIntoDB(record); kafkaConsumer.commitSync(); &#125;);&#125; 很明显不行，因为 insertIntoDB () 和 kafkaConsumer.commitSync () 两个方法做不到原子操作，如果 insertIntoDB () 成功了，但是提交 offset 的时候 KafkaConsumer 挂掉了，然后服务器重启，仍然会导致重复消费问题。 是否需要做到不重复消费？只要保证处理消息和提交 offset 的操作是原子操作，就可以做到不重复消费。我们可以自己管理 committed offset，而不让 Kafka 来进行管理。 比如如下使用方式： 1.如果消费的数据刚好需要存储在数据库，那么可以把 offset 也存在数据库，就可以在一个事物中提交这两个结果，保证原子操作。 2.借助搜索引擎，把 offset 和数据一起放到索引里面，比如 Elasticsearch。 每条记录都有自己的 offset，所以如果要管理自己的 offset 还得要做下面事情： 1.设置 enable.auto.commit 为 false； 2.使用每个 ConsumerRecord 提供的 offset 来保存消费的位置； 3.在重新启动时使用 seek (TopicPartition partition, long offset) 恢复上次消费的位置。 通过上面的方式就可以在消费端实现 ”Exactly Once” 的语义，即保证只消费一次。但是是否真的需要保证不重复消费呢？这个得看具体业务，如果重复消费数据对整体有什么影响，然后再来决定是否需要做到不重复消费。 再均衡 (reblance) 时怎么办？再均衡是指分区的所属权从一个消费者转移到另一个消费者的行为，再均衡期间，消费者组内的消费者无法读取消息。为了更精确的控制消息的消费，我们可以在订阅主题的时候，通过指定监听器的方式来设定发生再均衡动作前后的一些准备或者收尾的动作。 1234567891011kafkaConsumer.subscribe(Collections.singletonList(&quot;test&quot;), new ConsumerRebalanceListener() &#123; @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; // 再均衡之前和消费者停止读取消息之后被调用 &#125; @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; // 重新分配分区之后和消费者开始消费之前被调用 &#125;&#125;); 具体如何操作，得根据具体的业务逻辑来实现，如果消息比较重要，你可以在再均衡的时候处理 offset，如果不够重要，你可以什么都不做。 无法消费的数据怎么办？可能由于你的业务逻辑有些数据没法消费，这个时候怎么办？同样的还是的看你认为这个数据有多重要或者多不重要，如果重要可以记录日志，把它存入文件或者数据库，以便于稍候进行重试或者定向分析。如果不重要就当做什么事情都没有发生好了。 实际开发中我的处理方式我开发的项目中，用到 Kafka 的其中一个地方是消息通知 (谁给你发了消息，点赞，评论等)，大概的流程就是用户在 client 端做了某些操作，就会发送数据到 Kafka，然后把这些数据进行一定的处理之后插入到 HBase 中。 其中采用了 N consumer thread + N Event Handler 的方式来消费数据，并采用自动提交 offset。对于无法消费的数据往往只是简单处理下，打印下日志以及消息体 (无法消费的情况非常非常少)。 得益于 HBase 的多 version 控制，即使是重复消费了数据也无关紧要。这样做没有去避免重复消费的问题主要是基于以下几点考虑： 1.重复消费的概率较低，服务器整体性能稳定。 2.即便是重复消费了数据，入库了 HBase，获取数据也是只有一条，不影响结果的正确性。 3.有更高的吞吐量。 4.编程简单，不用单独去处理以及保存 offset。 本文参考http://generalthink.github.io/2019/05/06/kafka-consumer-use/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaProducer 部分源码解析","slug":"kafka-producer","date":"2020-10-26T06:15:52.000Z","updated":"2021-01-05T07:33:19.544Z","comments":true,"path":"2020/10/26/kafka-producer/","link":"","permalink":"http://example.com/2020/10/26/kafka-producer/","excerpt":"","text":"先来看一段创建 KafkaProducer 的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class KafkaProducerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // bootstrap.servers 必须设置 props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.239.131:9092&quot;); // key.serializer 必须设置 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // value.serializer 必须设置 props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // client.id props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;client-0&quot;); // retries props.put(ProducerConfig.RETRIES_CONFIG, 3); // acks props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;); // max.in.flight.requests.per.connection props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1); // linger.ms props.put(ProducerConfig.LINGER_MS_CONFIG, 100); // batch.size props.put(ProducerConfig.BATCH_SIZE_CONFIG, 10240); // buffer.memory props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10240); KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props); // 指定topic，key，value ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;test1&quot;, &quot;key1&quot;, &quot;value1&quot;); // 异步发送 kafkaProducer.send(record, (recordMetadata, exception) -&gt; &#123; if (exception != null) &#123; // 发送失败的处理逻辑 exception.printStackTrace(); &#125; else &#123; // 发送成功的处理逻辑 System.out.println(recordMetadata.topic()); &#125; &#125;); // 同步发送 // kafkaProducer.send(record).get(); // 关闭Producer kafkaProducer.close(); &#125;&#125; 主要流程图 简要说明： 1.new KafkaProducer () 后，创建一个后台线程 KafkaThread (实际运行线程是 Sender，KafkaThread 是对 Sender 的封装) 扫描 RecordAccumulator 中是否有消息； 2.调用 kafkaProducer.send () 发送消息，实际是将消息保存到 RecordAccumulator 中，实际上就是保存到一个 Map 中 (ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)，这条消息会被记录到同一个记录批次 (相同主题相同分区算同一个批次) 里面，这个批次的所有消息会被发送到相同的主题和分区上； 3.后台的独立线程扫描到 RecordAccumulator 中有消息后，会将消息发送到 Kafka 集群中 (不是一有消息就发送，而是要看消息是否 ready)； 4.如果发送成功 (消息成功写入 Kafka)，就返回一个 RecordMetaData 对象，它包换了主题和分区信息，以及记录在分区里的偏移量等信息； 5.如果写入失败，就会返回一个错误，生产者在收到错误之后会尝试重新发送消息 (如果允许的话，此时会将消息在保存到 RecordAccumulator 中)，达到重试次数之后如果还是失败就返回错误消息。 缓存器的创建123456789101112this.accumulator = new RecordAccumulator(logContext, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), this.compressionType, lingerMs(config), retryBackoffMs, deliveryTimeoutMs, metrics, PRODUCER_METRIC_GROUP_NAME, time, apiVersions, transactionManager, new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME)); 后台线程的创建12345678910111213141516171819202122this.sender = newSender(logContext, kafkaClient, this.metadata);String ioThreadName = NETWORK_THREAD_PREFIX + &quot; | &quot; + clientId;this.ioThread = new KafkaThread(ioThreadName, this.sender, true);this.ioThread.start();KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient( new Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), this.metrics, time, &quot;producer&quot;, channelBuilder, logContext), metadata, clientId, maxInflightRequests, producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG), producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG), producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG), producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG), requestTimeoutMs, ClientDnsLookup.forConfig(producerConfig.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)), time, true, apiVersions, throttleTimeSensor, logContext); 上述代码中，构造了一个 KafkaClient 负责和 broker 通信，同时构造一个 Sender 并启动一个异步线程，这个线程会被命名为：kafka-producer-network-thread | $&#123;clientId&#125;，如果你在创建 producer 的时候指定 client.id 的值为 myclient，那么线程名称就是 kafka-producer-network-thread | myclient。 发送消息 (缓存消息)发送消息有同步发送以及异步发送两种方式，我们一般不使用同步发送，毕竟太过于耗时，使用异步发送的时候可以指定回调函数，当消息发送完成的时候 (成功或者失败) 会通过回调通知生产者。 同步 send： 123public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record) &#123; return send(record, null);&#125; 异步 send： 12345public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; // intercept the record, which can be potentially modified; this method does not throw exceptions ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record); return doSend(interceptedRecord, callback);&#125; 可以看出，同步和异步实际上调用的是同一个方法，同步发送时，设置回调函数为 null。 消息发送之前，会先对 key 和 value 进行序列化： 12345678910111213141516byte[] serializedKey;try &#123; serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert key of class &quot; + record.key().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in key.serializer&quot;, cce);&#125;byte[] serializedValue;try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert value of class &quot; + record.value().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in value.serializer&quot;, cce);&#125; 计算分区： 1int partition = partition(record, serializedKey, serializedValue, cluster); 发送消息，实际上是将消息缓存起来，核心代码如下： 12RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs); RecordAccumulator 的核心数据结构是 ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;，会将相同 topic 相同 partition 的数据放到一个 Deque (双向队列) 中，这也是我们之前提到的同一个记录批次里面的消息会发送到同一个主题和分区的意思。append () 方法的核心源码如下： 123456789101112131415161718192021// 从batchs(ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)中，根据主题分区获取对应的队列，如果没有则new ArrayDeque&lt;&gt;返回Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);// 计算同一个记录批次占用空间大小，batchSize根据batch.size参数决定int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound( maxUsableMagic, compression, key, value, headers));// 为同一个topic，partition分配buffer，如果同一个记录批次的内存不足，那么会阻塞maxTimeToBlock(max.block.ms参数)这么长时间ByteBuffer buffer = free.allocate(size, maxTimeToBlock);synchronized (dq) &#123; // 创建MemoryRecordBuilder，通过buffer初始化appendStream(DataOutputStream)属性 MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic); ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds()); // 将key，value写入到MemoryRecordsBuilder中的appendStream(DataOutputStream)中 FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds())); // 将需要发送的消息放入到队列中 dq.addLast(batch);&#125; 发送消息到 Kafka上面已经将消息存储 RecordAccumulator 中去了，现在看看怎么发送消息。前面提到创建 KafkaProducer 的时候，会启动一个异步线程去从 RecordAccumulator 中取得消息然后发送到 Kafka，发送消息的核心代码在 Sender 中，它实现了 Runnable 接口并在后台一直运行处理发送请求并将消息发送到合适的节点，直到 KafkaProducer 被关闭。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata * requests to renew its view of the cluster and then sends produce requests to the appropriate nodes. */public class Sender implements Runnable &#123; /** * The main run loop for the sender thread */ public void run() &#123; // main loop, runs until close is called while (running) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // okay we stopped accepting requests but there may still be // requests in the transaction manager, accumulator or waiting for acknowledgment, // wait until these are completed. while (!forceClose &amp;&amp; ((this.accumulator.hasUndrained() || this.client.inFlightRequestCount() &gt; 0) || hasPendingTransactionalRequests())) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // Abort the transaction if any commit or abort didn&#x27;t go through the transaction manager&#x27;s queue while (!forceClose &amp;&amp; transactionManager != null &amp;&amp; transactionManager.hasOngoingTransaction()) &#123; if (!transactionManager.isCompleting()) &#123; log.info(&quot;Aborting incomplete transaction due to shutdown&quot;); transactionManager.beginAbort(); &#125; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; if (forceClose) &#123; // We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on // the futures. if (transactionManager != null) &#123; log.debug(&quot;Aborting incomplete transactional requests due to forced shutdown&quot;); transactionManager.close(); &#125; log.debug(&quot;Aborting incomplete batches due to forced shutdown&quot;); this.accumulator.abortIncompleteBatches(); &#125; &#125;&#125; KafkaProducer 的关闭方法有2个：close () 以及 close (Duration timeout)，close (long timeout, TimeUnit timUnit) 已被弃用，其中 timeout 参数的意思是等待生产者完成任何待处理请求的最长时间，第一种方式的 timeout 为 Long.MAX_VALUE 毫秒，如果采用第二种方式关闭，当 timeout = 0 的时候则表示强制关闭，直接关闭 Sender (设置 running = false)。 Send 中，runOnce () 方法，跳过对 transactionManager 的处理，查看发送消息的主要流程： 123456long currentTimeMs = time.milliseconds();// 将记录批次转移到每个节点的生产请求列表中long pollTimeout = sendProducerData(currentTimeMs);// 轮询进行消息发送client.poll(pollTimeout, currentTimeMs); 首先，查看 sendProducerData (currentTimeMs) 方法，它的核心逻辑在 sendProduceRequest (batches, now) 方法中： 123456789101112131415161718192021222324252627282930313233for (ProducerBatch batch : batches) &#123; TopicPartition tp = batch.topicPartition; // 将ProducerBatch中MemoryRecordsBuilder转换为MemoryRecords(发送的数据就在这里面) MemoryRecords records = batch.records(); // down convert if necessary to the minimum magic used. In general, there can be a delay between the time // that the producer starts building the batch and the time that we send the request, and we may have // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use // the new message format, but found that the broker didn&#x27;t support it, so we need to down-convert on the // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may // not all support the same message format version. For example, if a partition migrates from a broker // which is supporting the new magic version to one which doesn&#x27;t, then we will need to convert. if (!records.hasMatchingMagic(minUsedMagic)) records = batch.records().downConvert(minUsedMagic, 0, time).records(); produceRecordsByPartition.put(tp, records); recordsByPartition.put(tp, batch);&#125;ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout, produceRecordsByPartition, transactionalId);// 消息发送完成时的回调(消息发送失败后，在handleProduceResponse中处理)RequestCompletionHandler callback = new RequestCompletionHandler() &#123; public void onComplete(ClientResponse response) &#123; handleProduceResponse(response, recordsByPartition, time.milliseconds()); &#125;&#125;;// 根据参数构造ClientRequest，此时需要发送的消息在requestBuilder中ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, requestTimeoutMs, callback);// 将clientRequest转换成Send对象(Send.java，包含了需要发送数据的buffer)，给KafkaChannel设置该对象，记住这里还没有发送数据client.send(clientRequest, now); 在没有指定 KafkaClient 时，client.send (clientRequest, now) 方法，实际就是 NetworkClient.send (ClientRequest request, long now) 方法，所有的请求 (无论是 producer 发送消息的请求，还是获取 metadata 的请求) 都是通过该方法设置对应的 Send 对象： 1Send send = request.toSend(destination, header); 需要知道的是，上面只是设置了发送消息所需要准备的内容。 接下来，查看 client.poll (pollTimeout, currentTimeMs) 方法，进入到发送消息的主流程，发送消息的核心代码最终可以定位到 Selector 的 pollSelectionKeys (Set&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos) 方法中，代码如下： 12345678910111213141516/* if channel is ready write to any sockets that have space in their buffer and for which we have data */if (channel.ready() &amp;&amp; key.isWritable() &amp;&amp; !channel.maybeBeginClientReauthentication( () -&gt; channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos)) &#123; Send send; try &#123; // 底层实际调用的是java8 GatheringByteChannel的write方法 send = channel.write(); &#125; catch (Exception e) &#123; sendFailed = true; throw e; &#125; if (send != null) &#123; this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); &#125;&#125; 就这样，我们的消息就发送到了 broker 中了，发送流程分析完毕，注意，这个是完美发送的情况。但是总会有发送失败的时候 (消息过大或者没有可用的 leader 等)，那么发送失败后重发又是在哪里完成的呢？还记得上面的回调函数吗，没错，就是在回调函数这里设置的，先来看下回调函数源码： 12345678910111213141516171819202122232425262728293031323334/** * Handle a produce response */private void handleProduceResponse(ClientResponse response, Map&lt;TopicPartition, ProducerBatch&gt; batches, long now) &#123; RequestHeader requestHeader = response.requestHeader(); long receivedTimeMs = response.receivedTimeMs(); int correlationId = requestHeader.correlationId(); if (response.wasDisconnected()) &#123; // 如果是网络断开则构造Errors.NETWORK_EXCEPTION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION), correlationId, now, 0L); &#125; else if (response.versionMismatch() != null) &#123; // 如果是版本不匹配，则构造Errors.UNSUPPORTED_VERSION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.UNSUPPORTED_VERSION), correlationId, now, 0L); &#125; else &#123; // if we have a response, parse it(如果存在response就返回正常的response) if (response.hasResponse()) &#123; ProduceResponse produceResponse = (ProduceResponse) response.responseBody(); for (Map.Entry&lt;TopicPartition, ProduceResponse.PartitionResponse&gt; entry : produceResponse.responses().entrySet()) &#123; TopicPartition tp = entry.getKey(); ProduceResponse.PartitionResponse partResp = entry.getValue(); ProducerBatch batch = batches.get(tp); completeBatch(batch, partResp, correlationId, now, receivedTimeMs + produceResponse.throttleTimeMs()); &#125; this.sensors.recordLatency(response.destination(), response.requestLatencyMs()); &#125; else &#123; // this is the acks = 0 case, just complete all requests(如果acks=0，那么则构造Errors.NONE的响应，因为这种情况只需要发送不需要响应结果) for (ProducerBatch batch : batches.values()) &#123; completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NONE), correlationId, now, 0L); &#125; &#125; &#125;&#125; 在 completeBatch () 方法中我们主要关注失败的逻辑处理，核心源码如下： 12345678910111213141516171819202122232425262728293031/** * Complete or retry the given batch of records. * * @param batch The record batch * @param response The produce response * @param correlationId The correlation id for the request * @param now The current POSIX timestamp in milliseconds */private void completeBatch(ProducerBatch batch, ProduceResponse.PartitionResponse response, long correlationId, long now, long throttleUntilTimeMs) &#123; Errors error = response.error; if (error == Errors.MESSAGE_TOO_LARGE &amp;&amp; batch.recordCount &gt; 1 &amp;&amp; !batch.isDone() &amp;&amp; (batch.magic() &gt;= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) &#123; // If the batch is too large, we split the batch and send the split batches again. We do not decrement // the retry attempts in this case.(如果发送的消息太大，需要重新进行分割发送) this.accumulator.splitAndReenqueue(batch); maybeRemoveAndDeallocateBatch(batch); this.sensors.recordBatchSplit(); &#125; else if (error != Errors.NONE) &#123; // 发生了错误，如果此时可以retry(retry次数未达到限制以及产生的异常是RetriableException) if (canRetry(batch, response, now)) &#123; if (transactionManager == null) &#123; // 把需要重试的消息放入队列中，等待重试，实际就是调用deque.addFirst(batch) reenqueueBatch(batch, now); &#125; ... &#125; ... &#125;&#125; 以上，就是 KafkaProducer 发送消息的流程。 补充：分区算法在发送消息前，调用的计算分区方法如下： 123456789101112/** * computes partition for given record. * if the record has partition returns the value otherwise * calls configured partitioner class to compute the partition. */private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition : partitioner.partition( record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; 如果在创建 ProducerRecord 的时候，指定了 partition，则使用指定的，否则调用配置的 partitioner 类来计算分区。 如果没有配置自定义的分区器，Kafka 默认使用 org.apache.kafka.clients.producer.internals.DefaultPartitioner，源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * The default partitioning strategy: * &lt;ul&gt; * &lt;li&gt;If a partition is specified in the record, use it * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key * &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion */public class DefaultPartitioner implements Partitioner &#123; private final ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = new ConcurrentHashMap&lt;&gt;(); public void configure(Map&lt;String, ?&gt; configs) &#123;&#125; /** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; // 如果key为null，则使用Round Robin算法 int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition(根据key进行散列，使用murmur2算法) return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125; private int nextValue(String topic) &#123; AtomicInteger counter = topicCounterMap.get(topic); if (null == counter) &#123; counter = new AtomicInteger(ThreadLocalRandom.current().nextInt()); AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter); if (currentCounter != null) &#123; counter = currentCounter; &#125; &#125; return counter.getAndIncrement(); &#125; public void close() &#123;&#125;&#125; DefaultPartitioner 中对于分区的算法有两种情况： 1.如果键值为 null，那么记录键随机地发送到主题内各个可用的分区上。分区器使用轮询 (Round Robin) 算法键消息均衡地分布到各个分区上。 2.如果键不为 null，那么 Kafka 会对键进行散列 (使用 Kafka 自己的散列算法，即使升级 java 版本，散列值也不会发生变化) ，然后根据散列值把消息映射到特定的分区上。应该注意的是，同一个键总是被映射到同一个分区上 (如果分区数量发生了变化则不能保证)，映射的时候会使用主题所有的分区，而不仅仅是可用分区，所以如果写入数据分区是不可用的，那么就会发生错误，当然这种情况很少发生。 当然，如果你想要实现自定义分区，那么只需要实现 Partitioner 接口即可： 123456789101112131415161718192021222324/** * 将key的hash值，对分区总数取余，以确定消息发送到哪个分区 */public class KeyPartitioner implements Partitioner &#123; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; Integer numPartitions = cluster.partitionCountForTopic(topic); if (keyBytes == null) &#123; throw new InvalidRecordException(&quot;key can not be null&quot;); &#125; return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 然后，使用 partitioner.class 参数，指定你自定义的分区器的路径： 1props.put(&quot;partitioner.class&quot;, &quot;cn.xisun.partitioner.KeyPartitioner&quot;); 本文参考https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"什么是 Kafka","slug":"kafka-introduce","date":"2020-10-23T07:58:39.000Z","updated":"2021-01-05T07:33:11.926Z","comments":true,"path":"2020/10/23/kafka-introduce/","link":"","permalink":"http://example.com/2020/10/23/kafka-introduce/","excerpt":"","text":"分布式分布式系统由多个运行的计算机系统组成，所有这些计算机在一个集群中一起工作，对终端用户来讲只是一个单一节点。 Kafka 也是分布式的，因为它在不同的节点 (又被称为 broker) 上存储，接受以及发送消息，这样做的好处是具有很高的可扩展性和容错性。 水平可扩展性在这之前，先看看什么是垂直可扩展，比如你有一个传统的数据库服务器，它开始过度负载，解决这个问题的办法就是给服务器加配置 (cpu，内存，SSD)，这就叫做垂直扩展。但是这种方式存在两个巨大的劣势： 1.硬件存在限制，不可能无限的添加机器配置。 2.它需要停机时间，通常这是很多公司无法容忍的。 水平可扩展就是通过添加更多的机器来解决同样的问题，添加新机器不需要停机，而且集群中也不会对机器的数量有任何的限制。但问题在于并非所有系统都支持水平可伸缩性，因为它们不是设计用于集群中 (在集群中工作会更加复杂)。 容错性非分布式系统中容易最致命的问题就是单点失败，如果你唯一的服务器挂掉了，那么我相信你会很崩溃。 而分布式系统的设计方式就是可以以配置的方式来容许失败。比如在 5 个节点的 Kafka 集群中，即使其中两个节点挂掉了，你仍然可以继续工作。 需要注意的是，容错与性能直接相关，你的系统容错程度越高，性能就越差。 提交日志 (commit log)提交日志 (也被称为预写日志或者事物日志) 是仅支持附加的持久有序数据结构，你无法修改或者删除记录，它从左往右读并且保证日志的顺序。 是不是觉得 Kafka 的数据结构如此简单? 是的，从很多方面来讲，这个数据结构就是 Kafka 的核心。这个数据结构的记录是有序的，而有序的数据可以确保我们的处理流程。这两个在分布式系统中都是极其重要的问题。 Kafka 实际上将所有消息存储到磁盘并在数据结构中对它们进行排序，以便利用顺序磁盘读取。 1.读取和写入都是常量时间 O(1) (当确定了 record id)，与磁盘上其他结构的 O(log N)操作相比是一个巨大的优势，因为每个磁盘搜索都很耗时。 2.读取和写入不会相互影响，写不会锁住读，反之亦然。 这两点有着巨大的优势， 因为数据大小与性能完全分离。无论你的服务器上有 100 KB 还是 100 TB 的数据，Kafka 都具有相同的性能。 如何工作生产者消费者模式：生产者 (producer) 发送消息 (record) 到 Kafka 服务器 (broker)，这些消息存储在主题 (topic) 中，然后消费者 (consumer) 订阅该主题，接受新消息后并进行处理。 随着消息的越来越多，topic 也会越来越大，为了获得更好的性能和可伸缩性，可以在 topic 下建立多个更小的分区 (partition)，在发送消息时，可以根据实际情况，对消息进行分类，同一类的消息发送到同一个 partition (比如存储不同用户发送的消息，可以根据用户名的首字母进行分区匹配)。Kafka 保证 partition 内的所有消息都按照它们的顺序排序，区分特定消息的方式是通过其偏移量 (offset)，你可以将其视为普通数组索引，即为分区中的每个新消息递增的序列号。 Kafka 遵守着愚蠢的 broker 和聪明的 consumer 的准则。这意味着 Kafka 不会跟踪消费者读取了哪些记录并删除它们，而是会将它们存储一定的时间 (比如 1 天，以 log.retention 开头的来决定日志保留时间)，直到达到某个阈值。消费者自己轮询 Kafka 的新消息并且告诉它自己想要读取哪些记录，这允许它们按照自己的意愿递增/递减它们所处的偏移量，从而能够重放和重新处理事件。 需要注意的是消费者是属于消费者组的 (在创建 consumer 时，必须指定其所属的消费者组的 group.id)，消费者组有一个或多个消费者。为了避免两个进程读取同样的消息两次，每个 partition 只能被一个消费者组中的一个消费者访问。 持久化到硬盘正如之前提到的，Kafka 实际上是将所有记录存储到硬盘而不在 RAM 中保存任何内容，这背后有很多优化使得这个方案可行。 1.Kafka 有一个将消息分组的协议，这允许网络请求将消息组合在一起并减少网络开销，服务器反过来一次性保留大量消息，消费者一次获取大量线性块。 2.磁盘上线性读写非常快，现代磁盘非常慢的原因是由于大量磁盘寻址，但是在大量的线性操作中不是问题。 3.操作系统对线性操作进行了大量优化，通过预读 (预取大块多次) 和后写 (将小型逻辑写入组成大型物理写入) 技术。 4.操作系统将磁盘文件缓存在空闲 RAM 中。这称为 page cache，而 Kafka 的读写都大量使用了 page cache： ​ ① 写消息的时候消息先从 java 到 page cache，然后异步线程刷盘，消息从 page cache 刷入磁盘； ​ ② 读消息的时候先从 page cache 找，有就直接转入 socket，没有就先从磁盘 load 到 page cache，然后直接从 socket 发出去。 5.由于 Kafka 在整个流程 (producer → broker → consumer) 中以未经修改的标准化二进制格式存储消息，因此它可以使用零拷贝优化。那时操作系统将数据从 page cache 直接复制到 socket，有效地完全绕过了 Kafka broker。 所有这些优化都使 Kafka 能够以接近网络的速度传递消息。 数据分发和复制下面来谈谈 Kafka 如何实现容错以及它如何在节点之间分配数据。 为了使得一个 broker 挂掉的时候，数据还能得以保留，分区 (partition) 数据在多个 broker 中复制。 在任何时候，一个 broker 拥有一个 partition，应用程序读取/写入都要通过这个节点，这个节点叫做 partition leader。它将收到的数据复制到 N 个其他 broker，这些接收数据的 broker 叫做 follower，follower 也存储数据，一旦 leader 节点死掉的时候，它们就准备竞争上岗成为 leader。 这可以保证你成功发布的消息不会丢失，通过选择更改副本因子，你可以根据数据的重要性来交换性能以获得更强的持久性保证。 这样如果 leader 挂掉了，那么其中一个 follower 就会接替它称为 leader。包括 leader 在内的总副本数就是副本因子 (创建 topic 时，使用 --replication-factor 参数指定)，上图有 1 个 leader，2 个 follower，所以副本因子就是 3。 但是你可能会问：producer 或者 consumer 怎么知道 partition leader 是谁？ 对生产者/消费者对分区的写/读请求，它们需要知道分区的 leader 是哪一个，对吧？这个信息肯定是可以获取到的，Kafka 使用 ZooKeeper 来存储这些元数据。 什么是 ZooKeeperZooKeeper 是一个分布式键值存储。它针对读取进行了高度优化，但写入速度较慢。它最常用于存储元数据和处理集群的机制 (心跳，分发更新/配置等)。 它允许服务的客户 (Kafka broker) 订阅并在发生变更后发送给他们，这就是 Kafka 如何知道何时切换分区领导者。ZooKeeper 本身维护了一个集群，所以它就有很高的容错性，当然它也应该具有，毕竟 Kafka 很大程度上是依赖于它的。 ZooKeeper 用于存储所有的元数据信息，包括但不限于如下几项： 消费者组每个分区的偏移量 (现在客户端在单独的 Kafka topic 上存储偏移量) ACL —— 权限控制 生产者/消费者的流量控制——每秒生产/消费的数据大小。参考：Kafka - 流量控制 Quota 功能 partition leader 以及它们的健康信息 那么 producer/consumer 是如何知道谁是 partition leader 的呢？ 生产者和消费者以前常常直接连接 ZooKeeper 来获取这些信息，但是 Kafka 从 0.8 和 0.9 版本开始移除了这种强耦合关系。客户端直接从 Kafka broker 获取这些元数据，而让 Kafka broker 从 ZooKeeper 那里获取这些元数据。 更多 ZooKeeper 的讲解参考：漫画：什么是 ZooKeeper？ 流式处理 (Streaming)在 Kafka 中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理并生成数据流以输出到其他主题 (或者外部服务，数据库，容器等等)。 什么是数据流呢？首先，数据流是无边界数据集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的，是因为随着时间推移，新的记录会不断加入进来。比如信用卡交易，股票交易等事件都可以用来表示数据流。 我们可以使用 producer/consumer 的 API 直接进行简单处理，但是对于更加复杂的转换，比如将流连接到一起，Kafka 提供了集成 Stream API 库。 这个 API 是在你自己的代码中使用的，它并不是运行在 broker 上，它的工作原理和 consumer API 类似，可帮助你在多个应用程序 (类似于消费者组) 上扩展流处理工作。 无状态处理流的无状态处理是确定性处理，其不依赖于任何外部条件，对于任何给定的数据，将始终生成与其他任何内容无关的相同输出。举个例子，我们要做一个简单的数据转换—“zhangsan” → “Hello, zhangsan” 流-表二义性重要的是要认识到流和表实质上是一样的，流可以被解释称为表，表也可以被解释称为流。 流作为表流可以解释为数据的一系列更新，聚合后的结果就是表的最终结果，这项技术被称为事件溯源 (Event Sourcing)。 如果你了解数据库备份同步，你就会知道它们的技术实现被称为流式复制—将对表的每个更改都发送报副本服务器。比如 redis 中的 AOF 以及 Mysql 中的 binlog。 Kafka 流可以用相同的方式解释 - 当累积形成最终状态时的事件。此类流聚合保存在本地 RocksDB 中 (默认情况下)，被称为 KTable。 表作为流可以将表视为流中每个键的最新值的快照。与流记录可以生成表一样，表更新可以生成更改日志流。 有状态处理我们在 java 中常用的一些操作比如 map() 或者 filter() 是没有状态的，它不会要求你保留任何原始数据。但是现实中，大多数的操作都是有状态的 (比如 count())，因为这需要你存储当前累计的状态。 在流处理器上维护状态的问题是流处理器可能会失败！你需要在哪里保持这种状态才能容错？ 一种简单的方法是简单地将所有状态存储在远程数据库中，并通过网络连接到该存储，这样做的问题是大量的网络带宽会使得你的应用程序变慢。一个更微妙但重要的问题是你的流处理作业的正常运行时间将与远程数据库紧密耦合，并且作业将不是自包含的 (其他 team 更改数据库可能会破坏你的处理)。 那么什么是更好的办法呢？ 回想一下表和流的二元性。这允许我们将流转换为与我们的处理位于同一位置的表。它还为我们提供了一种处理容错的机制—通过将流存储在 Kafka broker 中。 流处理器可以将其状态保持在本地表 (例如 RocksDB) 中，该表将从输入流 (可能在某些任意转换之后) 更新。当进程失败时，它可以通过重放流来恢复其数据。 你甚至可以将远程数据库作为流的生产者，有效地广播用于在本地重建表的更改日志。 KSQL通常，我们不得不使用 JVM 语言编写流处理，因为这是唯一的官方 Kafka Streams API 客户端。2018 年 4 月，KSQL 作为一项新特性被发布，它允许你使用熟悉的类似 SQL 的语言编写简单的 stream jobs。你安装了 KSQL 服务器并通过 CLI 以交互方式查询以及管理。它使用相同的抽象 (KStream 和 KTable)，保证了 Streams API 的相同优点 (可伸缩性，容错性)，并大大简化了流的工作。 这听起来可能不是很多，但在实践中对于测试内容更有用，甚至允许开发之外的人 (例如产品所有者) 使用流处理，可以看看 Confluent 提供的这篇关于 ksql 的使用。 什么时候使用 kafka正如我们已经介绍的那样，Kafka 允许你通过集中式介质获取大量消息并存储它们，而不必担心性能或数据丢失等问题。 这意味着它非常适合用作系统架构的核心，充当连接不同应用程序的集中式媒体。Kafka 可以成为事件驱动架构的中心部分，使你可以真正地将应用程序彼此分离。 Kafka 允许你轻松地分离不同 (微) 服务之间的通信。使用 Streams API，现在可以比以往更轻松地编写业务逻辑，从而丰富 Kafka 主题数据以供服务使用。可能性很大，我恳请你探讨公司如何使用 Kafka。 总结Apache Kafka 是一个分布式流媒体平台，每天可处理数万亿个事件。Kafka 提供低延迟，高吞吐量，容错的发布和订阅管道，并能够处理事件流。我们回顾了它的基本语义 (producer，broker，consumer，topic)，了解了它的一些优化 (page cache)，通过复制数据了解了它的容错能力，并介绍了它不断增长的强大流媒体功能。Kafka 已经在全球数千家公司中大量采用，其中包括财富 500 强企业中的三分之一。随着 Kafka 的积极开发和最近发布的第一个主要版本 1.0 (2017 年 11 月 1 日)，有预测这个流媒体平台将会与关系数据库一样，是数据平台的重要核心。我希望这篇介绍能帮助你熟悉 Apache Kafka。 本文参考http://generalthink.github.io/2019/02/27/introduction-of-kafka/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"使用 hexo 搭建 github 博客","slug":"hexo-blog","date":"2020-10-23T03:33:51.000Z","updated":"2022-01-12T02:29:03.335Z","comments":true,"path":"2020/10/23/hexo-blog/","link":"","permalink":"http://example.com/2020/10/23/hexo-blog/","excerpt":"","text":"使用工具版本默认已经安装 node.js 和 git。 123git version: git version 2.27.0.windows.1npm version: 6.14.7hexo version: 4.2.0 git 客户端与 github 建立 SSH 连接1Please make sure you have the correct access rights and the repository exists. 当 git 客户端出现以上提示时，说明 SSH 连接过期，需要重新建立连接。参考如下方式： 先查看下 name 和 email 123456# 查看user的name和email$ git config user.name$ git config user.email# 如果没设置，按如下命令设置$ git config --global user.name &#123;$yourname&#125;$ git config --global user.email &#123;$youremail&#125; 删除 .ssh 文件夹下的 known_hosts，路径为：C:\\Users\\&#123;$userrname&#125;\\.ssh git bash 输入命令 1$ ssh-keygen -t rsa -C &#123;$youremail&#125; 一直按回车，等结束后，.ssh 文件夹下会生成两个文件：id_rsa 和 id_rsa.pub，将 id_rsa.pub 的内容全部复制。 登录个人 github 账户，进入 Settings → SSH and GPG keys，点击 New SSH key，将复制的内容粘贴到 Key 里，点击 Add SSH key。 git bash 输入命令 1$ ssh -T git@github.com 在弹出的确定对话框输入：yes。 hexo 安装在 git bash 中依次输入以下命令： 123456$ npm install hexo-cli -g$ cd f: # 可以是任何路径$ hexo init blog$ cd blog # 进入blog目录$ npm install$ npm install hexo-deployer-git --save 命令执行完成后，会在 F:\\ 目录下，多一个 blog 文件夹。 修改 _config.yml 文件修改 blog 根目录下的 _config.yml 文件，将 deploy 节点修改为如下内容： 1234deploy: type: git repo: git@github.com:&#123;$yourname&#125;/&#123;$yourname&#125;.github.io.git branch: master 说明：_config.yml 文件的配置均为 [key: value] 形式，value 前面必须要有一个空格。 然后在 git bash 中输入以下命令，发布博客： 1$ hexo deploy 访问自己的博客博客地址：https://&#123;$yourname&#125;.github.io/ 写一个自己的博客hexo 的项目结构是在网站根目录的 source\\_posts 目录下存放你的博客文档，以 .md 文档格式存储，默认已存在一个 hello-world.md 文章。 新建文章 1$ hexo new &lt;title&gt; 会在 blog 的 source\\_posts 目录下，新建一个名叫 &lt;title&gt;.md 文章，如： 12INFO Validating configINFO Created: F:\\blog\\source\\_posts\\tesss.md 之后，在文章中添加自己的内容即可，建议使用 Typora 编辑，其语法参考：如何使用 markdown？ 发布文章 1234$ hexo clean # 清楚缓存$ hexo generate # 生成静态页面$ hexo server # 本地发布，浏览器输入localhost:4000即可访问博客$ hexo deploy # 将public中的静态页面复制到.deploy_git文件夹中，并提交到github 至此，你的第一个自己的博客发布完成。 说明：以上 hexo 的命令，都要在 F:\\blog 目录下执行。 修改博客的 themes如果想修改自己博客的 themes，可以下载好想要的，然后拷贝到 blog 的 themes 目录下，然后修改 _config.yml 文件，将 theme 节点的值，修改为你下载好的 themes 的名称，如： 1theme: next 之后，再按照你下载的 themes 的使用说明，做相应修改即可。 参考：NexT 的使用 NexT 中 tags 的使用 修改 NexT 目录下的 _config.yml 文件，取消 menu 菜单下 tags 字段的注释 123menu: home: / || fa fa-home tags: /tags/ || fa fa-tags 在 blog 根目录的 source 目录下，新建 tags 目录 1$ hexo new page &quot;tags&quot; 修改 tags 目录下的 index.md 文件 1234title: tagsdate: 2020-10-27 16:35:56type: tagslayout: &quot;tags&quot; NexT 中添加字数统计、阅读时长 安装 hexo-symbols-count-time 插件 1$ npm install hexo-symbols-count-time 或者 1$ yarn add hexo-symbols-count-time hexo 配置，根目录下的 _config.yaml 文件，添加 symbols_count_time 节点 123456# Post wordcount display settingssymbols_count_time: symbols: true # 文章字数 time: true # 阅读时长 total_symbols: true # 所有文章总字数 total_time: true # 所有文章阅读中时长 NexT 配置，themes 目录下的 _config.yml 文件，symbols_count_time 节点 123456# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: true # 是否换行显示 字数统计 及 阅读时长 item_text_post: true # 文章 字数统计 阅读时长 使用图标 还是 文本表示 item_text_total: false # 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示 Next 中添加访客统计、访问次数统计、文章阅读次数统计 打开 next 主题配置文件 \\themes\\next\\_config.yml，搜索 busuanzi_count，把 enable 设置为 true。 12345678910# Show Views / Visitors of the website / page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzibusuanzi_count: enable: true total_visitors: true total_visitors_icon: fa fa-user total_views: true total_views_icon: fa fa-eye post_views: true post_views_icon: fa fa-eye 同样是在 next 主题配置文件 \\themes\\next\\_config.yml 下，搜索 footer，在它底下添加 counter，设值为 true。 12345678910111213141516171819202122232425262728293031footer: # Specify the date when the site was setup. If not defined, current year will be used. #since: 2015 # Icon between year and copyright info. icon: # Icon name in Font Awesome. See: https://fontawesome.com/icons name: fa fa-heart # If you want to animate the icon, set it to true. animated: false # Change the color of icon, using Hex Code. color: &quot;#ff0000&quot; # If not defined, `author` from Hexo `_config.yml` will be used. copyright: # Powered by Hexo &amp; NexT powered: true # Beian ICP and gongan information for Chinese users. See: https://beian.miit.gov.cn, http://www.beian.gov.cn beian: enable: false icp: # The digit in the num of gongan beian. gongan_id: # The full num of gongan beian. gongan_num: # The icon for gongan beian. See: http://www.beian.gov.cn/portal/download gongan_icon_url: counter: true 来到 themes\\next\\layout\\_partials，找到 footer.swig 文件，打开编辑，在底下添加代码。 123&#123;% if theme.footer.counter %&#125; &lt;script async src=&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 站点访客数、访问次数显示在网址底部，文章阅读次数在文章开头。 在博客中添加图片md 文件中插入图片的语法为：![]()。 其中，方括号是图片描述，圆括号是图片路径。一般来说有三种图片路径，分别是相对路径，绝对路径和网络路径。 相对而言，使用相对路径会更加方便，设置如下： 安装 hexo-renderer-marked 插件 1$ npm install hexo-renderer-marked 修改根目录下的 _config.yaml 配置 将： 1post_asset_folder: false 修改为： 1234post_asset_folder: truemarked: prependRoot: true postAsset: true 设置 Typora 点击文件 → 偏好设置，设置如下： 这样，在粘贴图片到文件中时，会自动将图片复制到 source\\_posts 目录下，与 .md 文件同名的目录中。 之后，在将博客发布时，会自动上传到文章所生成的静态网页同级目录之下。","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}],"categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"ceph","slug":"ceph","permalink":"http://example.com/tags/ceph/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://example.com/tags/hadoop/"},{"name":"movie","slug":"movie","permalink":"http://example.com/tags/movie/"},{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"},{"name":"database","slug":"database","permalink":"http://example.com/tags/database/"},{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"tool","slug":"tool","permalink":"http://example.com/tags/tool/"},{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"},{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"},{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"Poetry and Prose","slug":"Poetry-and-Prose","permalink":"http://example.com/tags/Poetry-and-Prose/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}