{"meta":{"title":"XiSun的博客","subtitle":null,"description":"心如止水者，虽世间繁华之红尘纷扰，已然空无一物","author":"XiSun","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"Kafka Producer源码","slug":"kafka-producer","date":"2020-10-26T06:15:52.000Z","updated":"2020-10-27T07:32:46.897Z","comments":true,"path":"2020/10/26/kafka-producer/","link":"","permalink":"http://example.com/2020/10/26/kafka-producer/","excerpt":"","text":"先来看一段创建KafkaProducer的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class KafkaProducerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // bootstrap.servers 必须设置 props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.239.131:9092&quot;); // key.serializer 必须设置 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // value.serializer 必须设置 props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // client.id props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;client-0&quot;); // retries props.put(ProducerConfig.RETRIES_CONFIG, 3); // acks props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;); // max.in.flight.requests.per.connection props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1); // linger.ms props.put(ProducerConfig.LINGER_MS_CONFIG, 100); // batch.size props.put(ProducerConfig.BATCH_SIZE_CONFIG, 10240); // buffer.memory props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10240); KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props); // 指定topic，key，value ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;test1&quot;, &quot;key1&quot;, &quot;value1&quot;); // 异步发送 kafkaProducer.send(record, (recordMetadata, exception) -&gt; &#123; if (exception != null) &#123; // 发送失败的处理逻辑 exception.printStackTrace(); &#125; else &#123; // 发送成功的处理逻辑 System.out.println(recordMetadata.topic()); &#125; &#125;); // 同步发送 // kafkaProducer.send(record).get(); // 关闭Producer kafkaProducer.close(); &#125;&#125; 1. 主要流程图 简要说明： 1.new KafkaProducer()后，创建一个后台线程KafkaThread(实际运行线程是Sender，KafkaThread是对Sender的封装)扫描RecordAccumulator中是否有消息； 2.调用KafkaProducer.send()发送消息，实际是将消息保存到RecordAccumulator中，实际上就是保存到一个Map中(ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)，这条消息会被记录到同一个记录批次(相同主题相同分区算同一个批次)里面，这个批次的所有消息会被发送到相同的主题和分区上； 3.后台的独立线程扫描到RecordAccumulator中有消息后，会将消息发送到Kafka集群中(不是一有消息就发送，而是要看消息是否ready)； 4.如果发送成功(消息成功写入Kafka)，就返回一个RecordMetaData对象，它包换了主题和分区信息，以及记录在分区里的偏移量等信息； 5.如果写入失败，就会返回一个错误，生产者在收到错误之后会尝试重新发送消息(如果允许的话，此时会将消息在保存到RecordAccumulator中)，达到重试次数之后如果还是失败就返回错误消息。 2. 缓存器的创建123456789101112this.accumulator = new RecordAccumulator(logContext, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), this.compressionType, lingerMs(config), retryBackoffMs, deliveryTimeoutMs, metrics, PRODUCER_METRIC_GROUP_NAME, time, apiVersions, transactionManager, new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME)); 3. 后台线程的创建12345678910111213141516171819202122this.sender = newSender(logContext, kafkaClient, this.metadata);String ioThreadName = NETWORK_THREAD_PREFIX + &quot; | &quot; + clientId;this.ioThread = new KafkaThread(ioThreadName, this.sender, true);this.ioThread.start();KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient( new Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), this.metrics, time, &quot;producer&quot;, channelBuilder, logContext), metadata, clientId, maxInflightRequests, producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG), producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG), producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG), producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG), requestTimeoutMs, ClientDnsLookup.forConfig(producerConfig.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)), time, true, apiVersions, throttleTimeSensor, logContext); 上述代码中，构造了一个KafkaClient负责和broker通信，同时构造一个Sender并启动一个异步线程，这个线程会被命名为：kafka-producer-network-thread | $&#123;clientId&#125;，如果你在创建producer的时候指定client.id的值为myclient，那么线程名称就是kafka-producer-network-thread | myclient。 4. 发送消息(缓存消息)发送消息有同步发送以及异步发送两种方式，我们一般不使用同步发送，毕竟太过于耗时，使用异步发送的时候可以指定回调函数，当消息发送完成的时候(成功或者失败)会通过回调通知生产者。 同步send： 123public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record) &#123; return send(record, null);&#125; 异步send： 12345public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; // intercept the record, which can be potentially modified; this method does not throw exceptions ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record); return doSend(interceptedRecord, callback);&#125; 可以看出，同步和异步实际上调用的是同一个方法，同步发送时，设置回调函数为null。 消息发送之前，会先对key和value进行序列化： 12345678910111213141516byte[] serializedKey;try &#123; serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert key of class &quot; + record.key().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in key.serializer&quot;, cce);&#125;byte[] serializedValue;try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert value of class &quot; + record.value().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in value.serializer&quot;, cce);&#125; 计算分区： 1int partition = partition(record, serializedKey, serializedValue, cluster); 发送消息实际上是将消息缓存起来，核心代码如下： 12RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs); RecordAccumulator的核心数据结构是ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;，会将相同topic相同partition的数据放到一个Deque(双向队列)中，这也是我们之前提到的同一个记录批次里面的消息会发送到同一个主题和分区的意思。append()方法的核心源码如下： 123456789101112131415161718192021// 从batchs(ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)中，根据主题分区获取对应的队列，如果没有则new ArrayDeque&lt;&gt;返回Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);// 计算同一个记录批次占用空间大小，batchSize根据batch.size参数决定int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound( maxUsableMagic, compression, key, value, headers));// 为同一个topic，partition分配buffer，如果同一个记录批次的内存不足，那么会阻塞maxTimeToBlock(max.block.ms参数)这么长时间ByteBuffer buffer = free.allocate(size, maxTimeToBlock);synchronized (dq) &#123; // 创建MemoryRecordBuilder，通过buffer初始化appendStream(DataOutputStream)属性 MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic); ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds()); // 将key，value写入到MemoryRecordsBuilder中的appendStream(DataOutputStream)中 FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds())); // 将需要发送的消息放入到队列中 dq.addLast(batch);&#125; 5. 发送消息到Kafka上面已经将消息存储RecordAccumulator中去了，现在看看怎么发送消息。前面提到创建KafkaProducer的时候，会启动一个异步线程去从RecordAccumulator中取得消息然后发送到Kafka，发送消息的核心代码在Sender.java中，它实现了Runnable接口并在后台一直运行处理发送请求并将消息发送到合适的节点，直到KafkaProducer被关闭。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata * requests to renew its view of the cluster and then sends produce requests to the appropriate nodes. */public class Sender implements Runnable &#123; /** * The main run loop for the sender thread */ public void run() &#123; // main loop, runs until close is called while (running) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // okay we stopped accepting requests but there may still be // requests in the transaction manager, accumulator or waiting for acknowledgment, // wait until these are completed. while (!forceClose &amp;&amp; ((this.accumulator.hasUndrained() || this.client.inFlightRequestCount() &gt; 0) || hasPendingTransactionalRequests())) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // Abort the transaction if any commit or abort didn&#x27;t go through the transaction manager&#x27;s queue while (!forceClose &amp;&amp; transactionManager != null &amp;&amp; transactionManager.hasOngoingTransaction()) &#123; if (!transactionManager.isCompleting()) &#123; log.info(&quot;Aborting incomplete transaction due to shutdown&quot;); transactionManager.beginAbort(); &#125; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; if (forceClose) &#123; // We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on // the futures. if (transactionManager != null) &#123; log.debug(&quot;Aborting incomplete transactional requests due to forced shutdown&quot;); transactionManager.close(); &#125; log.debug(&quot;Aborting incomplete batches due to forced shutdown&quot;); this.accumulator.abortIncompleteBatches(); &#125; &#125;&#125; KafkaProducer的关闭方法有2个：close()以及close(Duration timeout)，close(long timeout, TimeUnit timUnit)已被弃用，其中timeout参数的意思是等待生产者完成任何待处理请求的最长时间，第一种方式的timeout为Long.MAX_VALUE毫秒，如果采用第二种方式关闭，当timeout=0的时候则表示强制关闭，直接关闭Sender(设置running=false)。 Send.java中，runOnce()方法，跳过对transactionManager的处理，查看发送消息的主要流程： 123456long currentTimeMs = time.milliseconds();// 将记录批次转移到每个节点的生产请求列表中long pollTimeout = sendProducerData(currentTimeMs);// 轮询进行消息发送client.poll(pollTimeout, currentTimeMs); 首先，查看sendProducerData(currentTimeMs)方法，它的核心逻辑在sendProduceRequest(batches, now)方法中： 123456789101112131415161718192021222324252627282930313233for (ProducerBatch batch : batches) &#123; TopicPartition tp = batch.topicPartition; // 将ProducerBatch中MemoryRecordsBuilder转换为MemoryRecords(发送的数据就在这里面) MemoryRecords records = batch.records(); // down convert if necessary to the minimum magic used. In general, there can be a delay between the time // that the producer starts building the batch and the time that we send the request, and we may have // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use // the new message format, but found that the broker didn&#x27;t support it, so we need to down-convert on the // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may // not all support the same message format version. For example, if a partition migrates from a broker // which is supporting the new magic version to one which doesn&#x27;t, then we will need to convert. if (!records.hasMatchingMagic(minUsedMagic)) records = batch.records().downConvert(minUsedMagic, 0, time).records(); produceRecordsByPartition.put(tp, records); recordsByPartition.put(tp, batch);&#125;ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout, produceRecordsByPartition, transactionalId);// 消息发送完成时的回调(消息发送失败后，在handleProduceResponse中处理)RequestCompletionHandler callback = new RequestCompletionHandler() &#123; public void onComplete(ClientResponse response) &#123; handleProduceResponse(response, recordsByPartition, time.milliseconds()); &#125;&#125;;// 根据参数构造ClientRequest，此时需要发送的消息在requestBuilder中ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, requestTimeoutMs, callback);// 将clientRequest转换成Send对象(Send.java，包含了需要发送数据的buffer)，给KafkaChannel设置该对象，记住这里还没有发送数据client.send(clientRequest, now); 在没有指定kafkaclient时，client.send(clientRequest, now)方法，实际就是NetworkClient.send(ClientRequest request, long now)方法，所有的请求(无论是producer发送消息的请求，还是获取metadata的请求)都是通过该方法设置对应的Send对象： 1Send send = request.toSend(destination, header); 需要知道的是，上面只是设置了发送消息所需要准备的内容。 接下来，查看client.poll(pollTimeout, currentTimeMs)方法，进入到发送消息的主流程，发送消息的核心代码最终可以定位到Selector.java的pollSelectionKeys(Set&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos)方法中，代码如下： 12345678910111213141516/* if channel is ready write to any sockets that have space in their buffer and for which we have data */if (channel.ready() &amp;&amp; key.isWritable() &amp;&amp; !channel.maybeBeginClientReauthentication( () -&gt; channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos)) &#123; Send send; try &#123; // 底层实际调用的是java8 GatheringByteChannel的write方法 send = channel.write(); &#125; catch (Exception e) &#123; sendFailed = true; throw e; &#125; if (send != null) &#123; this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); &#125;&#125; 就这样，我们的消息就发送到了broker中了，发送流程分析完毕，注意，这个是完美发送的情况。但是总会有发送失败的时候(消息过大或者没有可用的leader等)，那么发送失败后重发又是在哪里完成的呢？还记得上面的回调函数吗，没错，就是在回调函数这里设置的，先来看下回调函数源码： 12345678910111213141516171819202122232425262728293031323334/** * Handle a produce response */private void handleProduceResponse(ClientResponse response, Map&lt;TopicPartition, ProducerBatch&gt; batches, long now) &#123; RequestHeader requestHeader = response.requestHeader(); long receivedTimeMs = response.receivedTimeMs(); int correlationId = requestHeader.correlationId(); if (response.wasDisconnected()) &#123; // 如果是网络断开则构造Errors.NETWORK_EXCEPTION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION), correlationId, now, 0L); &#125; else if (response.versionMismatch() != null) &#123; // 如果是版本不匹配，则构造Errors.UNSUPPORTED_VERSION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.UNSUPPORTED_VERSION), correlationId, now, 0L); &#125; else &#123; // if we have a response, parse it(如果存在response就返回正常的response) if (response.hasResponse()) &#123; ProduceResponse produceResponse = (ProduceResponse) response.responseBody(); for (Map.Entry&lt;TopicPartition, ProduceResponse.PartitionResponse&gt; entry : produceResponse.responses().entrySet()) &#123; TopicPartition tp = entry.getKey(); ProduceResponse.PartitionResponse partResp = entry.getValue(); ProducerBatch batch = batches.get(tp); completeBatch(batch, partResp, correlationId, now, receivedTimeMs + produceResponse.throttleTimeMs()); &#125; this.sensors.recordLatency(response.destination(), response.requestLatencyMs()); &#125; else &#123; // this is the acks = 0 case, just complete all requests(如果acks=0，那么则构造Errors.NONE的响应，因为这种情况只需要发送不需要响应结果) for (ProducerBatch batch : batches.values()) &#123; completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NONE), correlationId, now, 0L); &#125; &#125; &#125;&#125; 在completeBatch()方法中我们主要关注失败的逻辑处理，核心源码如下： 12345678910111213141516171819202122232425262728293031/** * Complete or retry the given batch of records. * * @param batch The record batch * @param response The produce response * @param correlationId The correlation id for the request * @param now The current POSIX timestamp in milliseconds */private void completeBatch(ProducerBatch batch, ProduceResponse.PartitionResponse response, long correlationId, long now, long throttleUntilTimeMs) &#123; Errors error = response.error; if (error == Errors.MESSAGE_TOO_LARGE &amp;&amp; batch.recordCount &gt; 1 &amp;&amp; !batch.isDone() &amp;&amp; (batch.magic() &gt;= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) &#123; // If the batch is too large, we split the batch and send the split batches again. We do not decrement // the retry attempts in this case.(如果发送的消息太大，需要重新进行分割发送) this.accumulator.splitAndReenqueue(batch); maybeRemoveAndDeallocateBatch(batch); this.sensors.recordBatchSplit(); &#125; else if (error != Errors.NONE) &#123; // 发生了错误，如果此时可以retry(retry次数未达到限制以及产生的异常是RetriableException) if (canRetry(batch, response, now)) &#123; if (transactionManager == null) &#123; // 把需要重试的消息放入队列中，等待重试，实际就是调用deque.addFirst(batch) reenqueueBatch(batch, now); &#125; ... &#125; ... &#125;&#125; 以上，就是Producer发送消息的流程。 6. 补充：分区算法在发送消息前，调用的计算分区方法如下： 123456789101112/** * computes partition for given record. * if the record has partition returns the value otherwise * calls configured partitioner class to compute the partition. */private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition : partitioner.partition( record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; 如果在创建ProducerRecord的时候，指定了partition，则使用指定的，否则调用配置的partitioner类来计算分区。 如果没有配置自定义的分区器，Kafka默认使用org.apache.kafka.clients.producer.internals.DefaultPartitioner，源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * The default partitioning strategy: * &lt;ul&gt; * &lt;li&gt;If a partition is specified in the record, use it * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key * &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion */public class DefaultPartitioner implements Partitioner &#123; private final ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = new ConcurrentHashMap&lt;&gt;(); public void configure(Map&lt;String, ?&gt; configs) &#123;&#125; /** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; // 如果key为null，则使用Round Robin算法 int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition(根据key进行散列，使用murmur2算法) return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125; private int nextValue(String topic) &#123; AtomicInteger counter = topicCounterMap.get(topic); if (null == counter) &#123; counter = new AtomicInteger(ThreadLocalRandom.current().nextInt()); AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter); if (currentCounter != null) &#123; counter = currentCounter; &#125; &#125; return counter.getAndIncrement(); &#125; public void close() &#123;&#125;&#125; DefaultPartitioner中对于分区的算法有两种情况： 1.如果键值为null，那么记录键随机地发送到主题内各个可用的分区上。分区器使用轮询(Round Robin)算法键消息均衡地分布到各个分区上。 2.如果键不为null，那么Kafka会对键进行散列(使用Kafka自己的散列算法，即使升级java版本，散列值也不会发生变化)，然后根据散列值把消息映射到特定的分区上。应该注意的是，同一个键总是被映射到同一个分区上(如果分区数量发生了变化则不能保证)，映射的时候会使用主题所有的分区，而不仅仅是可用分区，所以如果写入数据分区是不可用的，那么就会发生错误，当然这种情况很少发生。 当然，如果你想要实现自定义分区，那么只需要实现Partitioner接口即可： 123456789101112131415161718192021222324/** * 将key的hash值，对分区总数取余，以确定消息发送到哪个分区 */public class KeyPartitioner implements Partitioner &#123; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; Integer numPartitions = cluster.partitionCountForTopic(topic); if (keyBytes == null) &#123; throw new InvalidRecordException(&quot;key can not be null&quot;); &#125; return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 7. 本文参考https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系&#119;&#x64;&#x73;&#104;&#x66;&#117;&#x74;&#x40;&#49;&#54;&#x33;&#46;&#x63;&#111;&#x6d;","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"什么是Kafka","slug":"kafka-introduce","date":"2020-10-23T07:58:39.000Z","updated":"2020-10-27T07:36:13.467Z","comments":true,"path":"2020/10/23/kafka-introduce/","link":"","permalink":"http://example.com/2020/10/23/kafka-introduce/","excerpt":"","text":"1. 分布式分布式系统由多个运行的计算机系统组成，所有这些计算机在一个集群中一起工作，对终端用户来讲只是一个单一节点。 Kafka也是分布式的，因为它在不同的节点(又被称为broker)上存储，接受以及发送消息，这样做的好处是具有很高的可扩展性和容错性。 2. 水平可扩展性在这之前，先看看什么是垂直可扩展，比如你有一个传统的数据库服务器，它开始过度负载，解决这个问题的办法就是给服务器加配置(cpu，内存，SSD)，这就叫做垂直扩展。但是这种方式存在两个巨大的劣势： 1.硬件存在限制，不可能无限的添加机器配置 2.它需要停机时间，通常这是很多公司无法容忍的 水平可扩展就是通过添加更多的机器来解决同样的问题，添加新机器不需要停机，而且集群中也不会对机器的数量有任何的限制。但问题在于并非所有系统都支持水平可伸缩性，因为它们不是设计用于集群中(在集群中工作会更加复杂)。 3. 容错性非分布式系统中容易最致命的问题就是单点失败，如果你唯一的服务器挂掉了，那么我相信你会很崩溃。 而分布式系统的设计方式就是可以以配置的方式来容许失败。比如在5个节点的Kafka集群中，即使其中两个节点挂掉了，你仍然可以继续工作。 需要注意的是，容错与性能直接相关，你的系统容错程度越高，性能就越差。 4. 提交日志(commit log)提交日志(也被称为预写日志或者事物日志)是仅支持附加的持久有序数据结构，你无法修改或者删除记录，它从左往右读并且保证日志的顺序。 是不是觉得Kafka的数据结构如此简单? 是的，从很多方面来讲，这个数据结构就是Kafka的核心。这个数据结构的记录是有序的，而有序的数据可以确保我们的处理流程。这两个在分布式系统中都是极其重要的问题。 Kafka实际上将所有消息存储到磁盘并在数据结构中对它们进行排序，以便利用顺序磁盘读取。 1.读取和写入都是常量时间O(1)(当确定了record id)，与磁盘上其他结构的O(log N)操作相比是一个巨大的优势，因为每个磁盘搜索都很耗时。 2.读取和写入不会相互影响，写不会锁住读，反之亦然。 这两点有着巨大的优势， 因为数据大小与性能完全分离。无论你的服务器上有100KB还是100TB的数据，Kafka都具有相同的性能。 5. 如何工作生产者消费者模式：生产者(producer)发送消息(record)到Kafka服务器(broker)，这些消息存储在主题(topic)中，然后消费者(consumer)订阅该主题，接受新消息后并进行处理。 随着消息的越来越多，topic也会越来越大，为了获得更好的性能和可伸缩性，可以在topic下建立多个更小的分区(partition)，在发送消息时，可以根据实际情况，对消息进行分类，同一类的消息发送到同一个partition(比如存储不同用户发送的消息，可以根据用户名的首字母进行分区匹配)。Kafka保证partition内的所有消息都按照它们的顺序排序，区分特定消息的方式是通过其偏移量(offset)，你可以将其视为普通数组索引，即为分区中的每个新消息递增的序列号。 Kafka遵守着愚蠢的broker和聪明的consumer的准则。这意味着Kafka不会跟踪消费者读取了哪些记录并删除它们，而是会将它们存储一定的时间(比如1天，以log.retention开头的来决定日志保留时间)，直到达到某个阈值。消费者自己轮询Kafka的新消息并且告诉它自己想要读取哪些记录，这允许它们按照自己的意愿递增/递减它们所处的偏移量，从而能够重放和重新处理事件。 需要注意的是消费者是属于消费者组的(在创建consumer时，必须指定其所属的消费者组的group.id)，消费者组有一个或多个消费者。为了避免两个进程读取同样的消息两次，每个partition只能被一个消费者组中的一个消费者访问。 6. 持久化到硬盘正如之前提到的，Kafka实际上是将所有记录存储到硬盘而不在RAM中保存任何内容，这背后有很多优化使得这个方案可行。 1.Kafka有一个将消息分组的协议，这允许网络请求将消息组合在一起并减少网络开销，服务器反过来一次性保留大量消息，消费者一次获取大量线性块。 2.磁盘上线性读写非常快，现代磁盘非常慢的原因是由于大量磁盘寻址，但是在大量的线性操作中不是问题。 3.操作系统对线性操作进行了大量优化，通过预读(预取大块多次)和后写(将小型逻辑写入组成大型物理写入)技术。 4.操作系统将磁盘文件缓存在空闲RAM中。这称为page cache，而Kafka的读写都大量使用了page cache： ​ ① 写消息的时候消息先从java到page cache，然后异步线程刷盘，消息从page cache刷入磁盘； ​ ② 读消息的时候先从page cache找，有就直接转入socket，没有就先从磁盘load到page cache，然后直接从socket发出去。 5.由于Kafka在整个流程(producer - &gt;broker - &gt;consumer)中以未经修改的标准化二进制格式存储消息，因此它可以使用零拷贝优化。那时操作系统将数据从page cache直接复制到socket，有效地完全绕过了Kafka broker。 所有这些优化都使Kafka能够以接近网络的速度传递消息。 7. 数据分发和复制下面来谈谈Kafka如何实现容错以及它如何在节点之间分配数据。 为了使得一个broker挂掉的时候，数据还能得以保留，分区(partition)数据在多个broker中复制。 在任何时候，一个broker拥有一个partition，应用程序读取/写入都要通过这个节点，这个节点叫做—partition leader。它将收到的数据复制到N个其他broker，这些接收数据的broker叫做follower，follower也存储数据，一旦leader节点死掉的时候，它们就准备竞争上岗成为leader。 这可以保证你成功发布的消息不会丢失，通过选择更改副本因子，你可以根据数据的重要性来交换性能以获得更强的持久性保证。 这样如果leader挂掉了，那么其中一个follower就会接替它称为leader。包括leader在内的总副本数就是副本因子(创建topic时，使用–replication-factor参数指定)，上图有1个leader，2个follower，所以副本因子就是3。 但是你可能会问：producer或者consumer怎么知道partition leader是谁？ 对生产者/消费者对分区的写/读请求，它们需要知道分区的leader是哪一个，对吧？这个信息肯定是可以获取到的，Kafka使用ZooKeeper来存储这些元数据。 8. 什么是ZooKeeperZooKeeper是一个分布式键值存储。它针对读取进行了高度优化，但写入速度较慢。它最常用于存储元数据和处理集群的机制(心跳，分发更新/配置等)。 它允许服务的客户(Kafka broker)订阅并在发生变更后发送给他们，这就是Kafka如何知道何时切换分区领导者。ZooKeeper本身维护了一个集群，所以它就有很高的容错性，当然它也应该具有，毕竟Kafka很大程度上是依赖于它的。 ZooKeeper用于存储所有的元数据信息，包括但不限于如下几项： 消费者组每个分区的偏移量(现在客户端在单独的Kafka topic上存储偏移量) ACL —— 权限控制 生产者/消费者的流量控制——每秒生产/消费的数据大小。可以参考：Kafka-流量控制Quota功能 partition leader以及它们的健康信息 那么producer/consumer是如何知道谁是partition leader的呢？ 生产者和消费者以前常常直接连接ZooKeeper来获取这些信息，但是Kafka从0.8和0.9版本开始移除了这种强耦合关系。客户端直接从Kafka broker获取这些元数据，而让Kafka broker从ZooKeeper那里获取这些元数据。 更多ZooKeeper的讲解可以参考：漫画：什么是ZooKeeper？ 9. 流式处理(Streaming)在Kafka中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理并生成数据流以输出到其他主题(或者外部服务，数据库，容器等等)。 什么是数据流呢？首先，数据流是无边界数据集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的，是因为随着时间推移，新的记录会不断加入进来。比如信用卡交易，股票交易等事件都可以用来表示数据流。 我们可以使用producer/consumer的API直接进行简单处理，但是对于更加复杂的转换，比如将流连接到一起，Kafka提供了集成Stream API库。 这个API是在你自己的代码中使用的，它并不是运行在broker上，它的工作原理和consumer API类似，可帮助你在多个应用程序(类似于消费者组)上扩展流处理工作。 1) 无状态处理流的无状态处理是确定性处理，其不依赖于任何外部条件，对于任何给定的数据，将始终生成与其他任何内容无关的相同输出。举个例子，我们要做一个简单的数据转换—“zhangsan” → “Hello,zhangsan” 2) 流-表二义性重要的是要认识到流和表实质上是一样的，流可以被解释称为表，表也可以被解释称为流。 流作为表流可以解释为数据的一系列更新，聚合后的结果就是表的最终结果，这项技术被称为事件溯源(Event Sourcing)。 如果你了解数据库备份同步，你就会知道它们的技术实现被称为流式复制—将对表的每个更改都发送报副本服务器。比如redis中的AOF以及Mysql中的binlog。 Kafka流可以用相同的方式解释 - 当累积形成最终状态时的事件。此类流聚合保存在本地RocksDB中(默认情况下)，被称为KTable。 表作为流可以将表视为流中每个键的最新值的快照。与流记录可以生成表一样，表更新可以生成更改日志流。 3) 有状态处理我们在java中常用的一些操作比如map()或者filter()是没有状态的，它不会要求你保留任何原始数据。但是现实中，大多数的操作都是有状态的(比如count())，因为这需要你存储当前累计的状态。 在流处理器上维护状态的问题是流处理器可能会失败！你需要在哪里保持这种状态才能容错？ 一种简单的方法是简单地将所有状态存储在远程数据库中，并通过网络连接到该存储，这样做的问题是大量的网络带宽会使得你的应用程序变慢。一个更微妙但重要的问题是你的流处理作业的正常运行时间将与远程数据库紧密耦合，并且作业将不是自包含的(其他team更改数据库可能会破坏你的处理)。 那么什么是更好的办法呢？ 回想一下表和流的二元性。这允许我们将流转换为与我们的处理位于同一位置的表。它还为我们提供了一种处理容错的机制—通过将流存储在Kafka broker中。 流处理器可以将其状态保持在本地表(例如RocksDB)中，该表将从输入流(可能在某些任意转换之后)更新。当进程失败时，它可以通过重放流来恢复其数据。 你甚至可以将远程数据库作为流的生产者，有效地广播用于在本地重建表的更改日志。 4) KSQL通常，我们不得不使用JVM语言编写流处理，因为这是唯一的官方Kafka Streams API客户端。2018年4月，KSQL作为一项新特性被发布，它允许你使用熟悉的类似SQL的语言编写简单的stream jobs。你安装了KSQL服务器并通过CLI以交互方式查询以及管理。它使用相同的抽象(KStream和KTable)，保证了Streams API的相同优点(可伸缩性，容错性)，并大大简化了流的工作。 这听起来可能不是很多，但在实践中对于测试内容更有用，甚至允许开发之外的人(例如产品所有者)使用流处理，可以看看Confluent提供的这篇关于ksql的使用。 10. 什么时候使用kafka正如我们已经介绍的那样，Kafka允许你通过集中式介质获取大量消息并存储它们，而不必担心性能或数据丢失等问题。 这意味着它非常适合用作系统架构的核心，充当连接不同应用程序的集中式媒体。Kafka可以成为事件驱动架构的中心部分，使你可以真正地将应用程序彼此分离。 Kafka允许你轻松地分离不同(微)服务之间的通信。使用Streams API，现在可以比以往更轻松地编写业务逻辑，从而丰富Kafka主题数据以供服务使用。可能性很大，我恳请你探讨公司如何使用Kafka。 11. 总结Apache Kafka是一个分布式流媒体平台，每天可处理数万亿个事件。Kafka提供低延迟，高吞吐量，容错的发布和订阅管道，并能够处理事件流。我们回顾了它的基本语义(生产者，代理，消费者，主题)，了解了它的一些优化(page cache)，通过复制数据了解了它的容错能力，并介绍了它不断增长的强大流媒体功能。Kafka已经在全球数千家公司中大量采用，其中包括财富500强企业中的三分之一。随着Kafka的积极开发和最近发布的第一个主要版本1.0(2017年11月1日)，有预测这个流媒体平台将会与关系数据库一样，是数据平台的重要核心。我希望这篇介绍能帮助你熟悉Apache Kafka。 12. 本文参考http://generalthink.github.io/2019/02/27/introduction-of-kafka/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系&#x77;&#100;&#115;&#104;&#102;&#117;&#x74;&#64;&#x31;&#x36;&#x33;&#46;&#x63;&#x6f;&#109;","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"使用hexo搭建github博客","slug":"hexo-blog","date":"2020-10-23T03:33:51.000Z","updated":"2020-10-27T07:35:51.032Z","comments":true,"path":"2020/10/23/hexo-blog/","link":"","permalink":"http://example.com/2020/10/23/hexo-blog/","excerpt":"","text":"1. 使用工具版本默认已经安装node.js和git。 123git version: git version 2.27.0.windows.1npm version: 6.14.7hexo version: 4.2.0 2. git客户端与github建立SSH连接1Please make sure you have the correct access rights and the repository exists. 当git客服端出现以上提示时，说明SSH连接过期，需要重新建立连接。参考如下方式： 1.先查看下name和email 123456# 查看user的name和email$ git config user.name$ git config user.email# 如果没设置，按如下命令设置$ git config --global user.name &#123;$yourname&#125;$ git config --global user.email &#123;$youremail&#125; 2.删除.ssh文件夹下的known_hosts，路径为：C:\\Users\\{$userrname}\\.ssh 3.git bash输入命令 1$ ssh-keygen -t rsa -C &#123;$youremail&#125; 一直按回车，等结束后，.ssh文件夹下会生成两个文件：id_rsa和id_rsa.pub，将id_rsa.pub的内容全部复制。 4.登录个人github账户，进入Settings → SSH and GPG keys，点击New SSH key，将复制的内容粘贴到Key里，点击Add SSH key 5.git bash输入命令 1$ ssh -T git@github.com 在弹出的确定对话框输入：yes 3. hexo安装在git bash中输入以下命令： 123456$ npm install hexo-cli -g$ cd f: # 可以是任何路径$ hexo init blog$ cd blog # 进入blog目录$ npm install$ npm install hexo-deployer-git --save 命令执行完成后，会在F:\\目录下，多一个blog文件夹。 4. 修改_config.yml文件修改blog根目录下的_config.yml文件，将deploy节点修改为如下内容： 1234deploy: type: git repo: git@github.com:&#123;$yourname&#125;/&#123;$yourname&#125;.github.io.git branch: master 说明：_config.yml文件的配置均为[key: value]形式，value前面必须要有一个空格。 然后在git bash中输入以下命令，发布博客： 1$ hexo deploy 5. 访问自己的博客博客地址：https://{$yourname}.github.io/ 6. 写一个自己的博客hexo的项目结构是在网站根目录的source/_posts目录下存放你的博客文档，以.md文档格式存储，默认已存在一个hello-world.md文章。 1.新建文章 1$ hexo new &lt;title&gt; 会在blog的source\\_posts目录下，新建一个名叫&lt;title&gt;.md文章，如： 12INFO Validating configINFO Created: F:\\blog\\source\\_posts\\tesss.md 之后，在文章中添加自己的内容即可，建议使用Typora编辑，参考：Typora入门 2.发布文章 1234$ hexo clean # 清楚缓存$ hexo generate # 生成静态页面$ hexo server # 本地发布，浏览器输入localhost:4000即可访问博客$ hexo deploy # 将public中的静态页面复制到.deploy_git文件夹中，并提交到github 至此，你的第一个自己的博客发布完成。 说明：以上hexo的命令，都要在f:\\blog目录下执行。 7. 修改博客的themes如果向修改自己博客的themes，可以下载好想要的，然后拷贝到blog的themes目录下，然后修改_config.yml文件，将theme节点的值，修改为你下载好的themes的名称，如： 1theme: next 之后，再按照你下载的themes的使用说明，做相应修改即可。 参考：NexT的使用 8. 在博客中添加图片md文件中插入图片的语法为：![]() 其中，方括号是图片描述，圆括号是图片路径。一般来说有三种图片路径，分别是相对路径，绝对路径和网络路径。 相对而言，使用相对路径会更加方便，设置如下： 1.安装hexo-renderer-marked插件 1$ npm install hexo-renderer-marked 2.修改config.yaml配置 将： 1post_asset_folder: false 修改为： 1234post_asset_folder: truemarked: prependRoot: true postAsset: true 3.设置Typora 点击文件 → 偏好设置，设置如下： 这样，在粘贴图片到文件中时，会自动将图片复制到source\\_posts目录下，与.md文件同名的目录中。 之后，在将博客发布时，会自动上传到文章所生成的静态网页同级目录之下。","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}],"categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}