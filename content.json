{"meta":{"title":"XiSun的博客","subtitle":"Learning is endless","description":"心如止水者，虽世间繁华之红尘纷扰，已然空无一物","author":"XiSun","url":"http://example.com","root":"/"},"pages":[{"title":"tags","date":"2020-10-27T08:35:56.000Z","updated":"2020-10-27T08:40:16.641Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"maven 的配置文件","slug":"maven-configfiles","date":"2021-01-23T02:30:26.000Z","updated":"2021-01-27T02:31:46.290Z","comments":true,"path":"2021/01/23/maven-configfiles/","link":"","permalink":"http://example.com/2021/01/23/maven-configfiles/","excerpt":"","text":"settings.xmlsettings.xml是Maven的全局配置文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Licensed to the Apache Software Foundation (ASF) under oneor more contributor license agreements. See the NOTICE filedistributed with this work for additional informationregarding copyright ownership. The ASF licenses this fileto you under the Apache License, Version 2.0 (the&quot;License&quot;); you may not use this file except in compliancewith the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing,software distributed under the License is distributed on an&quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANYKIND, either express or implied. See the License for thespecific language governing permissions and limitationsunder the License.--&gt;&lt;!-- | 官方文档：https://maven.apache.org/settings.html | | maven提供以下两种level的配置: | | 1. User Level. 当前用户独享的配置，通常在$&#123;user.home&#125;/.m2/settings.xml目录下。 | 可在CLI命令行中通过以下参数设置：-s /path/to/user/settings.xml | | 2. Global Level. 同一台计算机上的所有maven用户共享的全局配置。通常在$&#123;maven.home&#125;/conf/settings.xml目录下。 | 可在CLI命令行中通过以下参数设置：-gs /path/to/global/settings.xml | | 备注： | 优先级：User Level &gt; Global Level | 默认情况，$&#123;user.home&#125;/.m2目录下没有settings.xml文件，需手动复制$&#123;maven.home&#125;/conf/settings.xml。 |--&gt;&lt;!-- | This is the configuration file for Maven. It can be specified at two levels: | | 1. User Level. This settings.xml file provides configuration for a single user, | and is normally provided in $&#123;user.home&#125;/.m2/settings.xml. | | NOTE: This location can be overridden with the CLI option: | | -s /path/to/user/settings.xml | | 2. Global Level. This settings.xml file provides configuration for all Maven | users on a machine (assuming they&#x27;re all using the same Maven | installation). It&#x27;s normally provided in | $&#123;maven.conf&#125;/settings.xml. | | NOTE: This location can be overridden with the CLI option: | | -gs /path/to/global/settings.xml | | The sections in this sample file are intended to give you a running start at | getting the most out of your Maven installation. Where appropriate, the default | values (values used when the setting is not specified) are provided. | |--&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;!-- 本地仓库路径，默认值：$&#123;user.home&#125;/.m2/repository --&gt; &lt;!-- localRepositor y | The path to the local repository maven will use to store artifacts. | | Default: $&#123;user.home&#125;/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;localRepository&gt;D:\\Program Files\\Maven\\apache-maven-3.6.3-maven-repository&lt;/localRepository&gt; &lt;!-- 当maven需要输入值的时候，是否交由用户输入，默认为true；false情况下maven将根据使用配置信息进行填充。 --&gt; &lt;!-- interactiveMode | This will determine whether maven prompts you when it needs input. If set to false, | maven will use a sensible default value, perhaps based on some other setting, for | the parameter in question. | | Default: true &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; --&gt; &lt;!-- 是否支持联网进行artifact下载、部署等操作，默认false。 --&gt; &lt;!-- offline | Determines whether maven should attempt to connect to the network when executing a build. | This will have an effect on artifact downloads, artifact deployment, and others. | | Default: false &lt;offline&gt;false&lt;/offline&gt; --&gt; &lt;!-- | 搜索插件时，如果groupId没有显式提供时，则以此处配置的groupId为默认值， | 可以简单理解为默认导入这些groupId下的所有artifact(需要时才下载)。 | 默认情况下该列表包含了：org.apache.maven.plugins和org.codehaus.mojo。 | | 查看插件信息： | mvn help:describe -Dplugin=org.apache.maven.plugins:maven-compiler-plugin:3.5.1 -Ddetail |--&gt; &lt;!-- pluginGroups | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e. | when invoking a command line like &quot;mvn prefix:goal&quot;. Maven will automatically add the group identifiers | &quot;org.apache.maven.plugins&quot; and &quot;org.codehaus.mojo&quot; if these are not already contained in the list. |--&gt; &lt;pluginGroups&gt; &lt;!-- pluginGroup | Specifies a further group identifier to use for plugin lookup. | plugin 的 groupId &lt;pluginGroup&gt;com.your.plugins&lt;/pluginGroup&gt; --&gt; &lt;/pluginGroups&gt; &lt;!-- 用来配置不同的代理，多代理profiles可以应对笔记本或移动设备的工作环境：通过简单的设置profile id就可以很容易的更换整个代理配置。 --&gt; &lt;!-- proxies | This is a list of proxies which can be used on this machine to connect to the network. | Unless otherwise specified (by system property or command-line switch), the first proxy | specification in this list marked as active will be used. |--&gt; &lt;proxies&gt; &lt;!-- proxy | Specification for one proxy, to be used in connecting to the network. | | 代理元素包含配置代理时需要的信息 &lt;proxy&gt; | 代理的唯一定义符，用来区分不同的代理元素 &lt;id&gt;optional&lt;/id&gt; | 该代理是否是激活的那个。true则激活代理。当我们声明了一组代理，而某个时候只需要激活一个代理的时候，该元素就可以派上用处。 &lt;active&gt;true&lt;/active&gt; | 代理的协议 &lt;protocol&gt;http&lt;/protocol&gt; | 代理服务器认证的登录名 &lt;username&gt;proxyuser&lt;/username&gt; | 代理服务器认证登录密码 &lt;password&gt;proxypass&lt;/password&gt; | 代理的主机名 &lt;host&gt;proxy.host.net&lt;/host&gt; | 代理的端口 &lt;port&gt;80&lt;/port&gt; | 不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，使用逗号分隔也很常见。 &lt;nonProxyHosts&gt;local.net|some.host.com&lt;/nonProxyHosts&gt; &lt;/proxy&gt; --&gt; &lt;/proxies&gt; &lt;!-- 进行远程服务器访问时所需的授权配置信息。通过系统唯一的server-id进行唯一关联。 --&gt; &lt;!-- servers | This is a list of authentication profiles, keyed by the server-id used within the system. | Authentication profiles can be used whenever maven must make a connection to a remote server. |--&gt; &lt;servers&gt; &lt;!-- server | Specifies the authentication information to use when connecting to a particular server, identified by | a unique name within the system (referred to by the &#x27;id&#x27; attribute below). | | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are | used together. | | 方式一：使用用户名和密码 &lt;server&gt; | 当前server的id，该id与distributionManagement中repository元素的id相匹配。 &lt;id&gt;deploymentRepo&lt;/id&gt; | 鉴权用户名 &lt;username&gt;repouser&lt;/username&gt; | 鉴权密码 &lt;password&gt;repopwd&lt;/password&gt; &lt;/server&gt; --&gt; &lt;!-- Another sample, using keys to authenticate. | 方式二：使用私钥 &lt;server&gt; &lt;id&gt;siteServer&lt;/id&gt; | 鉴权时使用的私钥位置，默认是/home/hudson/.ssh/id_dsa。 &lt;privateKey&gt;/path/to/private/key&lt;/privateKey&gt; | 鉴权时使用的私钥密码，非必要，非必要时留空。 &lt;passphrase&gt;optional; leave empty if not used.&lt;/passphrase&gt; &lt;/server&gt; --&gt; &lt;!-- 实例：对应pom.xml文件中配置的id为ChemAxon Public Repository的仓库。 --&gt; &lt;server&gt; &lt;id&gt;ChemAxon Public Repository&lt;/id&gt; &lt;username&gt;huxiongfeng95@gmail.com&lt;/username&gt; &lt;password&gt;AKCp5dL3HsJftZjXR4wLS7UMnJvQL7oarx8sad8Wh21UV7xQUMmNcZ7TMEHaBVoSrM8jAv48Q&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;!-- | 从远程仓库下载artifacts时，用于替代指定远程仓库的镜像服务器配置； | 例如当无法连接上国外的仓库时，可以指定连接到国内的镜像服务器； | 私服的配置推荐用profile配置而不是mirror。 |--&gt; &lt;!-- mirrors | This is a list of mirrors to be used in downloading artifacts from remote repositories. | | It works like this: a POM may declare a repository to use in resolving certain artifacts. | However, this repository may have problems with heavy traffic at times, so people have mirrored | it to several places. | | That repository definition will have a unique id, so we can create a mirror reference for that | repository, to be used as an alternate download site. The mirror site will be the preferred | server for that repository. |--&gt; &lt;mirrors&gt; &lt;!-- | mirrors匹配顺序： | 多个mirror优先级：按照id字母顺序进行排列，即与编写的顺序无关。 | 在第一个mirror找不到artifact，不会继续查找下一个镜像。 | 只有当前一个mirror无法链接的时候，才会尝试链接下一个镜像，类似容灾备份。 |--&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;!-- maven中央仓库的aliyun镜像，maven中央仓库的id为central。 --&gt; &lt;mirror&gt; &lt;!-- 当前镜像的唯一标识符，id用来区分不同的mirror元素，同时会套用使用server中id相同授权配置链接到镜像。 --&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;!-- 镜像名称，无特殊作用，可视为简述。 --&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;!-- 镜像地址 --&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;!-- 被镜像的服务器的id，必须与repository节点设置的id一致。但是&quot;This must not match the mirror id&quot;。 | mirrorOf 的配置语法: | * = 匹配所有远程仓库。这样所有pom中定义的仓库都不生效。 | external:* = 匹配除localhost、使用file://协议外的所有远程仓库。 | repo1,repo2 = 匹配仓库repo1和repo2。 | *,!repo1 = 匹配所有远程仓库，repo1除外。 |--&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;!-- profiles | This is a list of profiles which can be activated in a variety of ways, and which can modify | the build process. Profiles provided in the settings.xml are intended to provide local machine- | specific paths and repository locations which allow the build to work in the local environment. | | For example, if you have an integration testing plugin - like cactus - that needs to know where | your Tomcat instance is installed, you can provide a variable here such that the variable is | dereferenced during the build process to configure the cactus plugin. | | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles | section of this document (settings.xml) - will be discussed later. Another way essentially | relies on the detection of a system property, either matching a particular value for the property, | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a | value of &#x27;1.4&#x27; might activate a profile when the build is executed on a JDK version of &#x27;1.4.2_07&#x27;. | Finally, the list of active profiles can be specified directly from the command line. | | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact | repositories, plugin repositories, and free-form properties to be used as configuration | variables for plugins in the POM. | |--&gt; &lt;profiles&gt; &lt;!-- profile | Specifies a set of introductions to the build process, to be activated using one or more of the | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles/&gt; | or the command line, profiles have to have an ID that is unique. | | An encouraged best practice for profile identification is to use a consistent naming convention | for profiles, such as &#x27;env-dev&#x27;, &#x27;env-test&#x27;, &#x27;env-production&#x27;, &#x27;user-jdcasey&#x27;, &#x27;user-brett&#x27;, etc. | This will make it more intuitive to understand what the set of introduced profiles is attempting | to accomplish, particularly when you only have a list of profile id&#x27;s for debug. | | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo. &lt;profile&gt; &lt;id&gt;jdk-1.4&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.4&lt;/jdk&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jdk14&lt;/id&gt; &lt;name&gt;Repository for JDK 1.4 builds&lt;/name&gt; &lt;url&gt;http://www.myhost.com/maven/jdk14&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshotPolicy&gt;always&lt;/snapshotPolicy&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; --&gt; &lt;!-- | Here is another profile, activated by the system property &#x27;target-env&#x27; with a value of &#x27;dev&#x27;, | which provides a specific path to the Tomcat instance. To use this, your plugin configuration | might hypothetically look like: | | ... | &lt;plugin&gt; | &lt;groupId&gt;org.myco.myplugins&lt;/groupId&gt; | &lt;artifactId&gt;myplugin&lt;/artifactId&gt; | | &lt;configuration&gt; | &lt;tomcatLocation&gt;$&#123;tomcatPath&#125;&lt;/tomcatLocation&gt; | &lt;/configuration&gt; | &lt;/plugin&gt; | ... | | NOTE: If you just wanted to inject this configuration whenever someone set &#x27;target-env&#x27; to | anything, you could just leave off the &lt;value/&gt; inside the activation-property. | &lt;profile&gt; &lt;id&gt;env-dev&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;target-env&lt;/name&gt; &lt;value&gt;dev&lt;/value&gt; &lt;/property&gt; &lt;/activation&gt; &lt;properties&gt; &lt;tomcatPath&gt;/path/to/tomcat/instance&lt;/tomcatPath&gt; &lt;/properties&gt; &lt;/profile&gt; --&gt; &lt;/profiles&gt; &lt;!-- | 手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 | 任何activeProfile，不论环境设置如何，其对应的profile都会被激活，maven会忽略无效(找不到)的profile。 |--&gt; &lt;!-- activeProfiles | List of profiles that are active for all builds. | &lt;activeProfiles&gt; &lt;activeProfile&gt;alwaysActiveProfile&lt;/activeProfile&gt; &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;/activeProfile&gt; &lt;/activeProfiles&gt; --&gt;&lt;/settings&gt; 关于 profiles 节点的详解： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184&lt;!-- | 构建方法的配置清单，maven将根据不同环境参数来使用这些构建配置。 | settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。 | settings.xml负责的是整体的构建过程，pom.xml负责单独的项目对象构建过程。 | settings.xml只包含了id，activation，repositories，pluginRepositories和properties元素。 | | 如果settings.xml中的profile被激活，它的值会覆盖任何其它定义在pom.xml中或profile.xml中的相同id的profile。 | | 查看当前激活的profile: | mvn help:active-profiles |--&gt;&lt;profiles&gt; &lt;profile&gt; &lt;!-- 该配置的唯一标识符 --&gt; &lt;id&gt;profile_id&lt;/id&gt; &lt;!-- | profile的激活条件配置。 | 除此之外的其他激活方式： | 1. 通过settings.xml文件中的activeProfile元素进行指定激活。 | 2. 在命令行，使用-P标记和逗号分隔的列表来显式的激活，如：mvn clean package -P myProfile |--&gt; &lt;activation&gt; &lt;!-- 是否默认激活 --&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!-- 内建的java版本检测，匹配规则：https://maven.apache.org/enforcer/enforcer-rules/versionRanges.html --&gt; &lt;jdk&gt;9.9&lt;/jdk&gt; &lt;!-- 内建操作系统属性检测， 配置规则：https://maven.apache.org/enforcer/enforcer-rules/requireOS.html --&gt; &lt;os&gt; &lt;!-- 操作系统 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!-- 操作系统家族 --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!-- 操作系统 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!-- 操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!-- | 如果maven检测到某一个属性(其值可以在POM中通过$&#123;名称&#125;引用)，并且其拥有对应的名称和值，Profile就会被激活。 | 如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段。 |--&gt; &lt;property&gt; &lt;!-- 属性名 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!-- 属性值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!-- 根据文件存在/不存在激活profile --&gt; &lt;file&gt; &lt;!-- 如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/path/to/active_on_exists&lt;/exists&gt; &lt;!-- 如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/path/to/active_on_missing&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!-- 扩展属性设置。扩展属性可以在POM中的任何地方通过$&#123;扩展属性名&#125;进行引用。 | | 属性引用方式(包括扩展属性，共5种属性可以引用)： | | env.x：引用shell环境变量，例如，&quot;env.PATH&quot;指代了$path环境变量(在Linux/Windows上是%PATH%)。 | project.x：引用pom.xml(根元素是project)中xml元素内容。例如$&#123;project.artifactId&#125;可以获取pom.xml中设置的&lt;artifactId /&gt;元素的内容。 | settings.x：引用setting.xml(根元素是setting)中xml元素内容，例如$&#123;settings.offline&#125;。 | Java System Properties：所有可通过java.lang.System.getProperties()访问的属性都能在通过$&#123;property_name&#125;访问，例如$&#123;java.home&#125;。 | x：在&lt;properties/&gt;或者外部文件中设置的属性，都可以$&#123;someVar&#125;的形式使用。 | |--&gt; &lt;properties&gt; &lt;!-- 在当前profile被激活时，$&#123;profile.property&#125;就可以被访问到了。 --&gt; &lt;profile.property&gt;this.property.is.accessible.when.current.profile.actived&lt;/profile.property&gt; &lt;/properties&gt; &lt;!-- 远程仓库列表，settings.xml中的repositories不被直接支持，需要在profiles中配置。 --&gt; &lt;repositories&gt; &lt;!-- | releases vs snapshots | maven针对releases和snapshots有不同的处理策略，POM可以在每个单独的仓库中，为每种类型的artifact采取不同的策略。 | 例如： | 开发环境使用snapshots模式实时获取最新的快照版本进行构建 | 生成环境使用releases模式获取稳定版本进行构建 | 参见repositories/repository/releases元素。 |--&gt; &lt;!-- | 依赖包不更新问题： | 1. maven在下载依赖失败后会生成一个.lastUpdated为后缀的文件。如果这个文件存在，那么即使换一个有资源的仓库后， | maven依然不会去下载新资源。可以通过-U参数进行强制更新、手动删除.lastUpdated 文件： | find . -type f -name &quot;*.lastUpdated&quot; -exec echo &#123;&#125;&quot; found and deleted&quot; \\; -exec rm -f &#123;&#125; \\; | | 2. updatePolicy设置更新频率不对，导致没有触发maven检查本地artifact与远程artifact是否一致。 |--&gt; &lt;repository&gt; &lt;!-- 远程仓库唯一标识 --&gt; &lt;id&gt;maven_repository_id&lt;/id&gt; &lt;!-- 远程仓库名称 --&gt; &lt;name&gt;maven_repository_name&lt;/name&gt; &lt;!-- 远程仓库URL，按protocol://hostname/path形式。 --&gt; &lt;url&gt;http://host/maven&lt;/url&gt; &lt;!-- | 用于定位和排序artifact的仓库布局类型-可以是default(默认)或者legacy(遗留)。 | Maven2为其仓库提供了一个默认的布局；然而，Maven1.x有一种不同的布局。 | 我们可以使用该元素指定布局是default(默认)还是legacy(遗留)。 | --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;!-- 如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!-- 是否允许该仓库为artifact提供releases下载功能 --&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;!-- | 每次执行构建命令时，Maven会比较本地POM和远程POM的时间戳，该元素指定比较的频率。 | 有效选项是： | always ：每次构建都检查 | daily ：默认，距上次构建检查时间超过一天 | interval: x ：距上次构建检查超过x分钟 | never从不 ：从不 | | 重要： | 设置为daily时，如果artifact一天更新了几次，在一天之内进行构建，也不会从仓库中重新获取最新版本。 |--&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!-- 当maven验证artifact校验文件失败时该怎么做：ignore(忽略)，fail(失败)，或者warn(警告)。 --&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载 --&gt; &lt;snapshots&gt; &lt;!-- 是否允许该仓库为artifact提供snapshots下载功能 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;!-- | 国内可用的maven仓库地址(updated @ 2019-02-08)： | http://maven.aliyun.com/nexus/content/groups/public | http://maven.wso2.org/nexus/content/groups/public/ | http://jcenter.bintray.com/ | http://maven.springframework.org/release/ | http://repository.jboss.com/maven2/ | http://uk.maven.org/maven2/ | http://repo1.maven.org/maven2/ | http://maven.springframework.org/milestone | http://maven.jeecg.org/nexus/content/repositories/ | http://repo.maven.apache.org/maven2 | http://repo.spring.io/release/ | http://repo.spring.io/snapshot/ | http://mavensync.zkoss.org/maven2/ | https://repository.apache.org/content/groups/public/ | https://repository.jboss.org/nexus/content/repositories/releases/ |--&gt; &lt;/repositories&gt; &lt;!-- | maven插件的远程仓库配置。maven插件实际上是一种特殊类型的artifact。 | 插件仓库独立于artifact仓库。pluginRepositories元素的结构和repositories元素的结构类似。 |--&gt; &lt;!-- &lt;pluginRepositories&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; --&gt; &lt;/profile&gt;&lt;/profiles&gt; pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;!-- 父项目的坐标。如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值。 坐标包括groupID，artifactID和version。 --&gt; &lt;parent&gt; &lt;!-- 被继承的父项目的构件标识符 --&gt; &lt;artifactId /&gt; &lt;!-- 被继承的父项目的全球唯一标识符 --&gt; &lt;groupId /&gt; &lt;!-- 被继承的父项目的版本 --&gt; &lt;version /&gt; &lt;!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是：../pom.xml。 Maven首先在构建当前项目的地方寻找父项目的pom，其次在文件系统的这个位置(relativePath位置)， 然后在本地仓库，最后在远程仓库寻找父项目的pom。 --&gt; &lt;relativePath /&gt; &lt;/parent&gt; &lt;!-- 声明项目描述符遵循哪一个POM模型版本。模型本身的版本很少改变，虽然如此，但它仍然是必不可少的， 这是为了当Maven引入了新的特性或者其他模型变更的时候，确保稳定性。 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 项目的全球唯一标识符，通常使用全限定的包名区分该项目和其他项目。 并且构建时生成的路径也是由此生成，如com.mycompany.app生成的相对路径为：/com/mycompany/app --&gt; &lt;groupId&gt;asia.banseon&lt;/groupId&gt; &lt;!-- 构件的标识符，它和groupID一起唯一标识一个构件。换句话说，你不能有两个不同的项目拥有同样的artifactID和groupID； 在某个特定的groupID下，artifactID也必须是唯一的。 构件是项目产生的或使用的一个东西，Maven为项目产生的构件包括：JARs，源码，二进制发布和WARs等。 --&gt; &lt;artifactId&gt;banseon-maven2&lt;/artifactId&gt; &lt;!-- 项目产生的构件类型，例如jar、war、ear、pom。插件可以创建他们自己的构件类型，所以前面列的不是全部构件类型 --&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;!-- 项目当前版本，格式为：主版本.次版本.增量版本-限定版本号 --&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!-- 项目的名称，Maven产生的文档用 --&gt; &lt;name&gt;banseon-maven&lt;/name&gt; &lt;!-- 项目主页的URL，Maven产生的文档用 --&gt; &lt;url&gt;http://www.baidu.com/banseon&lt;/url&gt; &lt;!-- 项目的详细描述，Maven产生的文档用。当这个元素能够用HTML格式描述时(例如，CDATA中的文本会被解析器忽略， 就可以包含HTML标签)，不鼓励使用纯文本描述。如果你需要修改产生的web站点的索引页面， 你应该修改你自己的索引页文件，而不是调整这里的文档。 --&gt; &lt;description&gt;A maven project to study maven.&lt;/description&gt; &lt;!-- 描述了这个项目构建环境中的前提条件。 --&gt; &lt;prerequisites&gt; &lt;!-- 构建该项目或使用该插件所需要的Maven的最低版本 --&gt; &lt;maven /&gt; &lt;/prerequisites&gt; &lt;!-- 项目的问题管理系统(Bugzilla，Jira，Scarab，或任何你喜欢的问题管理系统)的名称和URL，本例为jira --&gt; &lt;issueManagement&gt; &lt;!-- 问题管理系统(例如jira)的名字 --&gt; &lt;system&gt;jira&lt;/system&gt; &lt;!-- 该项目使用的问题管理系统的URL --&gt; &lt;url&gt;http://jira.xxxx.com/xxxx&lt;/url&gt; &lt;/issueManagement&gt; &lt;!-- 项目持续集成信息 --&gt; &lt;ciManagement&gt; &lt;!-- 持续集成系统的名字，例如continuum --&gt; &lt;system /&gt; &lt;!-- 该项目使用的持续集成系统的URL(如果持续集成系统有web接口的话) --&gt; &lt;url /&gt; &lt;!-- 构建完成时，需要通知的开发者/用户的配置项。包括被通知者信息和通知条件(错误，失败，成功，警告) --&gt; &lt;notifiers&gt; &lt;!-- 配置一种方式，当构建中断时，以该方式通知用户/开发者 --&gt; &lt;notifier&gt; &lt;!-- 传送通知的途径 --&gt; &lt;type /&gt; &lt;!-- 发生错误时是否通知 --&gt; &lt;sendOnError /&gt; &lt;!-- 构建失败时是否通知 --&gt; &lt;sendOnFailure /&gt; &lt;!-- 构建成功时是否通知 --&gt; &lt;sendOnSuccess /&gt; &lt;!-- 发生警告时是否通知 --&gt; &lt;sendOnWarning /&gt; &lt;!-- 不赞成使用。通知发送到哪里 --&gt; &lt;address /&gt; &lt;!-- 扩展配置项 --&gt; &lt;configuration /&gt; &lt;/notifier&gt; &lt;/notifiers&gt; &lt;/ciManagement&gt; &lt;!-- 项目创建年份，4位数字。当产生版权信息时需要使用这个值。 --&gt; &lt;inceptionYear /&gt; &lt;!-- 项目相关邮件列表信息 --&gt; &lt;mailingLists&gt; &lt;!-- 该元素描述了项目相关的所有邮件列表。自动产生的网站引用这些信息。 --&gt; &lt;mailingList&gt; &lt;!-- 邮件的名称 --&gt; &lt;name&gt;Demo&lt;/name&gt; &lt;!-- 发送邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;post&gt;Demo@126.com&lt;/post&gt; &lt;!-- 订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;subscribe&gt;Demo@126.com&lt;/subscribe&gt; &lt;!-- 取消订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;unsubscribe&gt;Demo@126.com&lt;/unsubscribe&gt; &lt;!-- 你可以浏览邮件信息的URL --&gt; &lt;archive&gt;http://localhost:8080/demo/dev/&lt;/archive&gt; &lt;/mailingList&gt; &lt;/mailingLists&gt; &lt;!-- 项目开发者列表 --&gt; &lt;developers&gt; &lt;!-- 某个项目开发者的信息 --&gt; &lt;developer&gt; &lt;!-- SCM里项目开发者的唯一标识符 --&gt; &lt;id&gt;HELLO WORLD&lt;/id&gt; &lt;!-- 项目开发者的全名 --&gt; &lt;name&gt;youname&lt;/name&gt; &lt;!-- 项目开发者的email --&gt; &lt;email&gt;youname@qq.com&lt;/email&gt; &lt;!-- 项目开发者的主页的URL --&gt; &lt;url /&gt; &lt;!-- 项目开发者在项目中扮演的角色，角色元素描述了各种角色 --&gt; &lt;roles&gt; &lt;role&gt;Project Manager&lt;/role&gt; &lt;role&gt;Architect&lt;/role&gt; &lt;/roles&gt; &lt;!-- 项目开发者所属组织 --&gt; &lt;organization&gt;demo&lt;/organization&gt; &lt;!-- 项目开发者所属组织的URL --&gt; &lt;organizationUrl&gt;http://www.xxx.com/&lt;/organizationUrl&gt; &lt;!-- 项目开发者属性，如即时消息如何处理等 --&gt; &lt;properties&gt; &lt;dept&gt;No&lt;/dept&gt; &lt;/properties&gt; &lt;!-- 项目开发者所在时区， -11到12范围内的整数。 --&gt; &lt;timezone&gt;+8&lt;/timezone&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;!-- 项目的其他贡献者列表 --&gt; &lt;contributors&gt; &lt;!-- 项目的其他贡献者。参见developers/developer元素 --&gt; &lt;contributor&gt; &lt;name /&gt; &lt;email /&gt; &lt;url /&gt; &lt;organization /&gt; &lt;organizationUrl /&gt; &lt;roles /&gt; &lt;timezone /&gt; &lt;properties /&gt; &lt;/contributor&gt; &lt;/contributors&gt; &lt;!-- 该元素描述了项目所有license列表。应该只列出该项目的license列表，不要列出依赖项目的license列表。 如果列出多个license，用户可以选择它们中的一个而不是接受所有license。 --&gt; &lt;licenses&gt; &lt;!-- 描述了项目的license，用于生成项目的web站点的license页面，其他一些报表和validation也会用到该元素。 --&gt; &lt;license&gt; &lt;!-- license用于法律上的名称 --&gt; &lt;name&gt;Apache 2&lt;/name&gt; &lt;!-- 官方的license正文页面的URL --&gt; &lt;url&gt;http://www.xxxx.com/LICENSE-2.0.txt&lt;/url&gt; &lt;!-- 项目分发的主要方式：repo，可以从Maven库下载manual，用户必须手动下载和安装依赖 --&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;!-- 关于license的补充信息 --&gt; &lt;comments&gt;A business-friendly OSS license&lt;/comments&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;!-- SCM(Source Control Management)标签允许你配置你的代码库，供Maven web站点和其它插件使用。 --&gt; &lt;scm&gt; &lt;!-- SCM的URL，该URL描述了版本库和如何连接到版本库。欲知详情，请看SCMs提供的URL格式和列表。该连接只读。 --&gt; &lt;connection&gt; scm:svn:http://svn.xxxx.com/maven/xxxxx-maven2-trunk(dao-trunk) &lt;/connection&gt; &lt;!-- 给开发者使用的，类似connection元素。即该连接不仅仅只读。 --&gt; &lt;developerConnection&gt; scm:svn:http://svn.xxxx.com/maven/dao-trunk &lt;/developerConnection&gt; &lt;!-- 当前代码的标签，在开发阶段默认为HEAD --&gt; &lt;tag /&gt; &lt;!-- 指向项目的可浏览SCM库(例如ViewVC或者Fisheye)的URL。 --&gt; &lt;url&gt;http://svn.xxxxx.com/&lt;/url&gt; &lt;/scm&gt; &lt;!-- 描述项目所属组织的各种属性。Maven产生的文档用。 --&gt; &lt;organization&gt; &lt;!-- 组织的全名 --&gt; &lt;name&gt;demo&lt;/name&gt; &lt;!-- 组织主页的URL --&gt; &lt;url&gt;http://www.xxxxxx.com/&lt;/url&gt; &lt;/organization&gt; &lt;!-- 构建项目需要的信息 --&gt; &lt;build&gt; &lt;!-- 该元素设置了项目源码目录 当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;sourceDirectory /&gt; &lt;!-- 该元素设置了项目脚本源码目录 该目录和源码目录不同：绝大多数情况下，该目录下的内容会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。 --&gt; &lt;scriptSourceDirectory /&gt; &lt;!-- 该元素设置了项目单元测试使用的源码目录 当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;testSourceDirectory /&gt; &lt;!-- 被编译过的应用程序class文件存放的目录。 --&gt; &lt;outputDirectory /&gt; &lt;!-- 被编译过的测试class文件存放的目录。 --&gt; &lt;testOutputDirectory /&gt; &lt;!-- 使用来自该项目的一系列构建扩展 --&gt; &lt;extensions&gt; &lt;!-- 描述使用到的构建扩展。 --&gt; &lt;extension&gt; &lt;!-- 构建扩展的groupId --&gt; &lt;groupId /&gt; &lt;!-- 构建扩展的artifactId --&gt; &lt;artifactId /&gt; &lt;!-- 构建扩展的版本 --&gt; &lt;version /&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;!-- 当项目没有规定目标(Maven2叫做阶段)时的默认值 --&gt; &lt;defaultGoal /&gt; &lt;!-- 这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。 --&gt; &lt;resources&gt; &lt;!-- 这个元素描述了项目相关或测试相关的所有资源路径 --&gt; &lt;resource&gt; &lt;!-- 描述了资源的目标路径。该路径相对target/classes目录(例如$&#123;project.build.outputDirectory&#125;)。 举个例子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为: org/apache/maven/messages。然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。 --&gt; &lt;targetPath /&gt; &lt;!-- 是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。 --&gt; &lt;filtering /&gt; &lt;!-- 描述存放资源的目录，该路径相对POM路径 --&gt; &lt;directory /&gt; &lt;!-- 包含的模式列表，例如：**/*.xml --&gt; &lt;includes /&gt; &lt;!-- 排除的模式列表，例如：**/*.xml --&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;!-- 这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。 --&gt; &lt;testResources&gt; &lt;!-- 这个元素描述了测试相关的所有资源路径，参见build/resources/resource元素的说明 --&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;!-- 构建产生的所有文件存放的目录 --&gt; &lt;directory /&gt; &lt;!-- 产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。 --&gt; &lt;finalName /&gt; &lt;!-- 当filtering开关打开时，使用到的过滤器属性文件列表。 --&gt; &lt;filters /&gt; &lt;!-- 子项目可以引用的默认插件信息。该插件配置项直到被引用时才会被解析或绑定到生命周期。 给定插件的任何本地配置都会覆盖这里的配置。 --&gt; &lt;pluginManagement&gt; &lt;!-- 使用的插件列表 --&gt; &lt;plugins&gt; &lt;!-- plugin元素包含描述插件所需要的信息。 --&gt; &lt;plugin&gt; &lt;!-- 插件在仓库里的groupID --&gt; &lt;groupId /&gt; &lt;!-- 插件在仓库里的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 被使用的插件的版本(或版本范围) --&gt; &lt;version /&gt; &lt;!-- 是否从该插件下载Maven扩展，例如打包和类型处理器。 由于性能原因，只有在真需要下载时，该元素才被设置成enabled。 --&gt; &lt;extensions /&gt; &lt;!-- 在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --&gt; &lt;executions&gt; &lt;!-- execution元素包含了插件执行需要的信息 --&gt; &lt;execution&gt; &lt;!-- 执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --&gt; &lt;id /&gt; &lt;!-- 绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --&gt; &lt;phase /&gt; &lt;!-- 配置的执行目标 --&gt; &lt;goals /&gt; &lt;!-- 配置是否被传播到子POM --&gt; &lt;inherited /&gt; &lt;!-- 作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!-- 项目引入插件所需要的额外依赖 --&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!-- 作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!-- 使用的插件列表 --&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!-- 在列的项目构建profile，如果被激活，会修改构建处理。 --&gt; &lt;profiles&gt; &lt;!-- 根据环境参数或命令行参数激活某个构建处理 --&gt; &lt;profile&gt; &lt;!-- 构建配置的唯一标识符。即用于命令行激活，也用于在继承时合并具有相同标识符的profile。 --&gt; &lt;id /&gt; &lt;!-- 自动触发profile的条件逻辑。Activation是profile的开启钥匙。profile的力量来自于它， 能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。 activation元素并不是激活profile的唯一方式。 --&gt; &lt;activation&gt; &lt;!-- profile默认是否激活的标志 --&gt; &lt;activeByDefault /&gt; &lt;!-- 当匹配的jdk被检测到，profile被激活。 例如，&quot;1.4&quot;激活JDK1.4，1.4.0_2，而&quot;!1.4&quot;激活所有版本不是以1.4开头的JDK。 --&gt; &lt;jdk /&gt; &lt;!-- 当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!-- 激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!-- 激活profile的操作系统所属家族(如&quot;windows&quot;) --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!-- 激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x64&lt;/arch&gt; &lt;!-- 激活profile的操作系统版本 --&gt; &lt;version&gt;6.1.7100&lt;/version&gt; &lt;/os&gt; &lt;!-- 如果Maven检测到某一个属性(其值可以在POM中通过$&#123;名称&#125;引用)，其拥有对应的名称和值，Profile就会被激活。 如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段。 --&gt; &lt;property&gt; &lt;!-- 激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!-- 激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!-- 提供一个文件名，通过检测该文件的存在或不存在来激活profile。 exists：检查文件是否存在，如果存在则激活profile。 missing：检查文件是否存在，如果不存在则激活profile。 --&gt; &lt;file&gt; &lt;!-- 如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/usr/local/xxxx/xxxx-home/tomcat/maven-guide-zh-to-production/workspace/&lt;/exists&gt; &lt;!-- 如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/usr/local/xxxx/xxxx-home/tomcat/maven-guide-zh-to-production/workspace/&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!-- 构建项目所需要的信息。参见build元素。 --&gt; &lt;build&gt; &lt;defaultGoal /&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;directory /&gt; &lt;finalName /&gt; &lt;filters /&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!-- 模块(有时称作子项目)被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径。 --&gt; &lt;modules /&gt; &lt;!-- 发现依赖和扩展的远程仓库列表 --&gt; &lt;repositories&gt; &lt;!-- 参见repositories/repository元素 --&gt; &lt;repository&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!-- 发现插件的远程仓库列表，这些插件用于构建和报表 --&gt; &lt;pluginRepositories&gt; &lt;!-- 包含需要连接到远程插件仓库的信息。参见repositories/repository元素。 --&gt; &lt;pluginRepository&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!-- 该元素描述了项目相关的所有依赖。这些依赖组成了项目构建过程中的一个个环节。 它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 不赞成使用。现在Maven忽略该元素。 --&gt; &lt;reports /&gt; &lt;!-- 该元素包括使用报表插件产生报表的规范。当用户执行&quot;mvn site&quot;，这些报表就会运行。 在页面导航栏能看到所有报表的链接。参见reporting 元素。 --&gt; &lt;reporting&gt;......&lt;/reporting&gt; &lt;!-- 参见dependencyManagement元素 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 参见distributionManagement元素 --&gt; &lt;distributionManagement&gt;......&lt;/distributionManagement&gt; &lt;!-- 参见properties元素 --&gt; &lt;properties /&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!-- 模块(有时称作子项目)被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径。 --&gt; &lt;modules /&gt; &lt;!-- 发现依赖和扩展的远程仓库列表，配置多个repository时，按顺序依次查找。 --&gt; &lt;repositories&gt; &lt;!-- 包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!-- 远程仓库唯一标识符。可以用来匹配在settings.xml文件里配置的远程仓库。 --&gt; &lt;id&gt;banseon-repository-proxy&lt;/id&gt; &lt;!-- 远程仓库名称 --&gt; &lt;name&gt;banseon-repository-proxy&lt;/name&gt; &lt;!-- 远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://10.10.10.123:8080/repository/&lt;/url&gt; &lt;!-- 用于定位和排序构件的仓库布局类型-可以是default(默认)或者legacy(遗留)。 Maven2为其仓库提供了一个默认的布局；然而，Maven1.x有一种不同的布局。 我们可以使用该元素指定布局是default(默认)还是legacy(遗留)。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;!-- 如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!-- true或者false表示该仓库是否为下载某种类型构件(发布版，快照版)开启。 --&gt; &lt;enabled /&gt; &lt;!-- 该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。 选项：always(一直)，daily(默认，每日)，interval：X(这里X是以分钟为单位的时间间隔)，或者never(从不)。 --&gt; &lt;updatePolicy /&gt; &lt;!-- 当Maven验证构件校验文件失败时该怎么做：ignore(忽略)，fail(失败)，或者warn(警告)。 --&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载。 有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。 例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素。 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!-- 发现插件的远程仓库列表，这些插件用于构建和报表。 --&gt; &lt;pluginRepositories&gt; &lt;!-- 包含需要连接到远程插件仓库的信息。参见repositories/repository元素。 --&gt; &lt;pluginRepository&gt;......&lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!-- 该元素描述了项目相关的所有依赖。这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。 要获取更多信息，请看项目依赖机制。 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!-- 依赖的groupID --&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;!-- 依赖的artifactID --&gt; &lt;artifactId&gt;maven-artifact&lt;/artifactId&gt; &lt;!-- 依赖的版本号。在Maven2里，也可以配置成版本号的范围。 --&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;!-- 依赖类型，默认类型是jar。它通常表示依赖的文件的扩展名，但也有例外。 一个类型可以被映射成另外一个扩展名或分类器。类型经常和使用的打包方式对应，尽管这也有例外。 一些类型的例子：jar，war，ejb-client和test-jar。 如果设置extensions为true，就可以在plugin里定义新的类型。所以前面的类型的例子不完整。 --&gt; &lt;type&gt;jar&lt;/type&gt; &lt;!-- 依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面。 例如，如果你想要构建两个单独的构件成JAR，一个使用Java 1.4编译器，另一个使用Java 6编译器， 你就可以使用分类器来生成两个单独的JAR构件。 --&gt; &lt;classifier&gt;&lt;/classifier&gt; &lt;!-- 依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 - compile： 默认范围，用于编译 - provided： 类似于编译，但支持你期待jdk或者容器提供，类似于classpath - runtime： 在执行时需要使用 - test： 用于test任务时使用 - system： 需要外在提供相应的元素。通过systemPath来取得 - systemPath： 仅用于范围为system。提供相应的路径 - optional： 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 --&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;!-- 仅供system范围使用。注意，不鼓励使用这个元素，并且在新的版本中该元素可能被覆盖掉。 该元素为依赖规定了文件系统上的路径。需要绝对路径而不是相对路径。 推荐使用属性匹配绝对路径，例如$&#123;java.home&#125;。 --&gt; &lt;systemPath&gt;&lt;/systemPath&gt; &lt;!-- 当计算传递依赖时，从依赖构件列表里，列出被排除的依赖构件集。 即告诉maven你只依赖指定的项目，不依赖项目的依赖。此元素主要用于解决版本冲突问题。 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;!-- 可选依赖，如果你在项目B中把C依赖声明为可选，则要在依赖于B的项目(例如项目A)中显式的引用对C的依赖。 可选依赖阻断依赖的传递性。 --&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 不赞成使用，现在Maven忽略该元素 --&gt; &lt;reports&gt;&lt;/reports&gt; &lt;!-- 该元素描述使用报表插件产生报表的规范。当用户执行&quot;mvn site&quot;，这些报表就会运行。在页面导航栏能看到所有报表的链接。 --&gt; &lt;reporting&gt; &lt;!-- true，则网站不包括默认的报表。这包括&quot;项目信息&quot;菜单中的报表。 --&gt; &lt;excludeDefaults /&gt; &lt;!-- 所有产生的报表存放到哪里。默认值是$&#123;project.build.directory&#125;/site。 --&gt; &lt;outputDirectory /&gt; &lt;!-- 使用的报表插件和他们的配置 --&gt; &lt;plugins&gt; &lt;!-- plugin元素包含描述报表插件需要的信息 --&gt; &lt;plugin&gt; &lt;!-- 报表插件在仓库里的groupID --&gt; &lt;groupId /&gt; &lt;!-- 报表插件在仓库里的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 被使用的报表插件的版本(或版本范围) --&gt; &lt;version /&gt; &lt;!-- 任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!-- 报表插件的配置 --&gt; &lt;configuration /&gt; &lt;!-- 一组报表的多重规范，每个规范可能有不同的配置。一个规范(报表集)对应一个执行目标。 例如，有 1，2，3，4，5，6，7，8，9 个报表， 1，2，5 构成A报表集，对应一个执行目标， 2，5，8 构成B报表集，对应另一个执行目标。 --&gt; &lt;reportSets&gt; &lt;!-- 表示报表的一个集合，以及产生该集合的配置。 --&gt; &lt;reportSet&gt; &lt;!-- 报表集合的唯一标识符，POM继承时用到。 --&gt; &lt;id /&gt; &lt;!-- 产生报表集合时，被使用的报表的配置。 --&gt; &lt;configuration /&gt; &lt;!-- 配置是否被继承到子POMs --&gt; &lt;inherited /&gt; &lt;!-- 这个集合里使用到哪些报表 --&gt; &lt;reports /&gt; &lt;/reportSet&gt; &lt;/reportSets&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt; &lt;!-- 继承自该项目的所有子项目的默认依赖信息。 这部分的依赖信息不会被立即解析，而是当子项目声明一个依赖(必须描述groupID和artifactID信息)时，如果groupID 和artifactID以外的一些信息没有描述，则通过groupID和artifactID匹配到这里的依赖，并使用这里的依赖信息。 比如锁定子项目的一些依赖的版本时，即可在父项目中定义。 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 项目分发信息，在执行&quot;mvn deploy&quot;后表示要发布的位置。 有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。 --&gt; &lt;distributionManagement&gt; &lt;!-- 部署项目产生的构件到远程仓库需要的信息 --&gt; &lt;repository&gt; &lt;!-- 是分配给快照一个唯一的版本号(由时间戳和构建流水号)？还是每次都使用相同的版本号？ 参见repositories/repository元素 --&gt; &lt;uniqueVersion /&gt; &lt;id&gt;xxx-maven2&lt;/id&gt; &lt;name&gt;xxx maven2&lt;/name&gt; &lt;url&gt;file://$&#123;basedir&#125;/target/deploy&lt;/url&gt; &lt;layout /&gt; &lt;/repository&gt; &lt;!-- 构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库。 参见distributionManagement/repository元素 --&gt; &lt;snapshotRepository&gt; &lt;uniqueVersion /&gt; &lt;id&gt;xxx-maven2&lt;/id&gt; &lt;name&gt;xxx-maven2 Snapshot Repository&lt;/name&gt; &lt;url&gt;scp://svn.xxxx.com/xxx:/usr/local/maven-snapshot&lt;/url&gt; &lt;layout /&gt; &lt;/snapshotRepository&gt; &lt;!-- 部署项目的网站需要的信息 --&gt; &lt;site&gt; &lt;!-- 部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置 --&gt; &lt;id&gt;banseon-site&lt;/id&gt; &lt;!-- 部署位置的名称 --&gt; &lt;name&gt;business api website&lt;/name&gt; &lt;!-- 部署位置的URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;scp://svn.baidu.com/xxx:/var/www/localhost/web&lt;/url&gt; &lt;/site&gt; &lt;!-- 项目下载页面的URL。如果没有该元素，用户应该参考主页。 使用该元素的原因是：帮助定位那些不在仓库里的构件(由于license限制)。 --&gt; &lt;downloadUrl /&gt; &lt;!-- 如果构件有了新的groupID和artifactID(构件移到了新的位置)，这里列出构件的重定位信息。 --&gt; &lt;relocation&gt; &lt;!-- 构件新的groupID --&gt; &lt;groupId /&gt; &lt;!-- 构件新的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 构件新的版本号 --&gt; &lt;version /&gt; &lt;!-- 显示给用户的，关于移动的额外信息，例如原因。 --&gt; &lt;message /&gt; &lt;/relocation&gt; &lt;!-- 给出该构件在远程仓库的状态。不得在本地项目中设置该元素，因为这是工具自动更新的。有效的值有： - none： 默认 - converted： 仓库管理员从Maven1 POM转换过来 - partner： 直接从伙伴Maven 2仓库同步过来 - deployed： 从Maven 2实例部署 - verified： 被核实时正确的和最终的 --&gt; &lt;status /&gt; &lt;/distributionManagement&gt; &lt;!-- 以值替代名称，Properties可以在整个POM中使用，也可以作为触发条件(见settings.xml配置文件里activation元素的说明)。 格式是：&lt;name&gt;value&lt;/name&gt;。 --&gt; &lt;properties /&gt;&lt;/project&gt; 本文参考https://www.cnblogs.com/iceJava/p/10356309.html https://www.cnblogs.com/hongmoshui/p/10762272.html https://www.cnblogs.com/cxzdy/p/5126087.html 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"maven 仓库的配置方式以及依赖的下载顺序","slug":"maven-repository","date":"2021-01-19T08:17:18.000Z","updated":"2021-01-27T02:54:06.683Z","comments":true,"path":"2021/01/19/maven-repository/","link":"","permalink":"http://example.com/2021/01/19/maven-repository/","excerpt":"","text":"maven 仓库分为本地仓库和远程仓库，而远程仓库又分为 maven 中央仓库、其他远程仓库和私服 (私有服务器)。 maven 项目使用的仓库一般有如下几种方式： maven 中央仓库，这是默认的仓库。 镜像仓库，通过 sttings.xml 中的 settings.mirrors.mirror 配置。 全局 profile 仓库，通过 settings.xml 中的 settings.repositories.repository 配置。 项目仓库，通过 pom.xml 中的 project.repositories.repository 配置。 项目 profile 仓库，通过 pom.xml 中的 project.profiles.profile.repositories.repository 配置。 本地仓库。 如果所有仓库的配置都存在，那么依赖的搜索顺序也会变得异常复杂。 仓库的配置方式本地仓库maven 缺省的本地仓库地址为 ${user.home}/.m2/repository，也就是说，一个用户会对应的拥有一个本地仓库。 可以通过修改 ${user.home}/.m2/settings.xml，在 节点下添加配置： 1&lt;localRepository&gt;D:\\java\\maven-repo&lt;/localRepository&gt; 如果想让所有的用户使用统一的配置，那么可以修改 maven 主目录下的 setting.xml：${M2_HOME}/conf/setting.xml。 maven 中央仓库在 maven 安装目录的 lib 目录下，有一个 maven-model-builder-3.6.1.jar，里面的 org/apache/maven/model/pom-4.0.0.xml 文件定义了 maven 默认中央仓库的地址：https://repo.maven.apache.org/maven2，如下图所示： 1234567891011121314151617181920212223242526&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;updatePolicy&gt;never&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 一般使用阿里云镜像仓库代替默认的 maven 中央仓库，配置方式有两种： 第一种，全局配置 修改 ${M2_HOME}/conf/setting.xml 文件，在 节点下添加配置： 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;mirrors&gt; 修改全局配置后，所有使用此 maven 的工程都会生效。 &lt; mirrorOf&gt; 可以设置为哪个中央仓库做镜像，为名为 “central” 的中央仓库做镜像，写作 &lt; mirrorOf&gt;central&lt; /mirrorOf&gt;；为所有中央仓库做镜像，写作 &lt; mirrorOf&gt;*&lt; /mirrorOf&gt; (不建议)。maven 默认中央仓库的 id 为 central。id是唯一的。 第二种，局部配置 在需要使用阿里云镜像仓库的 maven 工程的 pom.xml 文件中添加： 123456789101112131415161718192021222324&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;aliyun&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun-plugin&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 修改局部配置后，只对当前工程有效。 私服 第一种，全局配置 修改 ${M2_HOME}/conf/setting.xml 文件，在 节点下添加配置： 1234567891011121314151617181920212223242526272829&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;matgene-nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-plugin&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 然后，在 节点下添加激活配置 (通过配置的 profile 的 id 标识进行激活)： 123&lt;activeProfiles&gt; &lt;activeProfile&gt;matgene-nexus&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 第二种，局部配置 在需要使用私服的 maven 工程的 pom.xml 文件中添加。 上传： settings.xml： 12345678910&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; pom.xml： 12345678910&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 下载： pom.xml： 123456789101112131415161718192021222324&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-plugin&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 在项目中，将私服地址更改为自己公司的实际地址。 依赖的下载顺序准备测试环境安装 jdk 和 maven。 使用如下命令创建测试项目： 1yes | mvn archetype:generate -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-webapp -DinteractiveMode=true -DgroupId=com.pollyduan -DartifactId=myweb -Dversion=1.0 -Dpackage=com.pollyduan 创建完成后，为了避免后续测试干扰，先执行一次 compile。 12cd mywebmvn compile 最后，修改 pom.xml 文件，将 junit 版本号改为 4.12 。我们要使用这个 jar 来测试依赖的搜索顺序。 默认情况首先确保 junit 4.12 不存在： 1rm -rf ~/.m2/repository/junit/junit/4.12 默认情况下没有配置任何仓库，也就是说，既没更改 $M2_HOME/conf/settings.xml，也没有添加 ~/.m2/settings.xml。 执行编译，查看日志中拉取 junit 的仓库。 1234mvn compile...Downloaded from central: https://repo.maven.apache.org/maven2/junit/junit/4.12/junit-4.12.pom (24 kB at 11 kB/s) 从显示的仓库 id 可以看出：默认是从 maven 中央仓库拉取的 jar。 配置镜像仓库 settings_mirror创建 ~/.m2/setttings.xml，配置 maven 中央仓库的镜像，如下： 123456789&lt;settings&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;settings_mirror&lt;/id&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt; 重新测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile 在日志中查看下载依赖的仓库： 1Downloaded from settings_mirror: https://maven.aliyun.com/repository/public/junit/junit/4.12/junit-4.12.pom (24 kB at 35 kB/s) 从显示的仓库 id 可以看出：是从 settings_mirror 中下载的 jar。 结论：settings_mirror 的优先级高于 central。 配置项目仓库 pom_repositories在 project 中的 pom.xml 文件中，增加如下配置： 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;pom_repositories&lt;/id&gt; &lt;name&gt;local&lt;/name&gt; &lt;url&gt;http://10.18.29.128/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;sapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 由于改变了 id 的名字，所以仓库地址无所谓，使用相同的地址也不影响测试。 执行测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile 在日志中查看下载依赖的仓库： 1Downloaded from pom_repositories: http://10.18.29.128/nexus/content/groups/public/junit/junit/4.12/junit-4.12.pom (24 kB at 95 kB/s) 从显示的仓库 id 可以看出：jar 是从 pom_repositories 中下载的。 结论：pom_repositories 优先级高于 settings_mirror。 配置全局 profile 仓库 settings_profile_repo在 ~/.m2/settings.xml 中 settings 的节点内增加： 123456789101112131415161718&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;s_profile&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;settings_profile_repo&lt;/id&gt; &lt;name&gt;netease&lt;/name&gt; &lt;url&gt;http://mirrors.163.com/maven/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 执行测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Ps_profile 在日志中查看下载依赖的仓库： 1Downloaded from settings_profile_repo: http://mirrors.163.com/maven/repository/maven-public/junit/junit/4.12/junit-4.12.pom (24 kB at 63 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：settings_profile_repo 优先级高于 pom_repositories 和 settings_mirror。 配置项目 profile 仓库 pom_profile_repo在 project 中的 pom.xml 文件中，增加如下配置： 123456789101112131415161718&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;p_profile&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;pom_profile_repo&lt;/id&gt; &lt;name&gt;local&lt;/name&gt; &lt;url&gt;http://10.18.29.128/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 执行测试： 123rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Ps_profile,p_profilemvn compile -Pp_profile,s_profile 在日志中查看下载依赖的仓库： 1Downloaded from settings_profile_repo: http://mirrors.163.com/maven/repository/maven-public/junit/junit/4.12/junit-4.12.pom (24 kB at 68 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：settings_profile_repo 优先级高于 pom_profile_repo。 进一步测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Pp_profile 在日志中查看下载依赖的仓库： 1Downloaded from pom_profile_repo: http://10.18.29.128/nexus/content/groups/public/junit/junit/4.12/junit-4.12.pom (24 kB at 106 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：pom_profile_repo 优先级高于 pom_repositories。 本地仓库 local_repo这不算测试了，只是一个结论，可以任意测试：只要 ~/.m2/repository 中包含依赖，无论怎么配置，都会优先使用 local 本地仓库中的 jar。 最终结论 settings_mirror 的优先级高于 central settings_profile_repo 优先级高于 settings_mirror settings_profile_repo 优先级高于 pom_repositories settings_profile_repo 优先级高于 pom_profile_repo pom_repositories 优先级高于 settings_mirror pom_profile_repo 优先级高于 pom_repositories 通过上面的比较，可以得出各种仓库完整的搜索顺序链： local_repo &gt; settings_profile_repo &gt; pom_profile_repo &gt; pom_repositories &gt; settings_mirror &gt; central 简单来说，查找依赖的顺序大致如下： 在本地仓库中寻找，如果没有则进入下一步。 在全局配置的私服仓库 (settings.xml 中配置的并被激活) 中寻找，如果没有则进入下一步。 在项目自身配置的私服仓库 (pom.xml) 中寻找，如果没有则进入下一步。 在中央仓库中寻找，如果没有则终止寻找。 说明： 如果在找寻的过程中，发现该仓库有镜像设置，则用镜像的地址代替，即假设现在进行到要在 respository A 仓库中查找某个依赖，但 A 仓库配置了 mirror，则会转到从 A 的 mirror 中查找该依赖，不会再从 A 中查找。 settings.xml 中配置的 profile (激活的) 下的 respository 优先级高于项目中 pom.xml 文件配置的 respository。 如果仓库的 id 设置成 “central”，则该仓库会覆盖 maven 默认的中央仓库配置。 本文参考https://blog.csdn.net/asdfsfsdgdfgh/article/details/96576665 https://www.cnblogs.com/default/p/11856188.html https://my.oschina.net/polly/blog/2120650 https://blog.csdn.net/fengdayuan/article/details/93089136 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"maven 构建分模块项目","slug":"maven-modules","date":"2021-01-12T09:17:19.000Z","updated":"2021-01-20T07:37:52.805Z","comments":true,"path":"2021/01/12/maven-modules/","link":"","permalink":"http://example.com/2021/01/12/maven-modules/","excerpt":"","text":"分模块构建 maven 工程分析在现实生活中，汽车厂家进行汽车生产时，由于整个生产过程非常复杂和繁琐，工作量非常大，所以车场都会将整个汽车的部件分开生产，最终再将生产好的部件进行组装，形成一台完整的汽车： 类似的，随着项目功能的增加，项目本身会变得越来越庞大，这个时候，代码的良好管理和规划就会变得很重要。为了提高效率，根据业务的不同将揉作一团的业务代码分离出来，业务划分上分割清晰，提高代码复用率，例如： 上述功能的实现就是代码这一层级的变动，可以采用多项目模式和多模块模式： 多项目：每个业务单独新建项目并编写相应逻辑 多模块：业务聚合在一个项目中的不同模块中，然后通过依赖调用实现业务逻辑 maven 工程的继承在 java 语言中，类之间是可以继承的，通过继承，子类就可以引用父类中非 private 的属性和方法。同样，在 maven 工程之间也可以继承，子工程继承父工程后，就可以使用在父工程中引入的依赖。继承的目的是为了消除重复代码。 maven 工程的聚合在 maven 工程的 pom.xml 文件中，可以使用 标签将其他 maven 工程聚合到一起，聚合的目的是为了进行统一操作。 例如，拆分后的 maven 工程有多个，如果要进行打包，就需要针对每个工程分别执行打包命令，操作起来非常繁琐。这时就可以使用 标签将这些工程统一聚合到 maven 工程中，需要打包的时候，只需要在此工程中执行一次打包命令，其下被聚合的工程就都会被打包了。 分模块构建 maven 工程构建父模块新建 maven 项目： 保留 pom.xml 文件，删除 src 目录： 构建子模块新建 module 1： 新建 module 2： 如果需要更多的模块，重复上述步骤。 父模块 pom 配置公用的 pom 配置，可以放在父模块的 pom.xml 文件中： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.matgene.reaction-extractor-assistant&lt;/groupId&gt; &lt;artifactId&gt;reaction-extractor-assistant&lt;/artifactId&gt; &lt;!-- parent必须使用pom格式打包并上传到仓库 --&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;patent-loader&lt;/module&gt; &lt;module&gt;consumer-log&lt;/module&gt; &lt;module&gt;consumer-reaction&lt;/module&gt; &lt;module&gt;consumer-timeout&lt;/module&gt; &lt;/modules&gt; &lt;!-- 全局版本管理 --&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.version&gt;3.8.1&lt;/maven.compiler.version&gt; &lt;maven.compiler.source&gt;$&#123;java.version&#125;&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;$&#123;java.version&#125;&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;!-- 全局依赖管理 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.danielwegener&lt;/groupId&gt; &lt;artifactId&gt;logback-kafka-appender&lt;/artifactId&gt; &lt;version&gt;0.2.0-RC1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.json-lib&lt;/groupId&gt; &lt;artifactId&gt;json-lib&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;classifier&gt;jdk15&lt;/classifier&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt; &lt;version&gt;5.6.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven.compiler.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;maven.compiler.source&#125;&lt;/source&gt; &lt;target&gt;$&#123;maven.compiler.target&#125;&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 子模块 pom 配置子模块单独使用的 pom 配置，放在子模块自己的 pom.xml 文件中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;reaction-extractor-assistant&lt;/artifactId&gt; &lt;groupId&gt;cn.matgene.reaction-extractor-assistant&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;patent-loader&lt;/artifactId&gt; &lt;properties&gt; &lt;app.main.class&gt;cn.matgene.patent.cn.matgene.patent.loader.PatentLoaderJob&lt;/app.main.class&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;configuration&gt; &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;manifestEntries&gt; &lt;Main-Class&gt;$&#123;app.main.class&#125;&lt;/Main-Class&gt; &lt;X-Compile-Source-JDK&gt;$&#123;maven.compiler.source&#125;&lt;/X-Compile-Source-JDK&gt; &lt;X-Compile-Target-JDK&gt;$&#123;maven.compiler.target&#125;&lt;/X-Compile-Target-JDK&gt; &lt;/manifestEntries&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 上面构建的项目比较简单，各模块之间不存在依赖关系，同时，因为每个模块都需要打包，因此把打包的插件放在每一个子模块的 pom.xml 文件中。 一个 spring web 项目的实例 父工程 maven_parent 构建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394 &lt;properties&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt; &lt;springmvc.version&gt;5.0.5.RELEASE&lt;/springmvc.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt;&lt;/properties&gt;&lt;!--锁定jar版本--&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- springMVC --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;springmvc.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 子工程 maven_pojo 构建 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_dao 构建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 &lt;dependencies&gt; &lt;!-- maven_pojo的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_pojo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis和mybatis与spring的整合 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- druid数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- junit测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_service 构建 12345678&lt;dependencies&gt; &lt;!-- maven_dao的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_dao&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_web 构建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- maven_service的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;finalName&gt;maven_web&lt;/finalName&gt; &lt;pluginManagement&gt;&lt;!-- lock down plugins versions to avoid using Maven defaults (may be moved to parent pom) --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/plugin&gt; &lt;!-- see http://maven.apache.org/ref/current/maven-core/default-bindings.html#Plugin_bindings_for_war_packaging --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.22.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 项目整体结构如下： maven_parent 为父工程，其余工程为子工程，都继承父工程 maven_parent； maven_parent 工程将其子工程都进行了聚合 ； 子工程之间存在依赖关系，比如 maven_dao 依赖 maven_pojo，maven_service 依赖 maven_dao，maven_web 依赖 maven_service。 本文参考https://juejin.cn/post/6844903970024980488 https://www.cnblogs.com/tianlong/p/10552848.html 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"linux 常见错误","slug":"linux-error","date":"2021-01-11T06:49:11.000Z","updated":"2021-01-11T08:00:48.503Z","comments":true,"path":"2021/01/11/linux-error/","link":"","permalink":"http://example.com/2021/01/11/linux-error/","excerpt":"","text":"No space left on device有时候，在创建新文件，或者往磁盘写内容时，会提示 No space left on device 异常。 一般来说，linux 空间占满有如两种情况： 空间占满通过 df -h 命令，查看空间的使用情况： 1234567891011121314$ df -hFilesystem Size Used Avail Use% Mounted onudev 3.9G 0 3.9G 0% /devtmpfs 799M 82M 718M 11% /run/dev/mapper/lin--vg-root 491G 195G 272G 42% /tmpfs 3.9G 8.0K 3.9G 1% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 472M 58M 390M 13% /boottmpfs 799M 0 799M 0% /run/user/1000192.168.1.152:/mnt/chenlei 10T 5.0T 4.6T 53% /home/lin/share/storage_server_1192.168.1.236:/mnt 10T 9.0T 520G 95% /home/lin/share/storage_server_2192.168.1.106:/mnt 40T 24T 15T 63% /home/lin/share/storage_server_3192.168.1.102:/home/lin/share 491G 195G 272G 42% /tmp/share 可以看出，各分区仍有较大的空间能够使用。如果某个分区的使用率达到了 100%，那也就无法再创建新文件，也无法再写入内容，需要删除一些文件。 inode 占满通过 df -i 命令，查看 inode 的使用情况。 1234567891011121314$ df -ihFilesystem Inodes IUsed IFree IUse% Mounted onudev 993K 421 993K 1% /devtmpfs 998K 749 998K 1% /run/dev/mapper/lin--vg-root 32M 1.6M 30M 6% /tmpfs 998K 2 998K 1% /dev/shmtmpfs 998K 3 998K 1% /run/locktmpfs 998K 16 998K 1% /sys/fs/cgroup/dev/sda1 122K 303 122K 1% /boottmpfs 998K 4 998K 1% /run/user/1000192.168.1.152:/mnt/chenlei 320M 42M 279M 13% /home/lin/share/storage_server_1192.168.1.236:/mnt 320M 69M 252M 22% /home/lin/share/storage_server_2192.168.1.106:/mnt 640M 641M 0M 100% /home/lin/share/storage_server_3192.168.1.102:/home/lin/share 32M 1.6M 30M 6% /tmp/share 可以看出，每个分区都有一定大小的 inode 空间，但 /home/lin/share/storage_server_3 分区的 inode 空间使用率达到 100%。因此，再往此分区创建新文件或写入内容时，会提示 No space left on device 异常。 解决方法：将 /home/lin/share/storage_server_3 分区上一些不必要的文件删除。 理解 inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做 “扇区” (Sector)，每个扇区储存 512 字节 (相当于 0.5KB)。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个 “块” (Block)。这种由多个扇区组成的 “块”，是文件存取的最小单位。”块” 的大小，最常见的是 4 KB，即连续八个 Sector 组成一个 Block。 文件数据都储存在 “块” 中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做 inode，中文译名为 “索引节点”。 每一个文件都有对应的 inode，里面包含了与该文件有关的一些信息。 某些时候，尽管一个分区的磁盘占用率未满，但是 inode 已经用完，可能是因为该分区的目录下存在大量小文件导致。尽管小文件占用的磁盘空间并不大，但是数量太多，也会导致 inode 用尽。本例中就是因为 /home/lin/share/storage_server_3 分区存在大量的小文件。","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"java 的日志处理","slug":"java-log","date":"2021-01-06T09:14:19.000Z","updated":"2021-01-12T07:26:56.110Z","comments":true,"path":"2021/01/06/java-log/","link":"","permalink":"http://example.com/2021/01/06/java-log/","excerpt":"","text":"常用日志处理工具常见的 log 日志处理工具有：log4j、Logging、commons-logging、slf4j、logback。其中，commons-loggin、slf4j 是一种日志抽象门面，不是具体的日志框架；log4j、logback 是具体的日志实现框架。 一般使用 slf4j + logback 处理日志，也可以使用 slf4j + log4j、commons-logging + log4j 这两种日志组合框架。 日志级别日志的输出都是分级别的，不同的场合设置不同的级别，以打印不同的日志。下面拿最普遍用的 log4j 日志框架来做个日志级别的说明，这个比较奇全，其他的日志框架也都大同小异。 log4j 的级别类 org.apache.log4j.Level 里面定义了日志级别，日志输出优先级由高到底分别为以下 8 种： 日志级别 描述 OFF 关闭：最高级别，不输出日志。 FATAL 致命：输出非常严重的可能会导致应用程序终止的错误。 ERROR 错误：输出错误，但应用还能继续运行。 WARN 警告：输出可能潜在的危险状况。 INFO 信息：输出应用运行过程的详细信息。 DEBUG 调试：输出更细致的对调试应用有用的信息。 TRACE 跟踪：输出更细致的程序运行轨迹。 ALL 所有：输出所有级别信息。 所以，日志优先级别标准顺序为： ALL &lt; TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL &lt; OFF 如果日志设置为 L ，一个级别为 P 的输出日志只有当 P &gt;= L 时日志才会输出。 即如果日志级别 L 设置 INFO，只有 P 的输出级别为 INFO、WARN，后面的日志才会正常输出。 具体的输出关系可以参考下图： LombokLombok 是一种 java 实用工具，可用来帮助开发人员消除 java 的冗长代码，尤其是对于简单的 java 对象 (POJO)。它通过注释实现这一目的。 引入IntelliJ 安装： Lombok 是侵入性很高的一个 library。 maven 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt;&lt;/dependency&gt; 注解说明常用注解： @Getter 和 @Setter 自动生成 getter 和 setter 方法。 @ToString 自动重写 toString() 方法，打印所有变量。也可以加其他参数，例如 @ToString(exclude=”id”) 排除 id 属性，或者 @ToString(callSuper=true, includeFieldNames=true) 调用父类的 toString() 方法，包含所有属性。 @EqualsAndHashCode 自动生成 equals(Object other) 和 hashcode() 方法，包括所有非静态变量和非 transient 的变量。 如果某些变量不想要加进判断，可以通过 exclude 排除，也可以使用 of 指定某些字段： java 中规定，当两个 object equals 时，它们的 hashcode 一定要相同，反之，当 hashcode 相同时，object 不一定 equals。所以 equals 和 hashcode 要一起 implement，免得出现违反 java 规定的情形。 @NoArgsConstructor，@AllArgsConstructor，@RequiredArgsConstructor 这三个很像，都是自动生成该类的 constructor，差別只在生成的 constructor 的参数不一样而已。 @NoArgsConstructor：生成一个沒有参数的 constructor。 在 java 中，如果沒有指定类的 constructor，java compiler 会自动生成一个无参构造器，但是如果自己写了 constructor 之后，java 就不会再自动生成无参构造器。但是，很多时候，无参构造器是必须的，因此，为避免不必要的麻烦，应在类上至少加上 @NoArgsConstrcutor。 @AllArgsConstructor ：生成一个包含所有参数的 constructor。 @RequiredArgsConstructor：生成一个包含 “特定参数” 的 constructor，特定参数指的是那些有加上 final 修饰词的变量。 如果所有的变量都沒有用 final 修饰，@RequiredArgsConstructor 会生成一个沒有参数的 constructor。 @Data 等于同时添加了以下注解：@Getter，@Setter，@ToString，@EqualsAndHashCode 和 @RequiredArgsConstructor。 @Value 把所有的变量都设成 final，其他的就跟 @Data 类似，等于同时添加了以下注解：@Getter，@ToString，@EqualsAndHashCode 和 @RequiredArgsConstructor。 @Builder 自动生成流式 set 值写法。 注意，虽然只要加上 @Builder 注解，我们就能用流式写法快速设定 Object 的值，但是 setter 还是不应该舍弃的，因为 Spring 或是其他框架，有很多地方都会用到 Object 的 getter/setter 方法来对属性取值/赋值。 所以，通常是 @Data 和 @Builder 会一起用在同个类上，既方便流式写 code，也方便框架做事。比如： 123456@Data@Builderpublic class User &#123; private Integer id; private String name;&#125; @Slf4j 自动生成该类的 log 静态常量，要打日志就可以直接打，不用再手动 new log 静态常量了。 除了 @Slf4j 之外，Lombok 也提供其他日志框架的几种注解，像是 @Log，@Log4j 等，他们都可以创建一个静态常量 log，只是使用的 library 不一样而已。 12345@Log // 对应的log语句如下private static final java.util.logging.Logger log = java.util.logging.Logger.getLogger(LogExample.class.getName());@Log4j // 对应的log语句如下private static final org.apache.log4j.Logger log = org.apache.log4j.Logger.getLogger(LogExample.class); 更多的参考：https://juejin.cn/post/6844903557016076302 Logback引入12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; logback 依赖中，含有对 slf4j 的依赖。 节点configuration 为主节点，其主要字节点如下： property定义变量值的标签，有两个属性，name 和 value，定义变量后，可以使 “${name}” 来使用变量。 1&lt;property name=&quot;logging.level&quot; value=&quot;info&quot;/&gt; appender日志打印的组件，定义打印过滤的条件、打印输出方式、滚动策略、编码方式、打印格式等。 种类： ConsoleAppender：把日志添加到控制台。 12345&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder charset=&quot;utf-8&quot;&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-6level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt; FileAppender：把日志添加到文件。 12345678910111213&lt;appender name=&quot;ReactionExtractorAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/reaction-extractor.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; RollingFileAppender：FileAppender 的子类，滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。 1234567891011121314&lt;appender name=&quot;ReactionExtractorRollingAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/reaction-extractork-%d&#123;yyyy-MM-dd&#125;.log&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; 属性： name：指定 appender 的名称。 class：指定 appender 的全限定名。 子节点： append：默认为 true，表示日志被追加到文件结尾，如果是 false，清空现存文件。 filter：过滤器，执行完一个过滤器后返回 DENY，NEUTRAL，ACCEPT 三个枚举值中的一个。 filter 的返回值含义： DENY：日志将立即被抛弃不再经过其他过滤器。 NEUTRAL：有序列表里的下个过滤器过接着处理日志。 ACCEPT：日志会被立即处理，不再经过剩余过滤器。 filter 的两种类型： ThresholdFilter：临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器返回 NEUTRAL，当日志级别低于临界值时，日志会被拒绝。 123&lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;INFO&lt;/level&gt;&lt;/filter&gt; LevelFilter：级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据 onMath (用于配置符合过滤条件的操作) 和 onMismatch (用于配置不符合过滤条件的操作) 接收或拒绝日志。 12345&lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; file：指定被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 rollingPolicy：滚动策略，只有 appender 的 class 是 RollingFileAppender 时才需要配置。 TimeBasedRollingPolicy：根据时间来制定滚动策略，既负责滚动也负责触发滚动。 12345678&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!-- 日志文件输出的文件名：按天回滚 daily --&gt; &lt;FileNamePattern&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log.%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; &lt;/FileNamePattern&gt; &lt;!-- 日志文件保留天数 --&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt;&lt;/rollingPolicy&gt; 每天生成一个日志文件，日志文件保存 30 天。 FixedWindowRollingPolicy：根据固定窗口算法重命名文件的滚动策略。 encoder：对记录事件进行格式化。主要作用是：把日志信息转换成字节数组，以及把字节数组写入到输出流。 12345&lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;!-- 格式化输出：%d表示日期；%thread表示线程名；%-5level：级别从左显示5个字符宽度；%logger&#123;50&#125; 表示logger名字最长50个字符，否则按照句点分割；%msg：日志消息；%n是换行符 --&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt;&lt;/encoder&gt; logger用来设置某一个包或者具体的某一个类的日志打印级别以及指定 appender。 属性： name：指定受此 logger 约束的某一个包或者具体的某一个类。 level：设置打印级别 (TRACE，DEBUG，INFO，WARN，ERROR，ALL 和 OFF)，还有一个值 INHERITED 或者同义词 NULL，代表强制执行上级的级别。如果没有设置此属性，那么当前 logger 将会继承上级的级别。 addtivity：设置是否向上级 logger 传递打印信息，默认为 true。 123&lt;logger name=&quot;com.glmapper.spring.boot.controller&quot; level=&quot;$&#123;logging.level&#125;&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;GLMAPPER-LOGGERONE&quot; /&gt;&lt;/logger&gt; com.glmapper.spring.boot.controller 这个包下的 ${logging.level} 级别的日志将会使用 GLMAPPER-LOGGERONE 来打印。 root根 logger，也是一种 logger，但只有一个 level 属性。 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187&lt;!-- 使用说明： 1. logback核心jar包：logback-core-1.2.3.jar，logback-classic-1.2.3.jar，slf4j-api-1.7.25.jar 1) logback官方建议配合slf4j使用 2) logback手动下载地址：https://repo1.maven.org/maven2/ch/qos/logback/ 3) slf4j手动下载地址：https://www.mvnjar.com/org.slf4j/slf4j-api/1.7.25/detail.html 4) jar包可以从maven仓库快速获取 2. logback分为3个组件：logback-core，logback-classic和logback-access 1) 其中logback-core提供了logback的核心功能，是另外两个组件的基础 2) logback-classic实现了slf4j的API，所以当想配合slf4j使用时，需要将logback-classic加入classpath 3) logback-access是为了集成servlet环境而准备的，可提供HTTP-access的日志接口 3. 配置中KafkaAppender的jar包：logback-kafka-appender-0.2.0-RC1.jar--&gt;&lt;!-- 参考： https://juejin.im/post/5b51f85c5188251af91a7525 https://my.oschina.net/Declan/blog/1793444--&gt;&lt;!-- 说明：logback.xml配置文件，需放置在项目的resources路径下 --&gt;&lt;!-- configuration属性： scan：热加载，当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true scanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟 debug：当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false packagingData：是否打印包的信息。默认值为false--&gt;&lt;configuration debug=&quot;false&quot; xmlns=&quot;http://ch.qos.logback/xml/ns/logback&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://ch.qos.logback/xml/ns/logback https://raw.githubusercontent.com/enricopulatzo/logback-XSD/master/src/main/xsd/logback.xsd&quot;&gt; &lt;!-- property：定义变量值，两个属性，name和value --&gt; &lt;property name=&quot;logging.path&quot; value=&quot;./&quot;/&gt; &lt;property name=&quot;logging.level&quot; value=&quot;INFO&quot;/&gt; &lt;!-- 日志格式化： %d：日期 %thread：线程名 %-5level：日志级别，从左显示5个字符宽度 %logger&#123;50&#125;：logger名字最长50个字符，超过的按照句点分割 %msg：日志消息 %n：换行符 %ex&#123;full, DISPLAY_EX_EVAL&#125;：异常信息，full表示全输出，可以替换为异常信息指定输出的行数 --&gt; &lt;property name=&quot;message.format&quot; value=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n%ex&#123;full, DISPLAY_EX_EVAL&#125;&quot;/&gt; &lt;!-- kafka topic --&gt; &lt;property name=&quot;topic.name&quot; value=&quot;log-collect&quot;/&gt; &lt;!-- 本地地址 --&gt; &lt;property name=&quot;bootstrap.servers&quot; value=&quot;192.168.1.71:9092&quot;/&gt; &lt;!-- 集群地址 --&gt; &lt;!-- &lt;property name=&quot;bootstrap.servers&quot; value=&quot;hadoopdatanode1:9092,hadoopdatanode2:9092,hadoopdatanode3:9092&quot;/&gt; --&gt; &lt;!-- appender种类： ConsoleAppender：把日志添加到控制台 FileAppender：把日志添加到文件 RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。FileAppender的子类 --&gt; &lt;!-- 控制台输出日志 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义输出日志到文件 --&gt; &lt;appender name=&quot;FileAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;!-- append：true，日志被追加到文件结尾；false，清空现存文件；默认是true --&gt; &lt;append&gt;true&lt;/append&gt; &lt;!-- 级别过滤器： ThresholdFilter：临界值过滤器，过滤掉低于指定临界值的日志 LevelFilter：级别过滤器，需配置onMatch和onMismatch --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/base.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义异常输出日志文件 --&gt; &lt;appender name=&quot;ErrorFileAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/error-file.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义输出日志：滚动记录日志 --&gt; &lt;appender name=&quot;RollingFileAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;!-- 滚动策略：每天生成一个日志文件，保存365天的日志文件 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!-- 日志文件输出的文件名：按天回滚 daily --&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/reaction-log-%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;.log&lt;/FileNamePattern&gt; &lt;!-- 日志文件保留天数 --&gt; &lt;MaxHistory&gt;365&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 日志文件最大的大小 --&gt; &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;MaxFileSize&gt;50MB&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 输出日志到kafka，参考：https://github.com/danielwegener/logback-kafka-appender --&gt; &lt;appender name=&quot;KafkaAppender&quot; class=&quot;com.github.danielwegener.logback.kafka.KafkaAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;topic&gt;$&#123;topic.name&#125;&lt;/topic&gt; &lt;keyingStrategy class=&quot;com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy&quot;/&gt; &lt;deliveryStrategy class=&quot;com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy&quot;/&gt; &lt;!-- Optional parameter to use a fixed partition --&gt; &lt;!-- &lt;partition&gt;0&lt;/partition&gt; --&gt; &lt;!-- Optional parameter to include log timestamps into the kafka message --&gt; &lt;!-- &lt;appendTimestamp&gt;true&lt;/appendTimestamp&gt; --&gt; &lt;!-- each &lt;producerConfig&gt; translates to regular kafka-client config (format: key=value) --&gt; &lt;!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs --&gt; &lt;!-- bootstrap.servers is the only mandatory producerConfig --&gt; &lt;producerConfig&gt;bootstrap.servers=$&#123;bootstrap.servers&#125;&lt;/producerConfig&gt; &lt;!-- this is the fallback appender if kafka is not available. --&gt; &lt;appender-ref ref=&quot;FileAppender&quot;/&gt; &lt;/appender&gt; &lt;!-- 异步输出日志 步骤：异步输出日志就是Logger.info负责往Queue(BlockingQueue)中放日志，然后再起个线程把Queue中的日志写到磁盘上 参考：https://blog.csdn.net/lkforce/article/details/76637071 --&gt; &lt;appender name=&quot;ASYNC&quot; class=&quot;ch.qos.logback.classic.AsyncAppender&quot;&gt; &lt;!-- 不丢失日志。默认的，如果队列的80%已满，则会丢弃TRACT、DEBUG、INFO级别的日志 --&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;!-- 更改默认的队列的深度，该值会影响性能。默认值为256 --&gt; &lt;queueSize&gt;100&lt;/queueSize&gt; &lt;!-- 添加附加的appender，最多只能添加一个，此处指定后，在root下不要再指定该appender，否则会输出两次 --&gt; &lt;appender-ref ref=&quot;KafkaAppender&quot;/&gt; &lt;/appender&gt; &lt;!--日志异步到数据库：未做测试，配置正确与否未知，先记录于此 --&gt; &lt;!--&lt;appender name=&quot;DB&quot; class=&quot;ch.qos.logback.classic.db.DBAppender&quot;&gt; &lt;connectionSource class=&quot;ch.qos.logback.core.db.DriverManagerConnectionSource&quot;&gt; &lt;dataSource class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;driverClass&gt;com.mysql.jdbc.Driver&lt;/driverClass&gt; &lt;url&gt;jdbc:mysql://127.0.0.1:3306/databaseName&lt;/url&gt; &lt;user&gt;root&lt;/user&gt; &lt;password&gt;root&lt;/password&gt; &lt;/dataSource&gt; &lt;/connectionSource&gt; &lt;/appender&gt;--&gt; &lt;!-- 关闭指定包下的日志输出，name里面的内容可以是包路径，或者具体要忽略的文件名称 --&gt; &lt;logger name=&quot;org.apache.flink&quot; level=&quot;OFF&quot;/&gt; &lt;!-- 将指定包下指定级别的日志，输出到指定的appender中 addtivity：是否向上级logger传递打印信息。默认是true。若此包下的日志单独输出到文件中，应设置为false，否则在root日志也会记录一遍 --&gt; &lt;logger name=&quot;org.apache.kafka&quot; level=&quot;ERROR&quot; addtivity=&quot;false&quot;&gt; &lt;!-- 指定此包下的error级别信息，输出到指定的收集文件 --&gt; &lt;appender-ref ref=&quot;ErrorFileAppender&quot;/&gt; &lt;/logger&gt; &lt;root level=&quot;$&#123;logging.level&#125;&quot;&gt; &lt;!--&lt;appender-ref ref=&quot;STDOUT&quot;/&gt;--&gt; &lt;!--&lt;appender-ref ref=&quot;FileAppender&quot;/&gt;--&gt; &lt;appender-ref ref=&quot;ASYNC&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt; 根据实际情况，对 appender 进行取舍，实际使用时不要所有的都添加到 logback.xml 配置文件中。 本文参考https://kucw.github.io/blog/2020/3/java-lombok/ https://juejin.cn/post/6844903641535479821 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"hadoop-hdfs","slug":"hadoop-hdfs","date":"2021-01-03T13:59:31.000Z","updated":"2021-01-05T07:30:50.217Z","comments":true,"path":"2021/01/03/hadoop-hdfs/","link":"","permalink":"http://example.com/2021/01/03/hadoop-hdfs/","excerpt":"","text":"HDFS 常用 shell 命令操作 HDFS 的 shell命令有三种： hadoop fs：适用于任何不同的文件系统，比如本地文件系统和 HDFS 文件系统。 hadoop dfs：只适用于 HDFS 文件系统。 hdfs dfs：只适用于 HDFS 文件系统。 官方不推荐使用第二种命令 hadoop dfs，有些 Hadoop 版本中已将这种命令弃用。 语法1hadoop fs [genericOptions] [commandOptions] 参数说明 HDFS 常用命令 说明 hadoop fs -ls 显示指定文件的详细信息 hadoop fs -cat 将指定文件的内容输出到标准输出 hadoop fs touchz 创建一个指定的空文件 hadoop fs -mkdir [-p] 创建指定的一个或多个文件夹，-p 选项用于递归创建 hadoop fs -cp 将文件从源路径复制到目标路径 hadoop fs -mv 将文件从源路径移动到目标路径 hadoop fs -rm 删除指定的文件，只删除非空目录和文件 hadoop fs -rm -r 删除指定的文件夹及其下的所有文件，-r 表示递归删除子目录 hadoop fs -chown 改变指定文件的所有者，该命令仅适用于超级用户 hadoop fs -chmod 将指定的文件权限更改为可执行文件，该命令仅适用于超级用户和文件所有者 hadoop fs -get 复制指定的文件到本地文件系统指定的文件或文件夹 hadoop fs -put 从本地文件系统中复制指定的单个或多个源文件到指定的目标文件系统 hadoop fs -moveFromLocal 与 -put 命令功能相同，但是文件上传结束后会删除源文件 hadoop fs -copyFromLocal 与 -put 命令功能相同，将本地源文件复制到路径指定的文件或文件夹中 hadoop fs -copyToLocal 与 -get命令功能相同，将目标文件复制到本地文件或文件夹中","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://example.com/tags/hadoop/"}]},{"title":"java 线程池","slug":"java-threadpool","date":"2021-01-01T11:44:47.000Z","updated":"2021-01-12T09:20:16.752Z","comments":true,"path":"2021/01/01/java-threadpool/","link":"","permalink":"http://example.com/2021/01/01/java-threadpool/","excerpt":"","text":"线程池的理解线程池是预先创建线程的一种技术，线程池在还没有任务到来之前，事先创建一定数量的线程，放入空闲队列中，然后对这些资源进行复用，从而减少频繁的创建和销毁对象。 系统启动一个新线程的成本是比较高的，因为它涉及与操作系统交互。在这种情形下，使用线程池可以很好地提高性能，尤其是当程序中需要创建大量生存期很短暂的线程时，更应该考虑使用线程池。 与数据库连接池类似的是，线程池在系统启动时即创建大量空闲的线程，程序将一个 Runnable 对象或 Callable 对象传给线程池，线程池就会启动一个线程来执行它们的 run() 或 call() 方法， 当 run() 或 call() 方法执行结束后， 该线程并不会死亡，而是再次返回线程池中成为空闲状态，等待执行下一个 Runnable 对象的 run() 或 call() 方法。 总结：由于系统创建和销毁线程都是需要时间和系统资源开销，为了提高性能，才考虑使用线程池。线程池会在系统启动时就创建大量的空闲线程，然后等待新的线程调用，线程执行结束并不会销毁，而是重新进入线程池，等待再次被调用。这样子就可以减少系统创建启动和销毁线程的时间，提高系统的性能。 线程池的使用使用 Executors 创建线程池 Executor 是线程池的顶级接口，接口中只定义了一个方法 void execute(Runnable command);，线程池的操作方法都是定义在 ExecutorService 子接口中的，所以说 ExecutorService 是线程池真正的接口。 newSingleThreadExecutor创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; newFixedThreadPool创建固定大小的线程池，每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到是大值就会保持不变。如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 123456public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);&#125; newCachedThreadPool创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲 (60 秒不执行任务) 的线程。当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对钱程池大小做限制，线程池大小完全依赖于操作系统 (或者说 JVM) 能够创建的最大线程大小。 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 使用 ThreadPoolExecutor 创建线程池123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 构造函数参数说明corePoolSize：核心线程数大小，当线程数小于 corePoolSize 的时候，会创建线程执行 runnable。 maximumPoolSize：最大线程数， 当线程数大于等于 corePoolSize 的时候，会把 runnable 放入 workQueue 中。 keepAliveTime：保持存活时间，当线程数大于 corePoolSize 的时候，空闲线程能保持的最大时间。 unit：时间单位。 workQueue：保存任务的阻塞队列。 threadFactory：创建线程的工厂。 handler：拒绝策略。 任务执行顺序 当线程数小于 corePoolSize 时，创建线程执行新任务。 当线程数大于等于 corePoolSize，并且 workQueue 没有满时，新任务放入 workQueue 中。 当线程数大于等于 corePoolSize，并且 workQueue 满时，新任务创建新线程运行，但线程总数要小于 maximumPoolSize。 当线程总数等于 maximumPoolSize，并且 workQueue 满时，执行 handler 的 rejectedExecution，也就是拒绝策略。 阻塞队列阻塞队列是一个在队列基础上又支持了两个附加操作的队列： 支持阻塞的插入方法：队列满时，队列会阻塞插入元素的线程，直到队列不满。 支持阻塞的移除方法：队列空时，获取元素的线程会等待队列变为非空。 阻塞队列的应用场景阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。简而言之，阻塞队列是生产者用来存放元素、消费者获取元素的容器。 阻塞队列的方法在阻塞队列不可用的时候，上述两个附加操作提供了四种处理方法： 方法处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e,time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() 不可用 不可用 阻塞队列的类型jdk 7 提供了 7 个阻塞队列，如下： ArrayBlockingQueue：数组结构组成的有界阻塞队列。 此队列按照先进先出 (FIFO) 的原则对元素进行排序，但是默认情况下不保证线程公平的访问队列，即如果队列满了，那么被阻塞在外面的线程对队列访问的顺序是不能保证线程公平 (即先阻塞，先插入) 的。 LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。 此队列按照先出先进的原则对元素进行排序。 PriorityBlockingQueue：支持优先级的无界阻塞队列。 DelayQueue：支持延时获取元素的无界阻塞队列，即可以指定多久才能从队列中获取当前元素。 SynchronousQueue：不存储元素的阻塞队列，每一个 put 必须等待一个 take 操作，否则不能继续添加元素。并且他支持公平访问队列。 LinkedTransferQueue：由链表结构组成的无界阻塞 TransferQueue 队列。 相对于其他阻塞队列，多了 tryTransfer 和 transfer 方法： transfer方法：如果当前有消费者正在等待接收元素 (take 或者待时间限制的 poll 方法)，transfer 可以把生产者传入的元素立刻传给消费者。如果没有消费者等待接收元素，则将元素放在队列的 tail 节点，并等到该元素被消费者消费了才返回。 tryTransfer方法：用来试探生产者传入的元素能否直接传给消费者。如果没有消费者在等待，则返回 false。和上述方法的区别是该方法无论消费者是否接收，方法立即返回，而 transfer 方法是必须等到消费者消费了才返回。 LinkedBlockingDeque：链表结构的双向阻塞队列，优势在于多线程入队时，减少一半的竞争。 拒绝策略当队列和线程池都满了，说明线程池处于饱和的状态，那么必须采取一种策略处理提交的新任务。ThreadPoolExecutor 默认有四个拒绝策略： ThreadPoolExecutor.AbortPolicy()：默认策略，直接抛出异常 RejectedExecutionException。 java.util.concurrent.RejectedExecutionException： 当线程池 ThreadPoolExecutor 执行方法 shutdown () 之后，再向线程池提交任务的时候，如果配置的拒绝策略是 AbortPolicy ，这个异常就会抛出来。 当设置的任务缓存队列过小的时候，或者说，线程池里面所有的线程都在干活（线程数等于 maxPoolSize)，并且任务缓存队列也已经充满了等待的队列， 这个时候，再向它提交任务，也会抛出这个异常。 ThreadPoolExecutor.CallerRunsPolicy()：直接调用 run () 方法并且阻塞执行。 ThreadPoolExecutor.DiscardPolicy()：不处理，直接丢弃后来的任务。 ThreadPoolExecutor.DiscardOldestPolicy()：丢弃在队列中队首的任务，并执行当前任务。 当然可以继承 RejectedExecutionHandler 来自定义拒绝策略。 线程池参数选择CPU 密集型：线程池的大小推荐为 CPU 数量 +1。CPU 数量可以根据 Runtime.getRuntime().availableProcessors() 方法获取。 IO 密集型：CPU 数量 * CPU 利用率 * (1 + 线程等待时间 / 线程 CPU 时间)。 混合型：将任务分为 CPU 密集型和 IO 密集型，然后分别使用不同的线程池去处理，从而使每个线程池可以根据各自的工作负载来调整。 阻塞队列：推荐使用有界队列，有界队列有助于避免资源耗尽的情况发生。 拒绝策略：默认采用的是 AbortPolicy 拒绝策略，直接在程序中抛出 RejectedExecutionException 异常，因为是运行时异常，不强制 catch，但这种处理方式不够优雅。处理拒绝策略有以下几种比较推荐： 在程序中捕获 RejectedExecutionException 异常，在捕获异常中对任务进行处理。针对默认拒绝策略。 使用 CallerRunsPolicy 拒绝策略，该策略会将任务交给调用 execute 的线程执行 (一般为主线程)，此时主线程将在一段时间内不能提交任何任务，从而使工作线程处理正在执行的任务。此时提交的线程将被保存在 TCP 队列中，TCP 队列满将会影响客户端，这是一种平缓的性能降低。 自定义拒绝策略，只需要实现 RejectedExecutionHandler 接口即可。 如果任务不是特别重要，使用 DiscardPolicy 和 DiscardOldestPolicy 拒绝策略将任务丢弃也是可以的。 如果使用 Executors 的静态方法创建 ThreadPoolExecutor 对象，可以通过使用 Semaphore 对任务的执行进行限流也可以避免出现 OOM 异常。 示例1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class TestThreadPoolExecutor &#123; public static void main(String[] args) &#123; long startTimeMillis = System.currentTimeMillis(); // 构造一个线程池 ThreadPoolExecutor threadPool = new ThreadPoolExecutor(5, 6, 3, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3) ); for (int i = 1; i &lt;= 10; i++) &#123; try &#123; String task = &quot;task = &quot; + i; System.out.println(&quot;创建任务并提交到线程池中：&quot; + task); threadPool.execute(new ThreadPoolTask(task)); Thread.sleep(100); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; try &#123; // 等待所有线程执行完毕当前任务 threadPool.shutdown(); boolean loop = true; do &#123; // 等待所有线程执行完毕，当前任务结束 loop = !threadPool.awaitTermination(2, TimeUnit.SECONDS);// 等待2秒 &#125; while (loop); if (!loop) &#123; System.out.println(&quot;所有线程执行完毕&quot;); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(&quot;耗时：&quot; + (System.currentTimeMillis() - startTimeMillis)); &#125; &#125;&#125; 12345678910111213141516171819import java.io.Serializable;public class ThreadPoolTask implements Runnable, Serializable &#123; private String attachData; public ThreadPoolTask(String tasks) &#123; this.attachData = tasks; &#125; public void run() &#123; try &#123; System.out.println(&quot;开始执行：&quot; + attachData + &quot;任务，使用的线程池，线程名称：&quot; + Thread.currentThread().getName() + &quot;\\r\\n&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; attachData = null; &#125;&#125; 运行结果，可以看到线程 pool-1-thread-1 到 pool-1-thread-5 循环使用： 1234567891011121314151617181920212223242526272829303132创建任务并提交到线程池中：task = 1开始执行：task = 1任务，使用的线程池，线程名称：pool-1-thread-1创建任务并提交到线程池中：task = 2开始执行：task = 2任务，使用的线程池，线程名称：pool-1-thread-2创建任务并提交到线程池中：task = 3开始执行：task = 3任务，使用的线程池，线程名称：pool-1-thread-3创建任务并提交到线程池中：task = 4开始执行：task = 4任务，使用的线程池，线程名称：pool-1-thread-4创建任务并提交到线程池中：task = 5开始执行：task = 5任务，使用的线程池，线程名称：pool-1-thread-5创建任务并提交到线程池中：task = 6开始执行：task = 6任务，使用的线程池，线程名称：pool-1-thread-1创建任务并提交到线程池中：task = 7开始执行：task = 7任务，使用的线程池，线程名称：pool-1-thread-2创建任务并提交到线程池中：task = 8开始执行：task = 8任务，使用的线程池，线程名称：pool-1-thread-3创建任务并提交到线程池中：task = 9开始执行：task = 9任务，使用的线程池，线程名称：pool-1-thread-4创建任务并提交到线程池中：task = 10开始执行：task = 10任务，使用的线程池，线程名称：pool-1-thread-5所有线程执行完毕耗时：1014 本文参考https://segmentfault.com/a/1190000011527245 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"java String 类的常用方法","slug":"java-string","date":"2021-01-01T11:31:54.000Z","updated":"2021-01-06T09:19:53.849Z","comments":true,"path":"2021/01/01/java-string/","link":"","permalink":"http://example.com/2021/01/01/java-string/","excerpt":"","text":"和 byte 数组转换不指定编码123456789101112public class Test &#123; public static void main(String[] args) &#123; //string 转 byte[] String str = &quot;Hello&quot;; byte[] strBytes = str.getBytes(); System.out.println(strBytes); // byte[] 转 string String res = new String(strBytes); System.out.println(res); &#125;&#125; 默认字符 UTF-8。 指定编码123456789101112public class Test &#123; public static void main(String[] args) &#123; String str = &quot;你好&quot;; try &#123; byte[] strByte = str.getBytes(&quot;GBK&quot;); String res = new String(strByte, &quot;GBK&quot;); System.out.println(res); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 包含12345678public class Test &#123; public static void main(String[] args) &#123; String a = &quot;szrfrrgdhjd&quot;; System.out.println(a.contains(&quot;g&quot;)); &#125;&#125;输出结果：true 位置12345678public class Test &#123; public static void main(String[] args) &#123; String a = &quot;qwertyu&quot;; System.out.println(a.indexOf(&quot;e&quot;)); &#125;&#125;输出结果：2 返回第一个出现的指定字符的位置。 分割12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; // 以A作为分割点，将字符串a分割为2个字符串数组 String a = &quot;abcdeAfghijk&quot;; String[] b = a.split(&quot;A&quot;); System.out.println(&quot;b[0]: &quot; + b[0] + &quot;, b[1]: &quot; + b[1]); // A有多个，则以每个A作为分割点 String a2 = &quot;abcdeAfghAijk&quot;; String[] b2 = a2.split(&quot;A&quot;); System.out.println(&quot;b2[0]: &quot; + b2[0] + &quot;, b2[1]: &quot; + b2[1] + &quot;, b2[2]: &quot; + b2[2]); &#125;&#125;输出结果：b[0]: abcde, b[1]: fghijkb2[0]: abcde, b2[1]: fgh, b2[2]: ijk 截取123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; String s = &quot;&#123;name=段炼, age=25, sex=男, id=12, hobby=吃饭&#125;&quot;; if (s.contains(&quot;id&quot;)) &#123; // 拿到id字段的位置 int start = s.indexOf(&quot;id&quot;); // 拿到hobby字段的位置 int end = s.indexOf(&quot;hobby&quot;); // start + 3：id开始的地方，end - 2：id结束的位置 String a = s.substring(start + 3, end - 2); // 得到id后转换成int类型 int id = Integer.parseInt(a); System.out.println(id); &#125; &#125;&#125;输出结果：12 与 indexOf 结合使用： 截取第二个 “-“ 之前的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(0, s.indexOf(&quot;-&quot;, s.indexOf(&quot;-&quot;) + 1)); System.out.println(r); &#125;&#125;输出结果：application-2005 截取第二个 “-“ 之后的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(s.indexOf(&quot;-&quot;, s.indexOf(&quot;-&quot;) + 1) + 1); System.out.println(r); &#125;&#125;输出结果：US20050154023A1-20050714 截取倒数第二个 “-“ 之前的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(0, s.lastIndexOf(&quot;-&quot;, s.lastIndexOf(&quot;-&quot;) - 1)); System.out.println(r); &#125;&#125;输出结果：application-2005 截取倒数第二个 “-“ 之后的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(s.lastIndexOf(&quot;-&quot;, s.lastIndexOf(&quot;-&quot;) - 1) + 1); System.out.println(r); &#125;&#125;输出结果：US20050154023A1-20050714 替换123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;abcde8fghijk8lmn&quot;; String a = s.replace(&#x27;8&#x27;, &#x27;Q&#x27;); System.out.println(a); &#125;&#125;输出结果：abcdeQfghijkQlmn 替换第一个： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;QQQQabcWWWabcGGGGabc&quot;; String a = s.replaceFirst(&quot;abc&quot;, &quot;PPP&quot;); System.out.println(a); &#125;&#125;输出结果：QQQQPPPWWWabcGGGGabc 全部替换： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;QQQQabcWWWabcGGGGabc&quot;; String a = s.replaceFirst(&quot;abc&quot;, &quot;PPP&quot;); System.out.println(a); &#125;&#125;输出结果：QQQQPPPWWWPPPGGGGPPP 去空格123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot; QQQQ abc WWW abc GGGG abc &quot;; String a = s.trim(); System.out.println(a); &#125;&#125;输出结果：QQQQ abc WWW abc GGGG abc 去除的是字符串前后的空格。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"redis 命令","slug":"redis-command","date":"2020-12-31T09:10:36.000Z","updated":"2020-12-31T09:12:10.982Z","comments":true,"path":"2020/12/31/redis-command/","link":"","permalink":"http://example.com/2020/12/31/redis-command/","excerpt":"","text":"查询当前库 key 的个数info可以看到所有库的key数量 dbsize则是当前库key的数量 keys *这种数据量小还可以，大的时候可以直接搞死生产环境。 dbsize和keys *统计的key数可能是不一样的，如果没记错的话，keys *统计的是当前db有效的key，而dbsize统计的是所有未被销毁的key（有效和未被销毁是不一样的，具体可以了解redis的过期策略）","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}]},{"title":"Kafka 命令行工具","slug":"kafka-command","date":"2020-12-30T02:19:35.000Z","updated":"2021-01-08T06:25:02.618Z","comments":true,"path":"2020/12/30/kafka-command/","link":"","permalink":"http://example.com/2020/12/30/kafka-command/","excerpt":"","text":"查看 Kafka topic 列表命令，返回 topic 名字列表 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --list 创建 Kafka topic 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181,hadoopdatanode2:2181,hadoopdatanode3:2181 --create --partitions 6 --replication-factor 2 --topic patent-grant 查看 Kafka 指定 topic 的详情命令，返回该 topic 的 parition 数量、replica 因子以及每个 partition 的 leader、replica 信息 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --describe --topic patent-grant 查看 Kafka 指定 topic 各 partition 的 offset 信息命令，–time 参数为 -1 时，表示各分区最大的 offset，为 -2 时，表示各分区最小的 offset 1$ ~/kafka_2.12-2.6.0/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list hadoopdatanode1:9092 --time -1 --topic patent-grant 删除 Kafka topic 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --delete -topic patent-grant 只有 topic 不再被使用时，才能被删除。 查看 kafka consumer group 命令，返回 consumer group 名字列表 (新版信息保存在 broker 中，老版信息保存在 zookeeper 中，二者命令不同) 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --list 老版命令：~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --zookeeper hadoopdatanode1:2181 --list 查看 Kafka 指定 consumer group 的详情命令，返回 consumer group 对应的 topic 信息、当前消费的 offset、总 offset、剩余待消费 offset 等信息 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --describe --group log-consumer 重置 Kafka 指定 consumer group 消费的 topic 的 offset 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --reset-offsets -to-offset 0 --execute --topic patent-app --group log-consumer 删除 Kafka 指定 consumer group 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --delete --group log-consumer 消费 Kafka 指定 topic 的内容命令 kafka-console-consumer.sh 脚本是一个简易的消费者控制台。该 shell 脚本的功能通过调用 kafka.tools 包下的 ConsoleConsumer 类，并将提供的命令行参数全部传给该类实现。 参数说明： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.shThis tool helps to read data from Kafka topics and outputs it to standard output.Option Description ------ ----------- --bootstrap-server &lt;String: server to REQUIRED: The server(s) to connect to. connect to&gt; --consumer-property &lt;String: A mechanism to pass user-defined consumer_prop&gt; properties in the form key=value to the consumer. --consumer.config &lt;String: config file&gt; Consumer config properties file. Note that [consumer-property] takes precedence over this config. --enable-systest-events Log lifecycle events of the consumer in addition to logging consumed messages. (This is specific for system tests.) --formatter &lt;String: class&gt; The name of a class to use for formatting kafka messages for display. (default: kafka.tools. DefaultMessageFormatter) --from-beginning If the consumer does not already have an established offset to consume from, start with the earliest message present in the log rather than the latest message. --group &lt;String: consumer group id&gt; The consumer group id of the consumer. --help Print usage information. --isolation-level &lt;String&gt; Set to read_committed in order to filter out transactional messages which are not committed. Set to read_uncommitted to read all messages. (default: read_uncommitted)--key-deserializer &lt;String: deserializer for key&gt; --max-messages &lt;Integer: num_messages&gt; The maximum number of messages to consume before exiting. If not set, consumption is continual. --offset &lt;String: consume offset&gt; The offset id to consume from (a non- negative number), or &#x27;earliest&#x27; which means from beginning, or &#x27;latest&#x27; which means from end (default: latest) --partition &lt;Integer: partition&gt; The partition to consume from. Consumption starts from the end of the partition unless &#x27;--offset&#x27; is specified. --property &lt;String: prop&gt; The properties to initialize the message formatter. Default properties include: print.timestamp=true|false print.key=true|false print.value=true|false key.separator=&lt;key.separator&gt; line.separator=&lt;line.separator&gt; key.deserializer=&lt;key.deserializer&gt; value.deserializer=&lt;value. deserializer&gt; Users can also pass in customized properties for their formatter; more specifically, users can pass in properties keyed with &#x27;key. deserializer.&#x27; and &#x27;value. deserializer.&#x27; prefixes to configure their deserializers. --skip-message-on-error If there is an error when processing a message, skip it instead of halt. --timeout-ms &lt;Integer: timeout_ms&gt; If specified, exit if no message is available for consumption for the specified interval. --topic &lt;String: topic&gt; The topic id to consume on. --value-deserializer &lt;String: deserializer for values&gt; --version Display Kafka version. --whitelist &lt;String: whitelist&gt; Regular expression specifying whitelist of topics to include for consumption. 参数说明参考：https://blog.csdn.net/qq_29116427/article/details/80206125 从头开始消费： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --from-beginning --topic log-collect 从头开始消费前 10 条消息，并显示 key： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --from-beginning --max-messages 10 --property print.key=true --topic log-collect 从指定分区、指定 offset 开始消费： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --partition 0 --offset 219000 --topic log-collect 从尾开始消费，必须指定分区： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --partition 0 --offset latest --topic log-collect","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"shell 脚本中的一些方法","slug":"shell","date":"2020-12-22T09:16:35.000Z","updated":"2021-01-01T08:13:35.647Z","comments":true,"path":"2020/12/22/shell/","link":"","permalink":"http://example.com/2020/12/22/shell/","excerpt":"","text":"字符串截取shell 截取字符串通常有两种方式：从指定位置开始截取和从指定字符 (子字符串) 开始截取。 从指定位置开始截取两个参数：起始位置，截取长度。 从字符串左边开始计数1$&#123;string: start :length&#125; 其中，string 是要截取的字符串，start 是起始位置 (从左边开始，从 0 开始计数)，length 是要截取的长度 (如果省略表示直到字符串的末尾)。 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 2: 9&#125;biancheng 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 2&#125;biancheng.net 从字符串右边开始计数1$&#123;string: 0-start :length&#125; 0- 是固定的写法，专门用来表示从字符串右边开始计数。 从左边开始计数时，起始数字是 0；从右边开始计数时，起始数字是 1。计数方向不同，起始数字也不同。 不管从哪边开始计数，截取方向都是从左到右。 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 0-13: 9&#125;biancheng 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 0-13&#125;biancheng.net 从右边数，b 是第 13 个字符。 从指定字符 (子字符串) 开始截取这种截取方式无法指定字符串长度，只能从指定字符 (子字符串) 截取到字符串末尾。shell 可以截取指定字符 (子字符串) 右边的所有字符，也可以截取左边的所有字符。 使用 # 截取指定字符 (子字符串)右边字符1$&#123;string#*chars&#125; 其中，string 表示要截取的字符，chars 是指定的字符 (子字符串)，* 是通配符的一种，表示任意长度的字符串。*chars 连起来使用的意思是：忽略左边的所有字符，直到遇见 chars (chars 不会被截取)。 从左往右看。 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*:&#125;//c.biancheng.net/index.html 以下写法也可以得到同样的结果： 12echo $&#123;url#*p:&#125;echo $&#123;url#*ttp:&#125; 如果不需要忽略 chars 左边的字符，那么也可以不写 *，例如： 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#http://&#125;c.biancheng.net/index.html 注意，以上写法遇到第一个匹配的字符 (子字符串) 就结束了。例如： 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*/&#125;/c.biancheng.net/index.html url 字符串中有三个 /，输出结果表明，shell 遇到第一个 / 就匹配结束了。 如果希望直到最后一个指定字符 (子字符串) 再匹配结束，那么可以使用 ##，具体格式为： 1$&#123;string##*chars&#125; 123456789#!/bin/bashurl=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*/&#125; # 结果为 /c.biancheng.net/index.htmlecho $&#123;url##*/&#125; # 结果为 index.htmlstr=&quot;---aa+++aa@@@&quot;echo $&#123;str#*aa&#125; # 结果为 +++aa@@@echo $&#123;str##*aa&#125; # 结果为 @@@ 使用 % 截取指定字符 (子字符串)左边字符1$&#123;string%chars*&#125; 注意 * 的位置，因为要截取 chars 左边的字符，而忽略 chars 右边的字符，所以 * 应该位于 chars 的右侧。其他方面 % 和 # 的用法相同。 从右往左看。 123456789#!/bin/bashurl=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url%/*&#125; # 结果为 http://c.biancheng.netecho $&#123;url%%/*&#125; # 结果为 http:str=&quot;---aa+++aa@@@&quot;echo $&#123;str%aa*&#125; # 结果为 ---aa+++echo $&#123;str%%aa*&#125; # 结果为 --- 汇总 格式 说明 ${string: start :length} 从 string 字符串的左边第 start 个字符开始，向右截取 length 个字符。 ${string: start} 从 string 字符串的左边第 start 个字符开始截取，直到最后。 ${string: 0-start :length} 从 string 字符串的右边第 start 个字符开始，向右截取 length 个字符。 ${string: 0-start} 从 string 字符串的右边第 start 个字符开始截取，直到最后。 ${string#*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string##*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string%*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 ${string%%*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 参考： http://c.biancheng.net/view/1120.html 字符串拼接12345678910111213141516171819202122#!/bin/bashname=&quot;Shell&quot;url=&quot;http://c.biancheng.net/shell/&quot;str1=$name$url # 中间不能有空格str2=&quot;$name $url&quot; # 如果被双引号包围，那么中间可以有空格str3=$name&quot;: &quot;$url # 中间可以出现别的字符串str4=&quot;$name: $url&quot; # 这样写也可以str5=&quot;$&#123;name&#125;Script: $&#123;url&#125;index.html&quot; # 这个时候需要给变量名加上大括号echo $str1echo $str2echo $str3echo $str4echo $str5Shellhttp://c.biancheng.net/shell/Shell http://c.biancheng.net/shell/Shell: http://c.biancheng.net/shell/Shell: http://c.biancheng.net/shell/ShellScript: http://c.biancheng.net/shell/index.html 对于第 7 行代码，$name 和 $url 之间之所以不能出现空格，是因为当字符串不被任何一种引号包围时，遇到空格就认为字符串结束了，空格后边的内容会作为其他变量或者命令解析。 对于第 10 行代码，加 { } 是为了帮助解释器识别变量的边界。 字符串分割以空格为分隔符比如有一个变量 “123 456 789”，要求以空格为分隔符把这个变量分隔，并把分隔后的字段分别赋值给变量，即 a=123；b=456；c=789。共有3中方法：方法一：先定义一个数组，然后把分隔出来的字段赋值给数组中的每一个元素方法二：通过 eval+ 赋值的方式方法三：通过多次 awk 把每个字段赋值 1234567891011121314151617181920212223242526272829#!/bin/basha=&quot;123 456 789&quot;# 方法一：通过数组的方式declare -a arrindex=0for i in $(echo $a | awk &#x27;&#123;print $1,$3&#125;&#x27;)do arr[$index]=$i let &quot;index+=1&quot;doneecho $&#123;arr[0]&#125; # 结果为 123echo $&#123;arr[1]&#125; # 结果为 789# 方法二：通过eval+赋值的方式b=&quot;&quot;c=&quot;&quot;eval $(echo $a | awk &#x27;&#123; printf(&quot;b=%s;c=%s&quot;,$2,$1)&#125;&#x27;)echo $b # 结果为 456echo $c # 结果为 123# 方法三：通过多次awk赋值的方式m=&quot;&quot;n=&quot;&quot;m=`echo $a | awk &#x27;&#123;print $1&#125;&#x27;`n=`echo $a | awk &#x27;&#123;print $2&#125;&#x27;`echo $m # 结果为 123echo $n # 结果为 456 指定分隔符123456789101112131415161718192021222324252627#!/bin/bashstring=&quot;hello,shell,haha&quot;# 方法一 array=($&#123;string//,/ &#125;) for var in $&#123;array[@]&#125;do echo $vardone# 方法二IFS=&quot;,&quot;OLD_IFS=&quot;$IFS&quot;IFS=&quot;$OLD_IFS&quot;array2=($string)for var2 in $&#123;array2[@]&#125;do echo $var2donehelloshellhahahelloshellhaha 变量赋值反引号： 1var=`command` $()： 1var=$(command) 例如： 1234567$ A=`date`$ echo $AFri Dec 25 20:02:30 CST 2020 # 变量A存放了date命令的执行结果$ B=$(date)$ echo $BFri Dec 25 20:03:12 CST 2020 # 变量B存放了date命令的执行结果 注意：= 号前后不要有空格。 参考： https://book.51cto.com/art/201411/457601.htm 判断文件夹是否存在1234567#!/bin/bashif [ ! -d testgrid ];then mkdir testgridelse echo dir existfi 外部传参： 12345678910111213141516#!/bin/bash# 判断传入的参数的个数是不是一个if [ ! $# -eq 1 ];then echo param error! exit 1fi# 判断目录是不是已经存在，如果不存在则创建，存在则输出&quot;dir exist&quot; dirname=$1echo &quot;the dir name is $dirname&quot;if [ ! -d $dirname ];then mkdir $dirnameelse echo dir existfi 循环类 C 语言： 123456# !/bin/shfor ((i=1; i&lt;=100; i ++))do echo $idone in 使用： 123456# !/bin/shfor i in &#123;1..100&#125;do echo $idone seq 使用： 123456# !/bin/shfor i in `seq 1 100`do echo $idone 发送微信消息https://blog.csdn.net/whatday/article/details/105781861","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"linux 命令","slug":"linux-command","date":"2020-12-15T09:31:47.000Z","updated":"2021-01-01T11:35:41.779Z","comments":true,"path":"2020/12/15/linux-command/","link":"","permalink":"http://example.com/2020/12/15/linux-command/","excerpt":"","text":"find用于在指定目录下查找文件。默认列出当前目录及子目录下所有的文件和文件夹。 语法1find path -option [ -print ] [ -exec -ok command ] &#123;&#125; \\; -print： find 命令将匹配到的文件输出到标准输出。 -exec： find 命令对匹配的文件执行该参数所给出的 Shell 命令。 -ok： 和 -exec 的作用相同，只是更安全，在执行每个命令之前，都会给出提示，让用户来确定是否执行。 参数说明1234567891011121314151617181920212223242526272829303132333435363738394041424344-mount, -xdev 只检查和指定目录在同一个文件系统下的文件，避免列出其它文件系统中的文件。-amin n 在过去n分钟内被读取过。-anewer file 比文件file更晚被读取过的文件。-atime n 在过去n天内被读取过的文件。 -cmin n 在过去n分钟内被修改过。 -cnewer file 比文件file更新的文件。 -ctime n 在过去n天内被修改过的文件。 -empty 空的文件。 -gid n gid是n。 -group name group名称是name。 -path p | -ipath p 路径名称符合p的文件。ipath忽略大小写。 -name name | -iname name 文件名称符合name的文件。iname忽略大小写。 -size n 文件大小是n单位，b代表512位元组的区块，c表示字元数，k表示kilo bytes，w是二个位元组。 -type d|f 文件类型是d|f的文件。 -pid n process id是n的文件。 文件类型：d — 目录，f — 一般文件，c — 字型装置文件，b — 区块装置文件，p — 具名贮列，l — 符号连结，s — socket。 实例 查询当前路径下的所有目录|普通文件 12$ find ./ -type d$ find ./ -type f 查询权限为 777 的普通文件 1$ find ./ -type f -perm 777 查询 .XML 文件，且权限不为 777 1$ find ./ -type f -name &quot;*.XML&quot; ! -perm 777 查询 .XML 文件，并统计查询结果的条数 1$ find ./ -name &quot;*.XML&quot; | wc -l 查询 .XML 文件，并复制查询结果到指定路径 12$ find ./ -name &quot;*.XML&quot; | xargs -i cp &#123;&#125; ../111$ find ./ -name &quot;*.XML&quot; -exec cp &#123;&#125; ../111 \\; 此命令不同于 cp，cp *.XML ../111 命令复制的是当前路径下符合条件的所有文件，子路径的不会被复制。 查询 .XML 文件，并删除 12$ find ./ -name &quot;*.XML&quot; | xargs -i rm &#123;&#125;$ find ./ -name &quot;*.XML&quot; -exec rm &#123;&#125; \\; 此命令不同于 rm，rm *.XML 命令删除的是当前路径下符合条件的所有文件，子路径的不会被删除。 查询 .XML 文件，并将查询结果以 “File: 文件名” 的形式打印出来 12$ find ./ -name &quot;*.XML&quot; | xargs -i printf &quot;File: %s\\n&quot; &#123;&#125;$ find ./ -name &quot;*.XML&quot; -exec printf &quot;File: %s\\n&quot; &#123;&#125; \\; 将当前路径及子路径下所有 3 天前的 .XML 格式的文件复制一份到指定路径 1$ find ./ -name &quot;*.XML&quot; -mtime +3 -exec cp &#123;&#125; ../111 \\; 查询多个文件后缀类型的文件 12$ find ./ -regextype posix-extended -regex &quot;.*\\.(java|xml|XML)&quot; # 查找所有的.java、.xml和.XML文件$ find ./ -name &quot;*.java&quot; -o -name &quot;*.xml&quot; -o -name &quot;*.XML&quot; # -o选项，适用于查询少量文件后缀类型 组合查询，可以多次拼接查询条件 1234$ find ./ -name &quot;file1*&quot; -a -name &quot;*.xml&quot; # -a：与，查找以file1开头，且以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; -o -name &quot;*.xml&quot; # -o：或，查找以file1开头，或以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; -not -name &quot;*.xml&quot; # -not：非，查找以file1开头，且不以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; ! -name &quot;file2*&quot; # !：同-not 查询当前目录下文件类型为 d 的文件，不包含子目录 1$ find ./ -maxdepth 1 -type d 和正则表达式的结合使用 12$ find ./ –name &quot;[^abc]*&quot; # 在当前路径中搜索不以a、b、c开头的所有文件$ find ./ -name &quot;[A-Z0-9]*&quot; # 在当前路径中搜索以大写字母或数字开头的所有文件 正则表达式符号含义： * 代表任意字符 (可以没有字符) ? 代表任意单个字符 [] 代表括号内的任意字符，如 [abc] 可以匹配 a\\b\\c 某个字符 [a-z] 可以匹配 a-z 的某个字母 [A-Z] 可以匹配 A-Z 的某个字符 [0-9] 可以匹配 0-9 的某个数字 ^ 用在 [] 内的前缀表示不匹配 [] 中的字符 [^a-z] 表示不匹配a-z的某个字符 参考： https://www.jianshu.com/p/b30a8aa4d1f1 https://www.oracle.com/cn/technical-resources/articles/linux-calish-find.html https://www.cnblogs.com/qmfsun/p/3811142.html https://www.cnblogs.com/ay-a/p/8017419.html cat用于连接文件或标准输入并打印。这个命令常用来显示文件内容，或者将几个文件连接起来显示，或者从标准输入读取内容并显示，它常与重定向符号配合使用。 语法1cat [-AbeEnstTuv] [--help] [--version] fileName 参数说明1234567891011121314151617181920212223242526-n | --number 由1开始对所有输出的行数编号。-b | --number-nonblank 和-n相似，只不过对于空白行不编号。-s | --squeeze-blank 当遇到有连续两行以上的空白行，就代换为一行的空白行。-v | --show-nonprinting 使用^和M-符号，除了LFD和TAB之外。-E | --show-ends 在每行结束处显示$。 -T | --show-tabs 将TAB字符显示为^I。 -A | --show-all 等价于&quot;-vET&quot;。 -e 等价于&quot;-vE&quot;。 -t 等价于&quot;-vT&quot;。 实例 把 textfile1 的文档内容加上行号后输入 textfile2 这个文档里 1$ cat -n textfile1 &gt; textfile2 清空 /etc/test.txt 文档内容 1$ cat /dev/null &gt; /etc/test.txt /dev/null：在类 Unix 系统中，/dev/null 称空设备，是一个特殊的设备文件，它丢弃一切写入其中的数据 (但报告写入操作成功)，读取它则会立即得到一个 EOF。 而使用 cat $filename &gt; /dev/null 则不会得到任何信息，因为我们将本来该通过标准输出显示的文件信息重定向到了 /dev/null 中。 使用 cat $filename 1 &gt; /dev/null 也会得到同样的效果，因为默认重定向的 1 就是标准输出。 如果你对 shell 脚本或者重定向比较熟悉的话，应该会联想到 2 ，也即标准错误输出。 如果我们不想看到错误输出呢？我们可以禁止标准错误 cat $badname 2 &gt; /dev/null。 合并多个文件内容 12$ cat b1.sql b2.sql b3.sql &gt; b_all.sql$ cat *.sql &gt; merge.sql headhttps://blog.csdn.net/zmx19951103/article/details/78575265 tailhttps://blog.csdn.net/luo200618/article/details/52510638 sedhttps://www.cnblogs.com/qmfsun/p/6626361.html jqhttps://www.jianshu.com/p/dde911234761 https://blog.csdn.net/whatday/article/details/105781861 https://blog.csdn.net/qq_26502245/article/details/100191694 https://blog.csdn.net/u011641885/article/details/45559031 basename用于去掉文件名的目录和后缀。 语法12 basename NAME [SUFFIX]or: basename OPTION... NAME... 去掉 NAME 中的目录部分和后缀 SUFFIX，如果输出结果没有，则输出 SUFFIX。 参数说明1234567891011121314-a | --multiple support multiple arguments and treat each as a NAME -s | --suffix=SUFFIX remove a trailing SUFFIX; implies -a -z | --zero end each output line with NUL, not newline(默认情况下，每条输出行以换行符结尾) --help display this help and exit --version output version information and exit 实例 去除目录 12$basename /usr/bin/sortsort 12$ basename /usr/include/stdio.h stdio.h 去除目录和后缀 12$ basename /usr/include/stdio.h .hstdio 12$ basename -s .h /usr/include/stdio.hstdio 12$ basename /usr/include/stdio.h stdio.h stdio.h 去除多个目录 1234$ basename -a any1/str1 any2/str2 any3/str3str1str2str3 dirname用于去除文件名中的非目录部分，删除最后一个 “\\“ 后面的路径，显示父目录。 语法1dirname [OPTION] NAME... 如果 NAME 中不包含 /，则输出 .，即当前目录。 参数说明12345678-z | --zero end each output line with NUL, not newline --help display this help and exit --version output version information and exit 实例1234567891011$ dirname /usr/bin//usr$ dirname /usr/bin/usr$ dirname /etc//$ dirname /etc/httpd/conf/httpd.conf/etc/httpd/conf 123$ dirname dir1/str dir2/strdir1dir2 12$ dirname stdio.h. xargsxargs 是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。 xargs 可以将管道或标准输入 (stdin) 数据转换成命令行参数，也能够从文件的输出中读取数据。 xargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。 xargs 默认的命令是 echo，这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。 xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。 之所以能用到这个命令，关键是由于很多命令不支持管道来传递参数，而日常工作中有有这个必要，所以就有了 xargs 命令，例如： 12find /sbin -perm +700 | ls -l #这个命令是错误的find /sbin -perm +700 | xargs ls -l #这样才是正确的 xargs 一般是和管道一起使用。 语法1some command | xargs -item command 参数说明1234567891011121314151617181920212223242526272829303132333435363738-a file 从文件中读入作为sdtin -e flag 注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。 -p 当每次执行一个argument的时候询问一次用户。 -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。-t 表示先打印命令，然后再执行。-i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给&#123;&#125;，可以用&#123;&#125;代替。-r no-run-if-empty，当xargs的输入为空的时候则停止xargs，不用再去执行了。-s num命令行的最大字符数，指的是xargs后面那个命令的最大命令行字符数。-L num 从标准输入一次读取num行送给command命令。-l 同 -L。-d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符。-x exit的意思，主要是配合-s使用。-P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。 实例 读取输入数据重新格式化后输出 定义一个测试文件，内有多行文本数据： 123456$ cat test.txta b c d e f gh i j k l m no p qr s tu v w x y z 单行输出： 12$ cat test.txt | xargsa b c d e f g h i j k l m n o p q r s t u v w x y z -n 选项自定义多行输出： 12345678910$ cat test.txt | xargs -n3a b cd e fg h ij k lm n op q rs t uv w xy z -d 选项自定义一个定界符： 12$ echo &quot;nameXnameXnameXname&quot; | xargs -dXname name name name 结合 -n 选项使用： 123$ echo &quot;nameXnameXnameXname&quot; | xargs -dX -n2name namename name 读取 stdin，将格式化后的参数传递给命令 假设一个命令为 sk.sh 和一个保存参数的文件 arg.txt： sk.sh 命令内容： 1234#!/bin/bash# 打印出所有参数。echo $* arg.txt 文件内容： 1234$ cat arg.txtaaabbbccc xargs 的一个选项 -I，使用 -I 指定一个替换字符串 {}，这个字符串在 xargs 扩展时会被替换掉，当 -I 与 xargs 结合使用，每一个参数命令都会被执行一次： 1234$ cat arg.txt | xargs -I &#123;&#125; ./sk.sh -p &#123;&#125; -l-p aaa -l-p bbb -l-p ccc -l 复制所有图片文件到 /data/images 目录下： 1ls *.jpg | xargs -n1 -I &#123;&#125; cp &#123;&#125; /data/images 结合 find 使用 用 rm 删除太多的文件时候，可能得到一个错误信息：**/bin/rm Argument list too long.**， 用 xargs 去避免这个问题： 1$ find . -type f -name &quot;*.log&quot; -print0 | xargs -0 rm -f xargs -0 将 \\0 作为定界符。 统计一个源代码目录中所有 php 文件的行数： 1$ find . -type f -name &quot;*.php&quot; -print0 | xargs -0 wc -l 查找所有的 jpg 文件，并且压缩它们： 1$ find . -type f -name &quot;*.jpg&quot; -print | xargs tar -zcvf images.tar.gz xargs 其他应用 假如你有一个文件包含了很多你希望下载的 URL，你能够使用 xargs 下载所有链接： 1$ cat url-list.txt | xargs wget -c 参考：？？？ https://www.cnblogs.com/wangqiguo/p/6464234.html crontablinux 内置的 cron 进程能实现定时任务需求。 crontab 命令是 cron table 的简写，它是 cron 的配置文件，也可以叫它作业列表。我们可以在以下文件夹内找到相关配置文件： **/var/spool/cron/**：该目录下存放的是每个用户包括 root 的 crontab 任务，每个任务以创建者的名字命名。 /etc/crontab：该文件负责调度各种管理和维护任务。 **/etc/cron.d/**：该目录用来存放任何要执行的crontab文件或脚本。 另外，还可以把脚本放在 /etc/cron.hourly、/etc/cron.daily、/etc/cron.weekly、/etc/cron.monthly 目录中，让它每小时/天/星期/月执行一次。 语法1crontab [-u user] [ -e | -l | -r ] 参数说明1234567891011121314151617-u user 用来设定某个用户的crontab服务，省略此参数表示操作当前用户的crontab。file file是命令文件的名字，表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入(键盘)上键入的命令，并将它们载入crontab。-e 编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。-l 显示某个用户的crontab文件内容。如果不指定用户，则表示显示当前用户的crontab文件内容。-r 从/var/spool/cron目录中删除某个用户的crontab文件。如果不指定用户，则默认删除当前用户的crontab文件。-i 在删除用户的crontab文件时给确认提示。 实例设置定时任务时，输入 crontab -e 命令，进入当前用户的工作表编辑，是常见的 vim 界面。每行是一条命令。 crontab 的命令格式： crontab 的命令构成： 时间 + 命令，其时间有分、时、日、月、周五种，时间的操作符有： *****：取值范围内的所有数字 /：每过多少个数字，间隔频率，例如：用在小时段的”*/2”表示每隔两小时 -：从X到Z，例如：”2-6”表示”2,3,4,5,6” ,：散列数字，例如：”1,2,5,7” crontab 的命令实例： 每 2 小时执行一次：0 */2 * * * command 上述命令的含义：能被2整除的整点的0分，执行命令，即 0、2、4、6、…、20、22、24。 crontab 的日志查看： 12$ tail -f /var/log/cron.log$ tail -f /var/spool/mail/[username] 注意事项： 环境变量问题 有时我们创建了一个 crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在 crontab 文件中没有配置环境变量引起的。 在 crontab 文件中定义多个调度任务时，需要特别注环境变量的设置，因为我们手动执行某个任务时，是在当前 shell 环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在 crontab 文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定 cron 知道所需要的特殊环境，它其实并不知道。所以你要保证在 shell 脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下 3 点： 脚本中涉及文件路径时写全局路径； 脚本执行要用到 java 或其他环境变量时，通过 source 命令引入环境变量，如： 12345$ cat start_cbp.sh!/bin/shsource /etc/profileexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; 或者通过在 crontab 中直接引入环境变量解决问题。如： 10 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 及时清理系统用户的邮件日志 每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。 例如，可以在 crontab 文件中设置如下形式，忽略日志输出： 10 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 “&gt;/dev/null 2&gt;&amp;1” 表示先将标准输出重定向到 /dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了 /dev/null，因此标准错误也会重定向到 /dev/null，这样日志输出问题就解决了。 其他注意事项 新创建的 cron job，不会马上执行，至少要过 2 分钟才执行。如果重启 cron 则马上执行。 当 crontab 失效时，可以尝试 /etc/init.d/crond restart 解决问题。或者查看日志看某个 job 有没有执行/报错 tail -f /var/log/cron。 **千万别乱运行 crontab -r**。它从 crontab 目录 (/var/spool/cron) 中删除用户的 crontab 文件，删除了该文件，则用户的所有 crontab 都没了。 在 crontab 中 % 是有特殊含义的，表示换行的意思。如果要用的话必须进行转义 %，如经常用的 date ‘+%Y%m%d’ 在 crontab 里是不会执行的，应该换成 date ‘+%Y%m%d’。 参考： https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html https://segmentfault.com/a/1190000021815907 mv用于为文件或目录改名、或将文件或目录移入其它位置。 语法123 mv [OPTION]... [-T] SOURCE DESTor: mv [OPTION]... SOURCE... DIRECTORYor: mv [OPTION]... -t DIRECTORY SOURCE... 参数说明1234567891011121314-b 当目标文件或目录sh存在时，在执行覆盖前，会为其创建一个备份。-i 如果指定移动的源目录或文件与目标的目录或文件同名，则会先询问是否覆盖旧文件，输入y表示直接覆盖，输入n表示取消该操作。-f 如果指定移动的源目录或文件与目标的目录或文件同名，不会询问，直接覆盖旧文件。-n 不要覆盖任何已存在的文件或目录。-u 当源文件比目标文件新或者目标文件不存在时，才执行移动操作。 实例 将源文件名 source_file 改为目标文件名 dest_file 1$ mv source_file(文件) dest_file(文件) 将文件 source_file 移动到目标目录 dest_directory 中 1$ mv source_file(文件) dest_directory(目录) 将源目录 source_directory 移动到 目标目录 dest_directory中 1$ mv source_directory(目录) dest_directory(目录) 若目录名 dest_directory 已存在，则 source_directory 移动到目录名 dest_directory 中； 若目录名 dest_directory 不存在，则 source_directory 改名为目录名 dest_directory。 rename用于实现文件或批量文件重命名。在不同的 linux 版本，命令的语法格式可能不同。 语法1rename [ -h|-m|-V ] [ -v ] [ -n ] [ -f ] [ -e|-E *perlexpr*]*|*perlexpr* [ *files* ] linux 版本： Linux version 4.4.0-116-generic (buildd@lgw01-amd64-021) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) ) #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 或者 1rename [options] expression replacement file... linux 版本： Linux version 3.10.0-1062.el7.x86_64 (&#109;&#111;&#x63;&#107;&#x62;&#117;&#x69;&#108;&#100;&#64;&#107;&#98;&#x75;&#105;&#x6c;&#x64;&#101;&#114;&#x2e;&#x62;&#x73;&#121;&#x73;&#46;&#x63;&#101;&#x6e;&#116;&#111;&#115;&#46;&#111;&#114;&#103;) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) ) #1 SMP Wed Aug 7 18:08:02 UTC 2019 参数说明123456789101112131415161718192021222324-v, -verbose Verbose: print names of files successfully renamed.-n, -nono No action: print names of files to be renamed, but don&#x27;t rename.-f, -force Over write: allow existing files to be over-written.-h, -help Help: print SYNOPSIS and OPTIONS.-m, -man Manual: print manual page.-V, -version Version: show version number.-e Expression: code to act on files name. May be repeated to build up code (like &quot;perl -e&quot;). If no -e, the first argument is used as code.-E Statement: code to act on files name, as -e but terminated by &#x27;;&#x27;. 或者 12345-v, --verbose explain what is being done-s, --symlink act on symlink target-h, --help display this help and exit-V, --version output version information and exit 实例 替换文件名中特定字段 1$ rename -v &quot;s/20/patent-application/&quot; *.tar.gz 1234567891011121314151617181920212223242526# lin @ lin in ~/share/storage_server_3/Download/test [14:52:08] $ lltotal 76G-rw-rw-r-- 1 lin lin 4.3G Dec 4 16:54 2005.tar.gz-rw-rw-r-- 1 lin lin 4.3G Dec 5 21:50 2006.tar.gz-rw-rw-r-- 1 lin lin 4.4G Dec 5 21:52 2007.tar.gz-rw-rw-r-- 1 lin lin 4.7G Dec 5 21:53 2008.tar.gz-rw-rw-r-- 1 lin lin 5.0G Dec 7 22:10 2009.tar.gz# lin @ lin in ~/share/storage_server_3/Download/test [14:52:08] $ rename -v &quot;s/20/patent-application/&quot; *.tar.gz2005.tar.gz renamed as patent-application05.tar.gz2006.tar.gz renamed as patent-application06.tar.gz2007.tar.gz renamed as patent-application07.tar.gz2008.tar.gz renamed as patent-application08.tar.gz2009.tar.gz renamed as patent-application09.tar.gz# lin @ lin in ~/share/storage_server_3/Download/test [14:53:55] $ lltotal 76G-rw-rw-r-- 1 lin lin 4.3G Dec 4 16:54 patent-application05.tar.gz-rw-rw-r-- 1 lin lin 4.3G Dec 5 21:50 patent-application06.tar.gz-rw-rw-r-- 1 lin lin 4.4G Dec 5 21:52 patent-application07.tar.gz-rw-rw-r-- 1 lin lin 4.7G Dec 5 21:53 patent-application08.tar.gz-rw-rw-r-- 1 lin lin 5.0G Dec 7 22:10 patent-application09.tar.gz 或者 1$ rename 20 patent-application-20 *.tar.gz 12345678910111213141516(base) [hadoop@client version-1.0]$ lltotal 79555796-rw-rw-r-- 1 hadoop hadoop 4527645498 Dec 4 16:54 2005.tar.gz-rw-rw-r-- 1 hadoop hadoop 4550889304 Dec 5 21:50 2006.tar.gz-rw-rw-r-- 1 hadoop hadoop 4712276001 Dec 5 21:52 2007.tar.gz-rw-rw-r-- 1 hadoop hadoop 4986740725 Dec 5 21:53 2008.tar.gz-rw-rw-r-- 1 hadoop hadoop 5311490484 Dec 7 22:10 2009.tar.gz(base) [hadoop@client version-1.0]$ rename 20 patent-application-20 *.tar.gz(base) [hadoop@client version-1.0]$ lltotal 79555796-rw-rw-r-- 1 hadoop hadoop 1372 Dec 16 09:15 hash_calculate.txt-rw-rw-r-- 1 hadoop hadoop 4527645498 Dec 4 16:54 patent-application-2005.tar.gz-rw-rw-r-- 1 hadoop hadoop 4550889304 Dec 5 21:50 patent-application-2006.tar.gz-rw-rw-r-- 1 hadoop hadoop 4712276001 Dec 5 21:52 patent-application-2007.tar.gz-rw-rw-r-- 1 hadoop hadoop 4986740725 Dec 5 21:53 patent-application-2008.tar.gz-rw-rw-r-- 1 hadoop hadoop 5311490484 Dec 7 22:10 patent-application-2009.tar.gz 参考： http://einverne.github.io/post/2018/01/rename-files-batch.html unzip用于解压缩 .zip 文件。 语法12 unzip [-cflptuvz][-agCjLMnoqsVX][-P &lt;密码&gt;][.zip文件][文件][-d &lt;目录&gt;][-x &lt;文件&gt;]or: unzip [-Z] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778-c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。-f 更新现有的文件。-l 显示压缩文件内所包含的文件。-p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。-t 检查压缩文件是否正确。-u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。-v 查看压缩文件目录信息，但是不解压该文件。-z 仅显示压缩文件的备注文字。-a 对文本文件进行必要的字符转换。-b 不要对文本文件进行字符转换。-C 压缩文件中的文件名称区分大小写。-j 不处理压缩文件中原有的目录路径。-L 将压缩文件中的全部文件名改为小写。-M 将输出结果送到more程序处理。-n 解压缩时不要覆盖原有的文件。-o 不必先询问用户，unzip执行后覆盖原有文件。-q 执行时不显示任何信息。-s 将文件名中的空白字符转换为底线字符。-V 保留VMS的文件版本信息。-X 解压缩时同时回存文件原来的UID/GID。-P &lt;密码&gt; 使用zip的密码选项。[.zip文件] 指定.zip压缩文件。[文件] 指定要处理.zip压缩文件中的哪些文件。-d &lt;目录&gt; 指定文件解压缩后所要存储的目录。-x &lt;文件&gt; 指定不要处理.zip压缩文件中的哪些文件。-Z &#x27;unzip -Z&#x27;等于执行zipinfo指令。 实例 查看压缩文件目录信息，但不解压 123456789101112131415$ unzip -v I20090212-SUPP.ZIP Archive: I20090212-SUPP.ZIP Length Method Size Cmpr Date Time CRC-32 Name-------- ------ ------- ---- ---------- ----- -------- ---- 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/ 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/DTDS/ 88199 Stored 88199 0% 2007-01-22 00:07 d5e3060f project/pdds/ICEApplication/I20090212-SUPP/DTDS/DTDS.zip 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/ 15664 Stored 15664 0% 2009-01-28 22:45 3dfa6c1c project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/US20090041797A1-20090212-SUPP.ZIP 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/ 901714 Stored 901714 0% 2009-01-28 22:45 75ce3ca6 project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044288A1-20090212-SUPP.ZIP 1911858 Stored 1911858 0% 2009-01-28 22:45 cbc1d0bd project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044297A1-20090212-SUPP.ZIP-------- ------- --- ------- 2917435 2917435 0% 8 files 解压 .zip 文件 12345678910$ unzip I20090212-SUPP.ZIP Archive: I20090212-SUPP.ZIP creating: project/pdds/ICEApplication/I20090212-SUPP/ creating: project/pdds/ICEApplication/I20090212-SUPP/DTDS/ extracting: project/pdds/ICEApplication/I20090212-SUPP/DTDS/DTDS.zip creating: project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/ extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/US20090041797A1-20090212-SUPP.ZIP creating: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/ extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044288A1-20090212-SUPP.ZIP extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044297A1-20090212-SUPP.ZIP 解压 .zip 文件，但不显示信息 1$ unzip -q I20090212-SUPP.ZIP 注意：如果压缩文件 .zip 是大于 2G 的，那 unzip 就无法使用，此时可以使用 7zip 解压。 参考： https://www.bbsmax.com/A/lk5aMEAP51/ zip用于压缩文件，压缩后的文件后缀名为 .zip。 语法1zip [-AcdDfFghjJKlLmoqrSTuvVwXyz$] [-b &lt;工作目录&gt;] [-ll] [-n &lt;字尾字符串&gt;] [-t &lt;日期时间&gt;] [-&lt;压缩效率&gt;] [压缩文件] [文件...] [-i &lt;范本样式&gt;] [-x &lt;范本样式&gt;] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101-A 调整可执行的自动解压缩文件。 -c 替每个被压缩的文件加上注释。-d 从压缩文件内删除指定的文件。-D 压缩文件内不建立目录名称。-f 更新现有的文件。-F 尝试修复已损坏的压缩文件。-g 将文件压缩后附加在既有的压缩文件之后，而非另行建立新的压缩文件。-h 在线帮助。-j 只保存文件名称及其内容，而不存放任何目录名称。-J 删除压缩文件前面不必要的数据。 -k 使用MS-DOS兼容格式的文件名称。 -l 压缩文件时，把LF字符置换成LF+CR字符。-L 显示版权信息。-m 将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中。-o 以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同。-q 不显示指令执行过程。-r 递归处理，将指定目录下的所有文件和子目录一并处理。-S 包含系统和隐藏文件。-T 检查备份文件内的每个文件是否正确无误。-u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。-v 显示指令执行过程或显示版本信息。-V 保存VMS操作系统的文件属性。-w 在文件名称里假如版本编号，本参数仅在VMS操作系统下有效。-X 不保存额外的文件属性。-y 直接保存符号连接，而非该连接所指向的文件，本参数仅在UNIX之类的系统下有效。-z 替压缩文件加上注释。-$ 保存第一个被压缩文件所在磁盘的卷册名称。 -b &lt;工作目录&gt; 指定暂时存放文件的目录。 -ll 压缩文件时，把LF+CR字符置换成LF字符。-n &lt;字尾字符串&gt; 不压缩具有特定字尾字符串的文件。-t &lt;日期时间&gt; 把压缩文件的日期设成指定的日期。-&lt;压缩效率&gt; 压缩效率是一个介于1-9的数值。-i &lt;范本样式&gt; 只压缩符合条件的文件。-x &lt;范本样式&gt; 压缩时排除符合条件的文件。 实例 将 /home/html/ 目录下所有文件和文件夹打包为当前目录下的 html.zip 1$ zip -q -r html.zip /home/html 如果当前在 /home/html 目录下，可以执行以下命令 1$ zip -q -r html.zip * 从压缩文件 cp.zip 中删除文件 a.c 1zip -dv cp.zip a.c tar用于打包、解包文件。 tar 本身不具有压缩功能，可以通过参数调用其他压缩工具实现压缩功能。 语法1tar [-ABcdgGhiklmMoOpPrRsStuUvwWxzZ] [-b &lt;区块数目&gt;] [-C &lt;目的目录&gt;] [-f &lt;备份文件&gt;] [-F &lt;Script文件&gt;] [-K &lt;文件&gt;] [-L &lt;媒体容量&gt;] [-N &lt;日期时间&gt;] [-T &lt;范本文件&gt;] [-V &lt;卷册名称&gt;] [-X &lt;范本文件&gt;] [-&lt;设备编号&gt;&lt;存储密度&gt;] [--after-date=&lt;日期时间&gt;] [--atime-preserve] [--backuup=&lt;备份方式&gt;] [--checkpoint] [--concatenate] [--confirmation] [--delete] [--exclude=&lt;范本样式&gt;] [--force-local] [--group=&lt;群组名称&gt;] [--help] [--ignore-failed-read] [--new-volume-script=&lt;Script文件&gt;] [--newer-mtime] [--no-recursion] [--null] [--numeric-owner] [--owner=&lt;用户名称&gt;] [--posix] [--erve] [--preserve-order] [--preserve-permissions] [--record-size=&lt;区块数目&gt;] [--recursive-unlink] [--remove-files] [--rsh-command=&lt;执行指令&gt;] [--same-owner] [--suffix=&lt;备份字尾字符串&gt;] [--totals] [--use-compress-program=&lt;执行指令&gt;] [--version] [--volno-file=&lt;编号文件&gt;] [文件或目录...] 语法结构：tar [必要参数] [可选参数] [文件] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212-A | --catenate 新增文件到已存在的备份文件。 -B | --read-full-records 读取数据时重设区块大小。 -c | --create 建立新的备份文件。 -d | --diff | --compare 对比备份文件内和文件系统上的文件的差异。 -g | --listed-incremental 处理GNU格式的大量备份。 -G | --incremental 处理旧的GNU格式的大量备份。 -h | --dereference 不建立符号连接，直接复制该连接所指向的原始文件。 -i | --ignore-zeros 忽略备份文件中的0 Byte区块，也就是EOF。 -k | --keep-old-files 解开备份文件时，不覆盖已有的文件。 -l | --one-file-system 复制的文件或目录存放的文件系统，必须与tar指令执行时所处的文件系统相同，否则不予复制。 -m | --modification-time 还原文件时，不变更文件的更改时间。 -M | --multi-volume 在建立，还原备份文件或列出其中的内容时，采用多卷册模式。 -o | --old-archive | --portability 将资料写入备份文件时使用V7格式。 -O | --stdout 把从备份文件里还原的文件输出到标准输出设备。 -p | --same-permissions 用原来的文件权限还原文件。 -P | --absolute-names 文件名使用绝对名称，不移除文件名称前的&quot;/&quot;号。 -r | --append 新增文件到已存在的备份文件的结尾部分。 -R | --block-number 列出每个信息在备份文件中的区块编号。 -s | --same-order 还原文件的顺序和备份文件内的存放顺序相同。 -S | --sparse 倘若一个文件内含大量的连续0字节，则将此文件存成稀疏文件。 -t | --list 列出备份文件的内容。 -u | --update 仅置换较备份文件内的文件更新的文件。 -U | --unlink-first 解开压缩文件还原文件之前，先解除文件的连接。 -v | --verbose 显示指令执行过程。 -w | --interactive 遭遇问题时先询问用户。 -W | --verify 写入备份文件后，确认文件正确无误。 -x | --extract | --get 从备份文件中还原文件。 -z | --gzip | --ungzip 通过gzip指令处理备份文件。 -Z | --compress | --uncompress 通过compress指令处理备份文件。-b &lt;区块数目&gt; | --blocking-factor=&lt;区块数目&gt; 设置每笔记录的区块数目，每个区块大小为12Bytes。-C &lt;目的目录&gt; | --directory=&lt;目的目录&gt; 切换到指定的目录。-f &lt;备份文件&gt; | --file=&lt;备份文件&gt; 指定备份文件。多个命令时需要放在最后面。 -F &lt;Script文件&gt; | --info-script=&lt;Script文件&gt; 每次更换磁带时，就执行指定的Script文件。-K &lt;文件&gt; | --starting-file=&lt;文件&gt; 从指定的文件开始还原。-L &lt;媒体容量&gt; | -tape-length=&lt;媒体容量&gt; 设置存放每体的容量，单位以1024Bytes计算。-N &lt;日期格式&gt; | --newer=&lt;日期时间&gt; 只将较指定日期更新的文件保存到备份文件里。-T &lt;范本文件&gt; | --files-from=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让tar解开或建立符合设置条件的文件。-V&lt;卷册名称&gt; | --label=&lt;卷册名称&gt; 建立使用指定的卷册名称的备份文件。-X &lt;范本文件&gt; | --exclude-from=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让tar排除符合设置条件的文件。-&lt;设备编号&gt;&lt;存储密度&gt; 设置备份用的外围设备编号及存放数据的密度。 --after-date=&lt;日期时间&gt; 此参数的效果和指定&quot;-N&quot;参数相同。--atime-preserve 不变更文件的存取时间。--backup=&lt;备份方式&gt; | --backup 移除文件前先进行备份。--checkpoint 读取备份文件时列出目录名称。--concatenate 此参数的效果和指定&quot;-A&quot;参数相同。--confirmation 此参数的效果和指定&quot;-w&quot;参数相同。--delete 从备份文件中删除指定的文件。--exclude=&lt;范本样式&gt; 排除符合范本样式的文件。--group=&lt;群组名称&gt; 把加入设备文件中的文件的所属群组设成指定的群组。--help 在线帮助。--ignore-failed-read 忽略数据读取错误，不中断程序的执行。--new-volume-script=&lt;Script文件&gt; 此参数的效果和指定&quot;-F&quot;参数相同。--newer-mtime 只保存更改过的文件。--no-recursion 不做递归处理，也就是指定目录下的所有文件及子目录不予处理。--null 从null设备读取文件名称。--numeric-owner 以用户识别码及群组识别码取代用户名称和群组名称。--owner=&lt;用户名称&gt; 把加入备份文件中的文件的拥有者设成指定的用户。--posix 将数据写入备份文件时使用POSIX格式。--preserve 此参数的效果和指定&quot;-ps&quot;参数相同。--preserve-order 此参数的效果和指定&quot;-A&quot;参数相同。--preserve-permissions 此参数的效果和指定&quot;-p&quot;参数相同。--record-size=&lt;区块数目&gt; 此参数的效果和指定&quot;-b&quot;参数相同。--recursive-unlink 解开压缩文件还原目录之前，先解除整个目录下所有文件的连接。--remove-files 文件加入备份文件后，就将其删除。--rsh-command=&lt;执行指令&gt; 设置要在远端主机上执行的指令，以取代rsh指令。--same-owner 尝试以相同的文件拥有者还原文件。--suffix=&lt;备份字尾字符串&gt; 移除文件前先行备份。--totals 备份文件建立后，列出文件大小。--use-compress-program=&lt;执行指令&gt; 通过指定的指令处理备份文件。--version 显示版本信息。--volno-file=&lt;编号文件&gt; 使用指定文件内的编号取代预设的卷册编号。 实例 打包，不压缩 1234567$ tar -cvf test.tar testtest/test/3test/1test/2test/5test/4 解包 1234567$ tar -xvf test.tar test/test/3test/1test/2test/5test/4 打包，并以 gzip 压缩 1234567$ tar -zcvf test.tar.gz testtest/test/3test/1test/2test/5test/4 在参数 f 之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加 z 参数，则以 .tar.gz 或 .tgz 来代表 gzip 压缩过的 tar 包。 解压 .tar.gz 1234567$ tar -zxvf test.tar.gz test/test/3test/1test/2test/5test/4 打包，以 bzip2 压缩 1234567$ tar -zcvf test.tar.bz2 testtest/test/3test/1test/2test/5test/4 解压 .tar.bz2 1234567$ tar -zxvf test.tar.bz2 test/test/3test/1test/2test/5test/4 查看 .tar.gz 或 .tar.bz2 压缩包内的文件，但不解压 123456789101112131415$ tar -ztvf test.tar.gz drwxrwxr-x lin/lin 0 2020-12-21 11:38 test/-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/3-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/1-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/2-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/5-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/4$ tar -ztvf test.tar.bz2 drwxrwxr-x lin/lin 0 2020-12-21 11:38 test/-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/3-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/1-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/2-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/5-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/4 解压 .tar.gz 压缩包内的部分文件 123$ tar -zxvf test.tar.gz test/2 test/3test/3test/2 解压 .tar.gz 到指定目录 1234567$ tar -zxvf test.tar.gz -C ../Download test/test/3test/1test/2test/5test/4 使用绝对路径打包压缩和解压 12345# 压缩$ tar -zcPf /home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0/patent-grant-2019.tar.gz /home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0/2019# 解压$ tar -zxPf patent-grant-2019.tar.gz tar 对文件打包时，一般不建议使用绝对路径。 如果使用绝对路径，需要加 -P 参数。如果不添加，会发出警告：tar: Removing leading &#39;/&#39; from member names。 对于使用绝对路径打包压缩的文件，解压时 tar 会在当前目录下创建压缩时的绝对路径所对应的目录，在上面例子中，即为在当前目录下创建一个子目录 home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0。 如果在解压时使用 -P 参数，需要保证系统存在压缩时的绝对路径。 使用 pigz 并发压缩和解压 安装 pigz： 1$ sudo apt install pigz 打包： 1$ tar --use-compress-program=pigz -cvpf package.tgz ./package 解包： 1$ tar --use-compress-program=pigz -xvpf package.tgz -C ./package pigz 是支持并行的 gzip，默认用当前逻辑 cpu 个数来并发压缩，无法检测个数的话，则并发 8 个线程。 另一种方式： 12345# 语法$ tar -cvpf - $Dir | pigz -9 -p 6 $target-name# 实例$ tar -cvpf - /usr/bin | pigz -9 -p 6 bin.tgz -9：代表压缩率-p ：代表 cpu 数量 time用于检测特定指令执行时所需消耗的时间及系统资源 (内存和 I/O) 等资讯。 语法1time [options] COMMAND [arguments] 参数说明12345678-o | --output=FILE 设定结果输出档。这个选项会将time的输出写入所指定的档案中。如果档案已经存在，系统将覆写其内容。 -a | --append 配合-o使用，会将结果写到档案的末端，而不会覆盖掉原来的内容。 -f FORMAT | --format=FORMAT 以FORMAT字串设定显示方式。当这个选项没有被设定的时候，会用系统预设的格式。不过你可以用环境变数time来设定这个格式，如此一来就不必每次登入系统都要设定一次。 实例 date 命令的运行时间 123$ time dateTue Dec 22 12:01:50 CST 2020date 0.00s user 0.01s system 8% cpu 0.092 total 查找文件并复制的运行时间 123$ time find /home/lin/share/storage_server_3/patent/application/unzip_version-1.0/2019 -iname &quot;*.xml&quot; | xargs -P 6 -i cp &#123;&#125; /home/lin/share/storage_server_3/patent/application-patent/patent_version-1.0/2019 find -iname &quot;*.xml&quot; 24.00s user 114.39s system 1% cpu 2:08:02.95 totalxargs -P 6 -i cp &#123;&#125; 4.35s user 28.35s system 0% cpu 2:08:02.99 total 参考： http://c.biancheng.net/linux/time.html mkdir语法参数说明实例 创建多级目录 1$ mkdir -p Project/a/src 创建多层次、多维度的目录树 1$ mkdir -p Project/&#123;a,b,c,d&#125;/src sh -csh -c 命令，可以让 bash 将一个字串作为完整的命令来执行。 比如，向 test.asc 文件中随便写入点内容，可以： 1$ echo &quot;信息&quot; &gt; test.asc 或者 1$ echo &quot;信息&quot; &gt;&gt; test.asc 下面，如果将 test.asc 权限设置为只有 root 用户才有权限进行写操作： 1$ sudo chown root.root test.asc 然后，我们使用 sudo 并配合 echo 命令再次向修改权限之后的 test.asc 文件中写入信息： 12$ sudo echo &quot;又一行信息&quot; &gt;&gt; test.asc-bash: test.asc: Permission denied 这时，可以看到 bash 拒绝这么做，说是权限不够。这是因为重定向符号 &gt; 和 &gt;&gt; 也是 bash 的命令。我们使用 sudo 只是让 echo 命令具有了 root 权限，但是没有让 &gt; 和 &gt;&gt; 命令也具有 root 权限，所以 bash 会认为这两个命令都没有向 test.asc 文件写入信息的权限。解决这一问题的途径有两种。 第一种是利用 sh -c 命令，它可以让 bash 将一个字串作为完整的命令来执行，这样就可以将 sudo 的影响范围扩展到整条命令。具体用法如下： 1$ sudo sh -c &#x27;echo &quot;又一行信息&quot; &gt;&gt; test.asc&#x27; 另一种方法是利用管道和 tee 命令，该命令可以从标准输入中读入信息并将其写入标准输出或文件中，具体用法如下： 1$ echo &quot;第三条信息&quot; | sudo tee -a test.asc 注意，tee 命令的 -a 选项的作用等同于 &gt;&gt; 命令，如果去除该选项，那么 tee 命令的作用就等同于 &gt; 命令。 1&gt;/dev/null 2&gt;&amp;1https://blog.csdn.net/ithomer/article/details/9288353 tophttps://www.jianshu.com/p/e9e0ce23a152 freepsmd5sumsha1sum用来为给定的文件或文件夹计算单个哈希，以校验文件或文件夹的完整性。 给文件： 12$ sha1sum patent-grant-2005.tar.gz 77b6416501d34b904bd25f9aa32ca60d3e14659a patent-grant-2005.tar.gz https://www.itranslater.com/qa/details/2326085750774825984 parallelhttps://linux.cn/article-9718-1.html https://www.myfreax.com/gnu-parallel/ https://www.hi-linux.com/posts/32794.html https://www.jianshu.com/p/c5a2369fa613 https://www.aqee.net/post/use-multiple-cpu-cores-with-your-linux-commands.html https://blog.csdn.net/orangefly0214/article/details/103701600","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"java 关闭 IO 流","slug":"java-io-close","date":"2020-12-14T13:54:17.000Z","updated":"2021-01-08T02:16:33.272Z","comments":true,"path":"2020/12/14/java-io-close/","link":"","permalink":"http://example.com/2020/12/14/java-io-close/","excerpt":"","text":"在操作 java 流对象后要将流关闭，但实际编写代码时，可能会出现一些误区，导致不能正确关闭流。 在 try 中关流，而没在 finally 中关流错误： 1234567try &#123; OutputStream out = new FileOutputStream(&quot;&quot;); // ...操作流代码 out.close();&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 修正： 12345678910111213141516OutputStream out = null;try &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 在一个 try 中关闭多个流错误： 1234567891011121314151617181920OutputStream out = null;OutputStream out2 = null;try &#123; out = new FileOutputStream(&quot;&quot;); out2 = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close();// 如果此处出现异常，则out2流没有被关闭 &#125; if (out2 != null) &#123; out2.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 修正： 123456789101112131415161718192021222324OutputStream out = null;OutputStream out2 = null;try &#123; out = new FileOutputStream(&quot;&quot;); out2 = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close();// 如果此处出现异常，则out2流也会被关闭 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; try &#123; if (out2 != null) &#123; out2.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 在循环中创建流，在循环外关闭，导致关闭的是最后一个流错误： 1234567891011121314151617OutputStream out = null;try &#123; for (int i = 0; i &lt; 10; i++) &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码 &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 修正： 1234567891011121314151617for (int i = 0; i &lt; 10; i++) &#123; OutputStream out = null; try &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 在 java 7 中，关闭流的方式得到很大的简化12345try (OutputStream out = new FileOutputStream(&quot;&quot;))&#123; // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 只要实现的自动关闭接口 (Closeable) 的类都可以在 try 结构体上定义，java 会自动帮我们关闭，即使在发生异常的情况下也会。 可以在 try 结构体上定义多个，用分号隔开即可，如： 123456try (OutputStream out = new FileOutputStream(&quot;&quot;); OutputStream out2 = new FileOutputStream(&quot;&quot;))&#123; // ...操作流代码&#125; catch (Exception e) &#123; throw e;&#125; Android SDK 20 版本对应 java 7，低于 20 版本无法使用 try-catch-resources 自动关流。 内存流的关闭内存流可以不用关闭。 ByteArrayOutputStream 和 ByteArrayInputStream 其实是伪装成流的字节数组 (把它们当成字节数据来看就好了)，他们不会锁定任何文件句柄和端口，如果不再被使用，字节数组会被垃圾回收掉，所以不需要关闭。 装饰流的关闭装饰流是指通过装饰模式实现的 java 流，又称为包装流，装饰流只是为原生流附加额外的功能或效果，java 中的缓冲流、桥接流也是属于装饰流。 例如： 123456789101112InputStream is = new FileInputStream(&quot;C:\\\\Users\\\\tang\\\\Desktop\\\\test.txt&quot;);InputStreamReader isr = new InputStreamReader(is);BufferedReader br = new BufferedReader(isr);String string = br.readLine();System.out.println(string);// 只需要关闭最后的br即可try &#123; br.close();&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 装饰流关闭时会调用原生流关闭。 BufferedReader.java 源码如下： 12345678910111213public void close() throws IOException &#123; synchronized (lock) &#123; if (in == null) return; try &#123; // 这里的in就是原生流 in.close(); &#125; finally &#123; in = null; cb = null; &#125; &#125;&#125; InputStreamReader.java 源码如下： 1234public void close() throws IOException &#123; // 这里的sd就是原生流的解码器(StreamDecoder)，解码器的close会调用原生流的close sd.close();&#125; 如上所示，有这样层层关闭的机制，我们就只需要关闭最外层的流就行了。 关闭流的顺序问题两个不相干的流的关闭顺序没有任何影响，如： 123// 这里的out1和out2谁先关谁后关都一样，没有任何影响out1 = new FileOutputStream(&quot;&quot;);out2 = new FileOutputStream(&quot;&quot;); 如果两个流有依赖关系，那么可以像上面说的，只关闭最外层的即可。 如果不嫌麻烦，非得一个个关闭，那么需要先关闭最里层，从里往外一层层进行关闭。 为什么不能从外层往里层逐步关闭？原因上面讲装饰流已经讲的很清楚了，关闭外层时，内层的流其实已经同时关闭了，你再去关内层的流，就会报错。 至于网上说的先声明先关闭，就是这个道理，先声明的是内层，最先申明的是最内层，最后声明的是最外层。 一定要关闭流的原因一个流绑定了一个文件句柄 (或网络端口)，如果流不关闭，该文件 (或端口) 将始终处于被锁定 (不能读取、写入、删除和重命名) 状态，占用大量系统资源却没有释放。 本文参考https://blog.csdn.net/u012643122/article/details/38540721 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"java 的 IO 流","slug":"java-io","date":"2020-11-27T03:33:09.000Z","updated":"2021-01-08T04:03:54.147Z","comments":true,"path":"2020/11/27/java-io/","link":"","permalink":"http://example.com/2020/11/27/java-io/","excerpt":"","text":"每次在使用 IO 流之前，应该做到明确分析如下四点： 明确要操作的数据是数据源还是数据目的，也就是要读还是要写； 数据源：InputStream、Reader 数据目的：OutputStream、Writer 明确要操作的设备上的数据是字节还是文本； 数据源：字节 —— InputStream，字符 —— Reader 数据目的：L字节 —— OutputStream，字符 —— Writer 明确数据所在的具体设备； 数据源设备：硬盘 —— 文件 File 开头；内存 —— 数组，字符串；屏幕 —— System.in；网络 —— Scoket 数据目的设备：硬盘 —— 文件 File 开头；内存 —— 数组，字符串；屏幕 —— System.out；网络—— Scoket 明确是否需要额外功能，比如是否需要转换流、高效流等。 需要转换：转换流 —— InputStreamReader 、OutputStreamWriter 需要高效：缓冲流 —— Bufferedxxx 多个源：序列流 —— SequenceInputStream 对象序列化：ObjectInputStream、ObjectOutputStream 保证数据的输出形式：打印流 —— PrintStream 、Printwriter 操作基本数据，保证字节原样性：DataOutputStream、DataInputStream 到这，先看看 IO 流的分类： File 类说到 IO 流，也就是输入输出流，从文件出发到文件结束，至始至终都离不开文件，所以 IO 流还得从文件 File 类讲起。 File 概述java.io.File 类是专门对文件进行操作的类，只能对文件本身进行操作，不能对文件内容进行操作。 java.io.File 类是文件和目录路径名的抽象表示，主要用于文件和目录的创建、查找和删除等操作。 怎么理解上面两句话？其实很简单！ 第一句就是说 File 跟流无关，File 类不能对文件进行读和写也就是输入和输出！第二句就是说 File 主要表示类似 D:\\\\文件目录1 与 D:\\\\文件目录1\\\\文件.txt，前者是文件夹 (Directory)，后者则是文件 (file)，而 File 类就是操作这两者的类。 构造方法在 java 中，一切皆是对象，File 类也不例外，不论是哪个对象都应该从该对象的构造方法说起： 我们主要来学习一下比较常用的三个： public File(String pathname) ：通过将给定的路径名字符串转换为抽象路径名来创建新的 File 实例。 public File(String parent, String child) ：从父路径名字符串和子路径名字符串创建新的 File 实例。 public File(File parent, String child) ：从父抽象路径名和子路径名字符串创建新的 File 实例。 举例如下： 1234567891011121314151617// 通过文件路径名 String path1 = &quot;D:\\\\123.txt&quot;;File file1 = new File(path1); // 通过文件路径名String path2 = &quot;D:\\\\1\\\\2.txt&quot;;File file2 = new File(path2); -------------相当于d:\\\\1\\\\2.txt// 通过父路径和子路径字符串 String parent = &quot;F:\\\\aaa&quot;; String child = &quot;bbb.txt&quot;; File file3 = new File(parent, child); --------相当于f:\\\\aaa\\\\bbb.txt// 通过父级File对象和子路径字符串File parentDir = new File(&quot;F:\\\\aaa&quot;);String child = &quot;bbb.txt&quot;;File file4 = new File(parentDir, child); --------相当于f:\\\\aaa\\\\bbb.txt 一个 File 对象代表硬盘中实际存在的一个文件或者目录。 File 类构造方法不会检验这个文件或文件夹是否真实存在，因此无论该路径下是否存在文件或者目录，都不影响 File 对象的创建。 常用方法File 的常用方法主要分为获取功能、获取绝对路径和相对路径功能、判断功能、创建删除功能。 获取功能的方法 public String getAbsolutePath() ：返回此 File 的绝对路径名字符串。 public String getPath() ：将此 File 转换为路径名字符串。 public String getName() ：返回由此 File 表示的文件或目录的名称。 public long length() ：返回由此 File 表示的文件的长度。 以上方法测试，代码如下【注意测试以自己的电脑文件夹为准】： 1234567891011121314151617181920212223242526public class FileGet &#123; public static void main(String[] args) &#123; File f = new File(&quot;f:/test/a.txt&quot;); System.out.println(&quot;文件绝对路径:&quot; + f.getAbsolutePath()); System.out.println(&quot;文件构造路径:&quot; + f.getPath()); System.out.println(&quot;文件名称:&quot; + f.getName()); System.out.println(&quot;文件长度:&quot; + f.length() + &quot;字节&quot;); File f2 = new File(&quot;f:/test&quot;); System.out.println(&quot;目录绝对路径:&quot; + f2.getAbsolutePath()); System.out.println(&quot;目录构造路径:&quot; + f2.getPath()); System.out.println(&quot;目录名称:&quot; + f2.getName()); System.out.println(&quot;目录长度:&quot; + f2.length()); &#125;&#125;输出结果：文件绝对路径: f:\\test\\a.txt文件构造路径: f:\\test\\a.txt文件名称: a.txt文件长度: 7字节 目录绝对路径: f:\\test目录构造路径: f:\\test目录名称: test目录长度: 0 length() 方法，表示文件的长度。但是如果 File 对象表示目录，则返回值未指定。 获取绝对路径和相对路径功能的方法绝对路径：一个完整的路径，以盘符开头，例如 f:/aaa.txt。 相对路径：一个简化的路径，不以盘符开头，例如 /aaa.tx。 路径不区分大小写。 路径中的文件名称分隔符 windows 使用反斜杠，反斜杠是转义字符，两个反斜杠代表一个普通的反斜杠。 123456789101112131415// 绝对路径public class FilePath &#123; public static void main(String[] args) &#123; // d盘下的bbb.java文件 File f = new File(&quot;d:\\\\bbb.java&quot;); System.out.println(f.getAbsolutePath()); // 项目下的bbb.java文件 File f2 = new File(&quot;bbb.java&quot;); System.out.println(f2.getAbsolutePath()); &#125;&#125;输出结果：d:\\bbb.javad:\\java\\bbb.java 判断功能的方法 public boolean exists() ：此 File 表示的文件或目录是否实际存在。 public boolean isDirectory() ：此 File 表示的是否为目录。 public boolean isFile() ：此 File 表示的是否为文件。 1234567891011121314151617public class FileIs &#123; public static void main(String[] args) &#123; File f = new File(&quot;d:\\\\aaa\\\\bbb.java&quot;); File f2 = new File(&quot;d:\\\\aaa&quot;); // 判断是否存在 System.out.println(&quot;d:\\\\aaa\\\\bbb.java 是否存在: &quot; + f.exists()); System.out.println(&quot;d:\\\\aaa 是否存在: &quot; + f2.exists()); // 判断是文件还是目录 System.out.println(&quot;d:\\\\aaa 文件?: &quot; + f2.isFile()); System.out.println(&quot;d:\\\\aaa 目录?: &quot; + f2.isDirectory()); &#125;&#125;输出结果：d:\\aaa\\bbb.java 是否存在: trued:\\aaa 是否存在: trued:\\aaa 文件?: falsed:\\aaa 目录?: true 创建删除功能的方法 public boolean createNewFile() ：文件不存在，创建一个新的空文件并返回 true，文件存在，不创建文件并返回 false。 public boolean delete() ：删除由此 File 表示的文件或目录。 public boolean mkdir() ：创建由此 File 表示的目录。 public boolean mkdirs() ：创建由此 File 表示的目录，包括任何必需但不存在的父目录。 其中，mkdir() 和 mkdirs() 方法类似，但 mkdir()，只能创建一级目录，mkdirs() 可以创建多级目录，比如 /a/b/c，所以开发中一般用 mkdirs() 方法。 delete() 方法，如果此 File 表示目录，则目录必须为空才能删除。 1234567891011121314151617181920212223242526272829public class FileCreateDelete &#123; public static void main(String[] args) throws IOException &#123; // 文件的创建 File f = new File(&quot;aaa.txt&quot;); System.out.println(&quot;是否存在: &quot; + f.exists());// false System.out.println(&quot;是否创建: &quot; + f.createNewFile());// true System.out.println(&quot;是否创建: &quot; + f.createNewFile());// 已经创建过了所以再使用createNewFile返回false System.out.println(&quot;是否存在: &quot; + f.exists());// true // 目录的创建 File f2 = new File(&quot;newDir&quot;); System.out.println(&quot;是否存在: &quot; + f2.exists());// false System.out.println(&quot;是否创建: &quot; + f2.mkdir());// true System.out.println(&quot;是否存在: &quot; + f2.exists());// true // 创建多级目录 File f3 = new File(&quot;newDira\\\\newDirb&quot;); System.out.println(f3.mkdir());// false File f4 = new File(&quot;newDira\\\\newDirb&quot;); System.out.println(f4.mkdirs());// true // 文件的删除 System.out.println(f.delete());// true // 目录的删除 System.out.println(f2.delete());// true System.out.println(f4.delete());// false &#125;&#125; 目录的遍历 public String[] list() ：返回一个 String 数组，表示该 File 目录中的所有子文件或目录。 public File[] listFiles() ：返回一个 File 数组，表示该 File 目录中的所有的子文件或目录。 其中，listFiles() 在获取指定目录下的文件或者文件夹时必须满足下面两个条件： 指定的目录必须存在； 指定的必须是目录，否则容易引发返回数组为 null，出现 NullPointerException 异常。 1234567891011121314151617public class FileFor &#123; public static void main(String[] args) &#123; File dir = new File(&quot;G:\\\\光标&quot;); // 获取当前目录下的文件以及文件夹的名称 String[] names = dir.list(); for(String name : names)&#123; System.out.println(name); &#125; // 获取当前目录下的文件以及文件夹对象，只要拿到了文件对象，那么就可以获取更多信息 File[] files = dir.listFiles(); for (File file : files) &#123; System.out.println(file); &#125; &#125;&#125; 递归遍历文件夹下所有文件以及子文件12345678910111213141516171819202122232425262728// 递归遍历文件夹下所有的文件public class RecursionDirectory &#123; public static void main(String[] args) &#123; File file = new File(&quot;D:\\\\java专属IO测试&quot;); Recursion(file); &#125; public static void Recursion(File file) &#123; // 判断传入的是否是目录 if (!file.isDirectory()) &#123; // 不是目录直接退出 return; &#125; // 已经确保了传入的file是目录 File[] files = file.listFiles(); // 遍历files for (File f : files) &#123; // 如果该目录下文件还是个文件夹就再进行递归遍历其子目录 if (f.isDirectory()) &#123; // 递归 Recursion(f); &#125; else &#123; // 如果该目录下文件是个文件，则打印对应的名字 System.out.println(f.getName()); &#125; &#125; &#125;&#125; 初探 IO 流什么是 IO数据的传输，可以看做是一种数据的流动，按照流动的方向，以内存为基准，分为输入 input 和输出 output，即流向内存是输入流，流出内存的输出流。 java 中 I/O 操作主要是指使用 java.io 包下的内容，进行输入、输出操作。输入也叫做读取数据，输出也叫做作写出数据。 IO 的分类 根据数据的流向分为：输入流和输出流。 输入流 ：把数据从其他设备上读取到内存中的流。 输出流 ：把数据从内存中写出到其他设备上的流。 根据数据的类型分为：字节流和字符流。 字节流 ：以字节为单位，读写数据的流。 字符流 ：以字符为单位，读写数据的流。 分类之后对应的超类 (超类也就是父类的意思)： 输入流 输出流 字节流 字节输入流： InputStream 字节输出流： OutputStream 字符流 字符输入流： Reader 字符输出流： Writer 这四个类的子类的名称基本都是以其父类名作为子类名的后缀。如：InputStream 的子类 FileInputStream，Reader 的子类 FileReader。 关于 IO 的分流向说明 字节流 InputStream 与 OutputStreamInputStream 与 OutputStream的继承关系： 文件的世界里一切皆为字节我们必须明确一点的是，一切文件数据 (文本、图片、视频等) 在存储时，都是以二进制数字的形式保存，都是一个一个的字节，那么传输时一样如此。所以，字节流可以传输任意文件数据。在操作流的时候，我们要时刻明确，无论使用什么样的流对象，底层传输的始终为二进制数据。 字节输入流 InputStreamjava.io.InputStream 抽象类是表示字节输入流的所有类的超类 (父类)，可以读取字节信息到内存中，它定义了字节输入流的基本共性功能方法。 字节输入流的基本共性功能方法： public void close() ：关闭此输入流并释放与此流相关联的任何系统资源。 public abstract int read()： 从输入流读取数据的下一个字节。 public int read(byte[] b)： 该方法返回的 int 值代表的是读取了多少个字节，读到几个返回几个，读取不到返回 -1。 FileInputStream 类java.io.FileInputStream 类是文件输入流，从文件中读取字节。 FileInputStream 的构造方法 FileInputStream(File file)： 通过打开与实际文件的连接来创建一个 FileInputStream ，该文件由文件系统中的 File对象 file命名。 FileInputStream(String name)： 通过打开与实际文件的连接来创建一个 FileInputStream ，该文件由文件系统中的路径名name命名。 推荐使用第二种构造方法： 1FileInputStream inputStream = new FileInputStream(&quot;test.txt&quot;); 当创建一个流对象时，必须传入一个文件路径。该路径下，如果没有该文件，会抛出 FileNotFoundException 。 举例如下： 12345678910public class FileInputStreamConstructor &#123; public static void main(String[] args) throws FileNotFoundException &#123; // 使用File对象创建流对象 File file = new File(&quot;a.txt&quot;); FileInputStream fos = new FileInputStream(file); // 使用文件名称创建流对象 FileInputStream fos2 = new FileInputStream(&quot;b.txt&quot;); &#125;&#125; FileInputStream 读取字节数据1. 读取单个字节：read() 方法，每次可以读取一个字节的数据，返回为 int 类型，读取到文件末尾，返回 -1。代码如下： 1234567891011121314151617181920212223242526272829public class FileRead &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象，read.txt文件中内容为：abcde FileInputStream fis = new FileInputStream(&quot;xisun-database/read.txt&quot;); // 读取数据，返回一个字节 int read = fis.read(); System.out.println((char) read); read = fis.read(); System.out.println((char) read); read = fis.read(); System.out.println((char) read); read = fis.read(); System.out.println((char) read); read = fis.read(); System.out.println((char) read); // 读取到末尾，返回-1 read = fis.read(); System.out.println( read); // 关闭资源 fis.close(); &#125;&#125;输出结果：abcde-1 流操作完毕后，必须释放系统资源，调用 close() 方法，千万记得。 循环改进读取方式： 1234567891011121314151617181920public class FileRead &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象，read.txt文件中内容为：abcde FileInputStream fis = new FileInputStream(&quot;xisun-database/read.txt&quot;); // 定义变量，保存数据 int b; // 循环读取 while ((b = fis.read()) != -1) &#123; System.out.println((char) b); &#125; // 关闭资源 fis.close(); &#125;&#125;输出结果：abcde 2. 使用字节数组读取：read(byte[] b)，每次读取 b 的长度个字节到数组中，返回读取到的有效字节个数，读取到末尾时，返回 -1。代码如下： 123456789101112131415161718192021public class FileRead &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象，read.txt文件中内容为：abcde FileInputStream fis = new FileInputStream(&quot;xisun-database/read.txt&quot;); // 定义变量，作为有效个数 int len; // 定义字节数组，作为装字节数据的容器 byte[] b = new byte[2]; // 循环读取 while ((len = fis.read(b)) != -1) &#123; // 每次读取后，把数组变成字符串打印 System.out.println(new String(b)); &#125; // 关闭资源 fis.close(); &#125;&#125;输出结果：abcded 由于 read.txt 文件中内容为 abcde，而错误数据 d，是由于最后一次读取时，只读取一个字节 e，数组中，上次读取的数据没有被完全替换【注意是替换，看下图】，所以要通过 len ，获取有效的字节。 改进代码如下： 123456789101112131415161718192021public class FileRead &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象，read.txt文件中内容为：abcde FileInputStream fis = new FileInputStream(&quot;xisun-database/read.txt&quot;); // 定义变量，作为每次读取的有效字节个数 int len; // 定义字节数组，作为装字节数据的容器 byte[] b = new byte[2]; // 循环读取 while ((len = fis.read(b)) != -1) &#123; // 每次读取后，把数组的有效字节部分，变成字符串打印 System.out.println(new String(b, 0, len)); &#125; // 关闭资源 fis.close(); &#125;&#125;输出结果：abcde 在开发中一般推荐使用数组读取文件，整体优化代码如下： 123456789101112131415public class FileRead &#123; public static void main(String[] args) &#123; try (FileInputStream inputStream = new FileInputStream(&quot;xisun-database/read.txt&quot;)) &#123; int len = 0; byte[] bys = new byte[1024]; while ((len = inputStream.read(bys)) != -1) &#123; System.out.println(new String(bys, 0, len)); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;输出结果：abcde 字节输出流 OutputStreamjava.io.OutputStream 抽象类是表示字节输出流的所有类的超类 (父类)，将指定的字节信息写出到目的地，它定义了字节输出流的基本共性功能方法。 字节输出流的基本共性功能方法： public void close() ：关闭此输出流并释放与此流相关联的任何系统资源。 public void flush() ：刷新此输出流并强制任何缓冲的输出字节被写出。 public void write(byte[] b)：将 b.length 个字节从指定的字节数组写入此输出流。 public void write(byte[] b, int off, int len) ：从指定的字节数组写入 len 字节，从偏移量 off 开始输出到此输出流，也就是说从 off 个字节数开始读取一直到 len 个字节结束 。 public abstract void write(int b) ：将指定的字节输出流。 以上五个方法是字节输出流都具有的方法，由父类 OutputStream 定义提供，子类都会共享以上方法。 FileOutputStream 类java.io.FileOutputStream 类是文件输出流，向文件中写入字节。 FileOutputStream 的构造方法 public FileOutputStream(File file)：根据 File 对象为参数创建对象。 public FileOutputStream(String name)： 根据名称字符串为参数创建对象。 推荐使用第二种构造方法： 1FileOutputStream outputStream = new FileOutputStream(&quot;test.txt&quot;); 以上面这句代码来讲，类似这样创建字节输出流对象都做了三件事情： 调用系统功能去创建文件，输出流对象会自动创建； 创建 outputStream 对象； 把 outputStream 对象指向这个文件。 创建输出流对象的时候，系统会自动去对应位置创建对应文件，而创建输入流对象的时候，文件不存在会报 FileNotFoundException 异常，也就是系统找不到指定的文件异常。 当创建一个流对象时，必须直接或者间接传入一个文件路径。比如现在我们创建一个 FileOutputStream 流对象，在该路径下，如果没有这个文件，会创建该文件。如果有这个文件，会清空这个文件的数据。代码如下： 12345678910public class FileOutputStreamConstructor &#123; public static void main(String[] args) throws FileNotFoundException &#123; // 使用File对象创建流对象 File file = new File(&quot;G:\\\\自动创建的文件夹\\\\a.txt&quot;); FileOutputStream fos = new FileOutputStream(file); // 使用文件名称创建流对象 FileOutputStream fos2 = new FileOutputStream(&quot;G:\\\\b.txt&quot;); &#125;&#125; FileOutputStream 写出字节数据使用 FileOutputStream 写出字节数据主要通过 write() 方法，而write() 方法分如下三种： 123public void write(int b)public void write(byte[] b)public void write(byte[] b, int off, int len) // 从off索引开始，len个字节 1. 写出字节：write(int b) 方法，每次可以写出一个字节数据。代码如下： 1234567891011121314public class IoWrite &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(&quot;xisun-database/fos.txt&quot;); // 写出数据 fos.write(97);// 写出第1个字节 fos.write(98);// 写出第2个字节 fos.write(99);// 写出第3个字节 // 关闭资源 fos.close(); &#125;&#125;输出结果：abc 虽然参数为 int 类型四个字节，但是只会保留一个字节的信息写出。 2. 写出字节数组：write(byte[] b)，每次可以写出数组中的数据。代码如下： 1234567891011121314public class IoWrite &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(&quot;xisun-database/fos.txt&quot;); // 字符串转换为字节数组 byte[] b = &quot;麻麻我想吃烤山药&quot;.getBytes(); // 写出字节数组数据 fos.write(b); // 关闭资源 fos.close(); &#125;&#125;输出结果：麻麻我想吃烤山药 3. 写出指定长度字节数组：write(byte[] b, int off, int len)，每次写出从 off 索引开始，len 个字节。代码如下： 1234567891011121314public class IoWrite &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(&quot;xisun-database/fos.txt&quot;); // 字符串转换为字节数组 byte[] b = &quot;abcde&quot;.getBytes(); // 写出从索引2开始，2个字节。索引2是c，2个字节，也就是cd。 fos.write(b, 2, 2); // 关闭资源 fos.close(); &#125;&#125;输出结果：cd FileOutputStream 实现数据追加续写、换行经过以上的代码测试，每次程序运行时创建输出流对象，都会清空目标文件中的数据。如何保留目标文件中数据，还能继续追加新数据呢？并且实现换行呢？其实很简单，这个时候我们又要再学习 FileOutputStream 的另外两个构造方法了，如下： public FileOutputStream(File file, boolean append) public FileOutputStream(String name, boolean append) 这两个构造方法，第二个参数中都需要传入一个 boolean 类型的值，true 表示追加数据，false 表示不追加也就是清空原有数据。这样创建的输出流对象，就可以指定是否追加续写了，至于 windows 系统换行符是 \\r\\n ，下面将会详细讲到。 实现数据追加续写代码如下： 12345678910111213public class FOSWrite &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(&quot;xisun-database/fos.txt&quot;, true); // 字符串转换为字节数组 byte[] b = &quot;abcde&quot;.getBytes(); fos.write(b); // 关闭资源 fos.close(); &#125;&#125;文件操作前：cd文件操作后：cdabcde Windows系统里，换行符号是\\r\\n ,具体代码如下： 123456789101112131415161718192021222324public class FOSWrite &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象 FileOutputStream fos = new FileOutputStream(&quot;xisun-database/fos.txt&quot;); // 定义字节数组 byte[] words = &#123;97, 98, 99, 100, 101&#125;; // 遍历数组 for (byte word : words) &#123; // 写出一个字节 fos.write(word); // 写出一个换行，换行符号转成数组写出 fos.write(&quot;\\r\\n&quot;.getBytes()); &#125; // 关闭资源 fos.close(); &#125;&#125;输出结果：abcde 回车符 \\r 和换行符 \\n： 回车符：回到一行的开头 (return)。 换行符：下一行 (newline)。 不同系统中的换行： Windows系统里，每行结尾是 回车 + 换行 ，即 \\r\\n； Unix系统里，每行结尾只有 换行 ，即 \\n； Mac系统里，每行结尾是 回车 ，即 \\r。从 Mac OS X开始与Linux统一。 FileInputStream 和 FileOutputStream 完成图片复制复制图片原理： 代码实现： 1234567891011121314151617public class Copy &#123; public static void main(String[] args) &#123; // 1.指定数据源fis和目的地fos try (FileInputStream fis = new FileInputStream(&quot;E:/test.png&quot;); FileOutputStream fos = new FileOutputStream(&quot;xisun-database/test_copy.jpg&quot;)) &#123; // 2.读和写数据 int len; byte[] b = new byte[1024]; while ((len = fis.read(b)) != -1) &#123; fos.write(b, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 复制文本、图片、mp3、视频等的方式一样。 字符流 Reader 和 WriterReader 和 Writer 的继承关系： 字符流的由来因为数据编码的不同，因而有了对字符进行高效操作的流对象，字符流本质其实就是基于字节流读取时，去查了指定的码表，而字节流直接读取数据会有乱码的问题 (读中文会乱码)。举例如下： 123456789101112131415public class CharaterStream &#123; public static void main(String[] args) &#123; // FileInputStream为操作文件的字符输入流，read.txt内容为：测试用例 try (FileInputStream inputStream = new FileInputStream(&quot;xisun-database/read.txt&quot;)) &#123; int len; while ((len = inputStream.read()) != -1) &#123; System.out.print((char) len); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;运行结果：æµè¯ç¨ä¾ 字节流读取中文字符时，可能不会显示完整的字符，那是因为一个中文字符占用多个字节存储。 如果不想乱码，可以用下面的方式，但比较繁琐： 1234567891011121314public class FileRead &#123; public static void main(String[] args) &#123; // FileInputStream为操作文件的字符输入流，read.txt内容为：测试用例 try (FileInputStream inputStream = new FileInputStream(&quot;xisun-database/read.txt&quot;)) &#123; byte[] bytes = new byte[1024]; int len; while ((len = inputStream.read(bytes)) != -1) &#123; System.out.print(new String(bytes, 0, len)); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 上述代码中，解码的是 String，查看 new String() 的源码，String 构造方法有解码功能，并且默认编码是 utf-8，代码如下： 1234public String(byte bytes[], int offset, int length) &#123; checkBounds(bytes, offset, length); this.value = StringCoding.decode(bytes, offset, length);&#125; 123456789101112131415161718192021static char[] decode(byte[] ba, int off, int len) &#123; String csn = Charset.defaultCharset().name(); try &#123; // use charset name decode() variant which provides caching. return decode(csn, ba, off, len); &#125; catch (UnsupportedEncodingException x) &#123; warnUnsupportedCharset(csn); &#125; try &#123; return decode(&quot;ISO-8859-1&quot;, ba, off, len); &#125; catch (UnsupportedEncodingException x) &#123; // If this code is hit during VM initialization, MessageUtils is // the only way we will be able to get any kind of error message. MessageUtils.err(&quot;ISO-8859-1 charset not available: &quot; + x.toString()); // If we can not find ISO-8859-1 (a required encoding) then things // are seriously wrong with the installation. System.exit(1); return null; &#125;&#125; 1234567891011121314public static Charset defaultCharset() &#123; if (defaultCharset == null) &#123; synchronized (Charset.class) &#123; String csn = AccessController.doPrivileged( new GetPropertyAction(&quot;file.encoding&quot;)); Charset cs = lookup(csn); if (cs != null) defaultCharset = cs; else defaultCharset = forName(&quot;UTF-8&quot;); &#125; &#125; return defaultCharset;&#125; 从上面可以看出，尽管字节流也能解决乱码问题，但是比较麻烦，于是 java 就有了字符流，以字符为单位读写数据，字符流专门用于处理文本文件。如果处理纯文本的数据，优先考虑字符流，其他情况就只能用字节流了 (图片、视频、等等只文本例外)。 从另一角度来说：字符流 = 字节流 + 编码表。 字符输入流 Readerjava.io.Reader 抽象类是字符输入流的所有类的超类 (父类)，可以读取字符信息到内存中，它定义了字符输入流的基本共性功能方法。 字符输入流的共性方法： public void close() ：关闭此流并释放与此流相关联的任何系统资源。 public int read()： 从输入流读取一个字符。 public int read(char[] cbuf)： 从输入流中读取一些字符，并将它们存储到字符数组 cbuf 中。 FileReader 类java.io.FileReader 类是读取字符文件的便利类。构造时使用系统默认的字符编码和默认字节缓冲区。 FileReader 的构造方法 public FileReader(File file)： 创建一个新的 FileReader，给定要读取的 File 对象。 public FileReader(String fileName)： 创建一个新的 FileReader ，给定要读取的文件的字符串名称。 12345678910public class FileReaderConstructor &#123; public static void main(String[] args) throws FileNotFoundException &#123; // 使用File对象创建流对象 File file = new File(&quot;a.txt&quot;); FileReader fr = new FileReader(file); // 使用文件名称创建流对象 FileReader fr2 = new FileReader(&quot;b.txt&quot;); &#125;&#125; FileReader 读取字符数据读取字符：read() 方法，每次可以读取一个字符的数据，返回为 int 类型，读取到文件末尾，返回 -1，循环读取。代码如下： 1234567891011121314public class FileReader &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象 FileReader fr = new FileReader(&quot;xisun-database/read.txt&quot;); // 定义变量，保存数据 int b; // 循环读取 while ((b = fr.read()) != -1) &#123; System.out.println((char) b); &#125; // 关闭资源 fr.close(); &#125;&#125; 字符输出流 Writerjava.io.Writer 抽象类是字符输出流的所有类的超类 (父类)，将指定的字符信息写出到目的地，它同样定义了字符输出流的基本共性功能方法。 字符输出流的基本共性功能方法： void write(int c)：写入单个字符。 void write(char[] cbuf)：写入字符数组。 abstract void write(char[] cbuf, int off, int len)：写入字符数组的某一部分，off 数组的开始索引，len 写的字符个数。 void write(String str)：写入字符串。 void write(String str, int off, int len)：写入字符串的某一部分，off 字符串的开始索引，len 写的字符个数。 void flush()：刷新该流的缓冲。 void close() ：关闭此流，但要先刷新它。 FileWriter 类java.io.FileWriter 类是写出字符到文件的便利类。构造时使用系统默认的字符编码和默认字节缓冲区。 FileWriter 的构造方法 public FileWriter(File file)： 创建一个新的 FileWriter，给定要读取的 File 对象。 public FileWriter(String fileName)： 创建一个新的 FileWriter，给定要读取的文件的名称。 12345678910public class FileWriterConstructor &#123; public static void main(String[] args) throws IOException &#123; // 第一种：使用File对象创建流对象 File file = new File(&quot;xisun-database/read.txt&quot;); FileWriter fw = new FileWriter(file); // 第二种：使用文件名称创建流对象 FileWriter fw2 = new FileWriter(&quot;b.txt&quot;); &#125;&#125; FileWriter 写出字符数据写出字符：write(int b) 方法，每次可以写出一个字符数据。代码如下： 1234567891011121314public class FWWrite &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象 FileWriter fw = new FileWriter(&quot;xisun-database/fw.txt&quot;); // 写出数据 fw.write(97); // 写出第1个字符 fw.write(&#x27;b&#x27;); // 写出第2个字符 fw.write(&#x27;C&#x27;); // 写出第3个字符 fw.close(); &#125;&#125;输出结果：abc 关闭资源时，与 FileOutputStream 不同，必须执行 close() 方法，如果不关闭，数据只是保存到缓冲区，并未保存到文件。 关闭 close() 方法和刷新 flush() 方法因为内置缓冲区的原因，如果不关闭输出流，无法写出字符到文件中。但是关闭的流对象，是无法继续写出数据的。如果我们既想写出数据，又想继续使用流，就需要 flush() 方法了。 flush()：刷新缓冲区，流对象可以继续使用。 close()：先刷新缓冲区，然后通知系统释放资源。流对象不可以再被使用了。 12345678910111213public class FlushDemo &#123; public static void main(String[] args) throws IOException &#123; // 源 也就是输入流【读取流】 读取read.txt文件，必须要存在read.txt文件，否则报FileNotFoundException异常 FileReader fr = new FileReader(&quot;xisun-database/read.txt&quot;); // 目的地 也就是输出流，系统会自动创建write.txt，因为它是输出流！ FileWriter fw = new FileWriter(&quot;xisun-database/write.txt&quot;); int len; while ((len = fr.read()) != -1) &#123; fw.write(len); &#125; // 注意这里没有使用close关闭流，开发中不能这样做，但是可以更好的体会flush的作用 &#125;&#125; 上面的代码执行完后，因为没有关闭流，write.txt 文件中依旧会是空的。在上面代码中再添加下面三句代码，write.txt 文件就能复制到源文件的数据了！ 123fr.close();fw.flush();fw.close(); flush() 方法是清空的意思，用于清空缓冲区的数据流，进行流的操作时，数据先被读到内存中，然后再写到文件中，那么当你数据读完时，如果这时调用 close() 方法关闭读写流，就可能造成数据丢失，为什么呢？因为，读入数据完成时不代表写入数据完成，一部分数据可能会留在缓存区中，这个时候使用 flush() 方法就格外重要了。 close() 方法使用代码示意如下： 1234567891011121314151617public class FWWrite &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象 FileWriter fw = new FileWriter(&quot;xisun-database/fw.txt&quot;); // 写出数据，通过flush fw.write(&#x27;刷&#x27;); // 写出第1个字符 fw.flush(); fw.write(&#x27;新&#x27;); // 继续写出第2个字符，写出成功 fw.flush(); // 写出数据，通过close fw.write(&#x27;关&#x27;); // 写出第1个字符 fw.close(); fw.write(&#x27;闭&#x27;); // 继续写出第2个字符,【报错】java.io.IOException: Stream closed fw.close(); &#125;&#125; 即便是 flush() 方法写出了数据，操作的最后还是要调用 close() 方法，释放系统资源。 FileWriter 的追加和换行追加和换行：操作类似于 FileOutputStream 操作。代码如下： 123456789101112131415161718public class FWWrite &#123; public static void main(String[] args) throws IOException &#123; // 使用文件名称创建流对象，true代表可以追加数据 FileWriter fw = new FileWriter(&quot;xisun-database/fw.txt&quot;, true); // 写出字符串 fw.write(&quot;测试&quot;); // 写出换行 fw.write(&quot;\\r\\n&quot;); // 写出字符串 fw.write(&quot;用例&quot;); // 关闭资源 fw.close(); &#125;&#125;输出结果：在fw.txt内容的后面，追加：测试用例 FileReader 和 FileWriter 完成文本文件复制12345678910111213141516171819202122232425262728293031323334353637383940public class CopyFile &#123; public static void main(String[] args) throws IOException &#123; // 创建输入流对象，若文件不存在会抛出java.io.FileNotFoundException FileReader fr = new FileReader(&quot;xisun-database/read.txt&quot;); // 创建输出流对象 FileWriter fw = new FileWriter(&quot;xisun-database/copyread.txt&quot;); /*创建输出流做的工作： * 1.调用系统资源创建了一个文件 * 2.创建输出流对象 * 3.把输出流对象指向文件 * */ // 文本文件复制，一次读一个字符 copyMethod1(fr, fw); // 文本文件复制，一次读一个字符数组 copyMethod2(fr, fw); fw.flush(); fw.close(); fr.close(); &#125; public static void copyMethod1(FileReader fr, FileWriter fw) throws IOException &#123; int ch; while ((ch = fr.read()) != -1) &#123;// 读数据 fw.write(ch);// 写数据 &#125; &#125; public static void copyMethod2(FileReader fr, FileWriter fw) throws IOException &#123; char[] chs = new char[1024]; int len = 0; while ((len = fr.read(chs)) != -1) &#123;// 读数据 fw.write(chs, 0, len);// 写数据 &#125; &#125;&#125; 字符流，只能操作文本文件，不能操作图片，视频等非文本文件。当我们单纯读或者写文本文件时，使用字符流，其他情况使用字节流。 IO 异常的处理我们在学习的过程中可能习惯把异常抛出，而实际开发中并不能这样处理，建议使用 try...catch...finally 代码块，处理异常部分，代码格式如下： 12345678910111213141516171819202122public class CopyFile &#123; public static void main(String[] args) &#123; // 声明变量 FileWriter fw = null; try &#123; //创建流对象 fw = new FileWriter(&quot;xisun-database/fw.txt&quot;); // 写出数据 fw.write(&quot;测试用例&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fw != null) &#123; fw.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 缓冲流缓冲流，也叫高效流，是对4个 FileXxx 流的 “增强流”。 缓冲流的基本原理： 使用了底层流对象从具体设备上获取数据，并将数据存储到缓冲区的数组内。 通过缓冲区的 read() 方法从缓冲区获取具体的字符数据，这样就提高了效率。 如果用 read() 方法读取字符数据，并存储到另一个容器中，直到读取到了换行符时，将另一个容器临时存储的数据转成字符串返回，就形成了 readLine() 功能。 也就是说在创建流对象时，会创建一个内置的默认大小的缓冲区数组，通过缓冲区读写，减少系统 IO 次数，从而提高读写的效率。 缓冲流书写格式为 BufferedXxx，按照数据类型分类： 字节缓冲流：BufferedInputStream，BufferedOutputStream 字符缓冲流：BufferedReader，BufferedWriter 字节缓冲流 BufferedInputStream 和 BufferedOutputStream构造方法 public BufferedInputStream(InputStream in) ：创建一个新的缓冲输入流，注意参数类型为 InputStream。 public BufferedOutputStream(OutputStream out)： 创建一个新的缓冲输出流，注意参数类型为 OutputStream。 举例如下： 1234567891011121314public class BufferConstructor &#123; public static void main(String[] args) throws IOException &#123; try (FileInputStream fis = new FileInputStream(&quot;xisun-database/read.txt&quot;); FileOutputStream fos = new FileOutputStream(&quot;xisun-database/write.txt&quot;);) &#123; // 创建字节缓冲输入流 BufferedInputStream bis = new BufferedInputStream(fis); // 创建字节缓冲输出流 BufferedOutputStream bos = new BufferedOutputStream(fos); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 高效的字节缓冲流缓冲流读写方法与基本的流是一致的，我们通过复制 370 多 MB 的大文件，测试它的效率。 基本流，代码如下： 123456789101112131415161718192021public class CopyFile &#123; public static void main(String[] args) throws IOException &#123; // 开始时间 long start = System.currentTimeMillis(); try (FileInputStream fis = new FileInputStream(&quot;xisun-database/read.txt&quot;); FileOutputStream fos = new FileOutputStream(&quot;xisun-database/write.txt&quot;);) &#123; // 读写数据 int b; while ((b = fis.read()) != -1) &#123; fos.write(b); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; // 结束时间 long end = System.currentTimeMillis(); System.out.println(&quot;普通流复制时间:&quot; + (end - start) + &quot; 毫秒&quot;); &#125;&#125; 缓冲流，代码如下： 123456789101112131415161718192021222324public class CopyFile &#123; public static void main(String[] args) throws IOException &#123; // 开始时间 long start = System.currentTimeMillis(); try (FileInputStream fis = new FileInputStream(&quot;xisun-database/read.txt&quot;); FileOutputStream fos = new FileOutputStream(&quot;xisun-database/write.txt&quot;);) &#123; BufferedInputStream bis = new BufferedInputStream(fis); BufferedOutputStream bos = new BufferedOutputStream(fos); // 读写数据 int b; while ((b = bis.read()) != -1) &#123; bos.write(b); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; // 结束时间 long end = System.currentTimeMillis(); System.out.println(&quot;缓冲流复制时间:&quot; + (end - start) + &quot; 毫秒&quot;); &#125;&#125; 经过测试，缓冲流能明显提高效率。 上述代码还能通过定义数组，来进一步优化，代码如下： 12345678910111213141516171819202122232425public class CopyFile &#123; public static void main(String[] args) throws IOException &#123; // 开始时间 long start = System.currentTimeMillis(); try (FileInputStream fis = new FileInputStream(&quot;xisun-database/read.txt&quot;); FileOutputStream fos = new FileOutputStream(&quot;xisun-database/write.txt&quot;);) &#123; BufferedInputStream bis = new BufferedInputStream(fis); BufferedOutputStream bos = new BufferedOutputStream(fos); // 读写数据 int len; byte[] bytes = new byte[8 * 1024]; while ((len = bis.read(bytes)) != -1) &#123; bos.write(bytes, 0, len); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; // 结束时间 long end = System.currentTimeMillis(); System.out.println(&quot;缓冲流优化后的复制时间:&quot; + (end - start) + &quot; 毫秒&quot;); &#125;&#125; 字符缓冲流 BufferedReader 和 BufferedWriter构造方法 public BufferedReader(Reader in) ：创建一个新的缓冲输入流，注意参数类型为 Reader。 public BufferedWriter(Writer out)： 创建一个新的缓冲输出流，注意参数类型为 Writer。 举例如下： 12345678910111213public class CopyFile &#123; public static void main(String[] args) throws IOException &#123; try (FileReader fr = new FileReader(&quot;xisun-database/read.txt&quot;); FileWriter fw = new FileWriter(&quot;xisun-database/writer.txt&quot;)) &#123; // 创建字符缓冲输入流 BufferedReader bfr = new BufferedReader(fr); // 创建字符缓冲输出流 BufferedWriter bfw = new BufferedWriter(fw); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 高效的字符缓冲流此处与字节缓冲流类似，优化后的代码如下： 12345678910111213141516171819202122232425public class CopyFile &#123; public static void main(String[] args) throws IOException &#123; // 开始时间 long start = System.currentTimeMillis(); try (FileReader fr = new FileReader(&quot;xisun-database/read.txt&quot;); FileWriter fw = new FileWriter(&quot;xisun-database/writer.txt&quot;)) &#123; BufferedReader bfr = new BufferedReader(fr); BufferedWriter bfw = new BufferedWriter(fw); // 读写数据 int len; char[] bytes = new char[1024]; while ((len = bfr.read(bytes)) != -1) &#123; bfw.write(bytes, 0, len); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 结束时间 long end = System.currentTimeMillis(); System.out.println(&quot;缓冲流优化后的复制时间:&quot; + (end - start) + &quot; 毫秒&quot;); &#125;&#125; 字符缓冲流的特有方法字符缓冲流的基本方法与普通字符流调用方式一致，这里不再阐述，我们来看字符缓冲流具备的特有方法： BufferedReader：public String readLine()：读一行数据，读取到最后返回 null。 BufferedWriter：public void newLine():：换行，由系统属性定义符号。 readLine() 方法示例代码如下： 123456789101112131415161718public class ReadLineDemo &#123; public static void main(String[] args) throws IOException &#123; try (FileReader fr = new FileReader(&quot;xisun-database/read.txt&quot;)) &#123; BufferedReader bfr = new BufferedReader(fr); // 定义字符串，保存读取的一行文字 String line = null; // 循环读取，读取到最后返回null while ((line = bfr.readLine()) != null) &#123; System.out.println(line); System.out.println(&quot;------&quot;); &#125; // 释放资源 bfr.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; newLine() 方法示例代码如下： 12345678910111213141516171819202122232425public class NewLineDemo &#123; public static void main(String[] args) throws IOException &#123; try (FileWriter fw = new FileWriter(&quot;xisun-database/write.txt&quot;)) &#123; BufferedWriter bfw = new BufferedWriter(fw); bfw.write(&quot;测&quot;); // 换行 bfw.newLine(); bfw.write(&quot;试&quot;); bfw.newLine(); bfw.write(&quot;用&quot;); bfw.newLine(); bfw.write(&quot;例&quot;); // 释放资源 bfw.flush(); bfw.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;输出结果：测试用例 转换流编码与解码众所周知，计算机中储存的信息都是用二进制数表示的，而我们在屏幕上看到的数字、英文、标点符号、汉字等字符是二进制数转换之后的结果。按照某种规则，将字符存储到计算机中，称为编码 。反之，将存储在计算机中的二进制数按照某种规则解析显示出来，称为解码 。注意，编码规则和解码规则要对应，否则会导致乱码。比如说，按照 A 规则存储，同样按照 A 规则解析，那么就能显示正确的文本符号。反之，按照 A 规则存储，再按照 B 规则解析，就会导致乱码现象。 简单一点说就是： 编码：字符 (能看懂的) → 字节 (看不懂的)。 解码：字节 (看不懂的) → 字符 (能看懂的)。 用代码解释则为： 12345678String(byte[] bytes, String charsetName): 通过指定的字符集解码字节数组byte[] getBytes(String charsetName): 使用指定的字符集合把字符串编码为字节数组编码: 把看得懂的变成看不懂的String -- byte[]解码: 把看不懂的变成看得懂的byte[] -- String 字符集字符集 Charset：也叫编码表。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等。 计算机要准确的存储和识别各种字符集符号，需要进行字符编码，一套字符集必然至少有一套字符编码。常见字符集有 ASCII 字符集、GBK 字符集、Unicode 字符集等。 可见，当指定了编码，它所对应的字符集自然就指定了，所以编码才是我们最终要关心的。 ASCII 字符集 ASCII (American Standard Code for Information Interchange，美国信息交换标准代码) 是基于拉丁字母的一套电脑编码系统，用于显示现代英语，主要包括控制字符 (回车键、退格、换行键等) 和可显示字符 (英文大小写字符、阿拉伯数字和西文符号)。 基本的 ASCII 字符集，使用 7 位（bits）表示一个字符，共 128 字符。ASCII 的扩展字符集使用 8 位（bits）表示一个字符，共 256 字符，方便支持欧洲常用字符。 ISO-8859-1 字符集 拉丁码表，别名 Latin-1，用于显示欧洲使用的语言，包括荷兰、丹麦、德语、意大利语、西班牙语等。 ISO-8859-1 使用单字节编码，兼容 ASCII 编码。 GBxxx 字符集 GB 就是国标的意思，是为了显示中文而设计的一套字符集。 GB2312：简体中文码表。一个小于 127 的字符的意义与原来相同。但两个大于 127 的字符连在一起时，就表示一个汉字，这样大约可以组合了包含 7000 多个简体汉字，此外数学符号、罗马希腊的字母、日文的假名们都编进去了，连在 ASCII 里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的 “全角” 字符，而原来在 127 号以下的那些就叫 “半角” 字符了。 GBK：最常用的中文码表。是在 GB2312 标准基础上的扩展规范，使用了双字节编码方案，共收录了 21003 个汉字，完全兼容 GB2312 标准，同时支持繁体汉字以及日韩汉字等。 GB18030：最新的中文码表。收录汉字 70244 个，采用多字节编码，每个字可以由 1 个、2 个或 4 个字节组成。支持中国国内少数民族的文字，同时支持繁体汉字以及日韩汉字等。 Unicode 字符集 Unicode 编码系统为表达任意语言的任意字符而设计，是业界的一种标准，也称为统一码、标准万国码。 它最多使用 4 个字节的数字来表达每个字母、符号，或者文字。有三种编码方案，UTF-8、UTF-16 和 UTF-32。最为常用的 UTF-8 编码。 UTF-8 编码，可以用来表示 Unicode 标准中任何字符，它是电子邮件、网页及其他存储或传送文字的应用中，优先采用的编码。互联网工程工作小组 (IETF) 要求所有互联网协议都必须支持 UTF-8 编码。所以，我们开发 Web 应用，也要使用 UTF-8 编码。它使用一至四个字节为每个字符编码，编码规则： 128 个 US-ASCII 字符，只需一个字节编码。 拉丁文等字符，需要二个字节编码。 大部分常用字 (含中文)，使用三个字节编码。 其他极少使用的 Unicode 辅助字符，使用四字节编码。 编码问题导致乱码在 java 开发工具 IDEA 中，使用 FileReader 读取项目中的文本文件。由于 IDEA 的设置，都是默认的 UTF-8 编码，所以没有任何问题。但是，当读取 Windows 系统中创建的文本文件时，由于 Windows 系统的默认是 GBK 编码，就会出现乱码。 1234567891011121314public class ReaderDemo &#123; public static void main(String[] args) &#123; try (FileReader fileReader = new FileReader(&quot;E:\\\\test.txt&quot;)) &#123; int read; while ((read = fileReader.read()) != -1) &#123; System.out.print((char) read); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;输出结果：�������� 那么如何读取 GBK 编码的文件呢？ 这个时候就得讲讲转换流了！ 从另一角度来讲：字符流 = 字节流 + 编码表。 InputStreamReader 类 — 字节流到字符流的桥梁转换流 java.io.InputStreamReader，是 Reader 的子类，从字面意思可以看出它是从字节流到字符流的桥梁。它读取字节，并使用指定的字符集将其解码为字符。它的字符集可以由名称指定，也可以接受平台的默认字符集。 构造方法 InputStreamReader(InputStream in)：创建一个使用默认字符集的字符流。 InputStreamReader(InputStream in, String charsetName)：创建一个指定字符集的字符流。 使用转换流解决编码问题 — 读文件12345678910111213141516171819202122232425262728293031323334public class ReaderDemo &#123; public static void main(String[] args) &#123; // 文件为gbk编码 String fileName = &quot;E:/test.txt&quot;; try &#123; // 创建流对象，默认UTF8编码 InputStreamReader isr = new InputStreamReader(new FileInputStream(fileName)); // 创建流对象，指定GBK编码 InputStreamReader isr2 = new InputStreamReader(new FileInputStream(fileName), &quot;GBK&quot;); // 定义变量，保存字符 int read; // 使用默认编码字符流读取，乱码 while ((read = isr.read()) != -1) &#123; System.out.print((char) read); &#125; isr.close(); System.out.print(&quot;\\r\\n******************\\r\\n&quot;); // 使用指定编码字符流读取，正常解析 while ((read = isr2.read()) != -1) &#123; System.out.print((char) read); &#125; isr2.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;输出结果：��������******************测试用例 OutputStreamWriter 类 — 字符流到字节流的桥梁转换流 java.io.OutputStreamWriter ，是 Writer 的子类，字面看容易混淆会误以为是转为字符流，其实不然，OutputStreamWriter 为从字符流到字节流的桥梁。使用指定的字符集将字符编码为字节。它的字符集可以由名称指定，也可以接受平台的默认字符集。 构造方法 OutputStreamWriter(OutputStream in)：创建一个使用默认字符集的字符流。 OutputStreamWriter(OutputStream in, String charsetName)：创建一个指定字符集的字符流。 使用转换流解决编码问题 — 写文件12345678910111213141516171819public class WriterDemo &#123; public static void main(String[] args) &#123; try &#123; // 创建流对象,默认UTF8编码 OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(&quot;E:/test.txt&quot;)); // 写出数据 osw.write(&quot;测试&quot;); // 保存为6个字节 osw.close(); // 创建流对象,指定GBK编码 OutputStreamWriter osw2 = new OutputStreamWriter(new FileOutputStream(&quot;E:/test2.txt&quot;), &quot;GBK&quot;); // 写出数据 osw2.write(&quot;用例&quot;);// 保存为4个字节 osw2.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 为了达到最高效率，可以考虑在 BufferedReader 内包装 InputStreamReader： 1BufferedReader in = new BufferedReader(new InputStreamReader(System.in))； 序列化流什么是序列化java 提供了一种对象序列化的机制。用一个字节序列可以表示一个对象，该字节序列包含该 对象的数据、对象的类型 和 对象中存储的属性等信息。字节序列写出到文件之后，相当于文件中持久保存了一个对象的信息。 反之，该字节序列还可以从文件中读取回来，重构对象，对它进行反序列化。对象的数据、对象的类型 和 对象中存储的属性 等信息，都可以用来在内存中创建对象。如图所示： ObjectOutputStream 类java.io.ObjectOutputStream 类，将 java 对象的原始数据类型写出到文件，实现对象的持久存储。 构造方法 public ObjectOutputStream(OutputStream out)： 创建一个指定 OutputStream 的 ObjectOutputStream。 举例如下： 12FileOutputStream fileOut = new FileOutputStream(&quot;aa.txt&quot;);ObjectOutputStream out = new ObjectOutputStream(fileOut); 序列化操作一个对象要想序列化，必须满足两个条件： 该类必须实现 java.io.Serializable 接口，Serializable 是一个标记接口，不实现此接口的类不能实现任何状态序列化或反序列化，否则会抛出 NotSerializableException。 该类的所有属性必须是可序列化的。如果有一个属性不需要可序列化的，则该属性必须注明是瞬态的，使用 transient 关键字修饰。 举例如下： 12345678public class Employee implements java.io.Serializable &#123; public String name; public String address; public transient int age; // transient瞬态修饰成员，不会被序列化 public void addressCheck() &#123; System.out.println(&quot;Address check : &quot; + name + &quot; -- &quot; + address); &#125;&#125; 写出对象方法： public final void writeObject(Object obj)： 将指定的对象写出。 123456789101112131415161718192021public class SerializeDemo &#123; public static void main(String[] args) &#123; Employee employee = new Employee(); employee.name = &quot;zhangsan&quot;; employee.address = &quot;beiqinglu&quot;; employee.age = 20; try &#123; // 创建序列化流对象 ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(&quot;xisun-database/employee.txt&quot;)); // 写出对象 out.writeObject(employee); // 释放资源 out.close(); System.out.println(&quot;Serialized data is saved&quot;); // 姓名，地址被序列化，年龄没有被序列化。 &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;输出结果：Serialized data is saved employee.txt 文件内容： ObjectInputStream类java.io.ObjectInputStream 类，将之前使用 ObjectOutputStream 序列化的原始数据恢复为对象。 构造方法 public ObjectInputStream(InputStream in)： 创建一个指定 InputStream 的 ObjectInputStream。 举例如下： 12FileInputStream fileIn = new FileInputStream(&quot;aa.txt&quot;);ObjectInputStream in = new ObjectInputStream(fileIn); 反序列化操作如果能找到一个对象的 class 文件，我们可以进行反序列化操作，调用 ObjectInputStream 读取对象的方法： public final Object readObject()：读取一个对象。 以前面序列化得到的 employee.txt 文件为例： 123456789101112131415161718192021222324public class DeserializeDemo &#123; public static void main(String[] args) &#123; Employee employee = null; try &#123; // 创建反序列化流 ObjectInputStream in = new ObjectInputStream(new FileInputStream(&quot;xisun-database/employee.txt&quot;)); // 读取一个对象 employee = (Employee) in.readObject(); // 释放资源 in.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); return; &#125; // 无异常，直接打印输出 System.out.println(&quot;Name: &quot; + employee.name); System.out.println(&quot;Address: &quot; + employee.address); System.out.println(&quot;Age: &quot; + employee.age); &#125;&#125;输出结果：Name: zhangsanAddress: beiqingluAge: 0 JVM 可以反序列化对象，但前提是必须能够找到 class 文件的类。如果找不到该类的 class 文件，则抛出一个 ClassNotFoundException 异常。 另外，当 JVM 反序列化对象时，如果找到的 class 文件，在序列化对象之后发生了修改，那么反序列化操作也会失败，抛出一个 InvalidClassException 异常。发生这个异常的原因如下： 该类的序列版本号与从流中读取的类描述符的版本号不匹配； 该类包含未知数据类型； 该类没有可访问的无参数构造方法。 序列版本号 serialVersionUID需要知道的是，只有实现了 Serializable 或 Externalizable 接口的类的对象才能被序列化。Externalizable 接口继承自 Serializable 接口，实现 Externalizable 接口的类完全由自身来控制序列化的行为，而仅实现 Serializable 接口的类可以采用默认的序列化方式 。 凡是实现 Serializable 接口的类都有一个表示序列化版本标识符的静态变量：private static final long serialVersionUID;，该版本号的目的在于验证序列化的对象和对应类是否版本匹配。 类的 serialVersionUID 的默认值完全依赖于 java 编译器的实现，对于同一个类，用不同的 java 编译器编译，有可能会导致不同的 serialVersionUID。 当然，也可以人为显式地定义 serialVersionUID，这种方式有两种用途： 在某些场合，希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有相同的 serialVersionUID；在某些场合，不希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有不同的 serialVersionUID。 当你序列化了一个类实例后，后续可能更改一个字段或添加一个字段。如果不设置 serialVersionUID，所做的任何更改都将导致无法反序化旧有实例，并在反序列化时抛出一个异常；如果你添加了 serialVersionUID，在反序列旧有实例时，新添加或更改的字段值将设为初始化值 (对象为 null，基本类型为相应的初始默认值)，字段被删除将不设置。 比如，对之前的 Employee 类添加新字段，同时希望能够继续反序列化未添加字段前得到的 employee.txt 文件： 12345678910111213public class Employee implements Serializable &#123; // 1.添加序列版本号，并保持和不添加新属性之前相同的serialVersionUID private static final long serialVersionUID = 5303622816239242438L; public String name; public String address; public transient int age; // transient瞬态修饰成员，不会被序列化 // 2.添加新的属性，重新反序列化之前序列化得到的文件，此时该属性赋为int类型的默认值0 public int eid; public void addressCheck() &#123; System.out.println(&quot;Address check : &quot; + name + &quot; -- &quot; + address); &#125;&#125; 此时，再次执行 DeserializeDemo 的方法，输出结果如下： 1234Name: zhangsanAddress: beiqingluAge: 0Eid：0 打印流什么是打印流平时我们在控制台打印输出，是调用 print 方法和 println 方法完成的，实际上，这两个方法都来自于 java.io.PrintStream 类，该类能够方便地打印各种数据类型的值，是一种便捷的输出方式。 打印流分类： 字节打印流 PrintStream 字符打印流 PrintWriter 打印流特点： 只操作目的地，不操作数据源。 可以操作任意类型的数据。 如果启用了自动刷新，在调用 println() 方法的时候，能够换行并刷新。 可以直接操作文件。 一般情况下，如果一个流的构造方法能够同时接收 File 和 String 类型的参数，都是可以直接操作文件的！ PrintStream 是 OutputStream 的子类，PrintWriter 是 Writer 的子类，两者处于对等的位置上，所以它们的 API 是非常相似的。二者区别无非一个是字节打印流，一个是字符打印流。 字节输出打印流 PrintStream 复制文本文件12345678910111213141516public class PrintStreamDemo &#123; public static void main(String[] args) &#123; try &#123; BufferedReader br = new BufferedReader(new FileReader(&quot;xisun-database/read.txt&quot;)); PrintStream ps = new PrintStream(&quot;xisun-database/printcopy.txt&quot;); String line; while ((line = br.readLine()) != null) &#123; ps.println(line); &#125; ps.close(); br.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 字符输出打印流 PrintWriter 复制文本文件12345678910111213141516public class PrintWriterDemo &#123; public static void main(String[] args) &#123; try &#123; BufferedReader br = new BufferedReader(new FileReader(&quot;xisun-database/read.txt&quot;)); PrintWriter pw = new PrintWriter(&quot;xisun-database/printcopy.txt&quot;); String line; while ((line = br.readLine()) != null) &#123; pw.println(line); &#125; pw.close(); br.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; Properties 属性类Properties 概述java.util.Properties 继承于 Hashtable，来表示一个持久的属性集。它使用键值结构存储数据，每个键及其对应值都是一个字符串。该类也被许多 java 类使用，比如获取系统属性时，System.getProperties() 方法就是返回一个 Properties 对象。 Properties 类构造方法 public Properties()：创建一个空的属性列表。 基本的存储方法 public Object setProperty(String key, String value) ： 保存一对属性。 public String getProperty(String key) ：使用此属性列表中指定的键搜索属性值。 public Set&lt;String&gt; stringPropertyNames() ：所有键的名称的集合。 举例如下： 123456789101112131415161718192021222324252627282930313233public class ProDemo &#123; public static void main(String[] args) &#123; // 创建属性集对象 Properties properties = new Properties(); // 添加键值对元素 properties.setProperty(&quot;filename&quot;, &quot;a.txt&quot;); properties.setProperty(&quot;length&quot;, &quot;209385038&quot;); properties.setProperty(&quot;location&quot;, &quot;D:\\\\a.txt&quot;); // 打印属性集对象 System.out.println(properties); // 通过键，获取属性值 System.out.println(properties.getProperty(&quot;filename&quot;)); System.out.println(properties.getProperty(&quot;length&quot;)); System.out.println(properties.getProperty(&quot;location&quot;)); // 遍历属性集，获取所有键的集合 Set&lt;String&gt; strings = properties.stringPropertyNames(); // 打印键值对 for (String key : strings) &#123; System.out.println(key + &quot; -- &quot; + properties.getProperty(key)); &#125; &#125;&#125;输出结果：&#123;location=D:\\a.txt, filename=a.txt, length=209385038&#125;a.txt209385038D:\\a.txtlocation -- D:\\a.txtfilename -- a.txtlength -- 209385038 与流相关的方法 public void load(InputStream inStream)： 从字节输入流中读取键值对。 public void load(Reader reader)：从字符输入流中读取键值对。 public void store(Writer writer,String comments) public void store(OutputStream out,String comments) 可以看出，参数中使用了字节输入流时，通过流对象，可以关联到某文件上，这样就能够加载文本中的数据了。 例如，现在文本数据格式如下： 123filename=Properties.txtlength=123location=C:/Properties.txt 文本中的数据，必须是键值对形式，可以使用空格、等号、冒号等符号分隔。 123456789101112131415161718192021public class ProDemo &#123; public static void main(String[] args) &#123; try (FileInputStream fis = new FileInputStream(&quot;xisun-database/Properties.txt&quot;)) &#123; // 创建属性集对象 Properties properties = new Properties(); // 加载文本中信息到属性集 properties.load(fis); // 遍历集合并打印 Set&lt;String&gt; strings = properties.stringPropertyNames(); for (String key : strings) &#123; System.out.println(key + &quot; -- &quot; + properties.getProperty(key)); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;输出结果：location -- C:/Properties.txtfilename -- Properties.txtlength -- 123 总结字节流 FileInputStream 和 FileOutputStream 用法读文件： 12345678910111213public class FileRead &#123; public static void main(String[] args) &#123; try (FileInputStream fis = new FileInputStream(&quot;E:/read.txt&quot;)) &#123; int len = 0; byte[] bytes = new byte[1024]; while ((len = fis.read(bytes)) != -1) &#123; System.out.println(new String(bytes, 0, len)); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 复制图片： 1234567891011121314public class PictureCopy &#123; public static void main(String[] args) &#123; try (FileInputStream fis = new FileInputStream(&quot;E:/input.png&quot;); FileOutputStream fos = new FileOutputStream(&quot;E:/copy-input.png&quot;)) &#123; int len; byte[] bytes = new byte[1024]; while ((len = fis.read(bytes)) != -1) &#123; fos.write(bytes, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 字符流 FileReader 和 FileWriter 的用法复制文件： 1234567891011121314public class FileCopy &#123; public static void main(String[] args) &#123; try (FileReader fr = new FileReader(&quot;E:/read.txt&quot;); FileWriter fw = new FileWriter(&quot;E:/copy-read.txt&quot;)) &#123; char[] chars = new char[1024]; int len; while ((len = fr.read(chars)) != -1) &#123; fw.write(chars, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 字节缓冲流 BufferedInputStream 和 BufferedOutputStream 的用法复制文件： 12345678910111213141516public class FileCopy &#123; public static void main(String[] args) &#123; try (FileInputStream fis = new FileInputStream(&quot;E:/read.txt&quot;); FileOutputStream fos = new FileOutputStream(&quot;E:/copy-read.txt&quot;); BufferedInputStream bis = new BufferedInputStream(fis); BufferedOutputStream bos = new BufferedOutputStream(fos)) &#123; int len; byte[] bytes = new byte[8 * 1024]; while ((len = bis.read(bytes)) != -1) &#123; bos.write(bytes, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 性能优于 FileInputStream 和 FileOutputStream。 字符缓冲流 BufferedReader 和 BufferedWriter 的用法复制文件： 12345678910111213141516public class FileCopy &#123; public static void main(String[] args) &#123; try (FileReader fr = new FileReader(&quot;E:/read.txt&quot;); FileWriter fw = new FileWriter(&quot;E:/copy-read.txt&quot;); BufferedReader bfr = new BufferedReader(fr); BufferedWriter bfw = new BufferedWriter(fw)) &#123; int len; char[] chars = new char[8 * 1024]; while ((len = bfr.read(chars)) != -1) &#123; bfw.write(chars, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 特有方法——按行读取和写入： 12345678910111213141516public class FileCopy &#123; public static void main(String[] args) &#123; try (FileReader fr = new FileReader(&quot;E:/read.txt&quot;); FileWriter fw = new FileWriter(&quot;E:/copy-read.txt&quot;); BufferedReader bfr = new BufferedReader(fr); BufferedWriter bfw = new BufferedWriter(fw)) &#123; String line; while ((line = bfr.readLine()) != null) &#123; bfw.write(line); bfw.newLine(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 性能优于 FileReader 和 FileWriter。 FileWriter 每次调用 write() 方法，都会直接写入磁盘，不但效率低，性能也差。而 BufferedWriter 每次调用 write() 方法，会先写入缓冲区，直到缓冲区满了才写入磁盘。查看 BufferedWriter 源码可知 defaultCharBufferSize = 8192;，即缓冲区大小默认是 8 K，可以在构造方法时，指定缓冲区的大小，如： 12BufferedReader bfr = new BufferedReader(fr, 5 * 1024 * 1024);BufferedWriter bfw = new BufferedWriter(fw, 5 * 1024 * 1024)； 对于文本文件，推荐使用 BufferedReader 和 BufferedWriter 两个字符缓冲流对象处理。 使用字符流复制文件时，建议采用逐字符复制和逐行复制两种方式。 关于 flush() 方法的特别说明使用缓冲流时，默认情况要等缓冲区满了才会写入磁盘，或者调用 close() 方法关闭文件时也会写入磁盘，如果希望立即将缓冲区中的数据写入磁盘，则需要调用 flush() 方法。 另外，创建的流对象均应放在 try 结构体上，这样 java 会自动关闭流对象。 1234567891011121314151617public class FileCopy &#123; public static void main(String[] args) &#123; try (FileReader fr = new FileReader(&quot;E:/read.txt&quot;); FileWriter fw = new FileWriter(&quot;E:/copy-read.txt&quot;)) &#123; BufferedReader bfr = new BufferedReader(fr); BufferedWriter bfw = new BufferedWriter(fw); String line; while ((line = bfr.readLine()) != null) &#123; bfw.write(line); bfw.newLine(); &#125; bfw.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 上面的写法，缓冲流对象没有放在 try 结构体上，如果不调用 flush() 方法，可能会造成数据丢失。 12345678910111213141516public class FileCopy &#123; public static void main(String[] args) &#123; try (FileReader fr = new FileReader(&quot;E:/read.txt&quot;); FileWriter fw = new FileWriter(&quot;E:/copy-read.txt&quot;); BufferedReader bfr = new BufferedReader(fr); BufferedWriter bfw = new BufferedWriter(fw)) &#123; String line; while ((line = bfr.readLine()) != null) &#123; bfw.write(line); bfw.newLine(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 上面的写法，缓冲流对象放在 try 结构体上，不需要调用 flush() 方法，java 会自动关闭流，在关闭之前，会将缓冲区中的数据写入磁盘。 本文参考https://juejin.cn/post/6844903985078337550#heading-55 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"KafkaConsumer 源码之 consumer 的 offset commit 机制和 partition 分配机制","slug":"kafka-consumer-commitandpartition","date":"2020-11-24T07:17:06.000Z","updated":"2021-01-05T07:32:30.026Z","comments":true,"path":"2020/11/24/kafka-consumer-commitandpartition/","link":"","permalink":"http://example.com/2020/11/24/kafka-consumer-commitandpartition/","excerpt":"","text":"紧接着上篇文章，这篇文章讲述 consumer 提供的 offset commit 机制和 partition 分配机制，具体如何使用是需要用户结合具体的场景进行选择，本文讲述一下其底层实现。 自动 offset commit 机制1234// 自动提交，默认trueprops.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// 设置自动每1s提交一次，默认为5sprops.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 通过上面设置，启动自动提交 offset 以及设置自动提交间隔时间。 手动 offset commit 机制先看下两种不同的手动 offset commit 机制，一种是同步 commit，一种是异步 commit，既然其作用都是 offset commit，应该不难猜到它们底层使用接口都是一样的，其调用流程如下图所示： 同步 commit1234567// 对poll()中返回的所有topics和partition列表进行commit// 这个方法只能将offset提交Kafka中，Kafka将会在每次rebalance之后的第一次拉取或启动时使用同步commit// 这是同步commit，它将会阻塞进程，直到commit成功或者遇到一些错误public void commitSync() &#123;&#125;// 只对指定的topic-partition列表进行commitpublic void commitSync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) &#123;&#125; 其实，从上图中，就已经可以看出，同步 commit 的实现方式，client.poll () 方法会阻塞直到这个 request 完成或超时才会返回。 异步 commit12345public void commitAsync() &#123;&#125;public void commitAsync(OffsetCommitCallback callback) &#123;&#125;public void commitAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback) &#123;&#125; 对于异步的 commit，最后调用的都是 doCommitOffsetsAsync () 方法，其具体实现如下： 12345678910111213141516171819202122232425262728private void doCommitOffsetsAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, final OffsetCommitCallback callback) &#123; // 发送offset-commit请求 RequestFuture&lt;Void&gt; future = sendOffsetCommitRequest(offsets); final OffsetCommitCallback cb = callback == null ? defaultOffsetCommitCallback : callback; future.addListener(new RequestFutureListener&lt;Void&gt;() &#123; @Override public void onSuccess(Void value) &#123; if (interceptors != null) interceptors.onCommit(offsets); // 添加成功的请求，以唤醒相应的回调函数 completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, null)); &#125; @Override public void onFailure(RuntimeException e) &#123; Exception commitException = e; if (e instanceof RetriableException) &#123; commitException = new RetriableCommitFailedException(e); &#125; // 添加失败的请求，以唤醒相应的回调函数 completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, commitException)); if (commitException instanceof FencedInstanceIdException) &#123; asyncCommitFenced.set(true); &#125; &#125; &#125;);&#125; 在异步 commit 中，可以添加相应的回调函数，如果 request 处理成功或处理失败，ConsumerCoordinator 会通过 invokeCompletedOffsetCommitCallbacks () 方法唤醒相应的回调函数。 上面简单的介绍了同步 commit 和异步 commit，更详细的分析参考：Kafka consumer 的 offset 的提交方式。 注意：手动 commit 时，提交的是下一次要读取的 offset。举例如下： 1234567891011121314151617181920try &#123; while(running) &#123; // 取得消息 ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE); // 根据分区来遍历数据 for (TopicPartition partition : records.partitions()) &#123; List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition); // 数据处理 for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123; System.out.println(record.offset() + &quot;: &quot; + record.value()); &#125; // 取得当前读取到的最后一条记录的offset long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); // 提交offset，记得要 + 1 consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1))); &#125; &#125;&#125; finally &#123; consumer.close();&#125; commit offset 请求的处理当 Kafka Server 端接收到来自 client 端的 offset commit 请求时，对于提交的 offset，GroupCoordinator 会记录在 GroupMetadata 对象中，至于其实现的逻辑细节，此处不再赘述。 partition 分配机制consumer 提供了三种不同的 partition 分配策略，可以通过 partition.assignment.strategy 参数进行配置，默认情况下使用的是 org.apache.kafka.clients.consumer.RangeAssignor，Kafka 中提供了另外两种 partition 的分配策略 org.apache.kafka.clients.consumer.RoundRobinAssignor 和 org.apache.kafka.clients.consumer.StickyAssignor，它们关系如下图所示： 通过上图可以看出，用户可以自定义相应的 partition 分配机制，只需要继承这个 AbstractPartitionAssignor 抽象类即可。 partition 分配策略，其实也就是 reblance 策略。 AbstractPartitionAssignorAbstractPartitionAssignor 有一个抽象方法，如下所示： 12345678910/** * Perform the group assignment given the partition counts and member subscriptions * @param partitionsPerTopic The number of partitions for each subscribed topic. Topics not in metadata will be excluded * from this map. * @param subscriptions Map from the memberId to their respective topic subscription * @return Map from each member to the list of partitions assigned to them. */// 根据partitionsPerTopic和subscriptions进行分配，具体的实现会在子类中实现(不同的子类，其实现方法不相同)public abstract Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions); assign () 这个方法，有两个参数： partitionsPerTopic：所订阅的每个 topic 与其 partition 数的对应关系，metadata 没有的 topic 将会被移除； subscriptions：每个 consumerId 与其所订阅的 topic 列表的关系。 继承 AbstractPartitionAssignor 的子类，通过实现 assign () 方法，来进行相应的 partition 分配。 RangeAssignor 分配模式assign () 方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; // 1.参数含义:(topic, List&lt;consumerId&gt;)，获取每个topic被多少个consumer订阅了 Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions); // 2.存储最终的分配方案 Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) assignment.put(memberId, new ArrayList&lt;&gt;()); for (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123; String topic = topicEntry.getKey(); List&lt;String&gt; consumersForTopic = topicEntry.getValue(); // 3.每个topic的partition数量 Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic == null) continue; Collections.sort(consumersForTopic); // 4.取商，表示平均每个consumer会分配到多少个partition int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size(); // 5.取余，表示平均分配后还剩下多少个partition未被分配 int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size(); List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic); // 6.这里是关键点，分配原则是将未能被平均分配的partition分配到前consumersWithExtraPartition个consumer for (int i = 0, n = consumersForTopic.size(); i &lt; n; i++) &#123; // 假设partition有7个，consumer有5个，则numPartitionsPerConsumer=1，consumersWithExtraPartition=2 // i=0, start: 0, length: 2, topic-partition: p0, p1 // i=1, start: 2, length: 2, topic-partition: p2, p3 // i=2, start: 4, length: 1, topic-partition: p4 // i=3, start: 5, length: 1, topic-partition: p5 // i=4, start: 6, length: 1, topic-partition: p6 int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition); int length = numPartitionsPerConsumer + (i + 1 &gt; consumersWithExtraPartition ? 0 : 1); assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length)); &#125; &#125; return assignment;&#125; 假设 topic 的 partition 数为 numPartitionsForTopic，group 中订阅这个 topic 的 member 数为 consumersForTopic.size()，首先需要算出两个值： numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size()：表示平均每个 consumer 会分配到几个 partition； consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size()：表示平均分配后还剩下多少个 partition 未分配。 分配的规则是：对于剩下的那些 partition 分配到前 consumersWithExtraPartition 个 consumer 上，也就是前 consumersWithExtraPartition 个 consumer 获得 topic-partition 列表会比后面多一个。 在上述的程序中，举了一个例子，假设有一个 topic 有 7 个 partition，group 有5个 consumer，这个5个 consumer 都订阅这个 topic，那么 range 的分配方式如下： 消费者 分配方案 consumer 0 start: 0, length: 2, topic-partition: p0, p1 consumer 1 start: 2, length: 2, topic-partition: p2, p3 consumer 2 start: 4, length: 1, topic-partition: p4 consumer 3 start: 5, length: 1, topic-partition: p5 consumer 4 start: 6, length: 1, topic-partition: p6 而如果 group 中有 consumer 没有订阅这个 topic，那么这个 consumer 将不会参与分配。下面再举个例子，假设有 2 个 topic，一个有 5 个 partition，另一个有 7 个 partition，group 中有 5 个 consumer，但是只有前 3 个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下： consumer 订阅 topic1 的列表 订阅 topic2 的列表 consumer 0 t1 p0, t1 p1 t2 p0, t2 p1 consumer 1 t1 p2, t1 p3 t2 p2, t2 p3 consumer 2 t1 p4 t2 p4 consumer 3 t2 p5 consumer 4 t2 p6 RoundRobinAssignorassign () 方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536@Overridepublic Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) assignment.put(memberId, new ArrayList&lt;&gt;()); // 环状链表，存储所有的consumer，一次迭代完之后又会回到原点 CircularIterator&lt;String&gt; assigner = new CircularIterator&lt;&gt;(Utils.sorted(subscriptions.keySet())); // 获取所有订阅的topic的partition总数 for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) &#123; final String topic = partition.topic(); while (!subscriptions.get(assigner.peek()).topics().contains(topic)) assigner.next(); assignment.get(assigner.next()).add(partition); &#125; return assignment;&#125;public List&lt;TopicPartition&gt; allPartitionsSorted(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; // 所有的topics(有序) SortedSet&lt;String&gt; topics = new TreeSet&lt;&gt;(); for (Subscription subscription : subscriptions.values()) topics.addAll(subscription.topics()); // 订阅的Topic的所有的TopicPartition集合 List&lt;TopicPartition&gt; allPartitions = new ArrayList&lt;&gt;(); for (String topic : topics) &#123; Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic != null) // topic的所有partition都添加进去 allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic)); &#125; return allPartitions;&#125; Round Robin 的实现原则，简单来说就是：列出所有 topic-partition 和列出所有的 consumer member，然后开始分配，一轮之后继续下一轮，假设有一个 topic，它有7个 partition，group 中有 3 个 consumer 都订阅了这个 topic，那么其分配方式为： 消费者 分配列表 consumer 0 p0, p3, p6 consumer 1 p1, p4 consumer 2 p2, p5 对于多个 topic 的订阅，假设有 2 个 topic，一个有 5 个 partition，一个有 7 个 partition，group 中有 5 个 consumer，但是只有前 3 个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下： 消费者 订阅 topic1 的列表 订阅的 topic2 的列表 consumer 0 t1 p0, t1 p3 t2 p0, t2 p5 consumer 1 t1 p1, t1 p4 t2 p1, t2 p6 consumer 2 t1 p2 t2 p2 consumer 3 t2 p3 consumer 4 t2 p4 StickyAssignorassign () 方法的实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;TopicPartition&gt;&gt; currentAssignment = new HashMap&lt;&gt;(); Map&lt;TopicPartition, ConsumerGenerationPair&gt; prevAssignment = new HashMap&lt;&gt;(); partitionMovements = new PartitionMovements(); prepopulateCurrentAssignments(subscriptions, currentAssignment, prevAssignment); boolean isFreshAssignment = currentAssignment.isEmpty(); // a mapping of all topic partitions to all consumers that can be assigned to them final Map&lt;TopicPartition, List&lt;String&gt;&gt; partition2AllPotentialConsumers = new HashMap&lt;&gt;(); // a mapping of all consumers to all potential topic partitions that can be assigned to them final Map&lt;String, List&lt;TopicPartition&gt;&gt; consumer2AllPotentialPartitions = new HashMap&lt;&gt;(); // initialize partition2AllPotentialConsumers and consumer2AllPotentialPartitions in the following two for loops for (Entry&lt;String, Integer&gt; entry: partitionsPerTopic.entrySet()) &#123; for (int i = 0; i &lt; entry.getValue(); ++i) partition2AllPotentialConsumers.put(new TopicPartition(entry.getKey(), i), new ArrayList&lt;&gt;()); &#125; for (Entry&lt;String, Subscription&gt; entry: subscriptions.entrySet()) &#123; String consumer = entry.getKey(); consumer2AllPotentialPartitions.put(consumer, new ArrayList&lt;&gt;()); entry.getValue().topics().stream().filter(topic -&gt; partitionsPerTopic.get(topic) != null).forEach(topic -&gt; &#123; for (int i = 0; i &lt; partitionsPerTopic.get(topic); ++i) &#123; TopicPartition topicPartition = new TopicPartition(topic, i); consumer2AllPotentialPartitions.get(consumer).add(topicPartition); partition2AllPotentialConsumers.get(topicPartition).add(consumer); &#125; &#125;); // add this consumer to currentAssignment (with an empty topic partition assignment) if it does not already exist if (!currentAssignment.containsKey(consumer)) currentAssignment.put(consumer, new ArrayList&lt;&gt;()); &#125; // a mapping of partition to current consumer Map&lt;TopicPartition, String&gt; currentPartitionConsumer = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry: currentAssignment.entrySet()) for (TopicPartition topicPartition: entry.getValue()) currentPartitionConsumer.put(topicPartition, entry.getKey()); List&lt;TopicPartition&gt; sortedPartitions = sortPartitions( currentAssignment, prevAssignment.keySet(), isFreshAssignment, partition2AllPotentialConsumers, consumer2AllPotentialPartitions); // all partitions that need to be assigned (initially set to all partitions but adjusted in the following loop) List&lt;TopicPartition&gt; unassignedPartitions = new ArrayList&lt;&gt;(sortedPartitions); for (Iterator&lt;Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt;&gt; it = currentAssignment.entrySet().iterator(); it.hasNext();) &#123; Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry = it.next(); if (!subscriptions.containsKey(entry.getKey())) &#123; // if a consumer that existed before (and had some partition assignments) is now removed, remove it from currentAssignment for (TopicPartition topicPartition: entry.getValue()) currentPartitionConsumer.remove(topicPartition); it.remove(); &#125; else &#123; // otherwise (the consumer still exists) for (Iterator&lt;TopicPartition&gt; partitionIter = entry.getValue().iterator(); partitionIter.hasNext();) &#123; TopicPartition partition = partitionIter.next(); if (!partition2AllPotentialConsumers.containsKey(partition)) &#123; // if this topic partition of this consumer no longer exists remove it from currentAssignment of the consumer partitionIter.remove(); currentPartitionConsumer.remove(partition); &#125; else if (!subscriptions.get(entry.getKey()).topics().contains(partition.topic())) &#123; // if this partition cannot remain assigned to its current consumer because the consumer // is no longer subscribed to its topic remove it from currentAssignment of the consumer partitionIter.remove(); &#125; else // otherwise, remove the topic partition from those that need to be assigned only if // its current consumer is still subscribed to its topic (because it is already assigned // and we would want to preserve that assignment as much as possible) unassignedPartitions.remove(partition); &#125; &#125; &#125; // at this point we have preserved all valid topic partition to consumer assignments and removed // all invalid topic partitions and invalid consumers. Now we need to assign unassignedPartitions // to consumers so that the topic partition assignments are as balanced as possible. // an ascending sorted set of consumers based on how many topic partitions are already assigned to them TreeSet&lt;String&gt; sortedCurrentSubscriptions = new TreeSet&lt;&gt;(new SubscriptionComparator(currentAssignment)); sortedCurrentSubscriptions.addAll(currentAssignment.keySet()); balance(currentAssignment, prevAssignment, sortedPartitions, unassignedPartitions, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer); return currentAssignment;&#125; sticky 分区策略是从 0.11 版本才开始引入的，它主要有两个目的： 分区的分配要尽可能均匀 分区的分配要尽可能与上次分配的保持相同 当两者冲突的时候，第一个目标优先于第二个目标。 sticky 的分区方式作用发生分区重分配的时候，尽可能地让前后两次分配相同，进而减少系统资源的损耗及其他异常情况的发生。因为 sticky 分区策略的代码，要比 range 和 roundrobin 复杂很多，此处不做具体的细节分析，只简单举例如下： 假设有 3 个 topic，一个有 2 个 partition，一个有 3 个 partition，另外一个有 4 个 partition，group 中有 3 个 consumer，第一个 consumer 订阅了第一个 topic，第二个 consumer 订阅了前两个 topic，第三个 consumer 订阅了三个 topic，那么它们的分配方案如下： 消费者 订阅 topic1 的列表 订阅 topic2 的列表 订阅 topic3 的列表 consumer1 t1 p0 consumer2 t1 p1, t2 p1 t2 p0, t2 p3 consumer3 t3 p0, t3 p1, t3 p2, t3 p3 上面三个分区策略有着不同的分配方式，在实际使用过程中，需要根据自己的需求选择合适的策略，但是如果你只有一个 consumer，那么选择哪个方式都是一样的，但是如果是多个 consumer 不在同一台设备上进行消费，那么 sticky 方式应该更加合适。 自定义分区策略如之前所说，只需要继承 AbstractPartitionAssignor 并复写其中方法即可 (当然也可以直接实现 PartitionAssignor 接口) 自定义分区策略，其中有两个方法需要复写： 1234public String name();public abstract Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions); 其中 assign () 方法表示的是分区分配方案的实现，而 name () 方法则表示了这个分配策略的唯一名称，比如之前提到的 range，roundrobin 和 sticky, 这个名字会在和 GroupCoordinator 的通信中返回，通过它 consumer leader 来确定整个 group 的分区方案 (分区策略是由 group 中的 consumer 共同投票决定的，谁使用的多，就使用哪个策略)。 本文参考http://generalthink.github.io/2019/06/06/kafka-consumer-partition-assign/ https://matt33.com/2017/11/19/consumer-two-summary/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。 至此，关于 Kafka 的学习暂时告一段落，未来有需要时，会继续学习。更多关于 Kafka 原理等知识的介绍，参考： http://generalthink.github.io/tags/Kafka/ https://matt33.com/tags/kafka/","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaConsumer 源码之 consumer 的两种订阅模式","slug":"kafka-consumer-subscribeandassign","date":"2020-11-24T02:18:47.000Z","updated":"2021-01-05T07:33:01.339Z","comments":true,"path":"2020/11/24/kafka-consumer-subscribeandassign/","link":"","permalink":"http://example.com/2020/11/24/kafka-consumer-subscribeandassign/","excerpt":"","text":"在前面的文章中，有简单的介绍了 KafkaConsumer 的两种订阅模式，本篇文章对此进行扩展说明一下。 KafkaConsumer 的两种订阅模式， subscribe () 模式和 assign () 模式，前者是 topic 粒度 (使用 group 管理)，后者是 topic-partition 粒度 (用户自己去管理)。 订阅模式KafkaConsumer 为订阅模式提供了 4 种 API，如下： 12345678910111213// 订阅指定的topic列表，并且会自动进行动态partition订阅// 当发生以下情况时，会进行rebalance:1.订阅的topic列表改变；2.topic被创建或删除；3.consumer线程die；4.加一个新的consumer线程// 当发生rebalance时，会唤醒ConsumerRebalanceListener线程public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123;&#125;// 同上，但是这里没有设置listenerpublic void subscribe(Collection&lt;String&gt; topics) &#123;&#125;// 订阅那些满足一定规则(pattern)的topicpublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123;&#125;// 同上，但是这里没有设置listenerpublic void subscribe(Pattern pattern) &#123;&#125; 以上 4 种 API 都是按照 topic 级别去订阅，可以动态地获取其分配的 topic-partition，这是使用 Group 动态管理，它不能与手动 partition 管理一起使用。当监控到发生下面的事件时，Group 将会触发 rebalance 操作： 订阅的 topic 列表变化； topic 被创建或删除； consumer group 的某个 consumer 实例挂掉； 一个新的 consumer 实例通过 join 方法加入到一个 group 中。 在这种模式下，当 KafkaConsumer 调用 poll () 方法时，第一步会首先加入到一个 group 中，并获取其分配的 topic-partition 列表，具体细节在前面的文章中已经分析过了。 这里介绍一下当调用 subscribe () 方法之后，consumer 所做的事情，分两种情况介绍，一种按 topic 列表订阅，一种是按 pattern 模式订阅： topic 列表订阅 topic 列表订阅，最终调用如下方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Subscribe to the given list of topics to get dynamically * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * As part of group management, the consumer will keep track of the list of consumers that belong to a particular * group and will trigger a rebalance operation if any one of the following events are triggered: * &lt;ul&gt; * &lt;li&gt;Number of partitions change for any of the subscribed topics * &lt;li&gt;A subscribed topic is created or deleted * &lt;li&gt;An existing member of the consumer group is shutdown or fails * &lt;li&gt;A new member is added to the consumer group * &lt;/ul&gt; * &lt;p&gt; * When any of these events are triggered, the provided listener will be invoked first to indicate that * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received. * Note that rebalances will only occur during an active call to &#123;@link #poll(Duration)&#125;, so callbacks will * also only be invoked during that time. * * The provided listener will immediately override any listener set in a previous call to subscribe. * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics * subscribed in this call. See &#123;@link ConsumerRebalanceListener&#125; for more details. * * @param topics The list of topics to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; acquireAndEnsureOpen(); try &#123; maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot be null&quot;); if (topics.isEmpty()) &#123; // treat subscribing to empty topic list as the same as unsubscribing this.unsubscribe(); &#125; else &#123; for (String topic : topics) &#123; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;); &#125; throwIfNoAssignorsConfigured(); fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(&quot;Subscribed to topic(s): &#123;&#125;&quot;, Utils.join(topics, &quot;, &quot;)); // 核心步骤在此处执行 if (this.subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; 将 SubscriptionType 类型设置为 AUTO_TOPICS，并更新 SubscriptionState 中记录的 subscription 属性 (记录的是订阅的 topic 列表)； 12345678910111213141516171819public synchronized boolean subscribe(Set&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_TOPICS); return changeSubscription(topics);&#125;private boolean changeSubscription(Set&lt;String&gt; topicsToSubscribe) &#123; if (subscription.equals(topicsToSubscribe)) return false; subscription = topicsToSubscribe; if (subscriptionType != SubscriptionType.USER_ASSIGNED) &#123; groupSubscription = new HashSet&lt;&gt;(groupSubscription); groupSubscription.addAll(topicsToSubscribe); &#125; else &#123; groupSubscription = new HashSet&lt;&gt;(topicsToSubscribe); &#125; return true;&#125; 请求更新 metadata。 1234567891011121314public synchronized void requestUpdateForNewTopics() &#123; // Override the timestamp of last refresh to let immediate update. this.lastRefreshMs = 0; this.requestVersion++; requestUpdate();&#125;/** * Request an update of the current cluster metadata info, return the current updateVersion before the update */public synchronized int requestUpdate() &#123; this.needUpdate = true; return this.updateVersion;&#125; pattern 模式订阅 pattern 模式订阅，最终调用如下方法： 123456789101112131415161718192021222324252627282930313233343536/** * Subscribe to all topics matching specified pattern to get dynamically assigned partitions. * The pattern matching will be done periodically against all topics existing at the time of check. * This can be controlled through the &#123;@code metadata.max.age.ms&#125; configuration: by lowering * the max metadata age, the consumer will refresh metadata more often and check for matching topics. * &lt;p&gt; * See &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125; for details on the * use of the &#123;@link ConsumerRebalanceListener&#125;. Generally rebalances are triggered when there * is a change to the topics matching the provided pattern and when consumer group membership changes. * Group rebalances only take place during an active call to &#123;@link #poll(Duration)&#125;. * * @param pattern Pattern to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If pattern or listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with topics, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; maybeThrowInvalidGroupIdException(); if (pattern == null) throw new IllegalArgumentException(&quot;Topic pattern to subscribe to cannot be null&quot;); acquireAndEnsureOpen(); try &#123; throwIfNoAssignorsConfigured(); log.info(&quot;Subscribed to pattern: &#x27;&#123;&#125;&#x27;&quot;, pattern); this.subscriptions.subscribe(pattern, listener); this.coordinator.updatePatternSubscription(metadata.fetch()); this.metadata.requestUpdateForNewTopics(); &#125; finally &#123; release(); &#125;&#125; 将 SubscriptionType 类型设置为 AUTO_PATTERN，并更新 SubscriptionState 中记录的 subscribedPattern 属性，设置为 pattern； 12345public synchronized void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_PATTERN); this.subscribedPattern = pattern;&#125; 调用 coordinator 的 updatePatternSubscription () 方法，遍历所有 topic 的 metadata，找到所有满足 pattern 的 topic 列表，更新到 SubscriptionState 的 subscriptions 属性，并请求更新 Metadata； 1234567891011121314151617181920212223242526272829public void updatePatternSubscription(Cluster cluster) &#123; final Set&lt;String&gt; topicsToSubscribe = cluster.topics().stream() .filter(subscriptions::matchesSubscribedPattern) .collect(Collectors.toSet()); if (subscriptions.subscribeFromPattern(topicsToSubscribe)) metadata.requestUpdateForNewTopics();&#125;public synchronized boolean subscribeFromPattern(Set&lt;String&gt; topics) &#123; if (subscriptionType != SubscriptionType.AUTO_PATTERN) throw new IllegalArgumentException(&quot;Attempt to subscribe from pattern while subscription type set to &quot; + subscriptionType); return changeSubscription(topics);&#125;private boolean changeSubscription(Set&lt;String&gt; topicsToSubscribe) &#123; if (subscription.equals(topicsToSubscribe)) return false; subscription = topicsToSubscribe; if (subscriptionType != SubscriptionType.USER_ASSIGNED) &#123; groupSubscription = new HashSet&lt;&gt;(groupSubscription); groupSubscription.addAll(topicsToSubscribe); &#125; else &#123; groupSubscription = new HashSet&lt;&gt;(topicsToSubscribe); &#125; return true;&#125; 1234567891011121314public synchronized void requestUpdateForNewTopics() &#123; // Override the timestamp of last refresh to let immediate update. this.lastRefreshMs = 0; this.requestVersion++; requestUpdate();&#125;/** * Request an update of the current cluster metadata info, return the current updateVersion before the update */public synchronized int requestUpdate() &#123; this.needUpdate = true; return this.updateVersion;&#125; 其他部分，两者基本一样，只是 pattern 模型在每次更新 topic-metadata 时，获取全局的 topic 列表，如果发现有新加入的符合条件的 topic，就立马去订阅，其他的地方，包括 group 管理、topic-partition 的分配都是一样的。 分配模式当调用 assign () 方法手动分配 topic-partition 列表时，不会使用 consumer 的 Group 管理机制，也即是当 consumer group member 变化或 topic 的 metadata 信息变化时，不会触发 rebalance 操作。比如：当 topic 的 partition 增加时，这里无法感知，需要用户进行相应的处理，Apache Flink 就是使用的这种方式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Manually assign a list of partitions to this consumer. This interface does not allow for incremental assignment * and will replace the previous assignment (if there is one). * &lt;p&gt; * If the given list of topic partitions is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * &lt;p&gt; * Manual topic assignment through this method does not use the consumer&#x27;s group management * functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic * metadata change. Note that it is not possible to use both manual partition assignment with &#123;@link #assign(Collection)&#125; * and group assignment with &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;. * &lt;p&gt; * If auto-commit is enabled, an async commit (based on the old assignment) will be triggered before the new * assignment replaces the old one. * * @param partitions The list of partitions to assign this consumer * @throws IllegalArgumentException If partitions is null or contains null or empty topics * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with topics or pattern * (without a subsequent call to &#123;@link #unsubscribe()&#125;) */@Overridepublic void assign(Collection&lt;TopicPartition&gt; partitions) &#123; acquireAndEnsureOpen(); try &#123; if (partitions == null) &#123; throw new IllegalArgumentException(&quot;Topic partition collection to assign to cannot be null&quot;); &#125; else if (partitions.isEmpty()) &#123; this.unsubscribe(); &#125; else &#123; for (TopicPartition tp : partitions) &#123; String topic = (tp != null) ? tp.topic() : null; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic partitions to assign to cannot have null or empty topic&quot;); &#125; fetcher.clearBufferedDataForUnassignedPartitions(partitions); // make sure the offsets of topic partitions the consumer is unsubscribing from // are committed since there will be no following rebalance if (coordinator != null) this.coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds()); log.info(&quot;Subscribed to partition(s): &#123;&#125;&quot;, Utils.join(partitions, &quot;, &quot;)); if (this.subscriptions.assignFromUser(new HashSet&lt;&gt;(partitions))) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; assign () 方法是手动向 consumer 分配一些 topic-partition 列表，并且这个接口不允许增加分配的 topic-partition 列表，将会覆盖之前分配的 topic-partition 列表，如果给定的 topic-partition 列表为空，它的作用将会与 unsubscribe () 方法一样。 这种手动 topic 分配也不会使用 consumer 的 group 管理，当 group 的 member 变化或 topic 的 metadata 变化时，也不会触发 rebalance 操作。 这里所说的 consumer 的 group 管理，就是前面所说的 consumer 如何加入 group 的管理过程。如果使用的是 assign 模式，也即是非 AUTO_TOPICS 或 AUTO_PATTERN 模式时，consumer 实例在调用 poll () 方法时，不会向 GroupCoordinator 发送 join-group、sync-group、heartbeat 请求，也就是说 GroupCoordinator 拿不到这个 consumer 实例的相关信息，也不会去维护这个 member 是否存活，这种情况下就需要用户自己管理自己的处理程序。但是这种模式可以进行 offset commit，这将在下一篇文章进行分析。 小结根据上面的讲述，这里做一下小结，两种模式对比如下图所示： 简单说明如下： 模式 不同之处 相同之处 subscribe () 使用 Kafka group 管理，自动进行 rebalance 操作 可以在 Kafka 保存 offset assign () 用户自己进行相关的处理 也可以进行 offset commit，但是尽量保证 group.id 唯一性，如果使用一个与上面模式一样的 group，offset commit 请求将会被拒绝 本文参考https://matt33.com/2017/11/18/consumer-subscribe/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"maven 的基础使用","slug":"maven","date":"2020-11-18T07:18:48.000Z","updated":"2021-01-27T03:48:19.323Z","comments":true,"path":"2020/11/18/maven/","link":"","permalink":"http://example.com/2020/11/18/maven/","excerpt":"","text":"maven 的功能maven 是一个项目管理工具，主要作用是在项目开发阶段对项目进行依赖管理和项目构建。 依赖管理：仅仅通过 jar 包的几个属性，就能确定唯一的 jar 包，在指定的文件 pom.xml 中，只要写入这些依赖属性，就会自动下载并管理 jar 包。 项目构建：内置很多的插件与生命周期，支持多种任务，比如校验、编译、测试、打包、部署、发布… 项目的知识管理：管理项目相关的其他内容，比如开发者信息，版本等等。 maven 的安装与配置 下载，地址：http://maven.apache.org/download.cgi 注意：安装 maven 之前，必须先确保你的机器中已经安装了 jdk，如果是 maven 3 则必须 jdk 1.7 以上。 解压，添加环境变量 MAVEN_HOME，值为解压后的 maven 路径。 在 Path 环境变量的变量值末尾添加 %MAVEN_HOME%\\bin; 。 在 cmd 窗口输入 mvn –version，显示 maven 版本信息，说明安装配置成功。 在 IDEA 中使用 maven maven 的仓库maven 仓库分为本地仓库和远程仓库，而远程仓库又分为 maven 中央仓库、其他远程仓库和私服 (私有服务器)。其中，中央仓库是由 maven 官方提供的，而私服就需要我们自己搭建。 本地仓库默认情况下，不管 linux 还是 windows，每个用户在自己的用户目录下都有一个路径名为 .m2\\repository 的仓库目录，如：C:\\Users\\XiSun\\.m2\\repository。如果不自定义本地仓库的地址，则会将下载的构件放到该目录下。 修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，可以自定义本地仓库地址，例如： 1&lt;localRepository&gt;D:\\Program Files\\Maven\\apache-maven-3.6.3-maven-repository&lt;/localRepository&gt; 运行 maven 的时候，maven 所需要的任何构件都是直接从本地仓库获取的。如果本地仓库没有，它会首先尝试从远程仓库下载构件至本地仓库，然后再使用本地仓库的构件。 远程仓库maven 中央仓库maven 中央仓库，是由 maven 社区提供的仓库，其中包含了大量常用的库。一般来说，简单的 java 项目依赖的构件都可以在这里下载到。 在 maven 安装目录的 lib 目录下，有一个 maven-model-builder-3.6.1.jar，里面的 org/apache/maven/model/pom-4.0.0.xml 文件定义了 maven 默认中央仓库的地址：https://repo.maven.apache.org/maven2 因为 maven 中央仓库默认在国外，国内使用难免很慢，推荐将其更换为阿里云的镜像。 全局配置：修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，在 mirrors 节点上，添加如下内容。 1234567891011121314151617181920&lt;mirrors&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 局部配置：修改项目的 pom.xml 文件，在 repositories 上，添加如下内容。 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 更多中央仓库地址参考：https://blog.csdn.net/Hello_World_QWP/article/details/82463799。 私服maven 私服就是公司局域网内的 maven 远程仓库，每个员工的电脑上安装 maven 软件并且连接 maven 私服，程序员可以将自己开发的项目打成 jar 并发布到私服，其它项目组成员就可以从私服下载所依赖的 jar。 私服还充当一个代理服务器的角色，当私服上没有 jar 包时，会从 maven 中央仓库自动下载。 nexus 是一个 maven 仓库管理器 (其实就是一个软件)，nexus 可以充当 maven 私服，同时 nexus 还提供强大的仓库管理、构件搜索等功能。 如果 maven 在中央仓库中也找不到依赖的文件，它会停止构建过程并输出错误信息到控制台。为避免这种情况，maven 提供了远程仓库的概念，它是开发人员自己定制的仓库，包含了所需要的代码库或者其他工程中用到的 jar 文件。 搭建 maven 私服 下载 nexus，地址：https://help.sonatype.com/repomanager2/download/download-archives---repository-manager-oss 安装 nexus 将下载的压缩包进行解压，进入 bin 目录： 打开 cmd 窗口并进入上面 bin 目录下，执行 nexus.bat install 命令安装服务 (注意需要以管理员身份运行 cmd 命令)： 启动 nexus 经过前面命令已经完成 nexus 的安装，可以通过如下两种方式启动 nexus 服务。 在 Windows 系统服务中启动 nexus 在命令行执行 nexus.bat start 命令启动 nexus 访问 nexus 启动 nexus 服务后，访问 http://localhost:8081/nexus，点击右上角 LogIn 按钮，使用默认用户名 admin 和密码 admin123 登录系统。 登录成功后，点击左侧菜单 Repositories，可以看到 nexus 内置的仓库列表，如下图： nexus 仓库类型通过前面的仓库列表可以看到，nexus 默认内置了很多仓库，这些仓库可以划分为 4 种类型，每种类型的仓库用于存放特定的 jar 包，具体说明如下。 hosted：宿主仓库，部署自己的 jar 到这个类型的仓库，包括 Releases 和 Snapshots 两部分，Releases 为公司内部发布版本仓库，Snapshots 为公司内部测试版本仓库。 proxy：代理仓库，用于代理远程的公共仓库，如 maven 中央仓库，用户连接私服，私服自动去中央仓库下载 jar 包或者插件。 group：仓库组，用来合并多个 hosted 或 proxy 仓库，通常我们配置自己的 maven 连接仓库组。 virtual (虚拟)：兼容 Maven1 版本的 jar 或者插件。 nexus 仓库类型与安装目录对应关系 将项目发布到 maven 私服maven 私服是搭建在公司局域网内的 maven 仓库，公司内的所有开发团队都可以使用。例如技术研发团队开发了一个基础组件，就可以将这个基础组件打成 jar 包发布到私服，其他团队成员就可以从私服下载这个 jar 包到本地仓库并在项目中使用。 具体操作步骤如下： 配置 maven 的 settings.xml 文件 12345678910&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 注意：一定要在 idea 工具中引入的 maven 的 settings.xml 文件中配置。 配置项目的 pom.xml 文件 12345678910&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 执行 mvn clean deploy 命令 从私服下载 jar 到本地仓库前面我们已经完成了将本地项目打成 jar 包发布到 maven 私服，下面我们就需要从 maven 私服下载 jar 包到本地仓库。 具体操作步骤如下： 在 maven 的 settings.xml 文件中配置下载模板 1234567891011121314151617181920212223242526&lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;!--仓库地址，即nexus仓库组的地址--&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;!--是否下载releases构件--&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;!--是否下载snapshots构件--&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;!-- 插件仓库，maven的运行依赖插件，也需要从私服下载插件 --&gt; &lt;pluginRepository&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/profile&gt; 在 maven 的 settings.xml 文件中配置激活下载模板 123&lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 将第三方 jar 安装到本地仓库和 maven 私服在 maven 工程的 pom.xml 文件中配置某个 jar 包的坐标后，如果本地的 maven 仓库不存在这个 jar 包，maven 工具会自动到配置的 maven 私服下载，如果私服中也不存在，maven 私服就会从 maven 中央仓库进行下载。 但是并不是所有的 jar 包都可以从中央仓库下载到，比如常用的 Oracle 数据库驱动的 jar 包在中央仓库就不存在。此时需要到 Oracle 的官网下载驱动 jar 包，然后将此 jar 包通过 maven 命令安装到我们本地的 maven 仓库或者 maven 私服中，这样在 maven 项目中就可以使用 maven 坐标引用到此 jar 包了。 将第三方 jar 安装到本地仓库 下载 Oracle 的 jar 包 mvn install 命令进行安装 1mvn install:install-file -Dfile=ojdbc14-10.2.0.4.0.jar -DgroupId=com.oracle -DartifactId=ojdbc14 –Dversion=10.2.0.4.0 -Dpackaging=jar 查看本地 maven 仓库，确认安装是否成功 再比如安装 Classifier4J-0.6.jar，打开 cmd 窗口，切换到 jar 包所在目录，输入 mvn 命令，命令格式如下： 1mvn install:install-file -DgroupId=net.sf(自定义，需要与pom.xml文件中的groupId一致) -DartifactId=classifier4j(自定义，需要与pom.xml文件中的artifaceId一致) -Dversion=0.6(自定义，需要与pom.xml文件中的version一致) -Dpackaging=jar -Dfile=Classifier4J-0.6.jar(本地jar包) -DgroupId、-DartifactId、-Dversion、-Dpackaging、-Dfile 前面均有一个空格。 使用示例如下： 之后，在 maven 的本地仓库，根据 groupId —— artifactId —— version，即可找到打包进来的本地 jar 包，也可以在项目中的 pom.xml 文件引入： 12345&lt;dependency&gt; &lt;groupId&gt;net.sf&lt;/groupId&gt; &lt;artifactId&gt;classifier4j&lt;/artifactId&gt; &lt;version&gt;0.6&lt;/version&gt;&lt;/dependency&gt; 将第三方 jar 安装到 maven 私服 下载 Oracle 的 jar 包 在 maven 的 settings.xml 配置文件中配置第三方仓库的 server 信息 12345&lt;server&gt; &lt;id&gt;thirdparty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 执行 mvn deploy 命令进行安装 1mvn deploy:deploy-file -Dfile=ojdbc14-10.2.0.4.0.jar -DgroupId=com.oracle -DartifactId=ojdbc14 –Dversion=10.2.0.4.0 -Dpackaging=jar –Durl=http://localhost:8081/nexus/content/repositories/thirdparty/ -DrepositoryId=thirdparty maven 的依赖搜索顺序一般情况下，当执行 maven 构建命令时，maven 按照以下顺序查找依赖的库： 步骤 1：在本地仓库中搜索，如果找不到，执行步骤 2，如果找到了则执行其他操作。 步骤 2：在中央仓库中搜索，如果找不到，并且有一个或多个远程仓库已经设置，则执行步骤 4，如果找到了则下载到本地仓库中以备将来引用。 步骤 3：如果远程仓库没有被设置，maven 将简单的停滞处理并抛出错误 (无法找到依赖的文件)。 步骤 4：在一个或多个远程仓库中搜索依赖的文件，如果找到则下载到本地仓库以备将来引用，否则 maven 将停止处理并抛出错误 (无法找到依赖的文件)。 maven 的常用命令 clean： 清理 compile：编译 test： 测试 package：打包 install： 安装 maven 的坐标书写规范12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt;&lt;/dependency&gt; maven 的依赖范围 依赖范围 对于编译 classpath 有效 对于测试 classpath 有效 对于运行 classpath 有效 例子 compile Y Y Y spring-core test - Y - Junit provided Y Y - servlet-api runtime - Y Y JDBC 驱动 system Y Y - 本地的，maven 仓库之外的类库 默认使用 compile 依赖范围。 使用 system 依赖范围的依赖时，必须通过 systemPath 元素显示地指定依赖文件的路径。由于此类依赖不是通过 maven 仓库解析的，而且往往与本机系统绑定，可能构成构建的不可移植，因此应该谨慎使用。systemPath 元素可以引用环境变量，例如： 1234567&lt;dependency&gt; &lt;groupId&gt;javax.sql&lt;/groupId&gt; &lt;artifactId&gt;jdbc-stdext&lt;/artifactId&gt; &lt;Version&gt;2.0&lt;/Version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;java.home&#125;/lib/rt.jar&lt;/systemPath&gt;&lt;/dependency&gt; maven 的依赖传递什么是依赖传递在 maven 中，依赖是可以传递的，假设存在三个项目，分别是项目 A，项目 B 以及项目 C。假设 C 依赖 B，B 依赖 A，那么根据 maven 项目依赖的特征，不难推出项目 C 也依赖 A。如图所示： ​ 通过上面的图可以看到， 在一个 web 项目中，直接依赖了 spring-webmvc，而 spring-webmvc 依赖了 spring-aop、spring-beans 等。最终的结果就是在这个 web 项目中，间接依赖了 spring-aop、spring-beans 等。 什么是依赖冲突由于依赖传递现象的存在，如图所示，spring-webmvc 依赖 spirng-beans-4.2.4，spring-aop 依赖 spring-beans-5.0.2，现在 spirng-beans-4.2.4 已经加入到了工程中，而我们希望 spring-beans-5.0.2 加入工程。这就造成了依赖冲突。 如何解决依赖冲突 使用 maven 提供的依赖调节原则 排除依赖 锁定版本 依赖调节原则路径近者优先原则当依赖声明不在同一个 pom.xml 文件中时，或者说存在依赖传递时，路径最短的 jar 包将被选为最终依赖。 上图中，Jar2.0 将被选为最终依赖。 第一声明者优先原则当依赖声明不在同一个 pom.xml 文件中时，或者说存在依赖传递时，并且依赖传递长度相同时，最先声明的依赖将被选为最终依赖。 上图中，spring-aop 和 spring-webmvc 都依赖了 spring-beans，但是因为 spring-aop 在前面，所以最终使用的 spring-beans 是由 spring-aop 传递过来的，而 spring-webmvc 传递过来的 spring-beans 则被忽略了。 覆盖优先当依赖声明在同一个 pom.xml 文件中时，后面声明的依赖将覆盖前面声明的依赖。 排除依赖使用 exclusions 标签将传递过来的依赖排除出去。 版本锁定采用直接锁定版本的方法确定依赖 jar 包的版本，版本锁定后则不考虑依赖的声明顺序或依赖的路径，以锁定的版本为准添加到工程中，此方法在企业开发中经常使用。 版本锁定的使用方式： 第一步：在 dependencyManagement 标签中锁定依赖的版本 第二步：在 dependencies 标签中声明需要导入的 maven 坐标 备注能查找依赖的网站：https://mvnrepository.com/ 本文参考https://juejin.cn/post/6844903543711907848 https://www.jianshu.com/p/a1d9fd97f568 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"KafkaConsumer 源码之 consumer 如何拉取 offset 和数据","slug":"kafka-consumer-offsetandfetcher","date":"2020-11-10T03:32:06.000Z","updated":"2021-01-05T07:32:53.038Z","comments":true,"path":"2020/11/10/kafka-consumer-offsetandfetcher/","link":"","permalink":"http://example.com/2020/11/10/kafka-consumer-offsetandfetcher/","excerpt":"","text":"上一篇文章讲了 consumer 如何加入 consumer group，现在加入 group 成功之后，就要准备开始消费。 kafkaConsumer.poll () 方法的核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private ConsumerRecords&lt;K, V&gt; poll(final Timer timer, final boolean includeMetadataInTimeout) &#123; // Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭 acquireAndEnsureOpen(); try &#123; if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123; throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;); &#125; // poll for new data until the timeout expires do &#123; client.maybeTriggerWakeup(); if (includeMetadataInTimeout) &#123; // Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息 if (!updateAssignmentMetadataIfNeeded(timer)) &#123; return ConsumerRecords.empty(); &#125; &#125; else &#123; while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123; log.warn(&quot;Still waiting for metadata&quot;); &#125; &#125; // Step3:拉取数据，核心步骤 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer); if (!records.isEmpty()) &#123; // before returning the fetched records, we can send off the next round of fetches // and avoid block waiting for their responses to enable pipelining while the user // is handling the fetched records. // // NOTE: since the consumed position has already been updated, we must not allow // wakeups or any other errors to be triggered prior to returning the fetched records. // 在返回数据之前，发送下次的fetch请求，避免用户在下次获取数据时线程block if (fetcher.sendFetches() &gt; 0 || client.hasPendingRequests()) &#123; client.pollNoWakeup(); &#125; return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records)); &#125; &#125; while (timer.notExpired()); return ConsumerRecords.empty(); &#125; finally &#123; release(); &#125;&#125; 紧跟上一篇文章，我们继续分析 consumer 加入 group 后的行为： 123456789101112/** * Visible for testing */boolean updateAssignmentMetadataIfNeeded(final Timer timer) &#123; // 1.上一篇主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group) if (coordinator != null &amp;&amp; !coordinator.poll(timer)) &#123; return false; &#125; // 2.本篇文章从updateFetchPositions(timer)方法开始继续分析(主要功能是:consumer获得partition的offset) return updateFetchPositions(timer);&#125; KafkaConsumer 的消费策略首先，我们应该知道，KafkaConsumer 关于如何消费的 2 种策略： 手动指定：调用 consumer.seek(TopicPartition, offset)，然后开始 poll ()。 自动指定：poll () 之前给集群发送请求，让集群告知客户端，当前该 TopicPartition 的 offset 是多少，这也是我们此次分析的重点。 在讲如何拉取 offset 之前，先认识下下面这个类 (SubscriptionState 的内部类)： 12345678910111213private static class TopicPartitionState &#123; private FetchState fetchState; private FetchPosition position; // last consumed position private Long highWatermark; // the high watermark from last fetch private Long logStartOffset; // the log start offset private Long lastStableOffset; private boolean paused; // whether this partition has been paused by the user private OffsetResetStrategy resetStrategy; // the strategy to use if the offset needs resetting private Long nextRetryTimeMs; private Integer preferredReadReplica; private Long preferredReadReplicaExpireTimeMs; ...&#125; consumer 实例订阅的每个 topic-partition 都会有一个对应的 TopicPartitionState 对象，在这个对象中会记录上面内容，最需要关注的就是 position 这个属性，它表示上一次消费的位置。通过 consumer.seek () 方式指定消费 offset 的时候，其实设置的就是这个 position 值。 updateFetchPositions - 拉取 offset在 consumer 成功加入 group 并开始消费之前，我们还需要知道 consumer 是从 offset 为多少的位置开始消费。consumer 加入 group 之后，就得去获取 offset 了，下面的方法，就是开始更新 position (offset)： 1234567891011121314151617181920212223242526272829303132333435363738/** * Set the fetch position to the committed position (if there is one) * or reset it using the offset reset policy the user has configured. * * @throws org.apache.kafka.common.errors.AuthenticationException if authentication fails. See the exception for more details * @throws NoOffsetForPartitionException If no offset is stored for a given partition and no offset reset policy is * defined * @return true iff the operation completed without timing out */private boolean updateFetchPositions(final Timer timer) &#123; // If any partitions have been truncated due to a leader change, we need to validate the offsets fetcher.validateOffsetsIfNeeded(); // Step1:查看TopicPartitionState的position是否为空，第一次消费肯定为空 cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions(); if (cachedSubscriptionHashAllFetchPositions) return true; // If there are any partitions which do not have a valid position and are not // awaiting reset, then we need to fetch committed offsets. We will only do a // coordinator lookup if there are partitions which have missing positions, so // a consumer with manually assigned partitions can avoid a coordinator dependence // by always ensuring that assigned partitions have an initial position. // Step2:如果没有有效的offset，那么需要从GroupCoordinator中获取 if (coordinator != null &amp;&amp; !coordinator.refreshCommittedOffsetsIfNeeded(timer)) return false; // If there are partitions still needing a position and a reset policy is defined, // request reset using the default policy. If no reset strategy is defined and there // are partitions with a missing position, then we will raise an exception. // Step3:如果还存在partition不知道position，并且设置了offsetreset策略，那么就等待重置，不然就抛出异常 subscriptions.resetMissingPositions(); // Finally send an asynchronous request to lookup and update the positions of any // partitions which are awaiting reset. // Step4:向PartitionLeader(GroupCoordinator所在机器)发送ListOffsetRequest重置position fetcher.resetOffsetsIfNeeded(); return true;&#125; 上面的代码主要分为 4 个步骤，具体如下： 首先，查看当前 TopicPartition 的 position 是否为空，如果不为空，表示知道下次 fetch position (即拉取数据时从哪个位置开始拉取)，但如果是第一次消费，这个 TopicPartitionState.position 肯定为空。 然后，通过 GroupCoordinator 为缺少 fetch position 的 partition 拉取 position (即 last committed offset)。 继而，仍不知道 partition 的 position (_consumer_offsets 中未保存位移信息)，且设置了 offsetreset 策略，那么就等待重置，如果没有设置重置策略，就抛出 NoOffsetForPartitionException 异常。 最后，为那些需要重置 fetch position 的 partition 发送 ListOffsetRequest 重置 position (consumer.beginningOffsets ()，consumer.endOffsets ()，consumer.offsetsForTimes ()，consumer.seek () 都会发送 ListOffRequest 请求)。 上面说的几个方法相当于都是用户自己自定义消费的 offset，所以可能出现越界 (消费位置无法在实际分区中查到) 的情况，所以也是会发送 ListOffsetRequest 请求的，即触发 auto.offset.reset 参数的执行。比如现在某个 partition 的可拉取 offset 最大值为 100，如果你指定消费 offset=200 的位置，那肯定拉取不到，此时就会根据 auto.offset.reset 策略将拉取位置重置为 100 (默认的 auto.offset.reset 为 latest)。 refreshCommittedOffsetsIfNeeded我们先看下 Setp 2 中 GroupCoordinator 是如何 fetch position 的： 1234567891011121314151617181920212223242526272829/** * Refresh the committed offsets for provided partitions. * * @param timer Timer bounding how long this method can block * @return true iff the operation completed within the timeout */public boolean refreshCommittedOffsetsIfNeeded(Timer timer) &#123; final Set&lt;TopicPartition&gt; missingFetchPositions = subscriptions.missingFetchPositions(); // 1.发送获取offset的请求，核心步骤 final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = fetchCommittedOffsets(missingFetchPositions, timer); if (offsets == null) return false; for (final Map.Entry&lt;TopicPartition, OffsetAndMetadata&gt; entry : offsets.entrySet()) &#123; final TopicPartition tp = entry.getKey(); // 2.获取response中的offset final OffsetAndMetadata offsetAndMetadata = entry.getValue(); final ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.leaderAndEpoch(tp); final SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(), leaderAndEpoch); log.info(&quot;Setting offset for partition &#123;&#125; to the committed offset &#123;&#125;&quot;, tp, position); entry.getValue().leaderEpoch().ifPresent(epoch -&gt; this.metadata.updateLastSeenEpochIfNewer(entry.getKey(), epoch)); // 3.实际就是设置SubscriptionState的position值 this.subscriptions.seekUnvalidated(tp, position); &#125; return true;&#125; fetchCommittedOffsets () 方法的核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Fetch the current committed offsets from the coordinator for a set of partitions. * * @param partitions The partitions to fetch offsets for * @return A map from partition to the committed offset or null if the operation timed out */public Map&lt;TopicPartition, OffsetAndMetadata&gt; fetchCommittedOffsets(final Set&lt;TopicPartition&gt; partitions, final Timer timer) &#123; if (partitions.isEmpty()) return Collections.emptyMap(); final Generation generation = generation(); if (pendingCommittedOffsetRequest != null &amp;&amp; !pendingCommittedOffsetRequest.sameRequest(partitions, generation)) &#123; // if we were waiting for a different request, then just clear it. pendingCommittedOffsetRequest = null; &#125; do &#123; if (!ensureCoordinatorReady(timer)) return null; // contact coordinator to fetch committed offsets final RequestFuture&lt;Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt; future; if (pendingCommittedOffsetRequest != null) &#123; future = pendingCommittedOffsetRequest.response; &#125; else &#123; // 1.封装FetchRequest请求 future = sendOffsetFetchRequest(partitions); pendingCommittedOffsetRequest = new PendingCommittedOffsetRequest(partitions, generation, future); &#125; // 2.通过KafkaClient发送请求 client.poll(future, timer); if (future.isDone()) &#123; pendingCommittedOffsetRequest = null; if (future.succeeded()) &#123; // 3.请求成功，获取请求的响应数据 return future.value(); &#125; else if (!future.isRetriable()) &#123; throw future.exception(); &#125; else &#123; timer.sleep(retryBackoffMs); &#125; &#125; else &#123; return null; &#125; &#125; while (timer.notExpired()); return null;&#125; 上面的步骤和我们之前提到的发送其他请求毫无区别，基本就是这三个套路。 在获取到响应之后，会通过 subscriptions.seekUnvalidated () 方法为每个 TopicPartition 设置 position 值后，就知道从哪里开始消费订阅 topic 下的 partition 了。 resetMissingPositions在 Step 3 中，什么时候发起 FetchRequest 拿不到 position 呢？ 我们知道消费位移 (consume offset) 是保存在 _consumer_offsets 这个 topic 里面的，当我们进行消费的时候需要知道上次消费到了什么位置。那么就会发起请求去看上次消费到了 topic 的 partition 的哪个位置，但是这个消费位移是有保存时长的，默认为 7 天 (broker 端通过 offsets.retention.minutes 设置)。 当隔了一段时间再进行消费，如果这个间隔时间超过了参数的配置值，那么原先的位移信息就会丢失，最后只能通过客户端参数 auto.offset.reset 来确定开始消费的位置。 如果我们第一次消费 topic，那么在 _consumer_offsets 中也是找不到消费位移的，所以就会执行第四个步骤，发起 ListOffsetRequest 请求根据配置的 reset 策略 (即 auto.offset.reset) 来决定开始消费的位置。 resetOffsetsIfNeeded在 Step 4 中，发起 ListOffsetRequest 请求和处理 response 的核心代码如下： 123456789101112131415161718192021222324252627/** * Reset offsets for all assigned partitions that require it. * * @throws org.apache.kafka.clients.consumer.NoOffsetForPartitionException If no offset reset strategy is defined * and one or more partitions aren&#x27;t awaiting a seekToBeginning() or seekToEnd(). */public void resetOffsetsIfNeeded() &#123; // Raise exception from previous offset fetch if there is one RuntimeException exception = cachedListOffsetsException.getAndSet(null); if (exception != null) throw exception; // 1.需要执行reset策略的partition Set&lt;TopicPartition&gt; partitions = subscriptions.partitionsNeedingReset(time.milliseconds()); if (partitions.isEmpty()) return; final Map&lt;TopicPartition, Long&gt; offsetResetTimestamps = new HashMap&lt;&gt;(); for (final TopicPartition partition : partitions) &#123; Long timestamp = offsetResetStrategyTimestamp(partition); if (timestamp != null) offsetResetTimestamps.put(partition, timestamp); &#125; // 2.执行reset策略 resetOffsetsAsync(offsetResetTimestamps);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940private void resetOffsetsAsync(Map&lt;TopicPartition, Long&gt; partitionResetTimestamps) &#123; Map&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; timestampsToSearchByNode = groupListOffsetRequests(partitionResetTimestamps, new HashSet&lt;&gt;()); for (Map.Entry&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; entry : timestampsToSearchByNode.entrySet()) &#123; Node node = entry.getKey(); final Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; resetTimestamps = entry.getValue(); subscriptions.setNextAllowedRetry(resetTimestamps.keySet(), time.milliseconds() + requestTimeoutMs); // 1.发送ListOffsetRequest请求 RequestFuture&lt;ListOffsetResult&gt; future = sendListOffsetRequest(node, resetTimestamps, false); // 2.为ListOffsetRequest请求添加监听器 future.addListener(new RequestFutureListener&lt;ListOffsetResult&gt;() &#123; @Override public void onSuccess(ListOffsetResult result) &#123; if (!result.partitionsToRetry.isEmpty()) &#123; subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs); metadata.requestUpdate(); &#125; for (Map.Entry&lt;TopicPartition, ListOffsetData&gt; fetchedOffset : result.fetchedOffsets.entrySet()) &#123; TopicPartition partition = fetchedOffset.getKey(); ListOffsetData offsetData = fetchedOffset.getValue(); ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition); // 3.发送ListOffsetRequest请求成功，对结果reset，如果reset策略设置的是latest，那么requestedReset.timestamp = -1，如果是earliest，requestedReset.timestamp = -2 resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData); &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; subscriptions.requestFailed(resetTimestamps.keySet(), time.milliseconds() + retryBackoffMs); metadata.requestUpdate(); if (!(e instanceof RetriableException) &amp;&amp; !cachedListOffsetsException.compareAndSet(null, e)) log.error(&quot;Discarding error in ListOffsetResponse because another error is pending&quot;, e); &#125; &#125;); &#125;&#125; sendListOffsetRequest () 方法的核心代码如下： 1234567891011121314151617181920212223242526/** * Send the ListOffsetRequest to a specific broker for the partitions and target timestamps. * * @param node The node to send the ListOffsetRequest to. * @param timestampsToSearch The mapping from partitions to the target timestamps. * @param requireTimestamp True if we require a timestamp in the response. * @return A response which can be polled to obtain the corresponding timestamps and offsets. */private RequestFuture&lt;ListOffsetResult&gt; sendListOffsetRequest(final Node node, final Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; timestampsToSearch, boolean requireTimestamp) &#123; ListOffsetRequest.Builder builder = ListOffsetRequest.Builder .forConsumer(requireTimestamp, isolationLevel) .setTargetTimes(timestampsToSearch); log.debug(&quot;Sending ListOffsetRequest &#123;&#125; to broker &#123;&#125;&quot;, builder, node); return client.send(node, builder) .compose(new RequestFutureAdapter&lt;ClientResponse, ListOffsetResult&gt;() &#123; @Override public void onSuccess(ClientResponse response, RequestFuture&lt;ListOffsetResult&gt; future) &#123; ListOffsetResponse lor = (ListOffsetResponse) response.responseBody(); log.trace(&quot;Received ListOffsetResponse &#123;&#125; from broker &#123;&#125;&quot;, lor, node); handleListOffsetResponse(timestampsToSearch, lor, future); &#125; &#125;);&#125; resetOffsetIfNeeded () 方法的核心代码如下： 1234567private void resetOffsetIfNeeded(TopicPartition partition, OffsetResetStrategy requestedResetStrategy, ListOffsetData offsetData) &#123; SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetData.offset, offsetData.leaderEpoch, metadata.leaderAndEpoch(partition)); offsetData.leaderEpoch.ifPresent(epoch -&gt; metadata.updateLastSeenEpochIfNewer(partition, epoch)); // reset对应的TopicPartition fetch的position subscriptions.maybeSeekUnvalidated(partition, position.offset, requestedResetStrategy);&#125; 这里解释下 auto.offset.reset 的两个值 (latest 和 earliest) 的区别： 假设我们现在要消费 MyConsumerTopic 的数据，它有 3 个分区，生产者往这个 topic 发送了 10 条数据，然后分区数据按照 MyConsumerTopic-0 (3 条数据)，MyConsumerTopic-1 (3 条数据)，MyConsumerTopic-2 (4 条数据) 这样分配。 当设置为 latest 的时候，返回的 offset 具体到每个 partition 就是 HW 值 (partition 0 是 3，partition 1 是 3，partition 2 是 4)。 当设置为 earliest 的时候，就会从起始处 (即 LogStartOffset，注意不是 LSO) 开始消费，这里就是从 0 开始。 Log Start Offset：表示 partition 的起始位置，初始值为 0，由于消息的增加以及日志清除策略影响，这个值会阶段性增大。尤其注意这个不能缩写为 LSO，LSO 代表的是 LastStableOffset，和事务有关。 Consumer Offset：消费位移，表示 partition 的某个消费者消费到的位移位置。 High Watermark：简称 HW，代表消费端能看到的 partition 的最高日志位移，HW 大于等于 ConsumerOffset 的值。 Log End Offset：简称 LEO，代表 partition 的最高日志位移，对消费者不可见，HW 到 LEO 这之间的数据未被 follwer 完全同步。 至此，我们成功的知道 consumer 消费的 partition 的 offset 位置在哪里，下面就开始拉取 partition 里的数据。 pollForFetches - 拉取数据现在万事俱备只欠东风了，consumer 成功加入 group，也确定了需要拉取的 topic partition 的 offset，那么现在就应该去拉取数据了，其核心源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) &#123; long pollTimeout = coordinator == null ? timer.remainingMs() : Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs()); // if data is available already, return it immediately // 1.获取fetcher已经拉取到的数据 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords(); if (!records.isEmpty()) &#123; return records; &#125; // 到此，说明上次fetch到的数据已经全部拉取了，需要再次发送fetch请求，从broker拉取新的数据 // send any new fetches (won&#x27;t resend pending fetches) // 2.发送fetch请求，会从多个topic-partition拉取数据(只要对应的topic-partition没有未完成的请求) fetcher.sendFetches(); // We do not want to be stuck blocking in poll if we are missing some positions // since the offset lookup may be backing off after a failure // NOTE: the use of cachedSubscriptionHashAllFetchPositions means we MUST call // updateAssignmentMetadataIfNeeded before this method. if (!cachedSubscriptionHashAllFetchPositions &amp;&amp; pollTimeout &gt; retryBackoffMs) &#123; pollTimeout = retryBackoffMs; &#125; Timer pollTimer = time.timer(pollTimeout); // 3.真正开始发送，底层同样使用NIO client.poll(pollTimer, () -&gt; &#123; // since a fetch might be completed by the background thread, we need this poll condition // to ensure that we do not block unnecessarily in poll() return !fetcher.hasCompletedFetches(); &#125;); timer.update(pollTimer.currentTimeMs()); // after the long poll, we should check whether the group needs to rebalance // prior to returning data so that the group can stabilize faster // 4.如果group需要rebalance，直接返回空数据，这样更快地让group进入稳定状态 if (coordinator != null &amp;&amp; coordinator.rejoinNeededOrPending()) &#123; return Collections.emptyMap(); &#125; // 5.返回拉取到的新数据 return fetcher.fetchedRecords();&#125; fetcher.sendFetches这里需要注意的是 fetcher.sendFetches () 方法，在发送请求的同时会注册回调函数，当有 response 的时候，会解析 response，将返回的数据放到 Fetcher 的成员变量中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/** * Set-up a fetch request for any node that we have assigned partitions for which doesn&#x27;t already have * an in-flight fetch or pending fetch data. * @return number of fetches sent */public synchronized int sendFetches() &#123; // Update metrics in case there was an assignment change sensors.maybeUpdateAssignment(subscriptions); // 1.创建FetchRequest Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; fetchRequestMap = prepareFetchRequests(); for (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData&gt; entry : fetchRequestMap.entrySet()) &#123; final Node fetchTarget = entry.getKey(); final FetchSessionHandler.FetchRequestData data = entry.getValue(); final FetchRequest.Builder request = FetchRequest.Builder .forConsumer(this.maxWaitMs, this.minBytes, data.toSend()) .isolationLevel(isolationLevel) .setMaxBytes(this.maxBytes) .metadata(data.metadata()) .toForget(data.toForget()) .rackId(clientRackId); if (log.isDebugEnabled()) &#123; log.debug(&quot;Sending &#123;&#125; &#123;&#125; to broker &#123;&#125;&quot;, isolationLevel, data.toString(), fetchTarget); &#125; // 2.发送FetchRequest RequestFuture&lt;ClientResponse&gt; future = client.send(fetchTarget, request); // We add the node to the set of nodes with pending fetch requests before adding the // listener because the future may have been fulfilled on another thread (e.g. during a // disconnection being handled by the heartbeat thread) which will mean the listener // will be invoked synchronously. this.nodesWithPendingFetchRequests.add(entry.getKey().id()); future.addListener(new RequestFutureListener&lt;ClientResponse&gt;() &#123; @Override public void onSuccess(ClientResponse resp) &#123; synchronized (Fetcher.this) &#123; try &#123; @SuppressWarnings(&quot;unchecked&quot;) FetchResponse&lt;Records&gt; response = (FetchResponse&lt;Records&gt;) resp.responseBody(); FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler == null) &#123; log.error(&quot;Unable to find FetchSessionHandler for node &#123;&#125;. Ignoring fetch response.&quot;, fetchTarget.id()); return; &#125; if (!handler.handleResponse(response)) &#123; return; &#125; Set&lt;TopicPartition&gt; partitions = new HashSet&lt;&gt;(response.responseData().keySet()); FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions); for (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&lt;Records&gt;&gt; entry : response.responseData().entrySet()) &#123; TopicPartition partition = entry.getKey(); FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition); if (requestData == null) &#123; String message; if (data.metadata().isFull()) &#123; message = MessageFormatter.arrayFormat( &quot;Response for missing full request partition: partition=&#123;&#125;; metadata=&#123;&#125;&quot;, new Object[]&#123;partition, data.metadata()&#125;).getMessage(); &#125; else &#123; message = MessageFormatter.arrayFormat( &quot;Response for missing session request partition: partition=&#123;&#125;; metadata=&#123;&#125;; toSend=&#123;&#125;; toForget=&#123;&#125;&quot;, new Object[]&#123;partition, data.metadata(), data.toSend(), data.toForget()&#125;).getMessage(); &#125; // Received fetch response for missing session partition throw new IllegalStateException(message); &#125; else &#123; long fetchOffset = requestData.fetchOffset; FetchResponse.PartitionData&lt;Records&gt; fetchData = entry.getValue(); log.debug(&quot;Fetch &#123;&#125; at offset &#123;&#125; for partition &#123;&#125; returned fetch data &#123;&#125;&quot;, isolationLevel, fetchOffset, partition, fetchData); // 3.发送FetchRequest请求成功，将返回的数据放到ConcurrentLinkedQueue&lt;CompletedFetch&gt;中 completedFetches.add(new CompletedFetch(partition, fetchOffset, fetchData, metricAggregator, resp.requestHeader().apiVersion())); &#125; &#125; sensors.fetchLatency.record(resp.requestLatencyMs()); &#125; finally &#123; nodesWithPendingFetchRequests.remove(fetchTarget.id()); &#125; &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; synchronized (Fetcher.this) &#123; try &#123; FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler != null) &#123; handler.handleError(e); &#125; &#125; finally &#123; nodesWithPendingFetchRequests.remove(fetchTarget.id()); &#125; &#125; &#125; &#125;); &#125; return fetchRequestMap.size();&#125; 该方法主要分为以下两步： prepareFetchRequests ()：为订阅的所有 topic-partition list 创建 fetch 请求 (只要该 topic-partition 没有还在处理的请求)，创建的 fetch 请求依然是按照 node 级别创建的； client.send ()：发送 fetch 请求，并设置相应的 Listener，请求处理成功的话，就加入到 completedFetches 中，在加入这个 completedFetches 队列时，是按照 topic-partition 级别去加入，这样也就方便了后续的处理。 从这里可以看出，在每次发送 fetch 请求时，都会向所有可发送的 topic-partition 发送 fetch 请求，调用一次 fetcher.sendFetches，拉取到的数据，可能需要多次 pollForFetches 循环才能处理完，因为 Fetcher 线程是在后台运行，这也保证了尽可能少地阻塞用户的处理线程，因为如果 Fetcher 中没有可处理的数据，用户的线程是会阻塞在 poll 方法中的。 fetcher.fetchedRecords这个方法的作用就是获取已经从 server 拉取到的 Records，其核心源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117/** * Return the fetched records, empty the record buffer and update the consumed position. * * NOTE: returning empty records guarantees the consumed position are NOT updated. * * @return The fetched records per partition * @throws OffsetOutOfRangeException If there is OffsetOutOfRange error in fetchResponse and * the defaultResetPolicy is NONE * @throws TopicAuthorizationException If there is TopicAuthorization error in fetchResponse. */public Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords() &#123; Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetched = new HashMap&lt;&gt;(); // 在max.poll.records中设置单词最大的拉取条数，默认500条 int recordsRemaining = maxPollRecords; try &#123; while (recordsRemaining &gt; 0) &#123; if (nextInLineRecords == null || nextInLineRecords.isFetched) &#123;// nextInLineRecords为空时 // Step1:当一个nextInLineRecords处理完，就从completedFetches处理下一个完成的Fetch请求 CompletedFetch completedFetch = completedFetches.peek(); if (completedFetch == null) break; try &#123; // Step2:获取下一个要处理的nextInLineRecords nextInLineRecords = parseCompletedFetch(completedFetch); &#125; catch (Exception e) &#123; // Remove a completedFetch upon a parse with exception if (1) it contains no records, and // (2) there are no fetched records with actual content preceding this exception. // The first condition ensures that the completedFetches is not stuck with the same completedFetch // in cases such as the TopicAuthorizationException, and the second condition ensures that no // potential data loss due to an exception in a following record. FetchResponse.PartitionData partition = completedFetch.partitionData; if (fetched.isEmpty() &amp;&amp; (partition.records == null || partition.records.sizeInBytes() == 0)) &#123; completedFetches.poll(); &#125; throw e; &#125; completedFetches.poll(); &#125; else &#123; // Step3:拉取records，更新position List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = fetchRecords(nextInLineRecords, recordsRemaining); TopicPartition partition = nextInLineRecords.partition; if (!records.isEmpty()) &#123; List&lt;ConsumerRecord&lt;K, V&gt;&gt; currentRecords = fetched.get(partition); if (currentRecords == null) &#123;// 正常情况下，一个node只会发送一个request，一般只会有一个 fetched.put(partition, records); &#125; else &#123; // this case shouldn&#x27;t usually happen because we only send one fetch at a time per partition, // but it might conceivably happen in some rare cases (such as partition leader changes). // we have to copy to a new list because the old one may be immutable List&lt;ConsumerRecord&lt;K, V&gt;&gt; newRecords = new ArrayList&lt;&gt;(records.size() + currentRecords.size()); newRecords.addAll(currentRecords); newRecords.addAll(records); fetched.put(partition, newRecords); &#125; recordsRemaining -= records.size(); &#125; &#125; &#125; &#125; catch (KafkaException e) &#123; if (fetched.isEmpty()) throw e; &#125; // Step4:返回相应的Records数据 return fetched;&#125;private List&lt;ConsumerRecord&lt;K, V&gt;&gt; fetchRecords(PartitionRecords partitionRecords, int maxRecords) &#123; if (!subscriptions.isAssigned(partitionRecords.partition)) &#123; // this can happen when a rebalance happened before fetched records are returned to the consumer&#x27;s poll call log.debug(&quot;Not returning fetched records for partition &#123;&#125; since it is no longer assigned&quot;, partitionRecords.partition); &#125; else if (!subscriptions.isFetchable(partitionRecords.partition)) &#123; // this can happen when a partition is paused before fetched records are returned to the consumer&#x27;s // poll call or if the offset is being reset // 这个topic-partition不能被消费了，比如调用了pause log.debug(&quot;Not returning fetched records for assigned partition &#123;&#125; since it is no longer fetchable&quot;, partitionRecords.partition); &#125; else &#123; SubscriptionState.FetchPosition position = subscriptions.position(partitionRecords.partition); if (partitionRecords.nextFetchOffset == position.offset) &#123;// offset对的上，也就是拉取是按顺序拉的 // 获取该topic-partition对应的records，并更新partitionRecords的fetchOffset(用于判断是否顺序) List&lt;ConsumerRecord&lt;K, V&gt;&gt; partRecords = partitionRecords.fetchRecords(maxRecords); if (partitionRecords.nextFetchOffset &gt; position.offset) &#123; SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition( partitionRecords.nextFetchOffset, partitionRecords.lastEpoch, position.currentLeader); log.trace(&quot;Returning fetched records at offset &#123;&#125; for assigned partition &#123;&#125; and update &quot; + &quot;position to &#123;&#125;&quot;, position, partitionRecords.partition, nextPosition); // 更新消费到的offset(the fetch position) subscriptions.position(partitionRecords.partition, nextPosition); &#125; // 获取Lag(即position与hw之间差值)，hw为null时，才返回null Long partitionLag = subscriptions.partitionLag(partitionRecords.partition, isolationLevel); if (partitionLag != null) this.sensors.recordPartitionLag(partitionRecords.partition, partitionLag); Long lead = subscriptions.partitionLead(partitionRecords.partition); if (lead != null) &#123; this.sensors.recordPartitionLead(partitionRecords.partition, lead); &#125; return partRecords; &#125; else &#123; // these records aren&#x27;t next in line based on the last consumed position, ignore them // they must be from an obsolete request log.debug(&quot;Ignoring fetched records for &#123;&#125; at offset &#123;&#125; since the current position is &#123;&#125;&quot;, partitionRecords.partition, partitionRecords.nextFetchOffset, position); &#125; &#125; partitionRecords.drain(); return emptyList();&#125; consumer 的 Fetcher 处理从 server 获取的 fetch response 大致分为以下几个过程： 通过 completedFetches.peek() 获取已经成功的 fetch response (在 fetcher.sendFetches () 方法中会把发送FetchRequest请求成功后的结果放在这个集合中，是拆分为 topic-partition 的粒度放进去的)； parseCompletedFetch() 处理上面获取的 completedFetch，构造成 PartitionRecords 类型； 通过 fetchRecords() 方法处理 PartitionRecords 对象，在这个里面会去验证 fetchOffset 是否能对得上，只有 fetchOffset 是一致的情况下才会去处理相应的数据，并更新 the fetch offset 的信息，如果 fetchOffset 不一致，这里就不会处理，the fetch offset 就不会更新，下次 fetch 请求时是会接着 the fetch offset 的位置去请求相应的数据； 返回相应的 Records 数据。 至此，KafkaConsumer 如何拉取消息的整体流程也分析完毕。 本文参考http://generalthink.github.io/2019/05/31/kafka-consumer-offset/ https://matt33.com/2017/11/11/consumer-pollonce/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"欧阳修","slug":"ouyangxiu","date":"2020-11-10T00:51:06.000Z","updated":"2020-11-23T08:48:11.442Z","comments":true,"path":"2020/11/10/ouyangxiu/","link":"","permalink":"http://example.com/2020/11/10/ouyangxiu/","excerpt":"","text":"伐树记署之东园，久茀不治。修至始辟之，粪瘠溉枯，为蔬圃十数畦，又植花果桐竹凡百本。 春阳既浮，萌者将动。园之守启曰：“园有樗焉，其根壮而叶大。根壮则梗地脉，耗阳气，而新植者不得滋；叶大则阴翳蒙碍，而新植者不得畅以茂。又其材拳曲臃肿，疏轻而不坚，不足养，是宜伐。”因尽薪之。明日，圃之守又曰：“圃之南有杏焉，凡其根庇之广可六七尺，其下之地最壤腴，以杏故，特不得蔬，是亦宜薪。”修曰：“噫！今杏方春且华，将待其实，若独不能损数畦之广为杏地邪？“因勿伐。 既而悟且叹曰：“吁！庄周之说曰：樗、栎以不材终其天年，桂、漆以有用而见伤夭。今樗诚不材矣，然一旦悉翦弃；杏之体最坚密，美泽可用，反见存。岂才不才各遭其时之可否邪？” 他日，客有过修者。仆夫曳薪过堂下，因指而语客以所疑。客曰： “是何怪邪？夫以无用处无用，庄周之贵也。以无用而贼有用，乌能免哉！彼杏之有华实也，以有生之具而庇其根，幸矣。若桂、漆之不能逃乎斤斧者，盖有利之者在死，势不得以生也，与乎杏实异矣。今樗之臃肿不材，而以壮大害物，其见伐，诚宜尔。与夫‘才者死、不才者生’之说，又异矣。凡物幸之与不幸，视其处之而已。”客既去，修善其言而记之。 非非堂记权衡之平物，动则轻重差，其于静也，锱铢不失。水之鉴物，动则不能有睹，其于静也，毫发可辨。在乎人，耳司听，目司视，动则乱于聪明，其于静也，闻见必审。处身者不为外物眩晃而动，则其心静，心静则智识明，是是非非，无所施而不中。夫是是近乎谄，非非近乎讪，不幸而过，宁讪无谄。是者，君子之常，是之何加？一以视之，未若非非之为正也。 予居洛之明年，既新厅事，有文纪于壁末。营其西偏作堂，户北向，植丛竹，辟户于其南，纳日月之光。设一几一榻，架书数百卷，朝夕居其中。以其静也，闭目澄心，览今照古，思虑无所不至焉。故其堂以非非为名云。 浪淘沙 · 把酒祝东风把酒祝东风，且共从容。垂杨紫陌洛城东。总是当时携手处，游遍芳丛。 聚散苦匆匆，此恨无穷。今年花胜去年红。可惜明年花更好，知与谁同？ 玉楼春 · 尊前拟把归期说尊前拟把归期说，欲语春容先惨咽。人生自是有情痴，此恨不关风与月。 离歌且莫翻新阕，一曲能教肠寸结。直须看尽洛城花，始共春风容易别。 生查子 · 元夕去年元夜时，花市灯如昼。 月上柳梢头，人约黄昏后。 今年元夜时，月与灯依旧。 不见去年人，泪满春衫袖。 读李翱文予始读翱《复性书》三篇，曰：此《中庸》之义疏尔。智者诚其性，当读《中庸》；愚者虽读此不晓也，不作可焉。又读《与韩侍郎荐贤书》，以谓翱特穷时愤世无荐己者，故丁宁如此；使其得志，亦未必。然以韩为秦汉间好侠行义之一豪俊，亦善论人者也。最后读《幽怀赋》，然后置书而叹，叹已复读，不自休。恨翱不生于今，不得与之交；又恨予不得生翱时，与翱上下其论也。 凡昔翱一时人，有道而能文者，莫若韩愈。愈尝有赋矣，不过羡二鸟之光荣，叹一饱之无时尔。此其心使光荣而饱，则不复云矣。若翱独不然，其赋曰：“众嚣嚣而杂处兮，咸叹老而嗟卑；视予心之不然兮，虑行道之犹非。”又怪神尧以一旅取天下，后世子孙不能以天下取河北，以为忧。呜呼！使当时君子皆易其叹老嗟卑之心为翱所忧之心，则唐之天下岂有乱与亡哉！ 然翱幸不生今时，见今之事，则其忧又甚矣。奈何今之人不忧也？余行天下，见人多矣，脱有一人能如翱忧者，又皆贱远，与翱无异；其余光荣而饱者，一闻忧世之言，不以为狂人，则以为病痴子，不怒则笑之矣。呜呼，在位而不肯自忧，又禁他人使皆不得忧，可叹也夫! 景祐三年十月十七日，欧阳修书。 答吴充秀才书修顿首白，先辈吴君足下。前辱示书及文三篇，发而读之，浩乎若千万言之多，及少定而视焉，才数百言尔。非夫辞丰意雄，沛然有不可御之势，何以至此！然犹自患伥伥莫有开之使前者，此好学之谦言也。 修材不足用于时，仕不足荣于世，其毁誉不足轻重，气力不足动人。世之欲假誉以为重，借力而后进者，奚取于修焉？先辈学精文雄，其施于时，又非待修誉而为重，力而后进者也。然而惠然见临，若有所责，得非急于谋道，不择其人而问焉者欤？ 夫学者未始不为道，而至者鲜焉；非道之于人远也，学者有所溺焉尔。盖文之为言，难工而可喜，易悦而自足。世之学者往往溺之，一有工焉，则曰：“吾学足矣。“甚者至弃百事不关于心，曰：“吾文士也，职于文而已。”此其所以至之鲜也。 昔孔子老而归鲁，六经之作，数年之顷尔。然读《易》者如无《春秋》，读《书》者如无《诗》，何其用功少而至于至也！圣人之文虽不可及，然大抵道胜者，文不难而自至也。故孟子皇皇不暇著书，荀卿盖亦晚而有作。若子云、仲淹，方勉焉以模言语，此道未足而强言者也。后之惑者，徒见前世之文传，以为学者文而已，故愈力愈勤而愈不至。此足下所谓”终日不出于轩序，不能纵横高下皆如意“者，道未足也。若道之充焉，虽行乎天地，入于渊泉，无不之也。 先辈之文浩乎沛然，可谓善矣。而又志于为道，犹自以为未广。若不止焉，孟、荀可至而不难也。修，学道而不至者，然幸不甘于所悦而溺于所止。因吾子之能不自止，又以励修之少进焉。幸甚！幸甚！修白。 答祖择之书修启。秀才人至，蒙示书一通，并诗赋杂文两策，谕之曰：“一览以为如何？”某既陋，不足以辱好学者之问；又其少贱而长穷，其素所为未有足称以取信于人。亦尝有人问者，以不足问之愚，而未尝答人之问。足下卒然及之，是以愧惧不知所言。虽然，不远数百里走使者以及门，意厚礼勤，何敢不报。 某闻古之学者必严其师，师严然后道尊，道尊然后笃敬，笃敬然后能自守，能自守然后果于用，果于用然后不畏而不迁。三代之衰，学校废。至两汉，师道尚存，故其学者各守其经以自用。是以汉之政理文章与其当时之事，后世莫及者，其所从来深矣。后世师，法渐坏，而今世无师，则学者不尊严，故自轻其道。轻之则不能至，不至则不能笃信，信不笃则不知所守，守不固则有所畏而物可移。是故学者惟俯仰徇时，以希禄利为急，至于忘本趋末，流而不返。夫以不信不固之心，守不至之学，虽欲果于自用，而莫知其所以用之之道，又况有禄利之诱、刑祸之惧以迁之哉！此足下所谓志古知道之士世所鲜，而未有合者，由此也。 足下所为文，用意甚高，卓然有不顾世俗之心，直欲自到于古人。今世之人用心如足下者有几？是则乡曲之中能为足下之师者谓谁，交游之间能发足下之议论者谓谁？学不师则守不一，议论不博则无所发明而究其深。足下之言高趣远，甚善，然所守未一而议论未精，此其病也。窃惟足下之交游能为足下称才誉美者不少，今皆舍之，远而见及，乃知足下是欲求其不至。此古君子之用心也，是以言之不敢隐。 夫世无师矣，学者当师经，师经必先求其意，意得则心定，心定则道纯，道纯则充于中者实，中充实则发为文者辉光，施于世者果致。三代、两汉之学，不过此也。足下患世未有合者，而不弃其愚，将某以为合，故敢道此。未知足下之意合否？ 与荆南乐秀才书修顿首白秀才足下。前者舟行往来，屡辱见过。又辱以所业一编，先之启事，及门而贽。田秀才西来，辱书；其后予家奴自府还县，比又辱书。仆有罪之人，人所共弃，而足下见礼如此，何以当之？当之未暇答，宜遂绝，而再辱书；再而未答，益宜绝，而又辱之。何其勤之甚也！如修者，天下穷贱之人尔，安能使足下之切切如是邪？盖足下力学好问，急于自为谋而然也。然蒙索仆所为文字者，此似有所过听也。 仆少从进士举于有司，学为诗赋，以备程试，凡三举而得第。与士君子相识者多，故往往能道仆名字，而又以游从相爱之私，或过称其文字。故使足下闻仆虚名，而欲见其所为者，由此也。 仆少孤贫，贪禄仕以养亲，不暇就师穷经，以学圣人之遗业。而涉猎书史，姑随世俗作所谓时文者，皆穿蠹经传，移此俪彼，以为浮薄，惟恐不悦于时人，非有卓然自立之言如古人者。然有司过采，屡以先多士。及得第已来，自以前所为不足以称有司之举而当长者之知，始大改其为，庶几有立。然言出而罪至，学成而身辱，为彼则获誉，为此则受祸，此明效也。 夫时文虽曰浮巧，然其为功，亦不易也。仆天姿不好而强为之，故比时人之为者尤不工，然已足以取禄仕而窃名誉者，顺时故也。先辈少年志盛，方欲取荣誉于世，则莫若顺时。天圣中，天子下诏书，敕学者去浮华，其后风俗大变。今时之士大夫所为，彬彬有两汉之风矣。先辈往学之，非徒足以顺时取誉而已，如其至之，是直齐肩于两汉之士也。若仆者，其前所为既不足学，其后所为慎不可学，是以徘徊不敢出其所为者，为此也。 在《易》之《困》曰：“有言不信。”谓夫人方困时，其言不为人所信也。今可谓困矣，安足为足下所取信哉？辱书既多且切，不敢不答。幸察。 答李诩第一书修白。人至，辱书及《性诠》三篇，曰以质其果是。夫自信笃者，无所待于人；有质于人者，自疑者也。今吾子自谓“夫子与孟、荀、扬、韩复生，不能夺吾言”，其可谓自信不疑者矣。而返以质于修。使修有过于夫子者，乃可为吾子辩，况修未及孟、荀、扬、韩之一二也。修非知道者，好学而未至者也。世无师久矣，尚赖朋友切磋之益，苟不自满而中止，庶几终身而有成。固常乐与学者论议往来，非敢以益于人，盖求益于人者也。况如吾子之文章论议，岂易得哉？固乐为吾子辩也。苟尚有所疑，敢不尽其所学以告，既吾子自信如是，虽夫子不能夺，使修何所说焉？人还索书，未知所答，惭惕惭惕。修再拜。 答李诩第二书修白。前辱示书及《性诠》三篇，见吾子好学善辩，而文能尽其意之详。令世之言性者多矣，有所不及也，故思与吾子卒其说。 修患世之学者多言性，故常为说曰“夫性，非学者之所急，而圣人之所罕言也。《易》六十四卦不言性，其言者动静得失吉凶之常理也；《春秋》二百四十二年不言性，其言者善恶是非之实录也；《诗》三百五篇不言性，其言者政教兴衰之美刺也；《书》五十九篇不言性，其言者尧、舜、三代之治乱也；《礼》、《乐》之书虽不完，而杂出于诸儒之记，然其大要，治国修身之法也。六经之所载，皆人事之切于世者，是以言之甚详。至于性也，百不一二言之，或因言而及焉，非为性而言也，故虽言而不究。 予之所谓不言者，非谓绝而无言，盖其言者鲜，而又不主于性而言也。《论语》所载七十二子之问于孔子者，问孝、问忠、问仁义、问礼乐、问修身、问为政、问朋友、问鬼神者有矣，未尝有问性者。孔子之告其弟子者，凡数千言，其及于性者一言而已。予故曰：非学者之所急，而圣人之罕言也。 《书》曰“习与性成”，《语》曰“性相近，习相远”者，戒人慎所习而言也。《中庸》曰“天命之谓性，率性之谓道”者，明性无常，必有以率之也。《乐记》亦曰“感物而动，性之欲”者，明物之感人无不至也。然终不言性果善果恶，但戒人慎所习与所感，而勤其所以率之者尔。予故曰“因言以及之，而不究也。 修少好学，知学之难。凡所谓六经之所载，七十二子之所问者，学之终身，有不能达者矣；于其所达，行之终身，有不能至者矣。以予之汲汲于此而不暇乎其他，因以知七十二子亦以是汲汲而不暇也，又以知圣人所以教人垂世，亦皇皇而不暇也。今之学者于古圣贤所皇皇汲汲者，学之行之，或未至其一二，而好为性说，以穷圣贤之所罕言而不究者，执后儒之偏说，事无用之空言，此予之所不暇也。 或有问曰：性果不足学乎？予曰：性者，与身俱生而人之所皆有也。为君子者，修身治人而已，性之善恶不必究也。使性果善邪，身不可以不修，人不可以不治；使性果恶邪，身不可以不修，人不可以不治。不修其身，虽君子而为小人，《书》曰“惟圣罔念作狂”是也；能修其身，虽小人而为君子，《书》曰“惟狂克念作圣”是也。治道备，人斯为善矣，《书》曰“黎民于变时雍”是也；治道失，人斯为恶矣，《书》曰“殷顽民”，又曰“旧染污俗”是也。故为君子者，以修身治人为急，而不穷性以为言。夫七十二子之不问，六经之不主言，或虽言而不究，岂略之哉，盖有意也。 或又问曰：然则三子言性，过欤？曰：不过也。其不同何也？曰：始异而终同也。使孟子曰人性善矣，遂怠而不教，则是过也；使荀子曰人性恶矣，遂弃而不教，则是过也；使扬子曰人性混矣，遂肆而不教，则是过也。然三子者，或身奔走诸侯以行其道，或著书累千万言以告于后世，未尝不区区以仁义礼乐为急。盖其意以谓善者一日不教，则失而入于恶；恶者勤而教之，则可使至于善；混者驱而率之，则可使去恶而就善也。其说与《书》之“习与性成”，《语》之“性近习远”，《中庸》之“有以率之”，《乐记》之“慎物所感”皆合。夫三子者，推其言则殊，察其用心则一，故予以为推其言不过始异而终同也。凡论三子者，以予言而一之，则譊譊者可以息矣。 予之所说如此，吾子其择焉。 醉翁亭记环滁皆山也。其西南诸峰，林壑尤美，望之蔚然而深秀者，琅琊也。山行六七里，渐闻水声潺潺，而泻出于两峰之间者，酿泉也。峰回路转，有亭翼然临于泉上者，醉翁亭也。作亭者谁？山之僧智仙也。名之者谁？太守自谓也。太守与客来饮于此，饮少辄醉，而年又最高，故自号曰醉翁也。醉翁之意不在酒，在乎山水之间也。山水之乐，得之心而寓之酒也。 若夫日出而林霏开，云归而岩穴暝，晦明变化者，山间之朝暮也。野芳发而幽香，佳木秀而繁阴，风霜高洁，水落而石出者，山间之四时也。朝而往，暮而归，四时之景不同，而乐亦无穷也。 至于负者歌于途，行者休于树，前者呼，后者应，伛偻提携，往来而不绝者，滁人游也。临溪而渔，溪深而鱼肥。酿泉为酒，泉香而酒洌；山肴野蔌，杂然而前陈者，太守宴也。宴酣之乐，非丝非竹，射者中，弈者胜，觥筹交错，起坐而喧哗者，众宾欢也。苍颜白发，颓然乎其间者，太守醉也。 已而夕阳在山，人影散乱，太守归而宾客从也。树林阴翳，鸣声上下，游人去而禽鸟乐也。然而禽鸟知山林之乐，而不知人之乐；人知从太守游而乐，而不知太守之乐其乐也。醉能同其乐，醒能述以文者，太守也。太守谓谁？庐陵欧阳修也。 朝中措 · 送刘仲原甫出守维扬平山阑槛倚晴空，山色有无中。手种堂前垂柳，别来几度春风？ 文章太守，挥毫万字，一饮千钟。行乐直须年少，尊前看取衰翁。 夜行船 · 忆昔西都欢纵忆昔西都欢纵。自别后、有谁能共。伊川山水洛川花，细寻思、旧游如梦。 今日相逢情愈重。愁闻唱、画楼钟动。白发天涯逢此景，倒金尊、殢谁相送。 伶官传序呜呼！盛衰之理，虽曰天命，岂非人事哉！原庄宗之所以得天下，与其所以失之者，可以知之矣。 世言晋王之将终也，以三矢赐庄宗而告之曰：“梁，吾仇也；燕王，吾所立，契丹，与吾约为兄弟，而皆背晋以归梁。此三者，吾遗恨也。与尔三矢，尔其无忘乃父之志！”庄宗受而藏之于庙。其后用兵，则遣从事以一少牢告庙，请其矢，盛以锦囊，负而前驱，及凯旋而纳之。 方其系燕父子以组，函梁君臣之首，入于太庙，还矢先王，而告以成功，其意气之盛，可谓壮哉！及仇雠已灭，天下已定，一夫夜呼，乱者四应，仓皇东出，未及见贼而士卒离散，君臣相顾，不知所归。至于誓天断发，泣下沾襟，何其衰也！岂得之难而失之易欤？抑本其成败之迹，而皆自于人欤？ 《书》曰：“满招损，谦得益。”忧劳可以兴国，逸豫可以亡身，自然之理也。故方其盛也，举天下之豪杰莫能与之争；及其衰也，数十伶人困之，而身死国灭，为天下笑。夫祸患常积于忽微，而智勇多困于所溺，岂独伶人也哉！作《伶官传》。","categories":[],"tags":[{"name":"Poetry and Prose","slug":"Poetry-and-Prose","permalink":"http://example.com/tags/Poetry-and-Prose/"}]},{"title":"KafkaConsumer 源码之 consumer 如何加入 consumer group","slug":"kafka-consumer-group","date":"2020-11-04T01:35:04.000Z","updated":"2021-01-05T07:32:45.777Z","comments":true,"path":"2020/11/04/kafka-consumer-group/","link":"","permalink":"http://example.com/2020/11/04/kafka-consumer-group/","excerpt":"","text":"Kafka 的 consumer 比 producer 要复杂许多，producer 没有 group 的概念，也不需要关注 offset，而 consumer 不一样，它有组织 (consumer group)，有纪律 (offset)。这些对 consumer 的要求就会很高，这篇文章就主要介绍 consumer 是如何加入 consumer group 的。 在这之前，我们需要先了解一下什么是 GroupCoordinator。简单地说，GroupCoordinator 是运行在服务器上的一个服务，Kafka 集群上的每一个 broker 节点启动的时候，都会启动一个 GroupCoordinator 服务，其功能是负责进行 consumer 的 group 成员与 offset 管理 (但每个 GroupCoordinator 只是管理一部分的 consumer group member 和 offset 信息)。 consumer group 对应的 GroupCoordinator 节点的确定，会通过如下方式： 将 consumer group 的 group.id 进行 hash，把得到的值的绝对值，对 _consumer_offsets 的 partition 总数取余，然后得到其对应的 partition 值，该 partition 的 leader 所在的 broker 即为该 consumer group 所对应的 GroupCoordinator 节点，GroupCoordinator 会存储与该 consumer group 相关的所有的 Meta 信息。 1._consumer_offsets 这个 topic 是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有 50 个 partition，每个 partition 默认有三个副本。 2.partition 计算方式：abs(GroupId.hashCode()) % NumPartitions，其中，NumPartitions 是 _consumer_offsets 的 partition 数，默认是 50 个。 3.比如，现在通过计算 abs(GroupId.hashCode()) % NumPartitions 的值为 35，然后就找第 35 个 partition 的 leader 在哪个 broker 上 (假设在 192.168.1.12)，那么 GroupCoordinator 节点就在这个 broker 上。 同时，这个 consumer group 所提交的消费 offset 信息也会发送给这个 partition 的 leader 所对应的 broker 节点，因此，这个节点不仅是 GroupCoordinator，而且还保存分区分配方案和组内消费者 offset 信息。 更多关于 GroupCoordinator 的解析，参考：Kafka 源码解析之 GroupCoordinator 详解。 KafkaConsumer 消费消息的主体流程接下来，我们回顾下 KafkaConsumer 消费消息的主体流程： 12345678// 创建消费者KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props);// 订阅主题kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;))// 从服务器拉取数据ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(100)); 创建 KafkaConsumer创建 KafkaConsumer 的时候，会创建一个 ConsumerCoordinator 服务，由它来负责和 GroupCoordinator 通信： 1234567891011121314151617181920// no coordinator will be constructed for the default (null) group idthis.coordinator = groupId == null ? null : new ConsumerCoordinator(logContext, this.client, groupId, this.groupInstanceId, maxPollIntervalMs, sessionTimeoutMs, new Heartbeat(time, sessionTimeoutMs, heartbeatIntervalMs, maxPollIntervalMs, retryBackoffMs), assignors, this.metadata, this.subscriptions, metrics, metricGrpPrefix, this.time, retryBackoffMs, enableAutoCommit, config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG), this.interceptors, config.getBoolean(ConsumerConfig.LEAVE_GROUP_ON_CLOSE_CONFIG)); 订阅 topicKafkaConsumer 订阅 topic 的方式有好几种，这在前面的文章有提到过。订阅的时候，会根据订阅的方式，设置其对应的订阅类型，默认存在四种订阅类型： 12345678910private enum SubscriptionType &#123; // 默认 NONE, // subscribe方式订阅 AUTO_TOPICS, // subscribe方式订阅 AUTO_PATTERN, // assign方式订阅 USER_ASSIGNED&#125; 比如，采用 kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;)) 方式订阅 topic 时，会将订阅类型设置为 SubscriptionType.AUTO_TOPICS，其核心代码如下： 12345678910111213141516171819202122232425/** * Subscribe to the given list of topics to get dynamically assigned partitions. * &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; It is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * This is a short-hand for &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;, which * uses a no-op listener. If you need the ability to seek to particular offsets, you should prefer * &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;, since group rebalances will cause partition offsets * to be reset. You should also provide your own listener if you are doing your own offset * management since the listener gives you an opportunity to commit offsets before a rebalance finishes. * * @param topics The list of topics to subscribe to * @throws IllegalArgumentException If topics is null or contains null or empty elements * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics) &#123; subscribe(topics, new NoOpConsumerRebalanceListener());&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Subscribe to the given list of topics to get dynamically * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * As part of group management, the consumer will keep track of the list of consumers that belong to a particular * group and will trigger a rebalance operation if any one of the following events are triggered: * &lt;ul&gt; * &lt;li&gt;Number of partitions change for any of the subscribed topics * &lt;li&gt;A subscribed topic is created or deleted * &lt;li&gt;An existing member of the consumer group is shutdown or fails * &lt;li&gt;A new member is added to the consumer group * &lt;/ul&gt; * &lt;p&gt; * When any of these events are triggered, the provided listener will be invoked first to indicate that * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received. * Note that rebalances will only occur during an active call to &#123;@link #poll(Duration)&#125;, so callbacks will * also only be invoked during that time. * * The provided listener will immediately override any listener set in a previous call to subscribe. * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics * subscribed in this call. See &#123;@link ConsumerRebalanceListener&#125; for more details. * * @param topics The list of topics to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; acquireAndEnsureOpen(); try &#123; maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot be null&quot;); if (topics.isEmpty()) &#123; // treat subscribing to empty topic list as the same as unsubscribing this.unsubscribe(); &#125; else &#123; for (String topic : topics) &#123; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;); &#125; throwIfNoAssignorsConfigured(); fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(&quot;Subscribed to topic(s): &#123;&#125;&quot;, Utils.join(topics, &quot;, &quot;)); if (this.subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; 12345public synchronized boolean subscribe(Set&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_TOPICS); return changeSubscription(topics);&#125; 123456789101112/** * This method sets the subscription type if it is not already set (i.e. when it is NONE), * or verifies that the subscription type is equal to the give type when it is set (i.e. * when it is not NONE) * @param type The given subscription type */private void setSubscriptionType(SubscriptionType type) &#123; if (this.subscriptionType == SubscriptionType.NONE) this.subscriptionType = type; else if (this.subscriptionType != type) throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);&#125; 从服务器拉取数据订阅完成后，就可以从服务器拉取数据了，应该注意的是，KafkaConsumer 没有后台线程默默的拉取数据，它的所有行为都集中在 poll () 方法中，KafkaConsumer 是线程不安全的，同时只能允许一个线程运行。 kafkaConsumer.poll () 方法的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445private ConsumerRecords&lt;K, V&gt; poll(final Timer timer, final boolean includeMetadataInTimeout) &#123; // Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭 acquireAndEnsureOpen(); try &#123; if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123; throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;); &#125; // poll for new data until the timeout expires do &#123; client.maybeTriggerWakeup(); if (includeMetadataInTimeout) &#123; // Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息 if (!updateAssignmentMetadataIfNeeded(timer)) &#123; return ConsumerRecords.empty(); &#125; &#125; else &#123; while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123; log.warn(&quot;Still waiting for metadata&quot;); &#125; &#125; // Step3:拉取数据 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer); if (!records.isEmpty()) &#123; // before returning the fetched records, we can send off the next round of fetches // and avoid block waiting for their responses to enable pipelining while the user // is handling the fetched records. // // NOTE: since the consumed position has already been updated, we must not allow // wakeups or any other errors to be triggered prior to returning the fetched records. if (fetcher.sendFetches() &gt; 0 || client.hasPendingRequests()) &#123; client.pollNoWakeup(); &#125; return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records)); &#125; &#125; while (timer.notExpired()); return ConsumerRecords.empty(); &#125; finally &#123; release(); &#125;&#125; 可以看出，在 Step 1 阶段， poll () 方法会先进行判定，如果有多个线程同时使用一个 KafkaConsumer 则会抛出异常： 1234567891011/** * Acquire the light lock and ensure that the consumer hasn&#x27;t been closed. * @throws IllegalStateException If the consumer has been closed */private void acquireAndEnsureOpen() &#123; acquire(); if (this.closed) &#123; release(); throw new IllegalStateException(&quot;This consumer has already been closed.&quot;); &#125;&#125; 123456789101112/** * Acquire the light lock protecting this consumer from multi-threaded access. Instead of blocking * when the lock is not available, however, we just throw an exception (since multi-threaded usage is not * supported). * @throws ConcurrentModificationException if another thread already has the lock */private void acquire() &#123; long threadId = Thread.currentThread().getId(); if (threadId != currentThread.get() &amp;&amp; !currentThread.compareAndSet(NO_CURRENT_THREAD, threadId)) throw new ConcurrentModificationException(&quot;KafkaConsumer is not safe for multi-threaded access&quot;); refcount.incrementAndGet();&#125; KafkaConsumer 如何加入 consumer group一个 KafkaConsumer 实例消费数据的前提是能够加入一个 consumer group 成功，并获取其要订阅的 tp（topic-partition）列表，因此首先要做的就是和 GroupCoordinator 建立连接，加入组织。 consumer 加入 group 的过程，也就是 reblance 的过程。如果出现了频繁 reblance 的问题，可能和 max.poll.interval.ms 和 max.poll.records 两个参数有关。 因此，我们先把目光集中在 ConsumerCoordinator 上，这个过程主要发生在 Step 2 阶段： 123456789101112/** * Visible for testing */boolean updateAssignmentMetadataIfNeeded(final Timer timer) &#123; // 1.本篇文章的内容主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group) if (coordinator != null &amp;&amp; !coordinator.poll(timer)) &#123; return false; &#125; // 2.updateFetchPositions(timer)方法留待下一篇文章分析(主要功能是:consumer获得partition的offset) return updateFetchPositions(timer);&#125; 关于对 ConsumerCoordinator 的处理都集中在 coordinator.poll () 方法中。其主要逻辑如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * Poll for coordinator events. This ensures that the coordinator is known and that the consumer * has joined the group (if it is using group management). This also handles periodic offset commits * if they are enabled. * (确保group的coordinator是已知的，并且这个consumer是已经加入到了group中，也用于offset周期性的commit) * &lt;p&gt; * Returns early if the timeout expires * * @param timer Timer bounding how long this method can block * @return true iff the operation succeeded */public boolean poll(Timer timer) &#123; maybeUpdateSubscriptionMetadata(); invokeCompletedOffsetCommitCallbacks(); // 如果是subscribe方式订阅的topic if (subscriptions.partitionsAutoAssigned()) &#123; // Always update the heartbeat last poll time so that the heartbeat thread does not leave the // group proactively due to application inactivity even if (say) the coordinator cannot be found. // 1.检查心跳线程运行是否正常，如果心跳线程失败则抛出异常，反之则更新poll调用的时间 pollHeartbeat(timer.currentTimeMs()); // 2.如果coordinator未知，则初始化ConsumeCoordinator if (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) &#123; return false; &#125; // 判断是否需要重新加入group，如果订阅的partition变化或者分配的partition变化，都可能需要重新加入group if (rejoinNeededOrPending()) &#123; // due to a race condition between the initial metadata fetch and the initial rebalance, // we need to ensure that the metadata is fresh before joining initially. This ensures // that we have matched the pattern against the cluster&#x27;s topics at least once before joining. if (subscriptions.hasPatternSubscription()) &#123; // For consumer group that uses pattern-based subscription, after a topic is created, // any consumer that discovers the topic after metadata refresh can trigger rebalance // across the entire consumer group. Multiple rebalances can be triggered after one topic // creation if consumers refresh metadata at vastly different times. We can significantly // reduce the number of rebalances caused by single topic creation by asking consumer to // refresh metadata before re-joining the group as long as the refresh backoff time has // passed. if (this.metadata.timeToAllowUpdate(timer.currentTimeMs()) == 0) &#123; this.metadata.requestUpdate(); &#125; if (!client.ensureFreshMetadata(timer)) &#123; return false; &#125; maybeUpdateSubscriptionMetadata(); &#125; // 3.确保group是active的，重新加入group，分配订阅的partition if (!ensureActiveGroup(timer)) &#123; return false; &#125; &#125; &#125; else &#123; // For manually assigned partitions, if there are no ready nodes, await metadata. // If connections to all nodes fail, wakeups triggered while attempting to send fetch // requests result in polls returning immediately, causing a tight loop of polls. Without // the wakeup, poll() with no channels would block for the timeout, delaying re-connection. // awaitMetadataUpdate() initiates new connections with configured backoff and avoids the busy loop. // When group management is used, metadata wait is already performed for this scenario as // coordinator is unknown, hence this check is not required. if (metadata.updateRequested() &amp;&amp; !client.hasReadyNodes(timer.currentTimeMs())) &#123; client.awaitMetadataUpdate(timer); &#125; &#125; // 4.如果设置的是自动commit,如果定时达到则自动commit maybeAutoCommitOffsetsAsync(timer.currentTimeMs()); return true;&#125; coordinator.poll () 方法中，具体实现可以分为四个步骤： pollHeartbeat ()：检测心跳线程运行是否正常，需要定时向 GroupCoordinator 发送心跳，如果超时未发送心跳，consumer 会离开 consumer group。 ensureCoordinatorReady ()：当通过 subscribe () 方法订阅 topic 时，如果 coordinator 未知，则初始化 ConsumerCoordinator (在 ensureCoordinatorReady () 中实现，该方法主要的作用是发送 FindCoordinatorRequest 请求，并建立连接)。 ensureActiveGroup ()：判断是否需要重新加入 group，如果订阅的 partition 变化或者分配的 partition 变化时，需要 rejoin，则通过 ensureActiveGroup () 发送 join-group、sync-group 请求，加入 group 并获取其 assign 的 TopicPartition list。 maybeAutoCommitOffsetsAsync ()：如果设置的是自动 commit，并且达到了发送时限则自动 commit offset。 关于 rejoin，下列几种情况会触发再均衡 (reblance) 操作： 订阅的 topic 列表变化 topic 被创建或删除 新的消费者加入消费者组 (第一次进行消费也属于这种情况) 消费者宕机下线 (长时间未发送心跳包) 消费者主动退出消费组，比如调用 unsubscrible () 方法取消对主题的订阅 消费者组对应的 GroupCoorinator 节点发生了变化 消费者组内所订阅的任一主题或者主题的分区数量发生了变化 取消 topic 订阅，consumer 心跳线程超时以及在 Server 端给定的时间内未收到心跳请求，这三个都是触发的 LEAVE_GROUP 请求。 下面重点介绍下第二步中的 ensureCoordinatorReady () 方法和第三步中的 ensureActiveGroup () 方法。 ensureCoordinatorReadyensureCoordinatorReady ()这个方法主要作用：选择一个连接数最少的 broker (还未响应请求最少的 broker)，发送 FindCoordinator 请求，找到 GroupCoordinator 后，建立对应的 TCP 连接。 方法调用流程是 ensureCoordinatorReady () → lookupCoordinator () → sendFindCoordinatorRequest ()。 如果 client 收到 server response，那么就与 GroupCoordinator 建立连接。 1234567891011121314151617181920212223242526272829303132333435363738/** * Visible for testing. * * Ensure that the coordinator is ready to receive requests. * * @param timer Timer bounding how long this method can block * @return true If coordinator discovery and initial connection succeeded, false otherwise */protected synchronized boolean ensureCoordinatorReady(final Timer timer) &#123; if (!coordinatorUnknown()) return true; do &#123; // 找到GroupCoordinator，并建立连接 final RequestFuture&lt;Void&gt; future = lookupCoordinator(); client.poll(future, timer); if (!future.isDone()) &#123; // ran out of time break; &#125; if (future.failed()) &#123; if (future.isRetriable()) &#123; log.debug(&quot;Coordinator discovery failed, refreshing metadata&quot;); client.awaitMetadataUpdate(timer); &#125; else throw future.exception(); &#125; else if (coordinator != null &amp;&amp; client.isUnavailable(coordinator)) &#123; // we found the coordinator, but the connection has failed, so mark // it dead and backoff before retrying discovery markCoordinatorUnknown(); timer.sleep(retryBackoffMs); &#125; &#125; while (coordinatorUnknown() &amp;&amp; timer.notExpired()); return !coordinatorUnknown();&#125; 12345678910111213protected synchronized RequestFuture&lt;Void&gt; lookupCoordinator() &#123; if (findCoordinatorFuture == null) &#123; // find a node to ask about the coordinator(找一个最少连接的broker，此处对应的应该就是文章开头处确定GroupCoordinator节点的发发) Node node = this.client.leastLoadedNode(); if (node == null) &#123; log.debug(&quot;No broker available to send FindCoordinator request&quot;); return RequestFuture.noBrokersAvailable(); &#125; else // 对找到的broker发送FindCoordinator请求，并对response进行处理 findCoordinatorFuture = sendFindCoordinatorRequest(node); &#125; return findCoordinatorFuture;&#125; 12345678910111213141516171819/** * Discover the current coordinator for the group. Sends a GroupMetadata request to * one of the brokers. The returned future should be polled to get the result of the request. * @return A request future which indicates the completion of the metadata request */private RequestFuture&lt;Void&gt; sendFindCoordinatorRequest(Node node) &#123; // initiate the group metadata request log.debug(&quot;Sending FindCoordinator request to broker &#123;&#125;&quot;, node); FindCoordinatorRequest.Builder requestBuilder = new FindCoordinatorRequest.Builder( new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.groupId)); // 发送请求，并将response转换为RequestFuture // compose的作用是将FindCoordinatorResponseHandler类转换为RequestFuture // 实际上就是为返回的Future类重置onSuccess()和onFailure()方法 return client.send(node, requestBuilder) .compose(new FindCoordinatorResponseHandler());&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142// 根据response返回的ip以及端口信息，和该broke上开启的GroupCoordinator建立连接private class FindCoordinatorResponseHandler extends RequestFutureAdapter&lt;ClientResponse, Void&gt; &#123; @Override public void onSuccess(ClientResponse resp, RequestFuture&lt;Void&gt; future) &#123; log.debug(&quot;Received FindCoordinator response &#123;&#125;&quot;, resp); clearFindCoordinatorFuture(); FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) resp.responseBody(); Errors error = findCoordinatorResponse.error(); if (error == Errors.NONE) &#123; // 如果正确获取broker上的GroupCoordinator，建立连接，并更新心跳时间 synchronized (AbstractCoordinator.this) &#123; // use MAX_VALUE - node.id as the coordinator id to allow separate connections // for the coordinator in the underlying network client layer int coordinatorConnectionId = Integer.MAX_VALUE - findCoordinatorResponse.data().nodeId(); AbstractCoordinator.this.coordinator = new Node( coordinatorConnectionId, findCoordinatorResponse.data().host(), findCoordinatorResponse.data().port()); log.info(&quot;Discovered group coordinator &#123;&#125;&quot;, coordinator); // 初始化tcp连接 client.tryConnect(coordinator); // 更新心跳时间 heartbeat.resetSessionTimeout(); &#125; future.complete(null); &#125; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else &#123; log.debug(&quot;Group coordinator lookup failed: &#123;&#125;&quot;, findCoordinatorResponse.data().errorMessage()); future.raise(error); &#125; &#125; @Override public void onFailure(RuntimeException e, RequestFuture&lt;Void&gt; future) &#123; clearFindCoordinatorFuture(); super.onFailure(e, future); &#125;&#125; 上面代码主要作用就是：往一个负载最小的 broker 节点发起 FindCoordinator 请求，Kafka 在走到这个请求后会根据 group_id 查找对应的 GroupCoordinator 节点 (文章开头处介绍的方法)，如果找到对应的 GroupCoordinator 则会返回其对应的 node_id，host 和 port 信息，并建立连接。 这里的 GroupCoordinator 节点的确定在文章开头提到过，是通过 group.id 和 partitionCount 来确定的。 ensureActiveGroup现在已经知道了 GroupCoordinator 节点，并建立了连接。ensureActiveGroup () 这个方法的主要作用：向 GroupCoordinator 发送 join-group、sync-group 请求，获取 assign 的 tp list。 调用过程是 ensureActiveGroup () → ensureCoordinatorReady () → startHeartbeatThreadIfNeeded () → joinGroupIfNeeded ()。 joinGroupIfNeeded () 方法中最重要的方法是 initiateJoinGroup ()，它的的调用流程是 disableHeartbeatThread () → sendJoinGroupRequest () → JoinGroupResponseHandler::handle () → onJoinLeader ()，onJoinFollower () → sendSyncGroupRequest ()。 12345678910111213141516171819/** * Ensure the group is active (i.e., joined and synced) * * @param timer Timer bounding how long this method can block * @return true iff the group is active */boolean ensureActiveGroup(final Timer timer) &#123; // always ensure that the coordinator is ready because we may have been disconnected // when sending heartbeats and does not necessarily require us to rejoin the group. // 1.确保GroupCoordinator已经连接 if (!ensureCoordinatorReady(timer)) &#123; return false; &#125; // 2.启动心跳线程，但是并不一定发送心跳，满足条件后才会发送心跳 startHeartbeatThreadIfNeeded(); // 3.发送joinGroup请求，并对返回的信息进行处理，核心步骤 return joinGroupIfNeeded(timer);&#125; 心跳线程就是在这里启动的，但是并不一定马上发送心跳包，会在满足条件之后才会开始发送。后面最主要的逻辑就集中在 joinGroupIfNeeded () 方法，它的核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Joins the group without starting the heartbeat thread. * * Visible for testing. * * @param timer Timer bounding how long this method can block * @return true iff the operation succeeded */boolean joinGroupIfNeeded(final Timer timer) &#123; while (rejoinNeededOrPending()) &#123; if (!ensureCoordinatorReady(timer)) &#123; return false; &#125; // call onJoinPrepare if needed. We set a flag to make sure that we do not call it a second // time if the client is woken up before a pending rebalance completes. This must be called // on each iteration of the loop because an event requiring a rebalance (such as a metadata // refresh which changes the matched subscription set) can occur while another rebalance is // still in progress. // 触发onJoinPrepare，包括offset commit和rebalance listener if (needsJoinPrepare) &#123; // 如果是自动提交，则要开始提交offset以及在join group之前回调reblance listener接口 onJoinPrepare(generation.generationId, generation.memberId); needsJoinPrepare = false; &#125; // 初始化joinGroup请求，并发送joinGroup请求，核心步骤 final RequestFuture&lt;ByteBuffer&gt; future = initiateJoinGroup(); client.poll(future, timer); if (!future.isDone()) &#123; // we ran out of time return false; &#125; // join succeed，这一步时，时间上sync-group已经成功了 if (future.succeeded()) &#123; // Duplicate the buffer in case `onJoinComplete` does not complete and needs to be retried. ByteBuffer memberAssignment = future.value().duplicate(); // 发送完成，consumer加入group成功，触发onJoinComplete()方法 onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment); // We reset the join group future only after the completion callback returns. This ensures // that if the callback is woken up, we will retry it on the next joinGroupIfNeeded. // 重置joinFuture为空 resetJoinGroupFuture(); needsJoinPrepare = true; &#125; else &#123; resetJoinGroupFuture(); final RuntimeException exception = future.exception(); if (exception instanceof UnknownMemberIdException || exception instanceof RebalanceInProgressException || exception instanceof IllegalGenerationException || exception instanceof MemberIdRequiredException) continue; else if (!future.isRetriable()) throw exception; timer.sleep(retryBackoffMs); &#125; &#125; return true;&#125; initiateJoinGroup () 方法的核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private synchronized RequestFuture&lt;ByteBuffer&gt; initiateJoinGroup() &#123; // we store the join future in case we are woken up by the user after beginning the // rebalance in the call to poll below. This ensures that we do not mistakenly attempt // to rejoin before the pending rebalance has completed. if (joinFuture == null) &#123; // fence off the heartbeat thread explicitly so that it cannot interfere with the join group. // Note that this must come after the call to onJoinPrepare since we must be able to continue // sending heartbeats if that callback takes some time. // Step1:rebalance期间，心跳线程停止运行 disableHeartbeatThread(); // 设置当前状态为rebalance state = MemberState.REBALANCING; // Step2:发送joinGroup请求，核心步骤 joinFuture = sendJoinGroupRequest(); // Step3:为joinGroup请求添加监听器，监听joinGroup请求的结果并做相应的处理 joinFuture.addListener(new RequestFutureListener&lt;ByteBuffer&gt;() &#123; @Override public void onSuccess(ByteBuffer value) &#123; // handle join completion in the callback so that the callback will be invoked // even if the consumer is woken up before finishing the rebalance synchronized (AbstractCoordinator.this) &#123; log.info(&quot;Successfully joined group with generation &#123;&#125;&quot;, generation.generationId); // 如果joinGroup成功，设置状态为stable state = MemberState.STABLE; rejoinNeeded = false; if (heartbeatThread != null) // Step4:允许心跳线程继续运行 heartbeatThread.enable(); &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; // we handle failures below after the request finishes. if the join completes // after having been woken up, the exception is ignored and we will rejoin synchronized (AbstractCoordinator.this) &#123; // 如果joinGroup失败，设置状态为unjoined state = MemberState.UNJOINED; &#125; &#125; &#125;); &#125; return joinFuture;&#125; 可以看到在 joinGroup 之前会让心跳线程暂时停下来，此时会将 ConsumerCoordinator 的状态设置为 rebalance 状态，当 joinGroup 成功之后会将状态设置为 stable 状态，同时让之前停下来的心跳线程继续运行。 sendJoinGroupRequest () 方法的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536/** * Join the group and return the assignment for the next generation. This function handles both * JoinGroup and SyncGroup, delegating to &#123;@link #performAssignment(String, String, List)&#125; if * elected leader by the coordinator. * * NOTE: This is visible only for testing * * @return A request future which wraps the assignment returned from the group leader */// 发送joinGroup请求RequestFuture&lt;ByteBuffer&gt; sendJoinGroupRequest() &#123; if (coordinatorUnknown()) return RequestFuture.coordinatorNotAvailable(); // send a join group request to the coordinator log.info(&quot;(Re-)joining group&quot;); JoinGroupRequest.Builder requestBuilder = new JoinGroupRequest.Builder( new JoinGroupRequestData() .setGroupId(groupId) .setSessionTimeoutMs(this.sessionTimeoutMs) .setMemberId(this.generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setProtocolType(protocolType()) .setProtocols(metadata()) .setRebalanceTimeoutMs(this.rebalanceTimeoutMs) ); log.debug(&quot;Sending JoinGroup (&#123;&#125;) to coordinator &#123;&#125;&quot;, requestBuilder, this.coordinator); // Note that we override the request timeout using the rebalance timeout since that is the // maximum time that it may block on the coordinator. We add an extra 5 seconds for small delays. int joinGroupTimeoutMs = Math.max(rebalanceTimeoutMs, rebalanceTimeoutMs + 5000); return client.send(coordinator, requestBuilder, joinGroupTimeoutMs) .compose(new JoinGroupResponseHandler());// Step5:处理joinGroup请求后的response&#125; 在发送 joinGroup 请求之后，会收到来自服务器的响应，然后针对这个响应再做一些重要的事情： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// 处理发送joinGroup请求后的response的handler(同步group信息)private class JoinGroupResponseHandler extends CoordinatorResponseHandler&lt;JoinGroupResponse, ByteBuffer&gt; &#123; @Override public void handle(JoinGroupResponse joinResponse, RequestFuture&lt;ByteBuffer&gt; future) &#123; Errors error = joinResponse.error(); if (error == Errors.NONE) &#123; log.debug(&quot;Received successful JoinGroup response: &#123;&#125;&quot;, joinResponse); sensors.joinLatency.record(response.requestLatencyMs()); synchronized (AbstractCoordinator.this) &#123; if (state != MemberState.REBALANCING) &#123; // if the consumer was woken up before a rebalance completes, we may have already left // the group. In this case, we do not want to continue with the sync group. future.raise(new UnjoinedGroupException()); &#125; else &#123; AbstractCoordinator.this.generation = new Generation(joinResponse.data().generationId(), joinResponse.data().memberId(), joinResponse.data().protocolName()); // Step6:joinGroup成功，下面需要进行sync-group，获取分配的tp列表 if (joinResponse.isLeader()) &#123; // 当前consumer是leader onJoinLeader(joinResponse).chain(future); &#125; else &#123; // 当前consumer是follower onJoinFollower().chain(future); &#125; &#125; &#125; &#125; else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS) &#123; log.debug(&quot;Attempt to join group rejected since coordinator &#123;&#125; is loading the group.&quot;, coordinator()); // backoff and retry future.raise(error); &#125; else if (error == Errors.UNKNOWN_MEMBER_ID) &#123; // reset the member id and retry immediately resetGeneration(); log.debug(&quot;Attempt to join group failed due to unknown member id.&quot;); future.raise(Errors.UNKNOWN_MEMBER_ID); &#125; else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123; // re-discover the coordinator and retry with backoff markCoordinatorUnknown(); log.debug(&quot;Attempt to join group failed due to obsolete coordinator information: &#123;&#125;&quot;, error.message()); future.raise(error); &#125; else if (error == Errors.FENCED_INSTANCE_ID) &#123; log.error(&quot;Received fatal exception: group.instance.id gets fenced&quot;); future.raise(error); &#125; else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL || error == Errors.INVALID_SESSION_TIMEOUT || error == Errors.INVALID_GROUP_ID || error == Errors.GROUP_AUTHORIZATION_FAILED || error == Errors.GROUP_MAX_SIZE_REACHED) &#123; // log the error and re-throw the exception log.error(&quot;Attempt to join group failed due to fatal error: &#123;&#125;&quot;, error.message()); if (error == Errors.GROUP_MAX_SIZE_REACHED) &#123; future.raise(new GroupMaxSizeReachedException(groupId)); &#125; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else &#123; future.raise(error); &#125; &#125; else if (error == Errors.UNSUPPORTED_VERSION) &#123; log.error(&quot;Attempt to join group failed due to unsupported version error. Please unset field group.instance.id and retry&quot; + &quot;to see if the problem resolves&quot;); future.raise(error); &#125; else if (error == Errors.MEMBER_ID_REQUIRED) &#123; // Broker requires a concrete member id to be allowed to join the group. Update member id // and send another join group request in next cycle. synchronized (AbstractCoordinator.this) &#123; AbstractCoordinator.this.generation = new Generation(OffsetCommitRequest.DEFAULT_GENERATION_ID, joinResponse.data().memberId(), null); AbstractCoordinator.this.rejoinNeeded = true; AbstractCoordinator.this.state = MemberState.UNJOINED; &#125; future.raise(Errors.MEMBER_ID_REQUIRED); &#125; else &#123; // unexpected error, throw the exception log.error(&quot;Attempt to join group failed due to unexpected error: &#123;&#125;&quot;, error.message()); future.raise(new KafkaException(&quot;Unexpected error in join group response: &quot; + error.message())); &#125; &#125;&#125; 上面代码的主要过程如下： 如果 group 是新的 group.id，那么此时 group 初始化的状态为 Empty； 当 GroupCoordinator 接收到 consumer 的 join-group 请求后，由于此时这个 group 的 member 列表还是空 (group 是新建的，每个 consumer 实例被称为这个 group 的一个 member)，第一个加入的 member 将被选为 leader，也就是说，对于一个新的 consumer group 而言，当第一个 consumer 实例加入后将会被选为 leader。如果后面 leader 挂了，会从其他 member 里面随机选择一个 member 成为新的 leader； 如果 GroupCoordinator 接收到 leader 发送 join-group 请求，将会触发 rebalance，group 的状态变为 PreparingRebalance； 此时，GroupCoordinator 将会等待一定的时间，如果在一定时间内，接收到 join-group 请求的 consumer 将被认为是依然存活的，此时 group 会变为 AwaitSync 状态，并且 GroupCoordinator 会向这个 group 的所有 member 返回其 response； consumer 在接收到 GroupCoordinator 的 response 后，如果这个 consumer 是 group 的 leader，那么这个 consumer 将会负责为整个 group assign partition 订阅安排（默认是按 range 的策略，目前也可选 roundrobin），然后 leader 将分配后的信息以 sendSyncGroupRequest () 请求的方式发给 GroupCoordinator，而作为 follower 的 consumer 实例会发送一个空列表； GroupCoordinator 在接收到 leader 发来的请求后，会将 assign 的结果返回给所有已经发送 sync-group 请求的 consumer 实例，并且 group 的状态将会转变为 Stable，如果后续再收到 sync-group 请求，由于 group 的状态已经是 Stable，将会直接返回其分配结果。 sync-group 发送请求核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 当consumer为follower时，从GroupCoordinator拉取分配结果private RequestFuture&lt;ByteBuffer&gt; onJoinFollower() &#123; // send follower&#x27;s sync group with an empty assignment SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder( new SyncGroupRequestData() .setGroupId(groupId) .setMemberId(generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setGenerationId(generation.generationId) .setAssignments(Collections.emptyList()) ); log.debug(&quot;Sending follower SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;, this.coordinator, requestBuilder); // 发送sync-group请求 return sendSyncGroupRequest(requestBuilder);&#125;// 当consumer客户端为leader时，对group下的所有实例进行分配，将assign的结果发送到GroupCoordinatorprivate RequestFuture&lt;ByteBuffer&gt; onJoinLeader(JoinGroupResponse joinResponse) &#123; try &#123; // perform the leader synchronization and send back the assignment for the group(assign 操作) Map&lt;String, ByteBuffer&gt; groupAssignment = performAssignment(joinResponse.data().leader(), joinResponse.data().protocolName(), joinResponse.data().members()); List&lt;SyncGroupRequestData.SyncGroupRequestAssignment&gt; groupAssignmentList = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, ByteBuffer&gt; assignment : groupAssignment.entrySet()) &#123; groupAssignmentList.add(new SyncGroupRequestData.SyncGroupRequestAssignment() .setMemberId(assignment.getKey()) .setAssignment(Utils.toArray(assignment.getValue())) ); &#125; SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder( new SyncGroupRequestData() .setGroupId(groupId) .setMemberId(generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setGenerationId(generation.generationId) .setAssignments(groupAssignmentList) ); log.debug(&quot;Sending leader SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;, this.coordinator, requestBuilder); // 发送sync-group请求 return sendSyncGroupRequest(requestBuilder); &#125; catch (RuntimeException e) &#123; return RequestFuture.failure(e); &#125;&#125;// 发送SyncGroup请求，获取对partition分配的安排private RequestFuture&lt;ByteBuffer&gt; sendSyncGroupRequest(SyncGroupRequest.Builder requestBuilder) &#123; if (coordinatorUnknown()) return RequestFuture.coordinatorNotAvailable(); return client.send(coordinator, requestBuilder) .compose(new SyncGroupResponseHandler());&#125;private class SyncGroupResponseHandler extends CoordinatorResponseHandler&lt;SyncGroupResponse, ByteBuffer&gt; &#123; @Override public void handle(SyncGroupResponse syncResponse, RequestFuture&lt;ByteBuffer&gt; future) &#123; Errors error = syncResponse.error(); if (error == Errors.NONE) &#123; // sync-group成功 sensors.syncLatency.record(response.requestLatencyMs()); future.complete(ByteBuffer.wrap(syncResponse.data.assignment())); &#125; else &#123; // join的标志位设置为true requestRejoin(); if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else if (error == Errors.REBALANCE_IN_PROGRESS) &#123; // group正在进行rebalance，任务失败 log.debug(&quot;SyncGroup failed because the group began another rebalance&quot;); future.raise(error); &#125; else if (error == Errors.FENCED_INSTANCE_ID) &#123; log.error(&quot;Received fatal exception: group.instance.id gets fenced&quot;); future.raise(error); &#125; else if (error == Errors.UNKNOWN_MEMBER_ID || error == Errors.ILLEGAL_GENERATION) &#123; log.debug(&quot;SyncGroup failed: &#123;&#125;&quot;, error.message()); resetGeneration(); future.raise(error); &#125; else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123; log.debug(&quot;SyncGroup failed: &#123;&#125;&quot;, error.message()); markCoordinatorUnknown(); future.raise(error); &#125; else &#123; future.raise(new KafkaException(&quot;Unexpected error from SyncGroup: &quot; + error.message())); &#125; &#125; &#125;&#125; 这个阶段主要是将分区分配方案同步给各个消费者，这个同步仍然是通过 GroupCoordinator 来转发的。 分区策略并非由 leader 消费者来决定，而是各个消费者投票决定的，谁的票多就采用什么分区策略。这里的分区策略是通过 partition.assignment.strategy 参数设置的，可以设置多个。如果选举出了消费者不支持的策略，那么就会抛出异常 IllegalArgumentException: Member does not support protocol。 经过上面的步骤，一个 consumer 实例就已经加入 group 成功了，加入 group 成功后，将会触发 ConsumerCoordinator 的 onJoinComplete () 方法，其作用就是：更新订阅的 tp 列表、更新其对应的 metadata 及触发注册的 listener。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 加入group成功@Overrideprotected void onJoinComplete(int generation, String memberId, String assignmentStrategy, ByteBuffer assignmentBuffer) &#123; // only the leader is responsible for monitoring for metadata changes (i.e. partition changes) if (!isLeader) assignmentSnapshot = null; PartitionAssignor assignor = lookupAssignor(assignmentStrategy); if (assignor == null) throw new IllegalStateException(&quot;Coordinator selected invalid assignment protocol: &quot; + assignmentStrategy); Assignment assignment = ConsumerProtocol.deserializeAssignment(assignmentBuffer); if (!subscriptions.assignFromSubscribed(assignment.partitions())) &#123; handleAssignmentMismatch(assignment); return; &#125; Set&lt;TopicPartition&gt; assignedPartitions = subscriptions.assignedPartitions(); // The leader may have assigned partitions which match our subscription pattern, but which // were not explicitly requested, so we update the joined subscription here. maybeUpdateJoinedSubscription(assignedPartitions); // give the assignor a chance to update internal state based on the received assignment assignor.onAssignment(assignment, generation); // reschedule the auto commit starting from now if (autoCommitEnabled) this.nextAutoCommitTimer.updateAndReset(autoCommitIntervalMs); // execute the user&#x27;s callback after rebalance ConsumerRebalanceListener listener = subscriptions.rebalanceListener(); log.info(&quot;Setting newly assigned partitions: &#123;&#125;&quot;, Utils.join(assignedPartitions, &quot;, &quot;)); try &#123; listener.onPartitionsAssigned(assignedPartitions); &#125; catch (WakeupException | InterruptException e) &#123; throw e; &#125; catch (Exception e) &#123; log.error(&quot;User provided listener &#123;&#125; failed on partition assignment&quot;, listener.getClass().getName(), e); &#125;&#125; 至此，一个 consumer 实例算是真正上意义上加入 group 成功。 然后 consumer 就进入正常工作状态，同时 consumer 也通过向 GroupCoordinator 发送心跳来维持它们与消费者组的从属关系，以及它们对分区的所有权关系。只要以正常的间隔发送心跳，就被认为是活跃的，但是如果 GroupCoordinator 没有响应，那么就会发送 LeaveGroup 请求退出消费者组。 本文参考http://generalthink.github.io/2019/05/15/how-to-join-kafka-consumer-group/ https://matt33.com/2017/10/22/consumer-join-group/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"Kafka 的核心配置参数","slug":"kafka-properties","date":"2020-10-30T07:48:10.000Z","updated":"2021-01-05T07:33:28.544Z","comments":true,"path":"2020/10/30/kafka-properties/","link":"","permalink":"http://example.com/2020/10/30/kafka-properties/","excerpt":"","text":"Kafka Producer 核心配置参数bootstrap.servers broke 服务器地址，多个服务器，用逗号隔开。 acks 发送应答，默认：1。 acks 参数指定了生产者希望 leader 返回的用于确认请求完成的确认数量，即必须要有多少个分区副本收到该消息，生产者才会认为消息写入是成功的。 允许以下设置： acks=0：生产者将完全不等待来自服务器的任何确认。记录将立即添加到 socket 缓冲区，并被认为已发送。在这种情况下，不能保证服务器已经收到记录，重试配置将不会生效 (因为客户机通常不会知道任何失败)。响应里来自服务端的 offset 总是-1。同时，由于不需要等待响应，所以可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量。 acks=1：只需要集群的 leader 收到消息，生产者就会收到一个来自服务器的成功响应。leader 会将记录写到本地日志中，但不会等待所有 follower 的完全确认。在这种情况下，如果 follower 复制数据之前，leader 挂掉，数据就会丢失。 acks=all / -1：当所有参与复制的节点全部收到消息的时候，生产者才会收到一个来自服务器的成功响应，最安全不过延迟比较高。如果需要保证消息不丢失, 需要使用该设置，同时需要设置 broke端 unclean.leader.election.enable 为 true，保证当 ISR 列表为空时，选择其他存活的副本作为新的 leader。 batch.size 批量发送大小，默认：16384，即 16 K。 当有多个消息需要被发送到同一个 partition 的时候，生产者会把他们放到同一个批次里面 (Deque)，该参数指定了一个批次可以使用的内存大小，按照字节数计算，当批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定会等到批次被填满才发送，半满甚至只包含一个消息的批次也有可能被发送。 生产者产生的消息缓存到本地，每次批量发送 batch.size 大小到服务器。太小的 batch 会降低吞吐，太大则会浪费内存。 linger.ms 发送延迟时间，默认：0。 指定了生产者在发送批次之前等待更多消息加入批次的时间。生产者会在批次填满或 linger.ms 达到上限时把批次发送出去。把 linger.ms 设置成比0大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次，虽然这样会增加延迟，但也会提升吞吐量。 说明：batch.size 和 linger.ms 满足任何一个条件都会发送。 buffer.memory 生产者最大可用缓存，默认：33554432，即 32 M。 生产者可以用来缓冲等待发送到服务器的记录的总内存字节。如果应用程序发送消息的速度超过生产者发送消息到服务器的速度，即超出 max.block.ms，将会抛出异常。 该设置应该大致与生产者将使用的总内存相对应，但不是硬绑定，因为生产者使用的内存并非全部都用于缓冲。一些额外的内存将用于压缩 (如果启用了压缩) 以及维护飞行中的请求。 max.block.ms 阻塞时间，默认：60000，即 1 分钟。 指定了在调用 send () 方法或者 partitionsFor () 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 max.block.ms 时，就会抛出 new TimeoutException(“Failed to allocate memory within the configured max blocking time “ + maxTimeToBlockMs + “ ms.”);。 用户提供的序列化器或分区程序中的阻塞将不计入此超时。 client.id 生产者 ID，默认：空。 请求时传递给服务器的 id 字符串，用来标识消息来源，后台线程会根据它命名。这样做的目的是通过允许在服务器端请求日志中包含逻辑应用程序名称，从而能够跟踪 ip/端口之外的请求源。 compression.type 生产者数据被发送到服务器之前被压缩的压缩类型，默认：none，即不压缩。 指定给定主题的最终压缩类型。此配置接受标准压缩编解码器 (“gzip”、“snappy”、“lz4”、“zstd”)。 “gzip”：压缩效率高，适合高内存、CPU。 “snappy”：适合带宽敏感性，压缩力度大。 retries 失败重试次数，默认：2147483647。 异常是 RetriableException 类型，或者 TransactionManager 允许重试 (transactionManager.canRetry () )。 RetriableException 类型异常如下： retry.backoff.ms 失败请求重试的间隔时间，默认：100。 这避免了在某些失败场景下以紧密循环的方式重复发送请求。 max.in.flight.requests.per.connection 单个连接上发送的未确认请求的最大数量，默认：5。 阻塞前客户端在单个连接上发送的未确认请求的最大数量。即指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。 如果设置为 1，可以保证消息是按照发送的顺序写入服务器的，即便发生了重试。 如果设置大于 1，在 retries 不为0的情况下可能会出现消息发送顺序的错误。例如将两个批发送到同一个分区，第一个批处理失败并重试，但是第二个批处理成功，那么第二个批处理中的记录可能会先出现。 delivery.timeout.ms 传输时间，默认：120000，即 2 分钟。 生产者发送完请求接受服务器 ACK 的时间，该时间允许重试 ，该配置应该大于 request.timeout.ms + linger.ms。 request.timeout.ms 请求超时时间，默认：30000，即30秒。 配置控制客户端等待请求响应的最长时间。 如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽，则该请求将失败。 这应该大于replica.lag.time.max.ms (broker 端配置)，以减少由于不必要的生产者重试引起的消息重复的可能性。 connections.max.idle.ms 连接空闲超时时间，默认：540000，即 9 分钟。 在此配置指定的毫秒数之后关闭空闲连接。 enable.idempotence 开启幂等，默认：false。 如果设置为 true ，将开启 exactly-once 模式，生产者将确保在流中准确地写入每个消息的副本。如果设置为 false，则由于代理失败而导致生产者重试，等等，可能会在流中写入重试消息的副本。 注意，启用幂等需要以下条件 ：max.in.flight.requests.per.connection 小于或等于 5，retries 大于 0， acks 必须为 all 或者 -1。如果用户没有显式地设置这些值，将选择合适的值。如果设置了不兼容的值，就会抛出 ConfigException。 key.serializer key 序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Serializer 。Kafka 提供以下几个默认的 key 序列化器： String：org.apache.kafka.common.serialization.StringSerializer。 value.serializer value 序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Serializer。Kafka 提供以下几个默认的 value 序列化器： byte[]：org.apache.kafka.common.serialization.ByteArraySerializer。 String：org.apache.kafka.common.serialization.StringSerializer。 max.request.size 请求的最大字节大小，默认：1048576，即 1 M。 该参数用于控制生产者发送的请求大小，单次发送的消息大小超过 max.request.size 时，会抛出异常 ，如：org.apache.kafka.common.errors.RecordTooLargeException: The message is 70459102 bytes when serialized which is larger than the maximum request size you have configured with the max.request.size configuration.。 注意：broker 对可接收的消息最大值也有自己的限制 (通过 message.max.bytes 参数设置)，所以两边的配置最好可以匹配，避免生产者发送的消息被 broker 拒绝。 metric.reporters 自定义指标报告器，默认：无。 用作指标报告器的类的列表，需要实现接口：org.apache.kafka.common.metrics.MetricsReporter，该接口允许插入将在创建新度量时得到通知的类。JmxReporter 始终包含在注册 JMX 统计信息中。 interceptor.classes 拦截器，默认：无。 用作拦截器的类的列表，需要实现接口：org.apache.kafka.clients.producer.ProducerInterceptor 。允许将生产者接收到的记录发布到 Kafka 集群之前拦截它们 (可能还会发生突变)。 partitioner.class 分区策略，默认：org.apache.kafka.clients.producer.internals.DefaultPartitioner。 如果自定义分区策略，需要实现接口： org.apache.kafka.clients.producer.Partitioner。 receive.buffer.bytes 默认：32768，即 32 K。 指定了 TCP socket 接收数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 send.buffer.bytes 默认：131072，即 128 K。 指定了 TCP socket 发送数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 transaction.timeout.ms 事务协调器等待生产者更新事务状态的最大毫秒数，默认：60000，即 1 分钟。 如果超过该时间，事务协调器会终止进行中的事务。 如果设置的时间大于 broker 端的 max.transaction.timeout.ms，会抛出 InvalidTransactionTimeout 异常。 transactional.id 用于事务传递的 TransactionalId，默认：空，即不使用事务。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的 TransactionalId 的事务已经完成。如果没有提供 TransactionalId，则生产者被限制为幂等传递。 注意：如果配置了 TransactionalId，则必须启用 enable.idempotence。 Kafka Consumer 核心配置参数bootstrap.servers broke 服务器地址，多个服务器，用逗号隔开。 enable.auto.commit 是否开启自动提交 offset，默认：true。 如果为 true，consumer 的偏移量将在后台定期提交，自动提交频率通过 auto.commit.interval.ms 设置。 auto.commit.interval.ms 自动提交频率，默认：5000。 auto.offset.reset 初始偏移量，默认：latest。 如果 Kafka 中没有初始偏移量，或者服务器上不再存在当前偏移量 (例如，该数据已被删除)，该怎么处理： earliest：自动重置偏移到最早的偏移。 latest：自动将偏移量重置为最新偏移量。 none：如果没有为使用者的组找到以前的偏移量，则向使用者抛出 exception。 anything else：向使用者抛出异常。 client.id 客户端 id，默认：空。 便于跟踪日志。 check.crcs 是否开启数据校验，默认：true。 自动检查消耗的记录的 CRC32。这确保不会发生对消息的在线或磁盘损坏。此检查增加了一些开销，因此在寻求极端性能的情况下可能禁用此检查。 group.id 消费者所属的群组，默认：空。 唯一标识用户群组，每个 partition 只会分配给同一个 group 里面的一个 consumer 来消费。 max.poll.records 拉取的最大记录，默认：500。 单次轮询调用 poll () 方法能返回的记录的最大数量。 max.poll.interval.ms 拉取记录间隔，默认：300000，即 5 分钟。 使用消费者组管理时轮询调用之间的最大延迟。这为使用者在获取更多记录之前空闲的时间设置了上限。如果在此超时过期之前没有调用 poll ()，则认为使用者失败，组将重新平衡，以便将分区重新分配给另一个成员。 request.timeout.ms 请求超时时间，默认：30000 。 配置控制客户机等待请求响应的最长时间。如果在超时之前没有收到响应，客户端将在需要时重新发送请求，或者在重试耗尽时失败请求。 session.timeout.ms consumer session 超时时间，默认：10000。 用于检测 worker 程序失败的超时。worker 定期发送心跳，以向代理表明其活性。如果在此会话超时过期之前代理没有接收到心跳，则代理将从组中删除。 注意：该值必须在 broker 端配置的 group.min.session.timeout 和 group.max.session.timeout.ms 范围之间。 heartbeat.interval.ms 心跳时间，默认：3000。 心跳是在 consumer 与 coordinator 之间进行的。心跳是确定 consumer 存活，加入或者退出 group 的有效手段。 这个值必须设置的小于 session.timeout.ms 的1/3，因为： 当 consumer 由于某种原因不能发 Heartbeat 到 coordinator 时，并且时间超过 session.timeout.ms 时，就会认为该 consumer 已退出，它所订阅的 partition 会分配到同一 group 内的其它的 consumer 上。 connections.max.idle.ms 连接空闲超时时间，默认：540000，即 9 分钟。 在此配置指定的毫秒数之后关闭空闲连接。 key.deserializer key 反序列化器，默认：无。 需要实现接口：org.apache.kafka.common.serialize.Deserializer。Kafka 提供以下几个默认的 key 反序列化器： String：org.apache.kafka.common.serialization.StringDeserializer。 value.deserializer value 反序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Deserializer。Kafka 提供以下几个默认的 value 反序列化器： String：org.apache.kafka.common.serialization.StringDeserializer。 partition.assignment.strategy consumer订阅分区策略，默认：org.apache.kafka.clients.consumer.RangeAssignor。 当使用组管理时，客户端将使用分区分配策略的类名在使用者实例之间分配分区所有权。 max.partition.fetch.bytes 一次 fetch 请求，从一个 partition 中取得的 records 的最大值，默认：1048576，即 1 M。 如果在从 topic 中第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。 broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 message.max.bytes 配置和 topic 端的 max.message.bytes 配置。 fetch.max.bytes 一次 fetch 请求，从一个 broker 中取得的 records 的最大值，默认：52428800，即 50 M。 如果在从 topic中 第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。 broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 message.max.bytes 配置 和 topic 端的 max.message.bytes 配置。 fetch.min.bytes 一次 fetch 请求，从一个 broker 中取得的 records 的最小值，默认：1。 如果 broker 中数据量不够的话会 wait，直到积累的数据大小满足这个条件。默认值设置为1的目的是：使得 consumer 的请求能够尽快的返回。将此设置为大于 1 的值将导致服务器等待更大数量的数据累积，这可以稍微提高服务器吞吐量，但代价是增加一些延迟。 fetch.max.wait.ms 拉取阻塞时间，默认：500。 如果没有足够的数据立即满足 fetch.min.bytes 提供的要求，服务器在响应 fetch 请求之前将阻塞的最长时间。 exclude.internal.topics 公开内部 topic，默认：true。 是否应该将来自内部主题 (如偏移量) 的记录公开给使用者，consumer 共享 offset。如果设置为 true，从内部主题接收记录的唯一方法是订阅它。 isolation.level 隔离级别，默认：read_uncommitted。 控制如何以事务方式读取写入的消息。如果设置为 read_committed，poll () 方法将只返回已提交的事务消息。如果设置为 read_uncommitted，poll () 方法将返回所有消息，甚至是已经中止的事务消息。在任何一种模式下，非事务性消息都将无条件返回。 Kafka Broker 核心配置参数zookeeper.connect zookeeper 地址，多个地址用逗号隔开。 broker.id 服务器的 broke id，默认：-1。 每一个 broker 在集群中的唯一表示，要求是正数。 如果未设置，将生成唯一的代理 id。为了避免 zookeeper 生成的 broke id 和用户配置的 broke id 之间的冲突，生成的代理 id 从 reserve.broker.max.id 开始 id + 1。 advertised.host.name 默认：null。 不赞成使用： 在 server.properties 里还有另一个参数是解决这个问题的， advertised.host.name 参数用来配置返回的 host.name值，把这个参数配置为外网 IP 地址即可。 这个参数默认没有启用，默认是返回的 java.net.InetAddress.getCanonicalHostName() 的值，在我的 mac 上这个值并不等于 hostname 的值而是返回 IP，但在 linux 上这个值就是 hostname 的值。 advertised.listeners hostname 和端口注册到 zookeeper 给生产者和消费者使用的，如果没有设置，将会使用 listeners 的配置，如果 listeners 也没有配置，将使用 java.net.InetAddress.getCanonicalHostName() 来获取这个 hostname 和 port，对于 ipv4，基本就是 localhost 了。 auto.create.topics.enable 是否允许自动创建 topic，默认：true。 如果为 true，第一次发动消息时，允许自动创建 topic。否则，只能通过命令创建 topic。 auto.leader.rebalance.enable 自动 rebalance，默认：true。 支持自动 leader balance。如果需要，后台线程定期检查并触发 leader balance。 background.threads 默认：10。 一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改。 compression.type 压缩类型，默认：producer。 对发送的消息采取的压缩编码方式 (‘gzip’，’snappy’，’lz4’)。 ‘uncompressed’：不压缩， ‘producer’：保持 producer 本身设置的压缩编码。 delete.topic.enable 是否允许删除 topic，默认：true。 如果关闭此配置，则通过管理工具删除主题将无效。 leader.imbalance.check.interval.seconds rebalance 检测频率，默认：300。 控制器触发分区 rebalance 检查的频率。 leader.imbalance.per.broker.percentage 触发 rebalance 得比率，默认：10，即 10%。 每个 broke 允许的 leader 不平衡比率。如果控制器超过每个 broke 的这个值，控制器将触发一个 leader balance。该值以百分比指定。 log.dir 保存日志数据的目录，默认：/tmp/kafka-logs。 log.dirs 保存日志数据的目录，默认：null。 可以指定多个存储路径，以逗号分隔。如果未设置，使用 log.dir 中设置的值。 log.flush.interval.messages 默认：9223372036854775807。 在将消息刷新到磁盘之前，日志分区上累积的消息数量。 log 文件 ”sync” 到磁盘之前累积的消息条数。因为磁盘 IO 操作是一个慢操作，但又是一个”数据可靠性”的必要手段。所以此参数的设置，需要在”数据可靠性”与”性能”之间做必要的权衡。 如果此值过大，将会导致每次 ”fsync” 的时间较长 (IO 阻塞)；如果此值过小，将会导致 ”fsync” 的次数较多，这也意味着整体的 client 请求有一定的延迟。 物理 server 故障，将会导致没有 fsync 的消息丢失。 log.flush.interval.ms 默认：null。 任何 topic 中的消息在刷新到磁盘之前保存在内存中的最长时间。如果没有设置，则使用 log.flush.scheduler.interval.ms 中的值。 log.flush.scheduler.interval.ms 日志刷新器检查是否需要将任何日志刷新到磁盘的频率，默认：9223372036854775807。 log.flush.offset.checkpoint.interval.ms 作为日志恢复点的上次刷新的持久记录的更新频率，默认：60000。 log.retention.bytes 删除前日志的最大大小，默认：-1。 topic 每个分区的最大文件大小，一个 topic 的大小限制 = 分区数 * log.retention.bytes。 log.retention.hours 日志文件最大保存时间 (小时)，默认：168，即 7 天。 删除日志文件之前保存它的小时数。 log.retention.minutes 日志文件最大保存时间 (分钟)，默认：null。 在删除日志文件之前保存它的分钟数，如果没有设置，则使用 log.retention.hours 中的值。 log.retention.ms 日志文件最大保存时间 (毫秒)，默认：null。 在删除日志文件之前保存它的毫秒数，如果没有设置，则使用 log.retention.minutes 中的值。如果设置为 -1，则没有时间限制。 log.roll.hours 新 segment 产生时间，默认：168，即 7 天。 即使文件没有到达 log.segment.bytes 设置的大小，只要文件创建时间到达此属性，也会强制创建新 segment。 log.roll.ms 新 segment 产生时间，默认：null。 如果未设置，则使用 log.roll.hours 中的值。 log.segment.bytes 单个 segment 文件的最大值，默认：1073741824，即 1 G。 log.segment.delete.delay.ms segment 删除前等待时间， 默认：60000，即 1 分钟。 message.max.bytes 最大 batch size，默认：1048588，即 1.000011 M。 Kafka 允许的最大 record batch size (如果启用了压缩，则是压缩后的大小)。如果增加了这个值，并且是 0.10.2 版本之前的 consumer，那么也必须增加 consumer 的 fetch 大小，以便他们能够获取这么大的 record batch。在最新的消息格式版本中，记录总是按批进行分组，以提高效率。在以前的消息格式版本中，未压缩记录没有分组成批，这种限制只适用于单个 record。针对每个 topic，可以使用 max.message.bytes 设置。 min.insync.replicas insync中最小副本值，默认：1。 当生产者将 acks 设置为 “all” (或 “-1”)时，min.insync.replicas 指定了必须确认写操作成功的最小副本数量。如果不能满足这个最小值，则生产者将抛出一个异常 (要么是 NotEnoughReplicas，要么是 NotEnoughReplicasAfterAppend)。 当一起使用时，min.insync.replicas 和 ack 允许你执行更大的持久性保证。一个典型的场景是创建一个复制因子为 3 的主题，设置 min.insync.replicas 为 2，生产者设置 acks 为 “all”，这将确保如果大多数副本没有收到写操作，则生产者会抛出异常。 num.io.threads 服务器用于处理请求的线程数，其中可能包括磁盘 I/O，默认：8。 num.network.threads 服务器用于接收来自网络的请求和向网络发送响应的线程数，默认：3。 num.recovery.threads.per.data.dir 每个数据目录在启动时用于日志恢复和在关闭时用于刷新的线程数，默认：1。 num.replica.alter.log.dirs.threads 可以在日志目录 (可能包括磁盘 I/O) 之间移动副本的线程数，默认：null。 num.replica.fetchers 从 leader 复制数据到 follower 的线程数，默认：1。 offset.metadata.max.bytes 与 offset 提交关联的 metadata 的最大大小，默认：4096。 offsets.commit.timeout.ms offset 提交将被延迟，直到偏移量主题的所有副本收到提交或达到此超时。这类似于生产者请求超时。默认：5000。 offsets.topic.num.partitions 偏移量提交主题的分区数量 (部署后不应再更改)，默认：50。 offsets.topic.replication.factor 副本大小，默认：3。 offsets.topic.segment.bytes 默认104857600，即 100 M。 segment 映射文件 (index) 文件大小，应该保持相对较小以便加快日志压缩和缓存负载。 queued.max.requests 阻塞网络线程之前，允许排队的请求数，默认：500。 replica.fetch.min.bytes 每个 fetch 响应所需的最小字节，默认：1。 如果字节不够，则等待 replicaMaxWaitTimeMs。 replica.lag.time.max.ms 默认：30000。 如果 follower 没有发送任何获取请求，或者至少在这段时间没有消耗到 leader 日志的结束偏移量，那么 leader 将从 isr 中删除 follower。 transaction.max.timeout.ms 默认：900000，即15分钟。 事务执行最长时间，超时则抛出异常。 unclean.leader.election.enable 默认：false。 指示是否在最后不得已的情况下启用 ISR 集中以外的副本作为 leader，即使这样做可能导致数据丢失。 zookeeper.connection.timeout.ms 默认：null。 客户端等待与 zookeeper 建立连接的最长时间。如果未设置，则使用 zookeeper.session.timeout.ms 中的值。 zookeeper.max.in.flight.requests 默认：10。 阻塞之前 consumer 将发送给 zookeeper 的未确认请求的最大数量。 group.max.session.timeout.ms 默认：1800000，即 30 分钟。 注册使用者允许的最大会话超时。超时时间越长，消费者在心跳之间处理消息的时间就越多，而检测故障的时间就越长。 group.min.session.timeout.ms 默认：6000。 注册使用者允许的最小会话超时。更短的超时导致更快的故障检测，但代价是更频繁的用户心跳，这可能会耗尽 broker 资源。 num.partitions 每个主题的默认日志分区数量，默认：1。 本文参考https://www.cnblogs.com/wangzhuxing/p/10111831.html#_label0_11 https://atbug.com/kafka-producer-config/ https://blog.csdn.net/jiecxy/article/details/53389892 本文只整理了部分有关 Kafka 的配置，仅作参考，更多的关于 broker，topic，producer 和 consumer 的配置，请参考 Kafka 官网。 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaConsumer 消费消息的基本流程","slug":"kafka-consumer","date":"2020-10-29T07:10:10.000Z","updated":"2021-01-05T07:33:06.237Z","comments":true,"path":"2020/10/29/kafka-consumer/","link":"","permalink":"http://example.com/2020/10/29/kafka-consumer/","excerpt":"","text":"如何消费数据在上一篇文章中，介绍了 KafkaProducer 如何发送数据到 Kafka，既然有数据发送，那么肯定就有数据消费，KafkaConsumer 也是 Kafka 整个体系中不可缺少的一环。 下面是一段创建 KafkaConsumer 的代码： 1234567891011121314151617181920212223242526272829public class KafkaConsumerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // 必须设置的属性 props.put(&quot;bootstrap.servers&quot;, &quot;192.168.239.131:9092&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;group.id&quot;, &quot;group1&quot;); // 可选设置的属性 props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;auto.offset.reset&quot;, &quot;earliest &quot;); props.put(&quot;client.id&quot;, &quot;test_client_id&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // 订阅主题 consumer.subscribe(Collections.singletonList(&quot;test&quot;)); while (true) &#123; // 拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); records.forEach(record -&gt; System.out.printf(&quot;topic = %s, partition = %d, offset = %d, key = %s, value = %s%n&quot;, record.topic(), record.partition(), record.offset(), record.key(), record.value())); &#125; &#125;&#125; 必须设置的属性创建 KafkaConsumer 时，必须设置的属性有 4 个： bootstrap.servers：连接 Kafka 集群的地址，多个地址以逗号分隔。 key.deserializer：消息中 key 反序列化类，需要和 KafkaProducer 中 key 序列化类相对应。 value.deserializer：消息中 value 的反序列化类，需要和 KafkaProducer 中 value 序列化类相对应。 group.id：消费者所属消费者组的唯一标识。 这里着重说一下 group.id 这个属性，KafkaConsumer 和 KafkaProducer 不一样，KafkaConsumer 中有一个 consumer group (消费者组)，由它来决定同一个 consumer group 中的消费者具体拉取哪个 partition 的数据，所以这里必须指定 group.id 属性。 订阅和取消主题 使用 subscribe () 方式订阅主题 1234// 订阅指定列表的topicpublic void subscribe(Collection&lt;String&gt; topics) &#123; subscribe(topics, new NoOpConsumerRebalanceListener());&#125; 1234// 订阅指定列表的topic，同时指定一个监听器public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; ...&#125; 1234// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行public void subscribe(Pattern pattern) &#123; subscribe(pattern, new NoOpConsumerRebalanceListener());&#125; 1234// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行，同时指定一个监听器public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; ...&#125; 使用 assign () 方式订阅主题和分区 1234// 手动将分区列表分配给consumerpublic void assign(Collection&lt;TopicPartition&gt; partitions) &#123; ...&#125; 使用示例 (仅作参考，assign() 方式的用法，应在使用时再做查询)： 123456List&lt;PartitionInfo&gt; partitionInfoList = kafkaConsumer.partitionsFor(&quot;test&quot;);if (partitionInfoList != null) &#123; for (PartitionInfo partitionInfo : partitionInfoList) &#123; kafkaConsumer.assign(Collections.singletonList(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()))); &#125;&#125; 取消主题的三种方式 123kafkaConsumer.unsubscribe();kafkaConsumer.subscribe(new ArrayList&lt;&gt;());kafkaConsumer.assign(new ArrayList&lt;TopicPartition&gt;()); 上面的三行代码作用相同，都是取消订阅，其中 unsubscribe () 方法即可以取消通过 subscribe () 方式实现的订阅，也可以取消通过 assign () 方式实现的订阅。 拉取数据KafkaConsumer 采用的是主动拉取 broker 数据进行消费的。 一般消息中间件存在推送 (push，server 推送数据给 consumer) 和拉取 (poll，consumer 主动去 server 拉取数据) 两种方式，这两种方式各有优劣。 如果是选择推送的方式，最大的阻碍就是 server 不清楚 consumer 的消费速度，如果 consumer 中执行的操作是比较耗时的，那么 consumer 可能会不堪重负，甚至会导致系统挂掉。 而采用拉取的方式则可以解决这种情况，consumer 根据自己的状态来拉取数据，可以对服务器的数据进行延迟处理。但是这种方式也有一个劣势就是 server 没有数据的时候可能会一直轮询，不过还好 KafkaConsumer 的 poll () 方法有参数允许 consumer 请求在”长轮询”中阻塞，以等待数据到达 (并且可选地等待直到给定数量的字节可用以确保传输大小)。 如何更好的消费数据文章开头处的代码展示了我们是如何消费数据的，但是代码未免过于简单，我们测试的时候这样写没有问题，但是实际开发过程中我们并不会这样写，我们会选择更加高效的方式，这里提供两种方式供大家参考。 一个 consumer group，多个 consumer，数量小于等于 partition 的数量 一个 consumer，多线程处理事件 第一种方式每个 consumer 都要维护一个独立的 TCP 连接，如果 partition 数和创建 consumer 线程的数量过多，会造成不小的系统开销。但是如果处理消息足够快速，消费性能也会提升，如果慢的话就会导致消费性能降低。 第二种方式是采用一个 consumer，多个消息处理线程来处理消息，其实在生产中，瓶颈一般是集中在消息处理上 (因为可能会插入数据到数据库，或者请求第三方 API)，所以我们采用多个线程来处理这些消息。 当然可以结合第一和第二两种方式，采用多 consumer + 多个消息处理线程来消费 Kafka 中的数据，核心代码如下： 1234567891011121314151617181920212223for (int i = 0; i &lt; consumerNum; i++) &#123; // 根据属性创建Consumer，并添加到consumer列表中 final Consumer&lt;String, byte[]&gt; consumer = consumerFactory.getConsumer(getServers(), groupId); consumerList.add(consumer); // 订阅主题 consumer.subscribe(Arrays.asList(this.getTopic())); // consumer.poll()拉取数据 BufferedConsumerRecords bufferedConsumerRecords = new BufferedConsumerRecords(consumer); getExecutor().scheduleWithFixedDelay(() -&gt; &#123; long startTime = System.currentTimeMillis(); // 进行消息处理 consumeEvents(bufferedConsumerRecords); long sleepTime = intervalMillis - (System.currentTimeMillis() - startTime); if (sleepTime &gt; 0) &#123; Thread.sleep(sleepTime); &#125; &#125;, 0, 1000, TimeUnit.MILLISECONDS);&#125; 不过这种方式不能顺序处理数据，如果你的业务是顺序处理，那么第一种方式可能更适合你。所以实际生产中请根据业务选择最适合自己的方式。 消费数据时应该考虑的问题什么是 offset？在 Kafka 中无论是 KafkarPoducer 往 topic 中写数据，还是 KafkaConsumer 从 topic 中读数据，都避免不了和 offset 打交道，关于 offset 主要有以下几个概念： Last Committed Offset：consumer group 最新一次 commit 的 offset，表示这个 consumer group 已经把 Last Committed Offset 之前的数据都消费成功了。 Current Position：consumer group 当前消费数据的 offset，也就是说，Last Committed Offset 到 Current Position 之间的数据已经拉取成功，可能正在处理，但是还未 commit。 High Watermark：HW，已经成功备份到其他 replica 中的最新一条数据的 offset，也就是说，High Watermark 与 Log End Offset 之间的数据已经写入到该 partition 的 leader 中，但是还未完全备份到其他的 replica 中，consumer 也无法消费这部分消息。 Log End Offset：LEO，记录底层日志 (log) 中的下一条消息的 offset。对 KafkaProducer 来说，就是即将插入下一条消息的 offset。 每个 Kafka 副本对象都有两个重要的属性：HW 和 LEO。注意是所有的副本，而不只是 leader 副本。关于这两者更详细解释，参考：[Kafka 的 High Watermark 与 leader epoch 的讨论 对于消费者而言，我们更多时候关注的是消费完成之后如何和服务器进行消费确认，告诉服务器这部分数据我已经消费过了。 这里就涉及到了 2 个 offset，一个是 Current Position，一个是处理完毕向服务器确认的 Last Committed Offset。显然，异步模式下 Last Committed Offset 是落后于 Current Position 的。如果 consumer 挂掉了，那么下一次消费数据又只会从 Last Committed Offset 的位置拉取数据，就会导致数据被重复消费。 如何选择 offset 的提交策略？Kafka 提供了三种提交 offset 的方式。 1. 自动提交 1234// 自动提交，默认trueprops.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// 设置自动每1s提交一次props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 2.手动同步提交 1kafkaConsumer.commitSync(); 3.手动异步提交 1kafkaConsumer.commitAsync(); 上面说了，既然异步提交 offset 可能会重复消费，那么我使用同步提交是否就可以解决数据重复消费的问题呢？我只能说 too young, too sample。且看如下代码： 1234567while (true) &#123; ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(100)); records.forEach(record -&gt; &#123; insertIntoDB(record); kafkaConsumer.commitSync(); &#125;);&#125; 很明显不行，因为 insertIntoDB () 和 kafkaConsumer.commitSync () 两个方法做不到原子操作，如果 insertIntoDB () 成功了，但是提交 offset 的时候 KafkaConsumer 挂掉了，然后服务器重启，仍然会导致重复消费问题。 是否需要做到不重复消费？只要保证处理消息和提交 offset 的操作是原子操作，就可以做到不重复消费。我们可以自己管理 committed offset，而不让 Kafka 来进行管理。 比如如下使用方式： 1.如果消费的数据刚好需要存储在数据库，那么可以把 offset 也存在数据库，就可以在一个事物中提交这两个结果，保证原子操作。 2.借助搜索引擎，把 offset 和数据一起放到索引里面，比如 Elasticsearch。 每条记录都有自己的 offset，所以如果要管理自己的 offset 还得要做下面事情： 1.设置 enable.auto.commit 为 false； 2.使用每个 ConsumerRecord 提供的 offset 来保存消费的位置； 3.在重新启动时使用 seek (TopicPartition partition, long offset) 恢复上次消费的位置。 通过上面的方式就可以在消费端实现 ”Exactly Once” 的语义，即保证只消费一次。但是是否真的需要保证不重复消费呢？这个得看具体业务，如果重复消费数据对整体有什么影响，然后再来决定是否需要做到不重复消费。 再均衡 (reblance) 时怎么办？再均衡是指分区的所属权从一个消费者转移到另一个消费者的行为，再均衡期间，消费者组内的消费者无法读取消息。为了更精确的控制消息的消费，我们可以在订阅主题的时候，通过指定监听器的方式来设定发生再均衡动作前后的一些准备或者收尾的动作。 1234567891011kafkaConsumer.subscribe(Collections.singletonList(&quot;test&quot;), new ConsumerRebalanceListener() &#123; @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; // 再均衡之前和消费者停止读取消息之后被调用 &#125; @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; // 重新分配分区之后和消费者开始消费之前被调用 &#125;&#125;); 具体如何操作，得根据具体的业务逻辑来实现，如果消息比较重要，你可以在再均衡的时候处理 offset，如果不够重要，你可以什么都不做。 无法消费的数据怎么办？可能由于你的业务逻辑有些数据没法消费，这个时候怎么办？同样的还是的看你认为这个数据有多重要或者多不重要，如果重要可以记录日志，把它存入文件或者数据库，以便于稍候进行重试或者定向分析。如果不重要就当做什么事情都没有发生好了。 实际开发中我的处理方式我开发的项目中，用到 Kafka 的其中一个地方是消息通知 (谁给你发了消息，点赞，评论等)，大概的流程就是用户在 client 端做了某些操作，就会发送数据到 Kafka，然后把这些数据进行一定的处理之后插入到 HBase 中。 其中采用了 N consumer thread + N Event Handler 的方式来消费数据，并采用自动提交 offset。对于无法消费的数据往往只是简单处理下，打印下日志以及消息体 (无法消费的情况非常非常少)。 得益于 HBase 的多 version 控制，即使是重复消费了数据也无关紧要。这样做没有去避免重复消费的问题主要是基于以下几点考虑： 1.重复消费的概率较低，服务器整体性能稳定。 2.即便是重复消费了数据，入库了 HBase，获取数据也是只有一条，不影响结果的正确性。 3.有更高的吞吐量。 4.编程简单，不用单独去处理以及保存 offset。 本文参考http://generalthink.github.io/2019/05/06/kafka-consumer-use/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaProducer 部分源码解析","slug":"kafka-producer","date":"2020-10-26T06:15:52.000Z","updated":"2021-01-05T07:33:19.544Z","comments":true,"path":"2020/10/26/kafka-producer/","link":"","permalink":"http://example.com/2020/10/26/kafka-producer/","excerpt":"","text":"先来看一段创建 KafkaProducer 的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class KafkaProducerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // bootstrap.servers 必须设置 props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.239.131:9092&quot;); // key.serializer 必须设置 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // value.serializer 必须设置 props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // client.id props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;client-0&quot;); // retries props.put(ProducerConfig.RETRIES_CONFIG, 3); // acks props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;); // max.in.flight.requests.per.connection props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1); // linger.ms props.put(ProducerConfig.LINGER_MS_CONFIG, 100); // batch.size props.put(ProducerConfig.BATCH_SIZE_CONFIG, 10240); // buffer.memory props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10240); KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props); // 指定topic，key，value ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;test1&quot;, &quot;key1&quot;, &quot;value1&quot;); // 异步发送 kafkaProducer.send(record, (recordMetadata, exception) -&gt; &#123; if (exception != null) &#123; // 发送失败的处理逻辑 exception.printStackTrace(); &#125; else &#123; // 发送成功的处理逻辑 System.out.println(recordMetadata.topic()); &#125; &#125;); // 同步发送 // kafkaProducer.send(record).get(); // 关闭Producer kafkaProducer.close(); &#125;&#125; 主要流程图 简要说明： 1.new KafkaProducer () 后，创建一个后台线程 KafkaThread (实际运行线程是 Sender，KafkaThread 是对 Sender 的封装) 扫描 RecordAccumulator 中是否有消息； 2.调用 kafkaProducer.send () 发送消息，实际是将消息保存到 RecordAccumulator 中，实际上就是保存到一个 Map 中 (ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)，这条消息会被记录到同一个记录批次 (相同主题相同分区算同一个批次) 里面，这个批次的所有消息会被发送到相同的主题和分区上； 3.后台的独立线程扫描到 RecordAccumulator 中有消息后，会将消息发送到 Kafka 集群中 (不是一有消息就发送，而是要看消息是否 ready)； 4.如果发送成功 (消息成功写入 Kafka)，就返回一个 RecordMetaData 对象，它包换了主题和分区信息，以及记录在分区里的偏移量等信息； 5.如果写入失败，就会返回一个错误，生产者在收到错误之后会尝试重新发送消息 (如果允许的话，此时会将消息在保存到 RecordAccumulator 中)，达到重试次数之后如果还是失败就返回错误消息。 缓存器的创建123456789101112this.accumulator = new RecordAccumulator(logContext, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), this.compressionType, lingerMs(config), retryBackoffMs, deliveryTimeoutMs, metrics, PRODUCER_METRIC_GROUP_NAME, time, apiVersions, transactionManager, new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME)); 后台线程的创建12345678910111213141516171819202122this.sender = newSender(logContext, kafkaClient, this.metadata);String ioThreadName = NETWORK_THREAD_PREFIX + &quot; | &quot; + clientId;this.ioThread = new KafkaThread(ioThreadName, this.sender, true);this.ioThread.start();KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient( new Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), this.metrics, time, &quot;producer&quot;, channelBuilder, logContext), metadata, clientId, maxInflightRequests, producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG), producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG), producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG), producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG), requestTimeoutMs, ClientDnsLookup.forConfig(producerConfig.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)), time, true, apiVersions, throttleTimeSensor, logContext); 上述代码中，构造了一个 KafkaClient 负责和 broker 通信，同时构造一个 Sender 并启动一个异步线程，这个线程会被命名为：kafka-producer-network-thread | $&#123;clientId&#125;，如果你在创建 producer 的时候指定 client.id 的值为 myclient，那么线程名称就是 kafka-producer-network-thread | myclient。 发送消息 (缓存消息)发送消息有同步发送以及异步发送两种方式，我们一般不使用同步发送，毕竟太过于耗时，使用异步发送的时候可以指定回调函数，当消息发送完成的时候 (成功或者失败) 会通过回调通知生产者。 同步 send： 123public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record) &#123; return send(record, null);&#125; 异步 send： 12345public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; // intercept the record, which can be potentially modified; this method does not throw exceptions ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record); return doSend(interceptedRecord, callback);&#125; 可以看出，同步和异步实际上调用的是同一个方法，同步发送时，设置回调函数为 null。 消息发送之前，会先对 key 和 value 进行序列化： 12345678910111213141516byte[] serializedKey;try &#123; serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert key of class &quot; + record.key().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in key.serializer&quot;, cce);&#125;byte[] serializedValue;try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert value of class &quot; + record.value().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in value.serializer&quot;, cce);&#125; 计算分区： 1int partition = partition(record, serializedKey, serializedValue, cluster); 发送消息，实际上是将消息缓存起来，核心代码如下： 12RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs); RecordAccumulator 的核心数据结构是 ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;，会将相同 topic 相同 partition 的数据放到一个 Deque (双向队列) 中，这也是我们之前提到的同一个记录批次里面的消息会发送到同一个主题和分区的意思。append () 方法的核心源码如下： 123456789101112131415161718192021// 从batchs(ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)中，根据主题分区获取对应的队列，如果没有则new ArrayDeque&lt;&gt;返回Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);// 计算同一个记录批次占用空间大小，batchSize根据batch.size参数决定int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound( maxUsableMagic, compression, key, value, headers));// 为同一个topic，partition分配buffer，如果同一个记录批次的内存不足，那么会阻塞maxTimeToBlock(max.block.ms参数)这么长时间ByteBuffer buffer = free.allocate(size, maxTimeToBlock);synchronized (dq) &#123; // 创建MemoryRecordBuilder，通过buffer初始化appendStream(DataOutputStream)属性 MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic); ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds()); // 将key，value写入到MemoryRecordsBuilder中的appendStream(DataOutputStream)中 FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds())); // 将需要发送的消息放入到队列中 dq.addLast(batch);&#125; 发送消息到 Kafka上面已经将消息存储 RecordAccumulator 中去了，现在看看怎么发送消息。前面提到创建 KafkaProducer 的时候，会启动一个异步线程去从 RecordAccumulator 中取得消息然后发送到 Kafka，发送消息的核心代码在 Sender 中，它实现了 Runnable 接口并在后台一直运行处理发送请求并将消息发送到合适的节点，直到 KafkaProducer 被关闭。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata * requests to renew its view of the cluster and then sends produce requests to the appropriate nodes. */public class Sender implements Runnable &#123; /** * The main run loop for the sender thread */ public void run() &#123; // main loop, runs until close is called while (running) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // okay we stopped accepting requests but there may still be // requests in the transaction manager, accumulator or waiting for acknowledgment, // wait until these are completed. while (!forceClose &amp;&amp; ((this.accumulator.hasUndrained() || this.client.inFlightRequestCount() &gt; 0) || hasPendingTransactionalRequests())) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // Abort the transaction if any commit or abort didn&#x27;t go through the transaction manager&#x27;s queue while (!forceClose &amp;&amp; transactionManager != null &amp;&amp; transactionManager.hasOngoingTransaction()) &#123; if (!transactionManager.isCompleting()) &#123; log.info(&quot;Aborting incomplete transaction due to shutdown&quot;); transactionManager.beginAbort(); &#125; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; if (forceClose) &#123; // We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on // the futures. if (transactionManager != null) &#123; log.debug(&quot;Aborting incomplete transactional requests due to forced shutdown&quot;); transactionManager.close(); &#125; log.debug(&quot;Aborting incomplete batches due to forced shutdown&quot;); this.accumulator.abortIncompleteBatches(); &#125; &#125;&#125; KafkaProducer 的关闭方法有2个：close () 以及 close (Duration timeout)，close (long timeout, TimeUnit timUnit) 已被弃用，其中 timeout 参数的意思是等待生产者完成任何待处理请求的最长时间，第一种方式的 timeout 为 Long.MAX_VALUE 毫秒，如果采用第二种方式关闭，当 timeout = 0 的时候则表示强制关闭，直接关闭 Sender (设置 running = false)。 Send 中，runOnce () 方法，跳过对 transactionManager 的处理，查看发送消息的主要流程： 123456long currentTimeMs = time.milliseconds();// 将记录批次转移到每个节点的生产请求列表中long pollTimeout = sendProducerData(currentTimeMs);// 轮询进行消息发送client.poll(pollTimeout, currentTimeMs); 首先，查看 sendProducerData (currentTimeMs) 方法，它的核心逻辑在 sendProduceRequest (batches, now) 方法中： 123456789101112131415161718192021222324252627282930313233for (ProducerBatch batch : batches) &#123; TopicPartition tp = batch.topicPartition; // 将ProducerBatch中MemoryRecordsBuilder转换为MemoryRecords(发送的数据就在这里面) MemoryRecords records = batch.records(); // down convert if necessary to the minimum magic used. In general, there can be a delay between the time // that the producer starts building the batch and the time that we send the request, and we may have // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use // the new message format, but found that the broker didn&#x27;t support it, so we need to down-convert on the // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may // not all support the same message format version. For example, if a partition migrates from a broker // which is supporting the new magic version to one which doesn&#x27;t, then we will need to convert. if (!records.hasMatchingMagic(minUsedMagic)) records = batch.records().downConvert(minUsedMagic, 0, time).records(); produceRecordsByPartition.put(tp, records); recordsByPartition.put(tp, batch);&#125;ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout, produceRecordsByPartition, transactionalId);// 消息发送完成时的回调(消息发送失败后，在handleProduceResponse中处理)RequestCompletionHandler callback = new RequestCompletionHandler() &#123; public void onComplete(ClientResponse response) &#123; handleProduceResponse(response, recordsByPartition, time.milliseconds()); &#125;&#125;;// 根据参数构造ClientRequest，此时需要发送的消息在requestBuilder中ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, requestTimeoutMs, callback);// 将clientRequest转换成Send对象(Send.java，包含了需要发送数据的buffer)，给KafkaChannel设置该对象，记住这里还没有发送数据client.send(clientRequest, now); 在没有指定 KafkaClient 时，client.send (clientRequest, now) 方法，实际就是 NetworkClient.send (ClientRequest request, long now) 方法，所有的请求 (无论是 producer 发送消息的请求，还是获取 metadata 的请求) 都是通过该方法设置对应的 Send 对象： 1Send send = request.toSend(destination, header); 需要知道的是，上面只是设置了发送消息所需要准备的内容。 接下来，查看 client.poll (pollTimeout, currentTimeMs) 方法，进入到发送消息的主流程，发送消息的核心代码最终可以定位到 Selector 的 pollSelectionKeys (Set&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos) 方法中，代码如下： 12345678910111213141516/* if channel is ready write to any sockets that have space in their buffer and for which we have data */if (channel.ready() &amp;&amp; key.isWritable() &amp;&amp; !channel.maybeBeginClientReauthentication( () -&gt; channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos)) &#123; Send send; try &#123; // 底层实际调用的是java8 GatheringByteChannel的write方法 send = channel.write(); &#125; catch (Exception e) &#123; sendFailed = true; throw e; &#125; if (send != null) &#123; this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); &#125;&#125; 就这样，我们的消息就发送到了 broker 中了，发送流程分析完毕，注意，这个是完美发送的情况。但是总会有发送失败的时候 (消息过大或者没有可用的 leader 等)，那么发送失败后重发又是在哪里完成的呢？还记得上面的回调函数吗，没错，就是在回调函数这里设置的，先来看下回调函数源码： 12345678910111213141516171819202122232425262728293031323334/** * Handle a produce response */private void handleProduceResponse(ClientResponse response, Map&lt;TopicPartition, ProducerBatch&gt; batches, long now) &#123; RequestHeader requestHeader = response.requestHeader(); long receivedTimeMs = response.receivedTimeMs(); int correlationId = requestHeader.correlationId(); if (response.wasDisconnected()) &#123; // 如果是网络断开则构造Errors.NETWORK_EXCEPTION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION), correlationId, now, 0L); &#125; else if (response.versionMismatch() != null) &#123; // 如果是版本不匹配，则构造Errors.UNSUPPORTED_VERSION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.UNSUPPORTED_VERSION), correlationId, now, 0L); &#125; else &#123; // if we have a response, parse it(如果存在response就返回正常的response) if (response.hasResponse()) &#123; ProduceResponse produceResponse = (ProduceResponse) response.responseBody(); for (Map.Entry&lt;TopicPartition, ProduceResponse.PartitionResponse&gt; entry : produceResponse.responses().entrySet()) &#123; TopicPartition tp = entry.getKey(); ProduceResponse.PartitionResponse partResp = entry.getValue(); ProducerBatch batch = batches.get(tp); completeBatch(batch, partResp, correlationId, now, receivedTimeMs + produceResponse.throttleTimeMs()); &#125; this.sensors.recordLatency(response.destination(), response.requestLatencyMs()); &#125; else &#123; // this is the acks = 0 case, just complete all requests(如果acks=0，那么则构造Errors.NONE的响应，因为这种情况只需要发送不需要响应结果) for (ProducerBatch batch : batches.values()) &#123; completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NONE), correlationId, now, 0L); &#125; &#125; &#125;&#125; 在 completeBatch () 方法中我们主要关注失败的逻辑处理，核心源码如下： 12345678910111213141516171819202122232425262728293031/** * Complete or retry the given batch of records. * * @param batch The record batch * @param response The produce response * @param correlationId The correlation id for the request * @param now The current POSIX timestamp in milliseconds */private void completeBatch(ProducerBatch batch, ProduceResponse.PartitionResponse response, long correlationId, long now, long throttleUntilTimeMs) &#123; Errors error = response.error; if (error == Errors.MESSAGE_TOO_LARGE &amp;&amp; batch.recordCount &gt; 1 &amp;&amp; !batch.isDone() &amp;&amp; (batch.magic() &gt;= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) &#123; // If the batch is too large, we split the batch and send the split batches again. We do not decrement // the retry attempts in this case.(如果发送的消息太大，需要重新进行分割发送) this.accumulator.splitAndReenqueue(batch); maybeRemoveAndDeallocateBatch(batch); this.sensors.recordBatchSplit(); &#125; else if (error != Errors.NONE) &#123; // 发生了错误，如果此时可以retry(retry次数未达到限制以及产生的异常是RetriableException) if (canRetry(batch, response, now)) &#123; if (transactionManager == null) &#123; // 把需要重试的消息放入队列中，等待重试，实际就是调用deque.addFirst(batch) reenqueueBatch(batch, now); &#125; ... &#125; ... &#125;&#125; 以上，就是 KafkaProducer 发送消息的流程。 补充：分区算法在发送消息前，调用的计算分区方法如下： 123456789101112/** * computes partition for given record. * if the record has partition returns the value otherwise * calls configured partitioner class to compute the partition. */private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition : partitioner.partition( record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; 如果在创建 ProducerRecord 的时候，指定了 partition，则使用指定的，否则调用配置的 partitioner 类来计算分区。 如果没有配置自定义的分区器，Kafka 默认使用 org.apache.kafka.clients.producer.internals.DefaultPartitioner，源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * The default partitioning strategy: * &lt;ul&gt; * &lt;li&gt;If a partition is specified in the record, use it * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key * &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion */public class DefaultPartitioner implements Partitioner &#123; private final ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = new ConcurrentHashMap&lt;&gt;(); public void configure(Map&lt;String, ?&gt; configs) &#123;&#125; /** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; // 如果key为null，则使用Round Robin算法 int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition(根据key进行散列，使用murmur2算法) return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125; private int nextValue(String topic) &#123; AtomicInteger counter = topicCounterMap.get(topic); if (null == counter) &#123; counter = new AtomicInteger(ThreadLocalRandom.current().nextInt()); AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter); if (currentCounter != null) &#123; counter = currentCounter; &#125; &#125; return counter.getAndIncrement(); &#125; public void close() &#123;&#125;&#125; DefaultPartitioner 中对于分区的算法有两种情况： 1.如果键值为 null，那么记录键随机地发送到主题内各个可用的分区上。分区器使用轮询 (Round Robin) 算法键消息均衡地分布到各个分区上。 2.如果键不为 null，那么 Kafka 会对键进行散列 (使用 Kafka 自己的散列算法，即使升级 java 版本，散列值也不会发生变化) ，然后根据散列值把消息映射到特定的分区上。应该注意的是，同一个键总是被映射到同一个分区上 (如果分区数量发生了变化则不能保证)，映射的时候会使用主题所有的分区，而不仅仅是可用分区，所以如果写入数据分区是不可用的，那么就会发生错误，当然这种情况很少发生。 当然，如果你想要实现自定义分区，那么只需要实现 Partitioner 接口即可： 123456789101112131415161718192021222324/** * 将key的hash值，对分区总数取余，以确定消息发送到哪个分区 */public class KeyPartitioner implements Partitioner &#123; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; Integer numPartitions = cluster.partitionCountForTopic(topic); if (keyBytes == null) &#123; throw new InvalidRecordException(&quot;key can not be null&quot;); &#125; return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 然后，使用 partitioner.class 参数，指定你自定义的分区器的路径： 1props.put(&quot;partitioner.class&quot;, &quot;cn.xisun.partitioner.KeyPartitioner&quot;); 本文参考https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"什么是 Kafka","slug":"kafka-introduce","date":"2020-10-23T07:58:39.000Z","updated":"2021-01-05T07:33:11.926Z","comments":true,"path":"2020/10/23/kafka-introduce/","link":"","permalink":"http://example.com/2020/10/23/kafka-introduce/","excerpt":"","text":"分布式分布式系统由多个运行的计算机系统组成，所有这些计算机在一个集群中一起工作，对终端用户来讲只是一个单一节点。 Kafka 也是分布式的，因为它在不同的节点 (又被称为 broker) 上存储，接受以及发送消息，这样做的好处是具有很高的可扩展性和容错性。 水平可扩展性在这之前，先看看什么是垂直可扩展，比如你有一个传统的数据库服务器，它开始过度负载，解决这个问题的办法就是给服务器加配置 (cpu，内存，SSD)，这就叫做垂直扩展。但是这种方式存在两个巨大的劣势： 1.硬件存在限制，不可能无限的添加机器配置。 2.它需要停机时间，通常这是很多公司无法容忍的。 水平可扩展就是通过添加更多的机器来解决同样的问题，添加新机器不需要停机，而且集群中也不会对机器的数量有任何的限制。但问题在于并非所有系统都支持水平可伸缩性，因为它们不是设计用于集群中 (在集群中工作会更加复杂)。 容错性非分布式系统中容易最致命的问题就是单点失败，如果你唯一的服务器挂掉了，那么我相信你会很崩溃。 而分布式系统的设计方式就是可以以配置的方式来容许失败。比如在 5 个节点的 Kafka 集群中，即使其中两个节点挂掉了，你仍然可以继续工作。 需要注意的是，容错与性能直接相关，你的系统容错程度越高，性能就越差。 提交日志 (commit log)提交日志 (也被称为预写日志或者事物日志) 是仅支持附加的持久有序数据结构，你无法修改或者删除记录，它从左往右读并且保证日志的顺序。 是不是觉得 Kafka 的数据结构如此简单? 是的，从很多方面来讲，这个数据结构就是 Kafka 的核心。这个数据结构的记录是有序的，而有序的数据可以确保我们的处理流程。这两个在分布式系统中都是极其重要的问题。 Kafka 实际上将所有消息存储到磁盘并在数据结构中对它们进行排序，以便利用顺序磁盘读取。 1.读取和写入都是常量时间 O(1) (当确定了 record id)，与磁盘上其他结构的 O(log N)操作相比是一个巨大的优势，因为每个磁盘搜索都很耗时。 2.读取和写入不会相互影响，写不会锁住读，反之亦然。 这两点有着巨大的优势， 因为数据大小与性能完全分离。无论你的服务器上有 100 KB 还是 100 TB 的数据，Kafka 都具有相同的性能。 如何工作生产者消费者模式：生产者 (producer) 发送消息 (record) 到 Kafka 服务器 (broker)，这些消息存储在主题 (topic) 中，然后消费者 (consumer) 订阅该主题，接受新消息后并进行处理。 随着消息的越来越多，topic 也会越来越大，为了获得更好的性能和可伸缩性，可以在 topic 下建立多个更小的分区 (partition)，在发送消息时，可以根据实际情况，对消息进行分类，同一类的消息发送到同一个 partition (比如存储不同用户发送的消息，可以根据用户名的首字母进行分区匹配)。Kafka 保证 partition 内的所有消息都按照它们的顺序排序，区分特定消息的方式是通过其偏移量 (offset)，你可以将其视为普通数组索引，即为分区中的每个新消息递增的序列号。 Kafka 遵守着愚蠢的 broker 和聪明的 consumer 的准则。这意味着 Kafka 不会跟踪消费者读取了哪些记录并删除它们，而是会将它们存储一定的时间 (比如 1 天，以 log.retention 开头的来决定日志保留时间)，直到达到某个阈值。消费者自己轮询 Kafka 的新消息并且告诉它自己想要读取哪些记录，这允许它们按照自己的意愿递增/递减它们所处的偏移量，从而能够重放和重新处理事件。 需要注意的是消费者是属于消费者组的 (在创建 consumer 时，必须指定其所属的消费者组的 group.id)，消费者组有一个或多个消费者。为了避免两个进程读取同样的消息两次，每个 partition 只能被一个消费者组中的一个消费者访问。 持久化到硬盘正如之前提到的，Kafka 实际上是将所有记录存储到硬盘而不在 RAM 中保存任何内容，这背后有很多优化使得这个方案可行。 1.Kafka 有一个将消息分组的协议，这允许网络请求将消息组合在一起并减少网络开销，服务器反过来一次性保留大量消息，消费者一次获取大量线性块。 2.磁盘上线性读写非常快，现代磁盘非常慢的原因是由于大量磁盘寻址，但是在大量的线性操作中不是问题。 3.操作系统对线性操作进行了大量优化，通过预读 (预取大块多次) 和后写 (将小型逻辑写入组成大型物理写入) 技术。 4.操作系统将磁盘文件缓存在空闲 RAM 中。这称为 page cache，而 Kafka 的读写都大量使用了 page cache： ​ ① 写消息的时候消息先从 java 到 page cache，然后异步线程刷盘，消息从 page cache 刷入磁盘； ​ ② 读消息的时候先从 page cache 找，有就直接转入 socket，没有就先从磁盘 load 到 page cache，然后直接从 socket 发出去。 5.由于 Kafka 在整个流程 (producer → broker → consumer) 中以未经修改的标准化二进制格式存储消息，因此它可以使用零拷贝优化。那时操作系统将数据从 page cache 直接复制到 socket，有效地完全绕过了 Kafka broker。 所有这些优化都使 Kafka 能够以接近网络的速度传递消息。 数据分发和复制下面来谈谈 Kafka 如何实现容错以及它如何在节点之间分配数据。 为了使得一个 broker 挂掉的时候，数据还能得以保留，分区 (partition) 数据在多个 broker 中复制。 在任何时候，一个 broker 拥有一个 partition，应用程序读取/写入都要通过这个节点，这个节点叫做 partition leader。它将收到的数据复制到 N 个其他 broker，这些接收数据的 broker 叫做 follower，follower 也存储数据，一旦 leader 节点死掉的时候，它们就准备竞争上岗成为 leader。 这可以保证你成功发布的消息不会丢失，通过选择更改副本因子，你可以根据数据的重要性来交换性能以获得更强的持久性保证。 这样如果 leader 挂掉了，那么其中一个 follower 就会接替它称为 leader。包括 leader 在内的总副本数就是副本因子 (创建 topic 时，使用 --replication-factor 参数指定)，上图有 1 个 leader，2 个 follower，所以副本因子就是 3。 但是你可能会问：producer 或者 consumer 怎么知道 partition leader 是谁？ 对生产者/消费者对分区的写/读请求，它们需要知道分区的 leader 是哪一个，对吧？这个信息肯定是可以获取到的，Kafka 使用 ZooKeeper 来存储这些元数据。 什么是 ZooKeeperZooKeeper 是一个分布式键值存储。它针对读取进行了高度优化，但写入速度较慢。它最常用于存储元数据和处理集群的机制 (心跳，分发更新/配置等)。 它允许服务的客户 (Kafka broker) 订阅并在发生变更后发送给他们，这就是 Kafka 如何知道何时切换分区领导者。ZooKeeper 本身维护了一个集群，所以它就有很高的容错性，当然它也应该具有，毕竟 Kafka 很大程度上是依赖于它的。 ZooKeeper 用于存储所有的元数据信息，包括但不限于如下几项： 消费者组每个分区的偏移量 (现在客户端在单独的 Kafka topic 上存储偏移量) ACL —— 权限控制 生产者/消费者的流量控制——每秒生产/消费的数据大小。参考：Kafka - 流量控制 Quota 功能 partition leader 以及它们的健康信息 那么 producer/consumer 是如何知道谁是 partition leader 的呢？ 生产者和消费者以前常常直接连接 ZooKeeper 来获取这些信息，但是 Kafka 从 0.8 和 0.9 版本开始移除了这种强耦合关系。客户端直接从 Kafka broker 获取这些元数据，而让 Kafka broker 从 ZooKeeper 那里获取这些元数据。 更多 ZooKeeper 的讲解参考：漫画：什么是 ZooKeeper？ 流式处理 (Streaming)在 Kafka 中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理并生成数据流以输出到其他主题 (或者外部服务，数据库，容器等等)。 什么是数据流呢？首先，数据流是无边界数据集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的，是因为随着时间推移，新的记录会不断加入进来。比如信用卡交易，股票交易等事件都可以用来表示数据流。 我们可以使用 producer/consumer 的 API 直接进行简单处理，但是对于更加复杂的转换，比如将流连接到一起，Kafka 提供了集成 Stream API 库。 这个 API 是在你自己的代码中使用的，它并不是运行在 broker 上，它的工作原理和 consumer API 类似，可帮助你在多个应用程序 (类似于消费者组) 上扩展流处理工作。 无状态处理流的无状态处理是确定性处理，其不依赖于任何外部条件，对于任何给定的数据，将始终生成与其他任何内容无关的相同输出。举个例子，我们要做一个简单的数据转换—“zhangsan” → “Hello, zhangsan” 流-表二义性重要的是要认识到流和表实质上是一样的，流可以被解释称为表，表也可以被解释称为流。 流作为表流可以解释为数据的一系列更新，聚合后的结果就是表的最终结果，这项技术被称为事件溯源 (Event Sourcing)。 如果你了解数据库备份同步，你就会知道它们的技术实现被称为流式复制—将对表的每个更改都发送报副本服务器。比如 redis 中的 AOF 以及 Mysql 中的 binlog。 Kafka 流可以用相同的方式解释 - 当累积形成最终状态时的事件。此类流聚合保存在本地 RocksDB 中 (默认情况下)，被称为 KTable。 表作为流可以将表视为流中每个键的最新值的快照。与流记录可以生成表一样，表更新可以生成更改日志流。 有状态处理我们在 java 中常用的一些操作比如 map() 或者 filter() 是没有状态的，它不会要求你保留任何原始数据。但是现实中，大多数的操作都是有状态的 (比如 count())，因为这需要你存储当前累计的状态。 在流处理器上维护状态的问题是流处理器可能会失败！你需要在哪里保持这种状态才能容错？ 一种简单的方法是简单地将所有状态存储在远程数据库中，并通过网络连接到该存储，这样做的问题是大量的网络带宽会使得你的应用程序变慢。一个更微妙但重要的问题是你的流处理作业的正常运行时间将与远程数据库紧密耦合，并且作业将不是自包含的 (其他 team 更改数据库可能会破坏你的处理)。 那么什么是更好的办法呢？ 回想一下表和流的二元性。这允许我们将流转换为与我们的处理位于同一位置的表。它还为我们提供了一种处理容错的机制—通过将流存储在 Kafka broker 中。 流处理器可以将其状态保持在本地表 (例如 RocksDB) 中，该表将从输入流 (可能在某些任意转换之后) 更新。当进程失败时，它可以通过重放流来恢复其数据。 你甚至可以将远程数据库作为流的生产者，有效地广播用于在本地重建表的更改日志。 KSQL通常，我们不得不使用 JVM 语言编写流处理，因为这是唯一的官方 Kafka Streams API 客户端。2018 年 4 月，KSQL 作为一项新特性被发布，它允许你使用熟悉的类似 SQL 的语言编写简单的 stream jobs。你安装了 KSQL 服务器并通过 CLI 以交互方式查询以及管理。它使用相同的抽象 (KStream 和 KTable)，保证了 Streams API 的相同优点 (可伸缩性，容错性)，并大大简化了流的工作。 这听起来可能不是很多，但在实践中对于测试内容更有用，甚至允许开发之外的人 (例如产品所有者) 使用流处理，可以看看 Confluent 提供的这篇关于 ksql 的使用。 什么时候使用 kafka正如我们已经介绍的那样，Kafka 允许你通过集中式介质获取大量消息并存储它们，而不必担心性能或数据丢失等问题。 这意味着它非常适合用作系统架构的核心，充当连接不同应用程序的集中式媒体。Kafka 可以成为事件驱动架构的中心部分，使你可以真正地将应用程序彼此分离。 Kafka 允许你轻松地分离不同 (微) 服务之间的通信。使用 Streams API，现在可以比以往更轻松地编写业务逻辑，从而丰富 Kafka 主题数据以供服务使用。可能性很大，我恳请你探讨公司如何使用 Kafka。 总结Apache Kafka 是一个分布式流媒体平台，每天可处理数万亿个事件。Kafka 提供低延迟，高吞吐量，容错的发布和订阅管道，并能够处理事件流。我们回顾了它的基本语义 (producer，broker，consumer，topic)，了解了它的一些优化 (page cache)，通过复制数据了解了它的容错能力，并介绍了它不断增长的强大流媒体功能。Kafka 已经在全球数千家公司中大量采用，其中包括财富 500 强企业中的三分之一。随着 Kafka 的积极开发和最近发布的第一个主要版本 1.0 (2017 年 11 月 1 日)，有预测这个流媒体平台将会与关系数据库一样，是数据平台的重要核心。我希望这篇介绍能帮助你熟悉 Apache Kafka。 本文参考http://generalthink.github.io/2019/02/27/introduction-of-kafka/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"使用 hexo 搭建 github 博客","slug":"hexo-blog","date":"2020-10-23T03:33:51.000Z","updated":"2021-01-27T06:29:29.019Z","comments":true,"path":"2020/10/23/hexo-blog/","link":"","permalink":"http://example.com/2020/10/23/hexo-blog/","excerpt":"","text":"使用工具版本默认已经安装 node.js 和 git。 123git version: git version 2.27.0.windows.1npm version: 6.14.7hexo version: 4.2.0 git 客户端与 github 建立 SSH 连接1Please make sure you have the correct access rights and the repository exists. 当 git 客户端出现以上提示时，说明 SSH 连接过期，需要重新建立连接。参考如下方式： 先查看下 name 和 email 123456# 查看user的name和email$ git config user.name$ git config user.email# 如果没设置，按如下命令设置$ git config --global user.name &#123;$yourname&#125;$ git config --global user.email &#123;$youremail&#125; 删除 .ssh 文件夹下的 known_hosts，路径为：C:\\Users\\&#123;$userrname&#125;\\.ssh git bash 输入命令 1$ ssh-keygen -t rsa -C &#123;$youremail&#125; 一直按回车，等结束后，.ssh 文件夹下会生成两个文件：id_rsa 和 id_rsa.pub，将 id_rsa.pub 的内容全部复制。 登录个人 github 账户，进入 Settings → SSH and GPG keys，点击 New SSH key，将复制的内容粘贴到 Key 里，点击 Add SSH key。 git bash 输入命令 1$ ssh -T git@github.com 在弹出的确定对话框输入：yes。 hexo 安装在 git bash 中依次输入以下命令： 123456$ npm install hexo-cli -g$ cd f: # 可以是任何路径$ hexo init blog$ cd blog # 进入blog目录$ npm install$ npm install hexo-deployer-git --save 命令执行完成后，会在 F:\\ 目录下，多一个 blog 文件夹。 修改 _config.yml 文件修改 blog 根目录下的 _config.yml 文件，将 deploy 节点修改为如下内容： 1234deploy: type: git repo: git@github.com:&#123;$yourname&#125;/&#123;$yourname&#125;.github.io.git branch: master 说明：_config.yml 文件的配置均为 [key: value] 形式，value 前面必须要有一个空格。 然后在 git bash 中输入以下命令，发布博客： 1$ hexo deploy 访问自己的博客博客地址：https://&#123;$yourname&#125;.github.io/ 写一个自己的博客hexo 的项目结构是在网站根目录的 source\\_posts 目录下存放你的博客文档，以 .md 文档格式存储，默认已存在一个 hello-world.md 文章。 新建文章 1$ hexo new &lt;title&gt; 会在 blog 的 source\\_posts 目录下，新建一个名叫 &lt;title&gt;.md 文章，如： 12INFO Validating configINFO Created: F:\\blog\\source\\_posts\\tesss.md 之后，在文章中添加自己的内容即可，建议使用 Typora 编辑，其语法参考：如何使用 markdown？ 发布文章 1234$ hexo clean # 清楚缓存$ hexo generate # 生成静态页面$ hexo server # 本地发布，浏览器输入localhost:4000即可访问博客$ hexo deploy # 将public中的静态页面复制到.deploy_git文件夹中，并提交到github 至此，你的第一个自己的博客发布完成。 说明：以上 hexo 的命令，都要在 F:\\blog 目录下执行。 修改博客的 themes如果想修改自己博客的 themes，可以下载好想要的，然后拷贝到 blog 的 themes 目录下，然后修改 _config.yml 文件，将 theme 节点的值，修改为你下载好的 themes 的名称，如： 1theme: next 之后，再按照你下载的 themes 的使用说明，做相应修改即可。 参考：NexT 的使用 NexT 中 tags 的使用 修改 NexT 目录下的 _config.yml 文件，取消 menu 菜单下 tags 字段的注释 123menu: home: / || fa fa-home tags: /tags/ || fa fa-tags 在 blog 根目录的 source 目录下，新建 tags 目录 1$ hexo new page &quot;tags&quot; 修改 tags 目录下的 index.md 文件 1234title: tagsdate: 2020-10-27 16:35:56type: tagslayout: &quot;tags&quot; NexT 中添加字数统计、阅读时长 安装 hexo-symbols-count-time 插件 1$ npm install hexo-symbols-count-time 或者 1$ yarn add hexo-symbols-count-time hexo 配置，根目录下的 _config.yaml 文件，添加 symbols_count_time 节点 123456# Post wordcount display settingssymbols_count_time: symbols: true # 文章字数 time: true # 阅读时长 total_symbols: true # 所有文章总字数 total_time: true # 所有文章阅读中时长 NexT 配置，themes 目录下的 _config.yml 文件，symbols_count_time 节点 123456# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: true # 是否换行显示 字数统计 及 阅读时长 item_text_post: true # 文章 字数统计 阅读时长 使用图标 还是 文本表示 item_text_total: false # 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示 Next 中添加访客统计、访问次数统计、文章阅读次数统计 打开 next 主题配置文件 \\themes\\next\\_config.yml，搜索 busuanzi_count，把 enable 设置为 true。 12345678910# Show Views / Visitors of the website / page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzibusuanzi_count: enable: true total_visitors: true total_visitors_icon: fa fa-user total_views: true total_views_icon: fa fa-eye post_views: true post_views_icon: fa fa-eye 同样是在 next 主题配置文件 \\themes\\next\\_config.yml 下，搜索 footer，在它底下添加 counter，设值为 true。 12345678910111213141516171819202122232425262728293031footer: # Specify the date when the site was setup. If not defined, current year will be used. #since: 2015 # Icon between year and copyright info. icon: # Icon name in Font Awesome. See: https://fontawesome.com/icons name: fa fa-heart # If you want to animate the icon, set it to true. animated: false # Change the color of icon, using Hex Code. color: &quot;#ff0000&quot; # If not defined, `author` from Hexo `_config.yml` will be used. copyright: # Powered by Hexo &amp; NexT powered: true # Beian ICP and gongan information for Chinese users. See: https://beian.miit.gov.cn, http://www.beian.gov.cn beian: enable: false icp: # The digit in the num of gongan beian. gongan_id: # The full num of gongan beian. gongan_num: # The icon for gongan beian. See: http://www.beian.gov.cn/portal/download gongan_icon_url: counter: true 来到 themes\\next\\layout\\_partials，找到 footer.swig 文件，打开编辑，在底下添加代码。 123&#123;% if theme.footer.counter %&#125; &lt;script async src=&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 站点访客数、访问次数显示在网址底部，文章阅读次数在文章开头。 在博客中添加图片md 文件中插入图片的语法为：![]()。 其中，方括号是图片描述，圆括号是图片路径。一般来说有三种图片路径，分别是相对路径，绝对路径和网络路径。 相对而言，使用相对路径会更加方便，设置如下： 安装 hexo-renderer-marked 插件 1$ npm install hexo-renderer-marked 修改根目录下的 _config.yaml 配置 将： 1post_asset_folder: false 修改为： 1234post_asset_folder: truemarked: prependRoot: true postAsset: true 设置 Typora 点击文件 → 偏好设置，设置如下： 这样，在粘贴图片到文件中时，会自动将图片复制到 source\\_posts 目录下，与 .md 文件同名的目录中。 之后，在将博客发布时，会自动上传到文章所生成的静态网页同级目录之下。","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}],"categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"hadoop","slug":"hadoop","permalink":"http://example.com/tags/hadoop/"},{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"},{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"Poetry and Prose","slug":"Poetry-and-Prose","permalink":"http://example.com/tags/Poetry-and-Prose/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}