{"meta":{"title":"XiSun的博客","subtitle":"Learning is endless","description":"心如止水者，虽世间繁华之红尘纷扰，已然空无一物","author":"XiSun","url":"http://example.com","root":"/"},"pages":[{"title":"tags","date":"2020-10-27T08:35:56.000Z","updated":"2020-10-27T08:40:16.641Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"spring-boot","slug":"spring-boot","date":"2021-06-12T07:31:58.000Z","updated":"2021-06-18T08:17:32.718Z","comments":true,"path":"2021/06/12/spring-boot/","link":"","permalink":"http://example.com/2021/06/12/spring-boot/","excerpt":"","text":"Spring Boot 的作用 官网：https://spring.io/projects/spring-boot Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can “just run”. Spring Boot 能快速创建出生产级别的 Spring 应用。 Spring Boot 的优点 Create stand-alone Spring applications 创建独立Spring应用 Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files) 内嵌web服务器 Provide opinionated ‘starter’ dependencies to simplify your build configuration 自动starter依赖，简化构建配置 Automatically configure Spring and 3rd party libraries whenever possible 自动配置Spring以及第三方功能 Provide production-ready features such as metrics, health checks, and externalized configuration 提供生产级别的监控、健康检查及外部化配置 Absolutely no code generation and no requirement for XML configuration 无代码生成、无需编写XML","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"}]},{"title":"Docker 入门","slug":"docker-base","date":"2021-05-17T02:30:51.000Z","updated":"2021-06-12T07:29:57.279Z","comments":true,"path":"2021/05/17/docker-base/","link":"","permalink":"http://example.com/2021/05/17/docker-base/","excerpt":"","text":"Docker 简介Docker 出现的背景 一款产品从开发到上线，从操作系统，到运行环境，再到应用配置。作为开发 + 运维之间的协作我们需要关心很多东西，这也是很多互联网公司都不得不面对的问题，特别是各种版本的迭代之后，不同版本环境的兼容，对运维人员都是考验。 环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。 Docker 之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。开发人员利用 Docker 可以消除协作编码时 “在我的机器上可正常工作” 的问题。 之前在服务器配置一个应用的运行环境，要安装各种软件。安装和配置这些东西有多麻烦就不说了，它还不能跨平台。假如我们是在 Windows 上安装的这些环境，到了 Linux 又得重新装。况且就算不跨操作系统，换另一台同样操作系统的服务器，要移植应用也是非常麻烦的。 传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等。而为了让这程序可以顺利执行，开发团队也得准备完整的部署文件，让运维团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件 + 所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker 镜像的设计，使得 Docker 得以打破过去「程序即应用」的观念。透过镜像 (images) 将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运作。 Docker 的理念 Docker 是基于 Go 语言实现的云开源项目。 Docker 的主要目标是 “Build, Ship and Run Any App, Anywhere“，也就是通过对应用组件的封装、分发、部署、运行等生命期的管理，使用户的 APP (可以是一个 WEB 应用或数据库应用等等) 及其运行环境能够做到 “一次封装，到处运行“。 Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。 总之，Docker 是一个解决了运行环境和配置问题的软件容器，是方便做持续集成并有助于整体发布的容器虚拟化技术。 Docker 的基本组成架构图 镜像 (Image) Docker 镜像就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 镜像与容器的关系类似于面向对象编程中的类与对象： Docker 面向对象 镜像 类 容器 对象 容器 (Container) Docker 利用容器独立运行一个或一组应用。容器是用镜像创建的运行实例。 容器可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 容器可以看做是一个简易版的 Linux 环境 (包括 root 用户权限、进程空间、用户空间和网络空间等) 和运行在其中的应用程序。 容器的定义和镜像几乎一模一样，也是一堆层的统一视角， 唯一区别在于容器的最上面那一层是可读可写的。 仓库 (Repository) 仓库是集中存放镜像文件的场所。 仓库和仓库注册服务器 (Registry) 是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多镜像，每个镜像有不同的标签 (tag) 。 仓库分为公开仓库 (Public) 和私有仓库 (Private) 两种形式。 最大的公开仓库是 Docker Hub ( https://hub.docker.com/ )，存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云、网易云等。 总结 Docker本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就是 image 镜像文件。只有通过这个镜像文件才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 image 文件生成的容器实例，本身也是一个文件，称为镜像文件。 一个容器运行一种服务，当我们需要的时候，就可以通过 Docker 客户端创建一个对应的运行实例，也就是我们的容器。 至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。 底层原理Docker 是怎样工作的 Docker 是一个 Client-Server 结构的系统，Docker 守护进程运行在主机上，然后通过 Socket 连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。容器，是一个运行时环境，就是我们前面说到的集装箱。 Docker 为什么比 VM 快 Docker 有着比虚拟机更少的抽象层。由于 Docker 不需要 Hypervisor 实现硬件资源虚拟化，运行在 Docker 容器上的程序直接使用的都是实际物理机的硬件资源。因此在 CPU、内存利用率上，Docker 将会在效率上有明显优势。 Docker 利用的是宿主机的内核，而不需要 Guest OS。因此，当新建一个容器时，Docker 不需要和虚拟机一样重新加载一个操作系统内核，因此可以避免引寻、加载操作系统内核这个比较费时费资源的过程，当新建一个虚拟机时，虚拟机软件需要加载 Guest OS，这个新建过程是分钟级别的。而 Docker 由于直接利用宿主机的操作系统，则省略了返个过程，因此新建一个 Docker 容器只需要几秒钟。 Docker 安装 官网：https://hub.docker.com/ Linux 安装：https://hub.docker.com/search?q=&amp;type=edition&amp;offering=community&amp;operating_system=linux WSL 安装 默认已经安装 WSL。 默认安装的 WSL version 是 1，在 Windows PowerShell 中查看： 123PS C:\\Users\\Xisun\\Desktop&gt; wsl --list -v NAME STATE VERSION* Ubuntu Running 1 在 Windows PowerShell 中，切换 WSL version 为 2： 启用 Hyper-V 功能： 启用 Hyper-V 功能后，需要重启电脑。 再按以下步骤依次执行： 参考：https://docs.microsoft.com/en-us/windows/wsl/install-win10 切换 version： 1wsl --set-version &lt;distribution name&gt; &lt;versionNumber&gt; 1234567PS C:\\Users\\Xisun\\Desktop&gt; wsl --set-version Ubuntu 2正在进行转换，这可能需要几分钟时间...有关与 WSL 2 的主要区别的信息，请访问 https://aka.ms/wsl2转换完成。PS C:\\Users\\Xisun\\Desktop&gt; wsl --list -v NAME STATE VERSION* Ubuntu Running 2 WSL 默认不支持 Docker，需要破解： 破解步骤： 参考：https://github.com/arkane-systems/genie 以管理员身份打开 Windows PowerShell：win + x 快捷键，然后执行命令 wsl，进入 WSL 控制台，并切换到 root 用户： 1234PS C:\\WINDOWS\\system32&gt; wslxisun@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32$ su rootPassword:root@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32# 如果忘记 root 用户密码，可以如下方式重置： 1231.以管理员身份打开Windows PowerShell;2.输入命令: wsl.exe --user root;3.输入命令: passwd root, 修改root用户密码。 安装 dotnet： 查看 Ubuntu 版本： 方式一： 12root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# cat /proc/versionLinux version 5.4.72-microsoft-standard-WSL2 (oe-user@oe-host) (gcc version 8.2.0 (GCC)) #1 SMP Wed Oct 28 23:40:43 UTC 2020 方式二： 123456root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 20.04.2 LTSRelease: 20.04Codename: focal 查看内核版本号： 12root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# uname -r5.4.72-microsoft-standard-WSL2 安装对应版本的 dotnet： 参考：https://docs.microsoft.com/zh-cn/dotnet/core/install/linux-ubuntu 将 Microsoft 包签名密钥添加到受信任密钥列表，并添加包存储库。 12wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.debsudo dpkg -i packages-microsoft-prod.deb 123456789101112131415161718root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.debo dpkg -i packages-microsoft-prod.deb--2021-06-08 21:15:31-- https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.debResolving packages.microsoft.com (packages.microsoft.com)... 65.52.183.205Connecting to packages.microsoft.com (packages.microsoft.com)|65.52.183.205|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 3124 (3.1K) [application/octet-stream]Saving to: ‘packages-microsoft-prod.deb’packages-microsoft-prod.deb 100%[=================================================&gt;] 3.05K --.-KB/s in 0s2021-06-08 21:15:32 (523 MB/s) - ‘packages-microsoft-prod.deb’ saved [3124/3124]root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sudo dpkg -i packages-microsoft-prod.debSelecting previously unselected package packages-microsoft-prod.(Reading database ... 47281 files and directories currently installed.)Preparing to unpack packages-microsoft-prod.deb ...Unpacking packages-microsoft-prod (1.0-ubuntu20.04.1) ...Setting up packages-microsoft-prod (1.0-ubuntu20.04.1) ... 安装 SDK： 1234sudo apt-get update; \\ sudo apt-get install -y apt-transport-https &amp;&amp; \\ sudo apt-get update &amp;&amp; \\ sudo apt-get install -y dotnet-sdk-5.0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sudo apt-get update; \\&gt; sudo apt-get install -y apt-transport-https &amp;&amp; \\&gt; sudo apt-get update &amp;&amp; \\&gt; sudo apt-get install -y dotnet-sdk-5.0Get:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InRelease [10.5 kB]Get:2 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 Packages [74.9 kB]Hit:3 http://archive.ubuntu.com/ubuntu focal InReleaseGet:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]Get:6 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [702 kB]Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [101 kB]Get:8 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [141 kB]Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1026 kB]Get:10 http://security.ubuntu.com/ubuntu focal-security/main amd64 c-n-f Metadata [7780 B]Get:11 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [247 kB]Get:12 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [36.1 kB]Get:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 c-n-f Metadata [456 B]Get:14 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [588 kB]Get:15 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [94.6 kB]Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [11.5 kB]Get:17 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [19.9 kB]Get:18 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [4316 B]Get:19 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [528 B]Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main Translation-en [229 kB]Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 c-n-f Metadata [13.5 kB]Get:22 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [266 kB]Get:23 http://archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [38.9 kB]Get:24 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 c-n-f Metadata [456 B]Get:25 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [781 kB]Get:26 http://archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [170 kB]Get:27 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [17.6 kB]Get:28 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [23.6 kB]Get:29 http://archive.ubuntu.com/ubuntu focal-updates/multiverse Translation-en [6376 B]Get:30 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [648 B]Fetched 4840 kB in 4s (1125 kB/s)Reading package lists... DoneReading package lists... DoneBuilding dependency treeReading state information... DoneThe following NEW packages will be installed: apt-transport-https0 upgraded, 1 newly installed, 0 to remove and 101 not upgraded.Need to get 1704 B of archives.After this operation, 161 kB of additional disk space will be used.Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.5 [1704 B]Fetched 1704 B in 0s (3469 B/s)Selecting previously unselected package apt-transport-https.(Reading database ... 47289 files and directories currently installed.)Preparing to unpack .../apt-transport-https_2.0.5_all.deb ...Unpacking apt-transport-https (2.0.5) ...Setting up apt-transport-https (2.0.5) ...Hit:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InReleaseHit:2 http://security.ubuntu.com/ubuntu focal-security InReleaseHit:3 http://archive.ubuntu.com/ubuntu focal InReleaseHit:4 http://archive.ubuntu.com/ubuntu focal-updates InReleaseHit:5 http://archive.ubuntu.com/ubuntu focal-backports InReleaseReading package lists... DoneReading package lists... DoneBuilding dependency treeReading state information... DoneThe following additional packages will be installed: aspnetcore-runtime-5.0 aspnetcore-targeting-pack-5.0 dotnet-apphost-pack-5.0 dotnet-host dotnet-hostfxr-5.0 dotnet-runtime-5.0 dotnet-runtime-deps-5.0 dotnet-targeting-pack-5.0 netstandard-targeting-pack-2.1The following NEW packages will be installed: aspnetcore-runtime-5.0 aspnetcore-targeting-pack-5.0 dotnet-apphost-pack-5.0 dotnet-host dotnet-hostfxr-5.0 dotnet-runtime-5.0 dotnet-runtime-deps-5.0 dotnet-sdk-5.0 dotnet-targeting-pack-5.0 netstandard-targeting-pack-2.10 upgraded, 10 newly installed, 0 to remove and 101 not upgraded.Need to get 95.1 MB of archives.After this operation, 396 MB of additional disk space will be used.Get:1 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-runtime-deps-5.0 amd64 5.0.6-1 [2642 B]Get:2 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-host amd64 5.0.6-1 [52.5 kB]Get:3 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-hostfxr-5.0 amd64 5.0.6-1 [140 kB]Get:4 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-runtime-5.0 amd64 5.0.6-1 [22.1 MB]Get:5 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 aspnetcore-runtime-5.0 amd64 5.0.6-1 [6086 kB]Get:6 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-targeting-pack-5.0 amd64 5.0.0-1 [2086 kB]Get:7 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 aspnetcore-targeting-pack-5.0 amd64 5.0.0-1 [1316 kB]Get:8 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-apphost-pack-5.0 amd64 5.0.6-1 [3412 kB]Get:9 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 netstandard-targeting-pack-2.1 amd64 2.1.0-1 [1476 kB]Get:10 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 dotnet-sdk-5.0 amd64 5.0.300-1 [58.4 MB]Fetched 95.1 MB in 1min 11s (1332 kB/s)Selecting previously unselected package dotnet-runtime-deps-5.0.(Reading database ... 47293 files and directories currently installed.)Preparing to unpack .../0-dotnet-runtime-deps-5.0_5.0.6-1_amd64.deb ...Unpacking dotnet-runtime-deps-5.0 (5.0.6-1) ...Selecting previously unselected package dotnet-host.Preparing to unpack .../1-dotnet-host_5.0.6-1_amd64.deb ...Unpacking dotnet-host (5.0.6-1) ...Selecting previously unselected package dotnet-hostfxr-5.0.Preparing to unpack .../2-dotnet-hostfxr-5.0_5.0.6-1_amd64.deb ...Unpacking dotnet-hostfxr-5.0 (5.0.6-1) ...Selecting previously unselected package dotnet-runtime-5.0.Preparing to unpack .../3-dotnet-runtime-5.0_5.0.6-1_amd64.deb ...Unpacking dotnet-runtime-5.0 (5.0.6-1) ...Selecting previously unselected package aspnetcore-runtime-5.0.Preparing to unpack .../4-aspnetcore-runtime-5.0_5.0.6-1_amd64.deb ...Unpacking aspnetcore-runtime-5.0 (5.0.6-1) ...Selecting previously unselected package dotnet-targeting-pack-5.0.Preparing to unpack .../5-dotnet-targeting-pack-5.0_5.0.0-1_amd64.deb ...Unpacking dotnet-targeting-pack-5.0 (5.0.0-1) ...Selecting previously unselected package aspnetcore-targeting-pack-5.0.Preparing to unpack .../6-aspnetcore-targeting-pack-5.0_5.0.0-1_amd64.deb ...Unpacking aspnetcore-targeting-pack-5.0 (5.0.0-1) ...Selecting previously unselected package dotnet-apphost-pack-5.0.Preparing to unpack .../7-dotnet-apphost-pack-5.0_5.0.6-1_amd64.deb ...Unpacking dotnet-apphost-pack-5.0 (5.0.6-1) ...Selecting previously unselected package netstandard-targeting-pack-2.1.Preparing to unpack .../8-netstandard-targeting-pack-2.1_2.1.0-1_amd64.deb ...Unpacking netstandard-targeting-pack-2.1 (2.1.0-1) ...Selecting previously unselected package dotnet-sdk-5.0.Preparing to unpack .../9-dotnet-sdk-5.0_5.0.300-1_amd64.deb ...Unpacking dotnet-sdk-5.0 (5.0.300-1) ...Setting up dotnet-host (5.0.6-1) ...Setting up dotnet-runtime-deps-5.0 (5.0.6-1) ...Setting up netstandard-targeting-pack-2.1 (2.1.0-1) ...Setting up dotnet-hostfxr-5.0 (5.0.6-1) ...Setting up dotnet-apphost-pack-5.0 (5.0.6-1) ...Setting up dotnet-targeting-pack-5.0 (5.0.0-1) ...Setting up aspnetcore-targeting-pack-5.0 (5.0.0-1) ...Setting up dotnet-runtime-5.0 (5.0.6-1) ...Setting up aspnetcore-runtime-5.0 (5.0.6-1) ...Setting up dotnet-sdk-5.0 (5.0.300-1) ...This software may collect information about you and your use of the software, and send that to Microsoft.Please visit http://aka.ms/dotnet-cli-eula for more information.Welcome to .NET!---------------------Learn more about .NET: https://aka.ms/dotnet-docsUse &#x27;dotnet --help&#x27; to see available commands or visit: https://aka.ms/dotnet-cli-docsTelemetry---------The .NET tools collect usage data in order to help us improve your experience. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#x27;1&#x27; or &#x27;true&#x27; using your favorite shell.Read more about .NET CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetryConfiguring...--------------A command is running to populate your local package cache to improve restore speed and enable offline access. This command takes up to one minute to complete and only runs once.Processing triggers for man-db (2.9.1-1) ... 安装运行时： 1234sudo apt-get update; \\ sudo apt-get install -y apt-transport-https &amp;&amp; \\ sudo apt-get update &amp;&amp; \\ sudo apt-get install -y aspnetcore-runtime-5.0 dotnet-sdk-5.0 安装成功后，会一起安装 aspnetcore-runtime-5.0。 检查 dotnet 版本： 123456789101112131415161718root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# dotnetUsage: dotnet [options]Usage: dotnet [path-to-application]Options: -h|--help Display help. --info Display .NET information. --list-sdks Display the installed SDKs. --list-runtimes Display the installed runtimes.path-to-application: The path to an application .dll file to execute.root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# dotnet --list-sdks5.0.300 [/usr/share/dotnet/sdk]root@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32# dotnet --list-runtimesMicrosoft.AspNetCore.App 5.0.6 [/home/xisun/.dotnet/shared/Microsoft.AspNetCore.App]Microsoft.NETCore.App 5.0.6 [/home/xisun/.dotnet/shared/Microsoft.NETCore.App] 安装 wsl-translinux： 参考：https://arkane-systems.github.io/wsl-transdebian/ 123456789101112apt install apt-transport-httpswget -O /etc/apt/trusted.gpg.d/wsl-transdebian.gpg https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpgchmod a+r /etc/apt/trusted.gpg.d/wsl-transdebian.gpgcat &lt;&lt; EOF &gt; /etc/apt/sources.list.d/wsl-transdebian.listdeb https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) maindeb-src https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) mainEOFapt update 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# apt install apt-transport-httpswget -O /etc/apt/trusted.gpg.d/wsl-transdebian.gpg https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpgchmod a+r /etc/apt/trusted.gpg.d/wsl-transdebian.gpgcat &lt;&lt; EOF &gt; /etc/apt/sources.list.d/wsl-transdebian.listdeb https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) maindeb-src https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) mainEOFReading package lists... DoneBuilding dependency treeReading state information... Doneapt-transport-https is already the newest version (2.0.5).0 upgraded, 0 newly installed, 0 to remove and 101 not upgraded.root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# wget -O /etc/apt/trusted.gpg.d/wsl-transdebian.gpg https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpg--2021-06-08 21:23:47-- https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpgResolving arkane-systems.github.io (arkane-systems.github.io)... 185.199.109.153, 185.199.108.153, 185.199.110.153, ...Connecting to arkane-systems.github.io (arkane-systems.github.io)|185.199.109.153|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 2280 (2.2K) [application/octet-stream]Saving to: ‘/etc/apt/trusted.gpg.d/wsl-transdebian.gpg’/etc/apt/trusted.gpg.d/wsl-tr 100%[=================================================&gt;] 2.23K --.-KB/s in 0s2021-06-08 21:23:49 (36.1 MB/s) - ‘/etc/apt/trusted.gpg.d/wsl-transdebian.gpg’ saved [2280/2280]root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# chmod a+r /etc/apt/trusted.gpg.d/wsl-transdebian.gpgroot@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# cat &lt;&lt; EOF &gt; /etc/apt/sources.list.d/wsl-transdebian.list&gt; deb https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) main&gt; deb-src https://arkane-systems.github.io/wsl-transdebian/apt/ $(lsb_release -cs) main&gt; EOFroot@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# apt updateHit:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InReleaseHit:2 http://archive.ubuntu.com/ubuntu focal InReleaseHit:3 http://security.ubuntu.com/ubuntu focal-security InReleaseHit:4 http://archive.ubuntu.com/ubuntu focal-updates InReleaseGet:5 https://arkane-systems.github.io/wsl-transdebian/apt focal InRelease [2495 B]Hit:6 http://archive.ubuntu.com/ubuntu focal-backports InReleaseGet:7 https://arkane-systems.github.io/wsl-transdebian/apt focal/main Sources [1338 B]Get:8 https://arkane-systems.github.io/wsl-transdebian/apt focal/main amd64 Packages [1897 B]Fetched 5730 B in 2s (3130 B/s)Reading package lists... DoneBuilding dependency treeReading state information... Done101 packages can be upgraded. Run &#x27;apt list --upgradable&#x27; to see them. 安装 genie： 12sudo apt updatesudo apt install -y systemd-genie 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sudo apt updateHit:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InReleaseHit:2 http://security.ubuntu.com/ubuntu focal-security InReleaseHit:3 http://archive.ubuntu.com/ubuntu focal InReleaseHit:4 http://archive.ubuntu.com/ubuntu focal-updates InReleaseHit:5 https://arkane-systems.github.io/wsl-transdebian/apt focal InReleaseHit:6 http://archive.ubuntu.com/ubuntu focal-backports InReleaseReading package lists... DoneBuilding dependency treeReading state information... Done101 packages can be upgraded. Run &#x27;apt list --upgradable&#x27; to see them.root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sudo apt install -y systemd-genieReading package lists... DoneBuilding dependency treeReading state information... DoneThe following additional packages will be installed: daemonize libnss-mymachines libnss-systemd libpam-systemd libsystemd0 systemd systemd-container systemd-sysv systemd-timesyncdThe following NEW packages will be installed: daemonize libnss-mymachines systemd-container systemd-genieThe following packages will be upgraded: libnss-systemd libpam-systemd libsystemd0 systemd systemd-sysv systemd-timesyncd6 upgraded, 4 newly installed, 0 to remove and 95 not upgraded.Need to get 5359 kB of archives.After this operation, 3892 kB of additional disk space will be used.Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libnss-systemd amd64 245.4-4ubuntu3.6 [95.8 kB]Get:2 https://arkane-systems.github.io/wsl-transdebian/apt focal/main amd64 systemd-genie amd64 1.42 [504 kB]Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 systemd-timesyncd amd64 245.4-4ubuntu3.6 [28.1 kB]Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 systemd-sysv amd64 245.4-4ubuntu3.6 [10.3 kB]Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpam-systemd amd64 245.4-4ubuntu3.6 [186 kB]Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 systemd amd64 245.4-4ubuntu3.6 [3805 kB]Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsystemd0 amd64 245.4-4ubuntu3.6 [269 kB]Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 daemonize amd64 1.7.8-1 [11.9 kB]Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 systemd-container amd64 245.4-4ubuntu3.6 [317 kB]Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libnss-mymachines amd64 245.4-4ubuntu3.6 [131 kB]Fetched 5359 kB in 5s (1137 kB/s)(Reading database ... 50558 files and directories currently installed.)Preparing to unpack .../0-libnss-systemd_245.4-4ubuntu3.6_amd64.deb ...Unpacking libnss-systemd:amd64 (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../1-systemd-timesyncd_245.4-4ubuntu3.6_amd64.deb ...Unpacking systemd-timesyncd (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../2-systemd-sysv_245.4-4ubuntu3.6_amd64.deb ...Unpacking systemd-sysv (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../3-libpam-systemd_245.4-4ubuntu3.6_amd64.deb ...Unpacking libpam-systemd:amd64 (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../4-systemd_245.4-4ubuntu3.6_amd64.deb ...Unpacking systemd (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Preparing to unpack .../5-libsystemd0_245.4-4ubuntu3.6_amd64.deb ...Unpacking libsystemd0:amd64 (245.4-4ubuntu3.6) over (245.4-4ubuntu3.4) ...Setting up libsystemd0:amd64 (245.4-4ubuntu3.6) ...Selecting previously unselected package daemonize.(Reading database ... 50558 files and directories currently installed.)Preparing to unpack .../daemonize_1.7.8-1_amd64.deb ...Unpacking daemonize (1.7.8-1) ...Selecting previously unselected package systemd-container.Preparing to unpack .../systemd-container_245.4-4ubuntu3.6_amd64.deb ...Unpacking systemd-container (245.4-4ubuntu3.6) ...Selecting previously unselected package systemd-genie.Preparing to unpack .../systemd-genie_1.42_amd64.deb ...Unpacking systemd-genie (1.42) ...Selecting previously unselected package libnss-mymachines:amd64.Preparing to unpack .../libnss-mymachines_245.4-4ubuntu3.6_amd64.deb ...Unpacking libnss-mymachines:amd64 (245.4-4ubuntu3.6) ...Setting up daemonize (1.7.8-1) ...Setting up systemd (245.4-4ubuntu3.6) ...Initializing machine ID from random generator.Setting up systemd-timesyncd (245.4-4ubuntu3.6) ...Setting up systemd-container (245.4-4ubuntu3.6) ...Created symlink /etc/systemd/system/multi-user.target.wants/machines.target → /lib/systemd/system/machines.target.Setting up systemd-sysv (245.4-4ubuntu3.6) ...Setting up systemd-genie (1.42) ...Created symlink /etc/systemd/system/sockets.target.wants/wslg-xwayland.socket → /lib/systemd/system/wslg-xwayland.socket.Setting up libnss-systemd:amd64 (245.4-4ubuntu3.6) ...Setting up libnss-mymachines:amd64 (245.4-4ubuntu3.6) ...First installation detected...Checking NSS setup...Setting up libpam-systemd:amd64 (245.4-4ubuntu3.6) ...Processing triggers for libc-bin (2.31-0ubuntu9.2) ...Processing triggers for man-db (2.9.1-1) ...Processing triggers for dbus (1.12.16-2ubuntu2.1) ... 破解完成之后，即可在 WSL 中安装 Docker (利用脚本安装)： 12curl -fsSL https://get.docker.com -o get-docker.shsh get-docker.sh 12345678910111213141516171819202122232425262728293031323334353637383940root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# curl -fsSL https://get.docker.com -o get-docker.shroot@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# sh get-docker.sh# Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737WSL DETECTED: We recommend using Docker Desktop for Windows.Please get Docker Desktop from https://www.docker.com/products/docker-desktopYou may press Ctrl+C now to abort this script.+ sleep 20+ sh -c apt-get update -qq &gt;/dev/null+ sh -c DEBIAN_FRONTEND=noninteractive apt-get install -y -qq apt-transport-https ca-certificates curl &gt;/dev/null+ sh -c curl -fsSL &quot;https://download.docker.com/linux/ubuntu/gpg&quot; | apt-key add -qq - &gt;/dev/nullWarning: apt-key output should not be parsed (stdout is not a terminal)+ sh -c echo &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable&quot; &gt; /etc/apt/sources.list.d/docker.list+ sh -c apt-get update -qq &gt;/dev/null+ [ -n ]+ sh -c apt-get install -y -qq --no-install-recommends docker-ce &gt;/dev/null+ [ -n 1 ]+ sh -c DEBIAN_FRONTEND=noninteractive apt-get install -y -qq docker-ce-rootless-extras &gt;/dev/null================================================================================To run Docker as a non-privileged user, consider setting up theDocker daemon in rootless mode for your user: dockerd-rootless-setuptool.sh installVisit https://docs.docker.com/go/rootless/ to learn about rootless mode.To run the Docker daemon as a fully privileged service, but granting non-rootusers access, refer to https://docs.docker.com/go/daemon-access/WARNING: Access to the remote API on a privileged Docker daemon is equivalent to root access on the host. Refer to the &#x27;Docker daemon attack surface&#x27; documentation for details: https://docs.docker.com/go/attack-surface/================================================================================ 参考：https://github.com/docker/docker-install Docker 安装成功后，启动 Docker 服务： 12root@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32# service docker start * Starting Docker: docker [ OK ] Docker 服务如果没有启动，执行 Docker 的命令时，会提示：Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?。 Docker 服务启动后，如果不手动关闭，会一直运行，即使关闭 Windows PowerShell 也不会关闭。 如果关闭电脑，需要重启 Docker 服务。 查看 Docker version： 1234567891011121314151617181920212223242526272829root@DESKTOP-OJKMETJ:/mnt/c/WINDOWS/system32# docker versionClient: Docker Engine - Community Version: 20.10.6 API version: 1.41 Go version: go1.13.15 Git commit: 370c289 Built: Fri Apr 9 22:47:17 2021 OS/Arch: linux/amd64 Context: default Experimental: trueServer: Docker Engine - Community Engine: Version: 20.10.7 API version: 1.41 (minimum version 1.12) Go version: go1.13.15 Git commit: b0f5bc3 Built: Wed Jun 2 11:54:50 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.6 GitCommit: d71fcd7d8303cbf684402823e425e9dd2e99285d runc: Version: 1.0.0-rc95 GitCommit: b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7 docker-init: Version: 0.19.0 GitCommit: de40ad0 测试 Docker，运行 hello-world： 12345678910111213141516171819202122232425262728293031root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# docker run hello-worldUnable to find image &#x27;hello-world:latest&#x27; locallylatest: Pulling from library/hello-worldb8dfde127a29: Pull completeDigest: sha256:9f6ad537c5132bcce57f7a0a20e317228d382c3cd61edae14650eec68b2b345cStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/root@DESKTOP-OJKMETJ:/mnt/c/Windows/system32# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest d1165f221234 3 months ago 13.3kB 关闭 Docker 服务： 12root@DESKTOP-OJKMETJ:/mnt/c/Users/Ziyoo# service docker stop * Stopping Docker: docker [ OK ] 启动和关闭 Docker 服务时，必须使用 root 用户。 Ubuntu 安装 参考：https://docs.docker.com/engine/install/ubuntu/ 按照官网指示一步步执行，即可安装 Docker。主要涉及如下命令，各命令的含义参考官网： 12345678910111213141516171819202122$ sudo apt-get remove docker docker-engine docker.io containerd runc$ sudo apt-get update$ sudo apt-get install apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \\ sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg$ echo \\ &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null$ sudo apt-get update$ sudo apt-get install docker-ce docker-ce-cli containerd.io 以上命令默认安装的为 Docker 最新版本，若需要安装特定版本，请参考官网。 Docker 常用命令帮助命令12345$ docker version$ docker info$ docker --help 镜像命令 列出本机上的镜像 1$ docker images [OPTIONS] OPTIONS 说明： 1234-a 列出本地所有的镜像(含中间映射层)-q 只显示镜像ID--digests 显示镜像的摘要信息--no-trunc 显示完整的镜像信息 查询某个镜像 1$ docker search [OPTIONS] 镜像名字 OPTIONS 说明： 123--no-trunc 显示完整的镜像描述-s 列出收藏数不小于指定值的镜像--automated 只列出 automated build类型的镜像 官方镜像仓库：https://hub.docker.com/ 下载镜像 1$ docker pull 镜像名字[:TAG] 一般设置从阿里云镜像下载。 docker pull tomcat 等价于 docker pull tomcat:latest，即默认下载最新版本。 删除镜像 删除单个镜像： 1$ docker rmi -f 镜像名[:TAG] 删除多个镜像： 1$ docker rmi -f 镜像名1[:TAG] 镜像名2[:TAG] 镜像名3[:TAG] ... 删除全部镜像： 1$ docker rmi -f $(docker images -qa) 容器命令 有镜像才能创建容器，这是根本前提。先下载一个 CentOS 镜像作为示例： 1$ docker pull centos 新建并启动容器 1$ docker run [OPTIONS] IMAGE [COMMAND][ARG] OPTIONS 说明： 12345678910--name 为容器指定一个名称，若不指定，由系统随机分配-d 后台运行容器，并返回容器ID，即启动守护式容器-i 以交互模式运行容器，通常与-t同时使用-t 为容器重新分配一个伪输入终端，通常与-i同时使用-P 随机端口映射-p 指定端口映射，有以下四种格式： ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 此时启动的是一个交互式的容器。 列出当前所有正在运行的容器 1$ docker ps [OPTIONS] OPTIONS 说明： 12345-a 列出当前所有正在运行的容器+历史上运行过的-| 显示最近创建的容器-n 显示最近n个创建的容器-q 静默模式，只显示容器编号--no-trunc 不截断输出 退出容器 方式一，停止容器并退出：exit。 方式二，不停止容器退出：ctrl + P + Q。 启动容器 1$ docker start 容器ID或容器名 重启容器 1$ docker restart 容器ID或容器名 停止容器 1$ docker stop 容器ID或容器名 强制停止容器 1$ docker kill 容器ID或容器名 删除已停止的容器 普通删除： 1$ docker rm 容器ID或容器名 强制删除： 1$ docker rm -f 容器ID或容器名 -f 可以删除没有停止的容器。 删除多个： 1$ docker rm -f $(docker ps -aq) 1$ docker ps -aq | xargs docker rm 启动守护式容器 1$ docker run -d IMAGE 例如，以后台模式启动一个 CentOS，docker run -d centos，然后 docker ps -a 进行查看，会发现容器已经退出。 Docker 容器若要后台运行，就必须有一个前台进程。 查看容器日志 启动一个一直运行的守护式容器： 1$ docker run -d centos /bin/sh -c &quot;while true; do echo hello xisun; sleep 2; done&quot; 查看该容器的日志： 1$ docker logs [OPTIONS] 容器ID或容器名 OPTION 说明： 12345-t 添加时间戳-f 跟随最新的日志打印--tail number 显示最后number条 查看容器内运行的进程 1$ docker top 容器ID或容器名 查看容器内部的细节 1$ docker inspect 容器ID或容器名 进入正在运行的容器并以命令行交互 方式一： 1$ docker attach 容器ID或容器名 直接进入容器启动命令的终端，不会启动新的进程。然后在该容器的终端内，执行相应的命令。 方式二： 1$ docker exec -t 容器ID或容器名 ls -l /tmp 在容器中打开新的终端，并且可以启动新的进程。然后执行后续的命令，并将结果显示在当前窗口。 1$ docker exec -t 容器ID或容器名 /bin/bash 与方式一等效。 针对执行 ctrl + P + Q 命令退出的容器。 从容器内拷贝文件到主机上 1$ docker cp 容器ID或容器名:容器内路径 目的主机路径 总结 Docker 镜像 镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的有内容，包括代码、运行时、库、环境变量和配置文件。 UnionFS (联合文件系统) UnionFS 是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下 (unite several directories into a single virtual file system)。 UnionFS 是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像 (没有父镜像)，可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。 Docker 镜像加载原理 Docker 的镜像实际上是由一层一层的文件系统组成，这种层级的文件系统即为 UnionFS。主要包含两个部分： bootfs (boot file system)：主要包含 bootloader 和 kernel，bootloader 主要是引导加载 kernel，Linux 刚启动时会加载bootfs文件系统。bootfs 是 Docker 镜像的最底层，这一层与我们典型的 Linux/Unix 系统是一样的，包含 boot 加载器和内核。当 boot 加载完成之后整个内核就都在内存中了，此时内存的使用权由 bootfs 转交给内核，系统也会卸载 bootfs。 rootfs (root file system)：在 bootfs 之上。包含的就是典型 Linux 系统中的 /dev， /proc，/bin，/etc 等标准目录和文件。rootfs 就是各种不同的操作系统发行版，比如 Ubuntu，CentOS 等等。 对于一个精简的 OS，rootfs 可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用 Host (宿主机) 的 kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的 Linux 发行版，bootfs基本是一致的，rootfs会有差别，因此不同的发行版可以公用 bootfs。比如：平时我们安装的虚拟机的 CentOS 都是好几个 G 大小，而 Docker 里才要 200 M 左右。 Docker 镜像是分层的 在执行 pull 命令时，可以看出 docker 的镜像时一层一层的在下载： 以 tomcat 为例，主要分为如下几个层次： Docker 镜像采用分层结构的原因 最大的一个好处就是：共享资源。 比如：有多个镜像都从相同的 base 镜像构建而来，那么宿主机只需在磁盘上保存一份 base 镜像，同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。进一步的，镜像的每一层都可以被共享。 Docker 镜像的特点 Docker 镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部，这一层通常被称为容器层，容器层之下都叫镜像层。 Docker 镜像的最外层是可写的，之下的都是封装好不可写的。 Docker 镜像 commit 操作 docker commit 命令，可以提交容器副本使之称为一个新的镜像。 1$ docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器ID 要创建的目标镜像名:[标签名] 案例演示： 从 Hub 上下载 tomcat 镜像到本地，然后运行。 1$ docker run -it -p 8888:8080 tomcat 12345-p 主机端口:容器端口，主机端口即为暴露的能访问docker的端口，容器端口即为docker内待访问特定容器的端口，如tomcat默认为8080-P 随机分配主机的端口-d 后台运行容器，并返回容器ID，即启动守护式容器-i 以交互模式运行容器，通常与-t同时使用-t 为容器重新分配一个伪输入终端，通常与-i同时使用 故意删除上一步镜像生成的 tomcat 容器的文档，会发现再次进入 tomcat 主页时，点击 Documentation 会返回 404。 也即当前的 tomcat 运行实例是一个没有文档内容的容器，现在，以此为模板 commit 一个没有 doc 文档的 tomcat 新镜像：atguigu/tomcat02:1.2。 启动新镜像并和原来的对比。 启动 atuigu/tomcat02，没有doc 1$ docker run -it -p 7777:8080 atuigu/tomcat02:1.2 启动原来 tomcat，有doc 1$ docker run -it -p 8888:8080 tomcat Docker 容器数据卷 Docker 容器产生的数据，如果不通过 docker commit 生成一个新的镜像，使得数据做为镜像的一部分保存下来，那么当容器删除后，数据自然也就没有了。 在 Docker 中，使用卷来保存数据。有点类似 Redis 里面的 rdb 和 aof 文件。 卷是目录或文件，存在于一个或多个容器中，由 Docker 挂载到容器，但不属于联合文件系统，因此能够绕过 UnionFS 提供一些用于持续存储或共享数据的特性。 卷的设计目的就是数据的持久化，卷完全独立于容器的生存周期，因此 Docker 不会在容器删除时删除其挂载的数据卷。 卷的特点： 数据卷可在容器之间共享或重用数据。 数据卷中的更改可以直接生效。 数据卷中的更改不会包含在镜像的更新中。 数据卷的生命周期一直持续到没有容器使用它为止。 容器内添加数据卷 直接命令添加 1$ docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 案例演示： 查看数据卷是否挂载成功： 1$ docker inspect 容器ID 此时，volume 权限是可读写的。可以在容器或主机内分别对卷进行数据的读写，读写的数据是共享的。 容器运行时，容器和宿主机之间数据能够共享： 容器停止退出后，主机修改后的数据也能同步： 带权限的命令： 1$ docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 此时，volume 权限是不可写的。可以在主机对卷进行数据的读写，读写的数据是共享的。但是，在容器内，只可对卷进行数据的读，不可写。 Dockerfile 添加 在主机根目录下新建 mydocker 文件夹并进入： 12$ mkdir /mydocker$ cd /mydocker 在 Dockerfile 中，使用 VOLUME 指令来给镜像添加一个或多个数据卷： 1VOLUME [&quot;/dataVolumeContainer&quot;,&quot;/dataVolumeContainer2&quot;,&quot;/dataVolumeContainer3&quot;] 出于可移植和分享的考虑，用 -v 主机目录:容器目录 这种方法不能直接在 Dockerfile 中实现。因为宿主机目录是依赖于特定宿主机的，不能保证在所有的宿主机上都存在这样的特定目录。 构建 Dockerfile： 1$ vim Dockerfile2 在 Dockerfile2 中添加如下内容： 12345# volume testFROM centosVOLUME [&quot;/dataVolumeContainer1&quot;,&quot;/dataVolumeContainer2&quot;]CMD echo &quot;finished,--------success1&quot;CMD /bin/bash 大致等同于命令：docker run -it -v /host1:/dataVolumeContainer1 -v /host2:/dataVolumeContainer2 centos /bin/bash。 执行 build 命令生成一个新镜像： 1$ docker builder -f /mydocker/Dockerfile2 -t zzyy/centos . 执行 run 命令启动容器，并查看容器内创建的卷的目录所在： 1$ docker run -it zzyy/centos /bin/bash 执行 inspect 命令查看主机对应的目录： 1$ docker inspect 容器ID 备注： Docker 挂载主机目录 Docker 访问出现 cannot open directory. Permission denied 异常时，在挂载目录后多加一个 --privileged=true 参数即可。 数据卷容器 命名的容器挂载数据卷，其它容器通过挂载这个 (父容器) 实现数据共享，挂载数据卷的容器，称之为数据卷容器。 案例演示： 先启动一个父容器 doc1，启动后在 dataVolumeContainer2 中新增内容 dc01_add.txt： 1$ docker run -it --name dc01 zzyy/centos 启动子容器 dc02 和 dc03，继承 dc01，启动后分别在 dataVolumeContainer2 中新增内容 dc02_add.txt 和 dc03_add.txt： 1$ docker run -it --name dc02 --volume-from dco1 zzyy/centos 1$ docker run -it --name dc03 --volume-from dco1 zzyy/centos 重新进入 dc01 容器，可以看到 dc02 和 dc03 容器内添加的数据，在卷 dataVolumeContainer2 中都可以共享： 1$ docker ps 1$ docker attach dc01 删除 dc01，dc02 和 dc03 仍然能够共享数据： 删除 dc02 后，dc03 仍然能够共享数据： 新建 dc04 继承 dc03，然后删除 dc03，dc04 仍然能够共享数据： 结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止。 Dockerfile 解析 Dockerfile 是用来构建 Docker 镜像的构建文件，由一系列命令和参数构成的脚本。 Dockerfile 构建的三步骤： 手动编写一个 Dockerfile 文件，必须要符合 Dockerfile 的规范； docker build 命令执行编写好的 Dockerfile 文件，获得一个自定义的镜像； doucker run 命令启动容器。 Dockerfile 构建过程解析 Dockerfile 内容基础知识： 每条保留字指令都必须为大写字母，且后面要跟随至少一个参数。 指令按照从上到下，顺序执行。 # 表示注释。 每条指令都会创建一个新的镜像层，并对镜像进行提交。 Docker 执行 Dockerfile 的大致流程： 第一步：Docker 从基础镜像运行一个容器； 第二步：执行一条指令并对容器作出修改； 第三步：执行类似 docker commit 的操作提交一个新的镜像层； 第四步：docker 再基刚提交的镜像运行一个新容器； 第五步：执行 Dockerfile 中的下一条指令，重复第二至第五步，直到所有指令都执行完成。 Dockerfile、 Docker 镜像与 Docker 容器三者的关系： 从应用软件的角度来看，Dockerfile、 Docker 镜像与 Docker 容器分别代表软件的三个不同阶段： Dockerfile 是软件的原材料。 Docker 镜像是软件的交付品。 Docker 容器可以认为是软件的运行态。 Dockerfile 面向开发，Docker 镜像为交付标准，Docker 容器则涉及部署与运维，三者缺一不可，合力充当 Docker 体系的基石。 Dockerfile 定义了进程需要的一切东西。Dockerfile 涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程 (当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计 namespace 的权限控制) 等等。 定义了 Dockerfile 文件后，docker build 命令产生一个 Docker 镜像。 对于生成的 Docker 镜像，docker run 命令生成 Docker 容器，容器是直接提供服务的。 Dockerfile 体系结构 (保留字指令) FROM：基础镜像，即当前新镜像是基于哪个镜像的。 MAINTAINER：镜像维护者的姓名和邮箱地址。 RUN：容器构建时需要运行的命令。 EXPOSE：当前容器对外暴露出的端口。 WORKDIR：指定在创建容器后，终端默认进入的工作目录。如果不指定，则为根目录。 ENV：用来在构建过程中设置环境变量。例如：ENV MY_PATH /usr/mytest，这个环境变量可以在后续的任何 RUN 指令中使用，如同在命令前制定了环境变量前缀；也可以在其他指令中直接使用这个环境变量，如 WORKDIR $MY_PATH。 ADD：将宿主机目录下的文件拷贝进镜像，并且能够自动处理 URL 和解压 tar 压缩包。 COPY：类似 ADD，拷贝文件和目录到镜像中 (只拷贝)。将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层镜像内的 &lt;目标路径&gt; 指向的位置。 COPY src dest COPY [&quot;src&quot;,&quot;dest&quot;] VOLUME：容器数据卷，用于数据保存和持久化工作。 CMD：指定一个容器启动时要运行的命令。 CMD 指令的格式和 RUN 相似： shell 格式：CMD &lt;命令&gt; exec 格式：CMD [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;...] 参数列表格式：CMD [&quot;参数1&quot;, &quot;参数2&quot;...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效。CMD 指令会被 docker run 之后的参数替换。 ENTRYPOINT：指定一个容器启动时要运行的命令。 ENTRYPOINT 的目的和 CMD 一样，不同的是，ENTRYPOINT 指令会被 docker run 之后的参数追加。 ONBUILD：当构建一个被继承的 Dockerfile 时运行命令，父镜像在被子镜像继承时，父镜像的 ONBUILD 指令触发， 案例演示Base 镜像 Docker Hub 中 99% 的镜像都是通过在 base 镜像中，安装和配置需要的软件构建出来的。例如 centos 镜像： 1234FROM scratchADD centos-8-x86_64.tar.xz /LABEL org.label-schema.schema-version=&quot;1.0&quot; org.label-schema.name=&quot;CentOS Base Image&quot; org.label-schema.vendor=&quot;CentOS&quot; org.label-schema.license=&quot;GPLv2&quot; org.label-schema.build-date=&quot;20201204&quot;CMD [&quot;/bin/bash&quot;] 自定义镜像 mycentos 编写 Dockerfile： Docker Hub 默认的 centos 镜像： 需求： 设置登陆后的默认路径； 增加 vim 编辑器； 增加查看网络配置 ifconfig 支持。 在主机 /mydocker 或其他目录下编写 Dockerfile 文件： 1234567891011121314FROM centosMAINTAINER ZZYY&lt;zzyy167@126.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo &quot;success--------------ok&quot;CMD /bin/bash 构建镜像：**docker build -f Dockerfile路径 -t 新镜像名字:TAG .** 1$ docker build -f /mydocker/Dockerfile -t mycentos:1.3 . 运行容器：**docker run -it 新镜像名字:TAG ** 1$ docker run -it mycentos:1.3 列出镜像的变更历史： 1$ docker history 镜像ID CMD/ENTRYPOINT 镜像案例 CMD/ENTRYPOINT 都是指定一个容器启动时要运行的命令。 Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效；另外，CMD 指令会被 docker run 命令之后的参数替换。 1234567891011121314151617181920212223242526FROM openjdk:16-jdk-busterENV CATALINA_HOME /usr/local/tomcatENV PATH $CATALINA_HOME/bin:$PATHRUN mkdir -p &quot;$CATALINA_HOME&quot;WORKDIR $CATALINA_HOME# let &quot;Tomcat Native&quot; live somewhere isolatedENV TOMCAT_NATIVE_LIBDIR $CATALINA_HOME/native-jni-libENV LD_LIBRARY_PATH $&#123;LD_LIBRARY_PATH:+$LD_LIBRARY_PATH:&#125;$TOMCAT_NATIVE_LIBDIR# see https://www.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/KEYS# see also &quot;update.sh&quot; (https://github.com/docker-library/tomcat/blob/master/update.sh)ENV GPG_KEYS A9C5DF4D22E99998D9875A5110C01C5A2F6059E7ENV TOMCAT_MAJOR 10ENV TOMCAT_VERSION 10.0.6ENV TOMCAT_SHA512 3d39b086b6fec86e354aa4837b1b55e6c16bfd5ec985a82a5dd71f928e3fab5370b2964a5a1098cfe05ca63d031f198773b18b1f8c7c6cdee6c90aa0644fb2f2RUN ...# verify Tomcat Native is working properlyRUN ...EXPOSE 8080CMD [&quot;catalina.sh&quot;, &quot;run&quot;] 以 tomcat 的 Dockerfile 为例，可以看到，文件的最后一条指令为 CMD 指令。 运行以下命令，tomcat 能够正常启动： 1$ docker run -it -p 7777:8080 tomcat 运行以下命令，tomcat 不能正常启动： 1$ docker run -it -p 7777:8080 tomcat ls -l 上面的 docker run 命令，末尾的 ls -l 参数会替换 Dockerfile 文件中的 CMD [&quot;catalina.sh&quot;, &quot;run&quot;] 指令，因此，tomcat 不会启动，只会列出 /usr/local/tomcat 路径下的文件。 不同于 CMD 指令，docker run 命令之后的参数，会传递给 ENTRYPOINT 指令，追加形成新的命令组合。 curl 命令解释： curl 命令可以用来执行下载、发送各种 HTTP 请求，指定 HTTP 头部等操作。 如果系统没有 curl，可以使用 yum install -y curl 命令安装。 curl 命令的 URL 如果指向的是 HTML 文档，那么缺省只显示文件头部，即 HTML 文档的 header，要全部显示，则加参数 -i。 CMD 版查询 IP 信息的容器： 123FROM centosRUN yum install -y curlCMD [&quot;curl&quot;, &quot;-s&quot;, &quot;http://ip.cn&quot;] 上面的容器，已经指定了 CMD 指令，如果希望查询结果包含 header，命令 docker run myip -i 会不生效。-i 参数会替换掉 CMD 指令。 ENTRYPOINT 版查询 IP 信息的容器： 123FROM centosRUN yum install -y curlENTRYPOINT [&quot;curl&quot;, &quot;-s&quot;, &quot;http://ip.cn&quot;] 上面的容器，使用的是 ENTRYPOINT 指令，如果希望查询结果包含 header，只需要使用命令 docker run myip -i 即可。-i 参数会追加到 ENTRYPOINT 指令后面。 自定义镜像 tomcat 创建目录： 1$ mkdir -p /zzyy/mydockerfile/tomcat9 再上述目录创建 c.txt： 123$ cd /zzyy/mydockerfile/tomcat9$ touch c.txt 将 JDK 和 tomcat 的安装压缩包拷贝进上一步目录： 123$ cp /opt/jdk-8u171-linux-x64.tar.gz /zzyy/mydockerfile/tomcat9$ cp /opt/apache-tomcat-9.0.8.tar.gz /zzyy/mydockerfile/tomcat9 在 zzyyuse/mydockerfile/tomcat9 目录下新建 Dockerfile 文件： 1$ vim Dockerfie 123456789101112131415161718192021222324FROM centosMAINTAINER zzyy&lt;zzyybs@ 126.com&gt;#把宿主机当前上下文的c.txt拷贝到容器/usr/local/路径下COPY c.txt /usr/local/cincontainer.txt#把java与tomcat添加到容器中ADD jdk-8u171-linux x64.tar.gz /usr/local/ADD apache-tomcat-9.0.8.tar.gz /usr/local/#安装vim编辑器RUN yum -y install vim#设置工作访问时候的WORKDIR路径，登录落脚点ENV MYPATH /usr/localWORKDIR $MYPATH#配置java与tomcat环境变量ENV JAVA_ HOME /usr/local/jdk1.8.0_171ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-9.0.8ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.8ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin#容器运行时监听的端口EXPOSE 8080#启动时运行tomcat# ENTRYPOINT [&quot;/usrl/local/apache-tomcat-9.0.8/bin/startup.sh&quot; ]# CMD [&quot;/usr/local/apache-tomcat-9.0.8/bin/catalina.sh&quot;,&quot;run&quot;]CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.8/in/logs/catalina.out 目录内容： 构建镜像： 1$ docker build -t zzyytomcat9 不添加 -f 参数，默认构建当前路径下的 Dockerfile。 运行容器： 1$ docker run -d -p 9080:8080 -name myt9 -v /zzyyuse/mydockerfile/tomcat9/test:/usrlocal/apache-tomcat9.0.8/webapps/test -v /zzyyuse/mydockerfile/tomcat9/tomcat9logs/:/usrlocal/apache-tomcat-9.0.8/logs -privileged=true zzyytomcat9 -v 参数设置两个数据卷，一个用于存放发布项目，一个用于存放日志记录。 -privileged=true 是 Docker 挂载主机目录 Docker 访问出现 cannot open directory : Permission denied 时的解决办法。 验证： 发布 web 服务 test： 在主机数据卷对应的目录 /zzyyuse/mydockerfile/tomcat9/test 目录下，新建 WEB-INF 目录，并添加 web.xml 文件。然后编写一个 a.jsp 文件作为测试： web.xml： 123456789&lt;?xml version=&quot;1 .0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmIns:xsi=&quot;http://www.w3.org/2001/XML Schema-instance&quot;xmIns=&quot;http://java sun.com/xm/ns/javaee&quot;xsi:schemaL ocation=&quot;http://java. sun.com/xml/ns/javaee htp:/:/java. sun.com/xml/ns/javaee/web-app_ 2_ _5.xsd&quot;id=&quot;WebApp_ ID&quot; version=&quot;2.5&quot;&gt; &lt;display-name&gt;test&lt;/display-name&gt;&lt;/web-app&gt; a.jsp： 123456789101112131415&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC“//W3C//DTD HTML 4.01 Transitional//EN&quot; http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;title&gt;Insert title here &lt;/title&gt; &lt;/head&gt; &lt;body&gt; ---------------welcome--------------- &lt;br&gt; &lt;%=&quot;i am in docker tomcat self &quot;%&gt; &lt;br&gt; &lt;% System.out.printIn(&quot;==========docker tomcat self&quot;);%&gt; &lt;/body&gt;&lt;/htmI&gt; docker restart 命令重新启动 tomcat，然后网页访问 localhost:9080/test/a.jsp，即可查看到 a.jsp 网页的内容。在主机目录下修改 a.jsp 的内容时，会同步到 tomcat 中。 主机上查看日志： 总结 Docker 常用安装总体步骤 搜索镜像 拉取镜像 查看镜像 启动镜像 停止镜像 移除镜像 安装 mysql docker hub 上查找 mysql 镜像： 从 docker hub (阿里云加速器) 拉取 mysql 镜像到本地，标签为 5.6： 使用 mysql:5.6 镜像创建容器 (也叫运行镜像)： 命令说明： 1234567891011121314151617docker run -p 12345:3306 --name mysql -v /zzyyuse/mysql/conf:/etc/mysql/conf.d -v /zzyyuse/mysql/logs:/logs -v /zzyyuse/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6----------------------------------------------命令说明:-p 12345:3306: 将主机的12345端口映射到docker容器的3306端口-name mysql: 运行服务名字-v /zzyyuse/mysql/conf:/etc/mysql/conf.d: 将主机/zzyyuse/mysq|目录下的conf/my.cnf挂载到容器的/etc/mysql/conf.d-v /zzyyuse/mysql/logs:/logs: 将主机/zzyyuse/mysql目录下的logs目录挂载到容器的/logs-v /zzyyuse/mysql/data:/var/lib/mysql: 将主机/zzyyuse/mysql目录下的data目录挂载到容器的/var/lib/mysql-e MYSQL_ROOT_PASSWORD=123456: 初始化root用户的密码-d mysql:5.6: 后台程序运行mysql5.6 ----------------------------------------------docker exec -it mysql运行成功后的容器ID /bin/bash---------------------------------------------- 将 mysql 数据备份测试： 1$ docker exec mysql运行成功后的容器ID sh -c &#x27;exec mysqldump --all-databases -uroot -p&quot;123456&quot;&#x27; &gt;/zzyyuse/all-database.sql 安装 redis 从 docker hub 上 (阿里云加速器) 拉取 redis 镜像到本地，标签为 3.2： 使用 redis:3.2 镜像创建容器 (也叫运行镜像)： 1$ docker run -p 6379:6379 -v /zzyyuse/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -v /zzyyuse/myredis/data:/data -d redis:3.2 redis-server /usr/local/etc/redis/redis.conf --appendonly yes 命令中的 redis.conf 是路径，不是文件。 在主机 /zzyyuse/myredis/conf/redis.conf 目录下新建 redis 配置文件 redis.conf，并添加如下内容： 1$ vim /zzyyuse/myredis/conf/redis.conf/redis.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000100110021003100410051006100710081009101010111012101310141015101610171018101910201021102210231024102510261027102810291030103110321033103410351036103710381039104010411042104310441045104610471048104910501051105210531054105510561057105810591060106110621063106410651066106710681069107010711072107310741075107610771078107910801081108210831084108510861087108810891090109110921093109410951096109710981099110011011102110311041105110611071108110911101111111211131114111511161117111811191120112111221123112411251126112711281129113011311132113311341135113611371138113911401141114211431144114511461147114811491150115111521153115411551156115711581159116011611162116311641165116611671168116911701171117211731174117511761177117811791180118111821183118411851186118711881189119011911192119311941195119611971198119912001201120212031204120512061207120812091210121112121213121412151216121712181219122012211222122312241225122612271228122912301231123212331234123512361237123812391240124112421243124412451246124712481249125012511252125312541255125612571258125912601261126212631264126512661267126812691270127112721273127412751276127712781279128012811282128312841285128612871288128912901291129212931294129512961297129812991300130113021303130413051306130713081309131013111312131313141315131613171318131913201321132213231324132513261327132813291330133113321333133413351336133713381339134013411342134313441345134613471348134913501351135213531354135513561357135813591360136113621363136413651366136713681369137013711372137313741375137613771378137913801381138213831384138513861387138813891390139113921393139413951396139713981399140014011402140314041405140614071408140914101411141214131414141514161417141814191420142114221423142414251426142714281429143014311432143314341435143614371438143914401441144214431444144514461447144814491450145114521453145414551456145714581459146014611462146314641465146614671468146914701471147214731474147514761477147814791480148114821483148414851486148714881489149014911492149314941495149614971498149915001501150215031504150515061507150815091510151115121513151415151516151715181519152015211522152315241525152615271528152915301531153215331534153515361537153815391540154115421543154415451546154715481549155015511552155315541555155615571558155915601561156215631564156515661567156815691570157115721573157415751576157715781579158015811582158315841585158615871588158915901591159215931594159515961597159815991600160116021603160416051606160716081609161016111612161316141615161616171618161916201621162216231624162516261627162816291630163116321633163416351636163716381639164016411642164316441645164616471648164916501651165216531654165516561657165816591660166116621663166416651666166716681669167016711672167316741675167616771678167916801681168216831684168516861687168816891690169116921693169416951696169716981699170017011702170317041705170617071708170917101711171217131714171517161717171817191720172117221723172417251726172717281729173017311732173317341735173617371738173917401741174217431744174517461747174817491750175117521753175417551756175717581759176017611762176317641765176617671768176917701771177217731774177517761777177817791780178117821783178417851786178717881789179017911792179317941795179617971798179918001801180218031804180518061807180818091810181118121813181418151816181718181819182018211822182318241825182618271828182918301831183218331834183518361837183818391840184118421843184418451846184718481849185018511852185318541855185618571858185918601861# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won&#x27;t be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you&#x27;d better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 loopback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.protected-mode yes# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# TLS/SSL ###################################### By default, TLS/SSL is disabled. To enable it, the &quot;tls-port&quot; configuration# directive can be used to define TLS-listening ports. To enable TLS on the# default port, use:## port 0# tls-port 6379# Configure a X.509 certificate and private key to use for authenticating the# server to connected clients, masters or cluster peers. These files should be# PEM formatted.## tls-cert-file redis.crt # tls-key-file redis.key# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange:## tls-dh-params-file redis.dh# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL# clients and peers. Redis requires an explicit configuration of at least one# of these, and will not implicitly use the system wide configuration.## tls-ca-cert-file ca.crt# tls-ca-cert-dir /etc/ssl/certs# By default, clients (including replica servers) on a TLS port are required# to authenticate using valid client side certificates.## If &quot;no&quot; is specified, client certificates are not required and not accepted.# If &quot;optional&quot; is specified, client certificates are accepted and must be# valid if provided, but are not required.## tls-auth-clients no# tls-auth-clients optional# By default, a Redis replica does not attempt to establish a TLS connection# with its master.## Use the following directive to enable TLS on replication links.## tls-replication yes# By default, the Redis Cluster bus uses a plain TCP connection. To enable# TLS for the bus protocol, use the following directive:## tls-cluster yes# Explicitly specify TLS versions to support. Allowed values are case insensitive# and include &quot;TLSv1&quot;, &quot;TLSv1.1&quot;, &quot;TLSv1.2&quot;, &quot;TLSv1.3&quot; (OpenSSL &gt;= 1.1.1) or# any combination. To enable only TLSv1.2 and TLSv1.3, use:## tls-protocols &quot;TLSv1.2 TLSv1.3&quot;# Configure allowed ciphers. See the ciphers(1ssl) manpage for more information# about the syntax of this string.## Note: this configuration applies only to &lt;= TLSv1.2.## tls-ciphers DEFAULT:!MEDIUM# Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more# information about the syntax of this string, and specifically for TLSv1.3# ciphersuites.## tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256# When choosing a cipher, use the server&#x27;s preference instead of the client# preference. By default, the server follows the client&#x27;s preference.## tls-prefer-server-ciphers yes# By default, TLS session caching is enabled to allow faster and less expensive# reconnections by clients that support it. Use the following directive to disable# caching.## tls-session-caching no# Change the default number of TLS sessions cached. A zero value sets the cache# to unlimited size. The default size is 20480.## tls-session-cache-size 5000# Change the default timeout of cached TLS sessions. The default timeout is 300# seconds.## tls-session-cache-timeout 60################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &#x27;yes&#x27; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set &#x27;syslog-enabled&#x27; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &#x27;databases&#x27;-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&#x27;s set to &#x27;yes&#x27; as it&#x27;s almost always a win.# If you want to save some CPU in the saving child set it to &#x27;no&#x27; but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# Remove RDB files used by replication in instances without persistence# enabled. By default this option is disabled, however there are environments# where for regulations or other security concerns, RDB files persisted on# disk by masters in order to feed replicas, or stored on disk by replicas# in order to load them for the initial synchronization, should be deleted# ASAP. Note that this option ONLY WORKS in instances that have both AOF# and RDB persistence disabled, otherwise is completely ignored.## An alternative (and sometimes better) way to obtain the same effect is# to use diskless replication on both master and replicas instances. However# in the case of replicas, diskless is not always an option.rdb-del-sync-files no# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &#x27;dbfilename&#x27; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition replicas automatically try to reconnect to masters# and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.## masterauth &lt;master-password&gt;## However this is not enough if you are using Redis ACLs (for Redis version# 6 or greater), and the default user is not capable of running the PSYNC# command and/or other commands needed for replication. In this case it&#x27;s# better to configure a special user to use with replication, and specify the# masteruser configuration as such:## masteruser &lt;username&gt;## When masteruser is specified, the replica will authenticate against its# master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;.# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to &#x27;yes&#x27; (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to &#x27;no&#x27; the replica will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It&#x27;s just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using &#x27;rename-command&#x27; to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## New replicas and reconnecting replicas that are not able to continue the# replication process just receiving differences, need to do what is called a# &quot;full synchronization&quot;. An RDB file is transmitted from the master to the# replicas.## The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child# producing the RDB file finishes its work. With diskless replication instead# once the transfer starts, new replicas arriving will be queued and a new# transfer will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple# replicas will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the# server waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# -----------------------------------------------------------------------------# WARNING: RDB diskless load is experimental. Since in this setup the replica# does not immediately store an RDB on disk, it may cause data loss during# failovers. RDB diskless load + Redis modules not handling I/O reads may also# cause Redis to abort in case of I/O errors during the initial synchronization# stage with the master. Use only if your do what you are doing.# -----------------------------------------------------------------------------## Replica can load the RDB it reads from the replication link directly from the# socket, or store the RDB to a file and read that file after it was completely# recived from the master.## In many cases the disk is slower than the network, and storing and loading# the RDB file may increase replication time (and even increase the master&#x27;s# Copy on Write memory and salve buffers).# However, parsing the RDB file directly from the socket may mean that we have# to flush the contents of the current database before the full rdb was# received. For this reason we have the following options:## &quot;disabled&quot; - Don&#x27;t use diskless load (store the rdb file to the disk first)# &quot;on-empty-db&quot; - Use diskless load only when it is completely safe.# &quot;swapdb&quot; - Keep a copy of the current db contents in RAM while parsing# the data directly from the socket. note that this requires# sufficient memory, if you don&#x27;t have it, you risk an OOM kill.repl-diskless-load disabled# Replicas send PINGs to server in a predefined interval. It&#x27;s possible to# change this interval with the repl_ping_replica_period option. The default# value is 10 seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a# replica wants to reconnect again, often a full resync is not needed, but a# partial resync is enough, just passing the portion of data the replica# missed while disconnected.## The bigger the replication backlog, the longer the time the replica can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a replica connected.## repl-backlog-size 1mb# After a master has no longer connected replicas for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last replica disconnected, for# the backlog buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO# output. It is used by Redis Sentinel in order to select a replica to promote# into a master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel# will pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a replica is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the replica to connect with the master.## Port: The port is communicated by the replica during the replication# handshake, and is normally the port that the replica is using to# listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may be actually reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234############################### KEYS TRACKING ################################## Redis implements server assisted support for client side caching of values.# This is implemented using an invalidation table that remembers, using# 16 millions of slots, what clients may have certain subsets of keys. In turn# this is used in order to send invalidation messages to clients. Please# to understand more about the feature check this page:## https://redis.io/topics/client-side-caching## When tracking is enabled for a client, all the read only queries are assumed# to be cached: this will force Redis to store information in the invalidation# table. When keys are modified, such information is flushed away, and# invalidation messages are sent to the clients. However if the workload is# heavily dominated by reads, Redis could use more and more memory in order# to track the keys fetched by many clients.## For this reason it is possible to configure a maximum fill value for the# invalidation table. By default it is set to 1M of keys, and once this limit# is reached, Redis will start to evict keys in the invalidation table# even if they were not modified, just to reclaim memory: this will in turn# force the clients to invalidate the cached values. Basically the table# maximum size is a trade off between the memory you want to spend server# side to track information about who cached what, and the ability of clients# to retain cached objects in memory.## If you set the value to 0, it means there are no limits, and Redis will# retain as many keys as needed in the invalidation table.# In the &quot;stats&quot; INFO section, you can find information about the number of# keys in the invalidation table at every given moment.## Note: when key tracking is used in broadcasting mode, no memory is used# in the server side so this setting is useless.## tracking-table-max-keys 1000000################################## SECURITY #################################### Warning: since Redis is pretty fast an outside user can try up to# 1 million passwords per second against a modern box. This means that you# should use very strong passwords, otherwise they will be very easy to break.# Note that because the password is really a shared secret between the client# and the server, and should not be memorized by any human, the password# can be easily a long string from /dev/urandom or whatever, so by using a# long and unguessable password no brute force attack will be possible.# Redis ACL users are defined in the following format:## user &lt;username&gt; ... acl rules ...## For example:## user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99## The special username &quot;default&quot; is used for new connections. If this user# has the &quot;nopass&quot; rule, then new connections will be immediately authenticated# as the &quot;default&quot; user without the need of any password provided via the# AUTH command. Otherwise if the &quot;default&quot; user is not flagged with &quot;nopass&quot;# the connections will start in not authenticated state, and will require# AUTH (or the HELLO command AUTH option) in order to be authenticated and# start to work.## The ACL rules that describe what an user can do are the following:## on Enable the user: it is possible to authenticate as this user.# off Disable the user: it&#x27;s no longer possible to authenticate# with this user, however the already authenticated connections# will still work.# +&lt;command&gt; Allow the execution of that command# -&lt;command&gt; Disallow the execution of that command# +@&lt;category&gt; Allow the execution of all the commands in such category# with valid categories are like @admin, @set, @sortedset, ...# and so forth, see the full list in the server.c file where# the Redis command table is described and defined.# The special category @all means all the commands, but currently# present in the server, and that will be loaded in the future# via modules.# +&lt;command&gt;|subcommand Allow a specific subcommand of an otherwise# disabled command. Note that this form is not# allowed as negative like -DEBUG|SEGFAULT, but# only additive starting with &quot;+&quot;.# allcommands Alias for +@all. Note that it implies the ability to execute# all the future commands loaded via the modules system.# nocommands Alias for -@all.# ~&lt;pattern&gt; Add a pattern of keys that can be mentioned as part of# commands. For instance ~* allows all the keys. The pattern# is a glob-style pattern like the one of KEYS.# It is possible to specify multiple patterns.# allkeys Alias for ~*# resetkeys Flush the list of allowed keys patterns.# &gt;&lt;password&gt; Add this passowrd to the list of valid password for the user.# For example &gt;mypass will add &quot;mypass&quot; to the list.# This directive clears the &quot;nopass&quot; flag (see later).# &lt;&lt;password&gt; Remove this password from the list of valid passwords.# nopass All the set passwords of the user are removed, and the user# is flagged as requiring no password: it means that every# password will work against this user. If this directive is# used for the default user, every new connection will be# immediately authenticated with the default user without# any explicit AUTH command required. Note that the &quot;resetpass&quot;# directive will clear this condition.# resetpass Flush the list of allowed passwords. Moreover removes the# &quot;nopass&quot; status. After &quot;resetpass&quot; the user has no associated# passwords and there is no way to authenticate without adding# some password (or setting it as &quot;nopass&quot; later).# reset Performs the following actions: resetpass, resetkeys, off,# -@all. The user returns to the same state it has immediately# after its creation.## ACL rules can be specified in any order: for instance you can start with# passwords, then flags, or key patterns. However note that the additive# and subtractive rules will CHANGE MEANING depending on the ordering.# For instance see the following example:## user alice on +@all -DEBUG ~* &gt;somepassword## This will allow &quot;alice&quot; to use all the commands with the exception of the# DEBUG command, since +@all added all the commands to the set of the commands# alice can use, and later DEBUG was removed. However if we invert the order# of two ACL rules the result will be different:## user alice on -DEBUG +@all ~* &gt;somepassword## Now DEBUG was removed when alice had yet no commands in the set of allowed# commands, later all the commands are added, so the user will be able to# execute everything.## Basically ACL rules are processed left-to-right.## For more information about ACL configuration please refer to# the Redis web site at https://redis.io/topics/acl# ACL LOG## The ACL Log tracks failed commands and authentication events associated# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in memory. You can reclaim memory with # ACL LOG RESET. Define the maximum entry length of the ACL Log below.acllog-max-len 128# Using an external ACL file## Instead of configuring users here in this file, it is possible to use# a stand-alone file just listing users. The two methods cannot be mixed:# if you configure users here and at the same time you activate the exteranl# ACL file, the server will refuse to start.## The format of the external ACL user file is exactly the same as the# format that is used inside redis.conf to describe users.## aclfile /etc/redis/users.acl# IMPORTANT NOTE: starting with Redis 6 &quot;requirepass&quot; is just a compatiblity# layer on top of the new ACL system. The option effect will be just setting# the password for the default user. Clients will still authenticate using# AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt;# if they follow the new protocol: both will work.## requirepass foobared# Command renaming (DEPRECATED).## ------------------------------------------------------------------------# WARNING: avoid using this option if possible. Instead use ACLs to remove# commands from the default user, and put them only in some admin user you# create for administrative purposes.# ------------------------------------------------------------------------## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &#x27;max number of clients reached&#x27;.## IMPORTANT: When Redis Cluster is used, the max number of connections is also# shared with the cluster bus: every node in the cluster will use two# connections, one incoming and another outgoing. It is important to size the# limit accordingly in case of very large clusters.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&#x27;t remove keys according to the policy, or if the policy is# set to &#x27;noeviction&#x27;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the &#x27;noeviction&#x27; policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is &#x27;noeviction&#x27;).## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select one from the following behaviors:## volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key having an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don&#x27;t evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica# to have a different memory setting, and you are sure all the writes performed# to the replica are idempotent, then you may change this default (but be sure# to understand what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory# and so forth). So make sure you monitor your replicas and make sure they# have enough memory to never hit a real out-of-memory condition before the# master hits the configured maxmemory setting.## replica-ignore-maxmemory yes# Redis reclaims expired keys in two ways: upon access when those keys are# found to be expired, and also in background, in what is called the# &quot;active expire key&quot;. The key space is slowly and interactively scanned# looking for expired keys to reclaim, so that it is possible to free memory# of keys that are expired and will never be accessed again in a short time.## The default effort of the expire cycle will try to avoid having more than# ten percent of expired keys still in memory, and will try to avoid consuming# more than 25% of total memory and to add latency to the system. However# it is possible to increase the expire &quot;effort&quot; that is normally set to# &quot;1&quot;, to a greater value, up to the value &quot;10&quot;. At its maximum value the# system will use more CPU, longer cycles (and technically may introduce# more latency), and will tollerate less already expired keys still present# in the system. It&#x27;s a tradeoff betweeen memory, CPU and latecy.## active-expire-effort 1############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It&#x27;s up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives.lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no# It is also possible, for the case when to replace the user code DEL calls# with UNLINK calls is not easy, to modify the default behavior of the DEL# command to act exactly like UNLINK, using the following configuration# directive:lazyfree-lazy-user-del no################################ THREADED I/O ################################## Redis is mostly single threaded, however there are certain threaded# operations such as UNLINK, slow I/O accesses and other things that are# performed on side threads.## Now it is also possible to handle Redis clients socket reads and writes# in different I/O threads. Since especially writing is so slow, normally# Redis users use pipelining in order to speedup the Redis performances per# core, and spawn multiple instances in order to scale more. Using I/O# threads it is possible to easily speedup two times Redis without resorting# to pipelining nor sharding of the instance.## By default threading is disabled, we suggest enabling it only in machines# that have at least 4 or more cores, leaving at least one spare core.# Using more than 8 threads is unlikely to help much. We also recommend using# threaded I/O only if you actually have performance problems, with Redis# instances being able to use a quite big percentage of CPU time, otherwise# there is no point in using this feature.## So for instance if you have a four cores boxes, try to use 2 or 3 I/O# threads, if you have a 8 cores, try to use 6 threads. In order to# enable I/O threads use the following configuration directive:## io-threads 4## Setting io-threads to 1 will just use the main thread as usually.# When I/O threads are enabled, we only use threads for writes, that is# to thread the write(2) syscall and transfer the client buffers to the# socket. However it is also possible to enable threading of reads and# protocol parsing using the following configuration directive, by setting# it to yes:## io-threads-do-reads no## Usually threading reads doesn&#x27;t help much.## NOTE 1: This configuration directive cannot be changed at runtime via# CONFIG SET. Aso this feature currently does not work when SSL is# enabled.## NOTE 2: If you want to test the Redis speedup using redis-benchmark, make# sure you also run the benchmark itself in threaded mode, using the# --threads option to match the number of Redis theads, otherwise you&#x27;ll not# be able to notice the improvements.############################ KERNEL OOM CONTROL ############################### On Linux, it is possible to hint the kernel OOM killer on what processes# should be killed first when out of memory.## Enabling this feature makes Redis actively control the oom_score_adj value# for all its processes, depending on their role. The default scores will# attempt to have background child processes killed before all others, and# replicas killed before masters.oom-score-adj no# When oom-score-adj is used, this directive controls the specific values used# for master, replica and background child processes. Values range -1000 to# 1000 (higher means more likely to be killed).## Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)# can freely increase their value, but not decrease it below its initial# settings.## Values are used relative to the initial value of oom_score_adj when the server# starts. Because typically the initial value is 0, they will often match the# absolute values.oom-score-adj-values 0 200 800############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don&#x27;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that&#x27;s usually the right compromise between# speed and data safety. It&#x27;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&#x27;s snapshotting),# or on the contrary, use &quot;always&quot; that&#x27;s very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&#x27;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&#x27;t happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&#x27;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################ Normal Redis instances can&#x27;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages# in order to try to give an advantage to the replica with the best# replication offset (more data from the master processed).# Replicas will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the replica will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they&#x27;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&#x27;t be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# This option, when set to yes, allows nodes to serve read traffic while the# the cluster is in a down state, as long as it believes it owns the slots. ## This is useful for two cases. The first case is for when an application # doesn&#x27;t require consistency of data during node failures or network partitions.# One example of this is a cache, where as long as the node has the data it# should be able to serve it. ## The second use case is for configurations that don&#x27;t meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the# entire cluster without this option set, with it set there is only a write outage.# Without a quorum of masters, slot ownership will not change automatically. ## cluster-allow-reads-when-down no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&#x27;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# t Stream commands# m Key-miss events (Note: It is not included in the &#x27;A&#x27; class)# A Alias for g$lshzxet, so that the &quot;AKE&quot; string means all the events# (Except key-miss events which are excluded from &#x27;A&#x27; due to their# unique nature).## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don&#x27;t need# this feature and the feature has some overhead. Note that if you don&#x27;t# specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### GOPHER SERVER ################################## Redis contains an implementation of the Gopher protocol, as specified in# the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt).## The Gopher protocol was very popular in the late &#x27;90s. It is an alternative# to the web, and the implementation both server and client side is so simple# that the Redis server has just 100 lines of code in order to implement this# support.## What do you do with Gopher nowadays? Well Gopher never *really* died, and# lately there is a movement in order for the Gopher more hierarchical content# composed of just plain text documents to be resurrected. Some want a simpler# internet, others believe that the mainstream internet became too much# controlled, and it&#x27;s cool to create an alternative space for people that# want a bit of fresh air.## Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol# as a gift.## --- HOW IT WORKS? ---## The Redis Gopher support uses the inline protocol of Redis, and specifically# two kind of inline requests that were anyway illegal: an empty request# or any request that starts with &quot;/&quot; (there are no Redis commands starting# with such a slash). Normal RESP2/RESP3 requests are completely out of the# path of the Gopher protocol implementation and are served as usually as well.## If you open a connection to Redis when Gopher is enabled and send it# a string like &quot;/foo&quot;, if there is a key named &quot;/foo&quot; it is served via the# Gopher protocol.## In order to create a real Gopher &quot;hole&quot; (the name of a Gopher site in Gopher# talking), you likely need a script like the following:## https://github.com/antirez/gopher2redis## --- SECURITY WARNING ---## If you plan to put Redis on the internet in a publicly accessible address# to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance.# Once a password is set:## 1. The Gopher server (when enabled, not by default) will still serve# content via Gopher.# 2. However other commands cannot be called before the client will# authenticate.## So use the &#x27;requirepass&#x27; option to protect your instance.## To enable Gopher support uncomment the following line and set# the option from no (the default) to yes.## gopher-enabled no############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&#x27;t start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don&#x27;t compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&#x27;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&#x27;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&#x27;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here, but must be 1mb or greater## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it&#x27;s maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don&#x27;t have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag no# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage, to be used when the lower# threshold is reached# active-defrag-cycle-min 1# Maximal effort for defrag in CPU percentage, to be used when the upper# threshold is reached# active-defrag-cycle-max 25# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000# Jemalloc background thread for purging will be enabled by defaultjemalloc-bg-thread yes# It is possible to pin different threads and processes of Redis to specific# CPUs in your system, in order to maximize the performances of the server.# This is useful both in order to pin different Redis threads in different# CPUs, but also in order to make sure that multiple Redis instances running# in the same host will be pinned to different CPUs.## Normally you can do this using the &quot;taskset&quot; command, however it is also# possible to this via Redis configuration directly, both in Linux and FreeBSD.## You can pin the server/IO threads, bio threads, aof rewrite child process, and# the bgsave child process. The syntax to specify the cpu list is the same as# the taskset command:## Set redis server/io threads to cpu affinity 0,2,4,6:# server_cpulist 0-7:2## Set bio threads to cpu affinity 1,3:# bio_cpulist 1,3## Set aof rewrite child process to cpu affinity 8,9,10,11:# aof_rewrite_cpulist 8-11## Set bgsave child process to cpu affinity 1,10,11# bgsave_cpulist 1,10-11 测试 redis-cli 连接： 1$ docker exec -it 运行的redis服务容器的ID redis-cli 查看持久化文件生成： 推送镜像到阿里云本地镜像发布到阿里云的流程 本地镜像推送到阿里云 本地镜像素材原型： 升级为 1.4 (按需)： 1$ docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] -a：提交镜像的作者；-m：提交时的文字说明。 阿里云开发者平台： https://promotion.aliyun.com/ntms/act/kubernetes.html 创建镜像仓库： 将镜像推送到阿里云： 123$ sudo docker login --username= registry.cn-shenzhen.aliyuncs.com$ sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/[命名空间]/[仓库名称]:[镜像版本号]$ sudo docker push registry.cn-shenzhen.aliyuncs.com/[命名空间]/[仓库名称]:[镜像版本号] ImageId 即为要推送的本地镜像。阿里云仓库中的镜像版本号可以与本地镜像 tag 保持一致，也可以不一致。 查看： 下载： 本文参考https://www.bilibili.com/video/BV1Ls411n7mx https://gitee.com/jack-GCQ/brain-map 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}]},{"title":"Flink 入门","slug":"flink","date":"2021-04-25T05:01:45.000Z","updated":"2021-06-02T13:47:14.124Z","comments":true,"path":"2021/04/25/flink/","link":"","permalink":"http://example.com/2021/04/25/flink/","excerpt":"","text":"Flink 流处理简介Flink 官网：https://flink.apache.org/ Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Apache Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行状态计算。 为什么选择 Flink： 流数据更真实地反映了我们的生活方式。 传统的数据架构是基于有限数据集的。 我们的目标： 低延迟 高吞吐 结果的准确性和良好的容错性 哪些行业需要处理流数据： 电商和市场营销 数据报表、广告投放、业务流程需要。 物联网 (IOT) 传感器实时数据采集和显示、实时报警，交通运输业。 电信业 基站流量调配。 银行和金融业 实时结算和通知推送，实时检测异常行为。 传统数据处理架构： 事务处理架构： 特点：实时性好，但数据量大时，难以进行高并发处理。(低延迟、低吞吐) 分析处理架构： 特点：将数据从业务数据库复制到数仓，再进行分析和查询。能够处理大数据，高并发，但实时性差。(高延迟、高吞吐) 有状态的流式处理 (第一代)： 数据存储于内存当中，达到 Periodic Checkpoint 条件时，执行持久化存储。能够做到低延迟、高吞吐，但分布式架构下，难以保证数据的顺序。 lambda 架构 (第二代)： 采用两套系统，同时保证低延迟和结果准确： 批处理系统处理速度慢，但准确性高。 流处理系统处理速度快，但准确性差。 缺点：实现一个功能，但需要维护两套系统，开发成本高。 流处理系统的演变： Storm 能够做到低延迟，Spark Streaming 能做到高吞吐，而 Flink 不仅综合了它们的优点，同时还能做得更好。 Flink 可以看作第三代流处理架构。 Flink 的特点： 事件驱动 (Event-driven) 基于流的世界观 在 Flink 的世界观中，一切都是由流组成的，离线数据是有界的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。 分层 API 越顶层越抽象，表达含义越简明，使用越方便。 越底层越具体，表达能力越丰富，使用越灵活。 支持事件时间 (event-time) 和处理时间 (processing-time) 语义。 精确一次 (exactly-once) 的状态一致性保证。 低延迟，每秒处理数百万个事件，毫秒级延迟。 与众多常用存储系统的连接。 高可用，动态扩展，实现 7 * 24 小时全天候运行。 Flink vs Spark Streaming： Flink 是流处理 (stream) 架构，Spark Streaming 是微批处理 (micro-batching) 架构。 数据模型 Spark 采用 RDD 模型，Spark Streaming 的 DStream 实际上也就是一组组小批数据 RDD 的集合。—&gt; 底层实现基于微批 Flink 基本数据模型是数据流，以及事件 (Event) 序列。—&gt; 底层实现就是流，一个一个的处理 运行时架构 Spark 是批计算，将 DAG 划分为不同的 stage，一个完成后才可以计算下一个。—&gt; 需要等待 Flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理。—&gt; 不需要等待 QuickStart 添加依赖： 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.xisun.flink&lt;/groupId&gt; &lt;artifactId&gt;xisun-flink&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;flink.version&gt;1.12.1&lt;/flink.version&gt; &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; flink-streaming-java_2.12：2.12 是 scala 版本，Flink 底层组件依赖 scala。 Flink 1.11 版本之后，需要添加 flink-clients_2.12 依赖，否则会报异常 java.lang.IllegalStateException: No ExecutorFactory found to execute the application.。 批处理实现 WordCount hello.txt 文件内容： 1234567hello javahello worldhello flinkhello sparkhello scalahow are youfine thank you 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package cn.xisun.flink;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.util.Collector;/** * @author XiSun * @Date 2021/4/25 20:39 */public class WordCount &#123; /** * 自定义类，实现FlatMapFunction接口 * 参数说明： * String：传入数据类型 * Tuple2&lt;String, Integer&gt;：传出数据类型 * Tuple2&lt;T0, T1&gt;：Flink自身实现的元组，注意不要用scala的 */ public static class MyFlatMapper implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; // 按空格分词 String[] words = line.split(&quot; &quot;); // 遍历所有word，包装成二元组输出 for (String str : words) &#123; // 每一个word，都包装成一个二元组对象，并计数为1，然后用out收集 out.collect(new Tuple2&lt;&gt;(str, 1)); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; // 1.创建批处理执行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2.从resources路径下的文件中单线程读取数据，是按照一行一行读取的 String inputPath = &quot;src/main/resources/hello.txt&quot;; DataSet&lt;String&gt; inputDataSet = env.readTextFile(inputPath).setParallelism(1); // 3.对数据集进行处理，按空格分词展开，转换成(word, 1)这样的二元组进行统计 DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; resultSet = inputDataSet.flatMap(new MyFlatMapper()) .groupBy(0)// 按照元组第一个位置的word分组 .sum(1);// 按照元组第二个位置上的数据求和 // 4.打印输出 resultSet.print(); &#125;&#125; 输出结果： 12345678910111213141516SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.(scala,1)(you,2)(flink,1)(world,1)(hello,5)(are,1)(java,1)(thank,1)(fine,1)(how,1)(spark,1)Process finished with exit code 0 流处理实现 WordCount 基于文件读取数据，非真正的流式数据。 批处理 —&gt; 几组或所有数据到达后才处理；流处理 —&gt; 有数据来就直接处理，不等数据堆叠到一定数量级。 这里不像批处理有 groupBy —&gt; 所有数据统一处理，而是用流处理的 keyBy —&gt; 每一个数据都对 key 进行 hash 计算，进行类似分区的操作，来一个数据就处理一次，所有中间过程都有输出！ 并行度：本地 IDEA 执行环境的并行度默认就是计算机的 CPU 逻辑核数。 代码实现： 12345678910111213141516171819202122232425262728293031323334package cn.xisun.flink;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;/** * @author XiSun * @Date 2021/4/25 20:45 */public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度，可选操作，默认值 = 当前计算机的CPU逻辑核数(设置成1即为单线程处理) env.setParallelism(4); // 2.从resources路径下的文件中多线程读取数据，是按照一行一行读取的 String inputPath = &quot;src/main/resources/hello.txt&quot;; DataStream&lt;String&gt; inputDataStream = env.readTextFile(inputPath); // 3.对数据集进行处理，按空格分词展开，转换成(word, 1)这样的二元组进行统计 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); // 4.打印输出 resultStream.print(); // 5.执行任务 env.execute(); &#125;&#125; 不同于批处理，env.execute() 之前的代码，可以理解为是在定义任务，只有执行 env.execute() 后，Flink 才把前面的代码片段当作一个任务整体 (每个线程根据这个任务操作，并行处理流数据)。 输出结果： 123456789101112131415161718192021SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.2&gt; (java,1)7&gt; (flink,1)1&gt; (scala,1)1&gt; (spark,1)4&gt; (are,1)3&gt; (hello,1)3&gt; (hello,2)6&gt; (how,1)3&gt; (thank,1)3&gt; (hello,3)3&gt; (hello,4)5&gt; (fine,1)5&gt; (you,1)5&gt; (you,2)5&gt; (world,1)3&gt; (hello,5)Process finished with exit code 0 因为是流处理，所以所有中间过程都会被输出，前面的序号就是并行执行任务的线程编号。 线程最大编号为 7，是因为本机配置是 4 核 8 处理器，默认并行度为 8。 流式数据源测试 开启适用于 Linux 的 Windows 子系统。 第一次使用时，需要先安装 ubuntu 系统： 通过 nc -lk &lt;port&gt; 命令打开一个 socket 服务，用于模拟实时的流数据。 代码实现： 12345678910111213141516171819202122232425262728293031323334package cn.xisun.flink;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;/** * @author XiSun * @Date 2021/4/26 10:40 */public class StreamWordCount &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度，默认值 = 当前计算机的CPU逻辑核数(设置成1即为单线程处理) env.setParallelism(4); // 2.从数据流中读取数据 DataStream&lt;String&gt; inputDataStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.对数据集进行处理，按空格分词展开，转换成(word, 1)这样的二元组进行统计 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1); // 4.打印输出 resultStream.print(); // 5.执行任务 env.execute(); &#125;&#125; 生产环境时，一般是在程序的启动参数中设置主机和端口号，此时，可以通过 ParameterTool 工具提取参数： 参数设置格式：--host localhost --port 7777。 1234567// 用parameter tool工具从程序启动参数中提取配置项ParameterTool parameterTool = ParameterTool.fromArgs(args);String host = parameterTool.get(&quot;host&quot;);int port = parameterTool.getInt(&quot;port&quot;);// 2.从数据流中读取数据DataStream&lt;String&gt; inputDataStream = env.socketTextStream(host, port); 输出结果：在本地开启的 socket 中输入数据，并观察 IDEA 的 console 输出。 1234xisun@DESKTOP-OM8IACS:/mnt/c/Users/Xisun/Desktop$ nc -lk 7777hello worldhello flinkhello scala 123456789SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.3&gt; (world,1)2&gt; (hello,1)4&gt; (flink,1)2&gt; (hello,2)1&gt; (scala,1)2&gt; (hello,3) Flink 部署Standalone 模式 下载：https://flink.apache.org/downloads.html#apache-flink-1122 地址：https://flink.apache.org/downloads.html#apache-flink-1122 Flink 组件： 其他组件 (hadoop)： 解压到指定目录，并把 hadoop 的组件添加到 Flink 解压路径的 lib 目录下： 打开 wsl 控制台，安装 JDK： 12xisun@DESKTOP-OM8IACS:/mnt/d/tools/flink-1.12.2$sudo apt updatexisun@DESKTOP-OM8IACS:/mnt/d/tools/flink-1.12.2$sudo apt install openjdk-8-jdk 1234xisun@DESKTOP-OM8IACS:/mnt/d/tools/flink-1.12.2$ java -versionopenjdk version &quot;1.8.0_282&quot;OpenJDK Runtime Environment (build 1.8.0_282-8u282-b08-0ubuntu1~20.04-b08)OpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode) 在 Ubuntu 20.04 系统下安装 OpenJDK 11 和 OpenJDK 8 的方法：https://ywnz.com/linuxjc/6984.html 查看 JDK 安装路径，并在 Flink 的 conf/flink-conf.yaml 文件中配置 Java 环境： 12# Javaenv.java.home: /usr/lib/jvm/java-8-openjdk-amd64 建议不要使用 Windows 系统下安装的 JDK 路径，可能会有问题。 启动 Flink 集群： 1234xisun@DESKTOP-OM8IACS:/mnt/c/Users/Ziyoo/Desktop$ bash /mnt/d/tools/flink-1.12.2/bin/start-cluster.shStarting cluster.Starting standalonesession daemon on host DESKTOP-OM8IACS.Starting taskexecutor daemon on host DESKTOP-OM8IACS. 1234xisun@DESKTOP-OM8IACS:/mnt/c/Users/Ziyoo/Desktop$ jps7347 Jps6956 StandaloneSessionClusterEntrypoint7246 TaskManagerRunner 前端页面访问：localhost:8081。(8081 是默认端口) 上传 jar 包：Submit New Job —&gt; Add New，将打包好的 jar 包添加上来。 设置启动参数： Show Plan 查看任务执行计划： Submit 提交任务： 提交失败： 提交任务时，如果 slot 的数量低于设置的线程数量，会提交失败，会一直等待分配更多的资源。—&gt; 增加 slot 数量或者降低任务线程数量。 提交成功：(需要先启动本机的 socket 服务) 1234xisun@DESKTOP-OM8IACS:/mnt/c/Users/Ziyoo/Desktop$ nc -tl 7777hello worldhello flinkhellojava^[[D^[[D^[[D s s 并行度优先级：任务 -&gt; 程序全局 -&gt; 提交job时设置的并行度 -&gt; 配置文件默认的并行度 Flink 运行架构Flink 运行时的组件 作业管理器 (JobManager) 控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的 JobManager 所控制执行。 JobManager 会先接收到要执行的应用程序，这个应用程序会包括：作业图 (JobGraph)、逻辑数据流图 (logical dataflow graph) 和打包了所有的类、库和其它资源的 Jar 包。 JobManager 会把 JobGraph 转换成一个物理层面的数据流图，这个图被叫做 “执行图” (ExecutionGraph)，包含了所有可以并发执行的任务。 JobManager 会向资源管理器 (ResourceManager) 请求执行任务必要的资源，也就是任务管理器 (TaskManager) 上的插槽 (slot)。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的 TaskManager 上。而在运行过程中，JobManager 会负责所有需要中央协调的操作，比如说检查点 (checkpoints) 的协调。 任务管理器 (TaskManager) Flink 中的工作进程。通常在 Flink 中会有多个 TaskManager 运行，每一个 TaskManager 都包含了一定数量的插槽 (slots)。插槽的数量限制了 TaskManager 能够执行的任务数量。 启动之后，TaskManager 会向资源管理器 (ResourceManager) 注册它的插槽；收到资源管理器 (ResourceManager) 的指令后，TaskManager 就会将一个或者多个插槽提供给 JobManager 调用，然后 JobManager 就可以向插槽分配任务 (tasks) 来执行了。 在执行过程中，一个 TaskManager 可以跟其它运行同一应用程序的 TaskManager 交换数据。 资源管理器 (ResourceManager) 主要负责管理任务管理器 (TaskManager) 的插槽 (slot)，TaskManger 插槽是 Flink 中定义的处理资源单元。 Flink 为不同的环境和资源管理工具提供了不同资源管理器，比如 YARN、Mesos、K8s，以及 Standalone 部署。 当 JobManager 申请插槽资源时，ResourceManager 会将有空闲插槽的 TaskManager 分配给 JobManager。如果 ResourceManager 没有足够的插槽来满足 JobManager 的请求，它还可以向资源提供平台发起会话，以提供启动 TaskManager 进程的容器。 另外，ResourceManager 还负责终止空闲的 TaskManager，释放计算资源。 分发器 (Dispatcher) 可以跨作业运行，它为应用提交提供了 REST 接口。 当一个应用被提交执行时，分发器就会启动并将应用移交给一个 JobManager。 Dispatcher 也会启动一个 Web UI，用来方便地展示和监控作业执行的信息。 Dispatcher 在架构中可能并不是必需的，这取决于应用提交运行的方式。 任务提交流程当一个应用提交执行时，Flink 的各个组件交互协作的过程如下： 上图中 步骤 7 指 TaskManager 为 JobManager 提供 slots，步骤 8 表示 JobManager 提交要在 slots 中执行的任务给 TaskManager。 上图是从一个较为高层级的视角来看应用中各组件的交互协作。 如果部署的集群环境不同 (例如 YARN，Mesos，Kubernetes，Standalone等)，其中一些步骤可以被省略，或是有些组件会运行在同一个 JVM 进程中。 具体地，如果我们将 Flink 集群部署到 YARN 上，那么就会有如下的提交流程： Flink 任务提交后，Client 向 HDFS 上传 Flink 的 Jar 包和配置。 之后，Client 向 Yarn ResourceManager 提交任务，Yarn ResourceManager 分配 Container 资源并通知对应的 NodeManager 启动 ApplicationMaster. ApplicationMaster 启动后加载 Flink 的 Jar 包和配置构建环境，然后启动 JobManager，之后，JobManager 向 Flink 自身的 ResourceManager 申请资源，Flink 自身的 ResourceManager 再向 Yarn 的 ResourceManager 申请资源 (因为是 yarn 模式，所有资源归 yarn ResourceManager 管理)，申请到资源后，启动 TaskManager。 Yarn ResourceManager 分配 Container 资源后 ， 由 ApplicationMaster 通知资源所在节点的 NodeManager 启动 TaskManager。 NodeManager 加载 Flink 的 Jar 包和配置构建环境并启动 TaskManager，TaskManager 启动后向 JobManager 发送心跳包，并等待 JobManager 向其分配任务。 任务调度原理 客户端不是运行时和程序执行的一部分，但它用于准备并发送 dataflow (JobGraph) 给 Master (JobManager)，然后，客户端断开连接或者维持连接以等待接收计算结果。 当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 Client 为提交 Job 的客户端，可以是运行在任何机器上 (与 JobManager 环境连通即可)。提交 Job 后，Client 可以结束进程 (Streaming 的任务)，也可以不结束并等待结果返回。 JobManager 会产生一个执行图 (Dataflow Graph)，主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 Jar 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 TaskManager 在启动的时候就设置好了槽位数 (Slot)，每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。(如果一个 Slot 中启动多个线程，那么这几个线程类似 CPU 调度一样共用同一个 slot) TaskManger 与 SlotsFlink 流处理 API EnvironmentgetExecutionEnvironment 创建一个执行环境，表示当前执行程序的上下文。 如果程序是独立调用的，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回此集群的执行环境，也就是说，getExecutionEnvironment 会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。 批处理执行环境： 1ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 流处理执行环境： 1StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 在生产环境时，如果没有设置并行度，会以 flink-conf.yaml 中的配置为准，默认是 1： 123# The parallelism used for programs that did not specify and other parallelism.parallelism.default: 1 在本地 IDEA 执行环境时，默认并行度是本地计算机的 CPU 逻辑核数，本地计算机为 4 核 8 处理器，即默认并行度为 8，本文测试代码以此为基准。 createLocalEnvironment 返回本地执行环境，需要在调用时指定默认的并行度： 1LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1); createRemoteEnvironment 返回集群执行环境，将 Jar 提交到远程服务器。需要在调用时指定 JobManager 的 IP 和端口号，并指定要在集群中运行的 Jar 包。 12StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(&quot;jobmanage-hostname&quot;, 6123, &quot;YOURPATH//WordCount.jar&quot;); Source从集合读取数据 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 传感器温度读数的数据类型 * * @author XiSun * @Date 2021/4/28 20:59 */public class SensorReading &#123; // 属性：id，时间戳，温度值 private String id; private Long timestamp; private Double temperature; public SensorReading() &#123; &#125; public SensorReading(String id, Long timestamp, Double temperature) &#123; this.id = id; this.timestamp = timestamp; this.temperature = temperature; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public Long getTimestamp() &#123; return timestamp; &#125; public void setTimestamp(Long timestamp) &#123; this.timestamp = timestamp; &#125; public Double getTemperature() &#123; return temperature; &#125; public void setTemperature(Double temperature) &#123; this.temperature = temperature; &#125; @Override public String toString() &#123; return &quot;SensorReading&#123;&quot; + &quot;id=&#x27;&quot; + id + &#x27;\\&#x27;&#x27; + &quot;, timestamp=&quot; + timestamp + &quot;, temperature=&quot; + temperature + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617181920212223242526272829/** * @author XiSun * @Date 2021/4/28 20:59 */public class SourceTest1_Collection &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.Source: 从集合读取数据 DataStream&lt;SensorReading&gt; sensorDataStream = env.fromCollection( Arrays.asList( new SensorReading(&quot;sensor_1&quot;, 1547718199L, 35.8), new SensorReading(&quot;sensor_6&quot;, 1547718201L, 15.4), new SensorReading(&quot;sensor_7&quot;, 1547718202L, 6.7), new SensorReading(&quot;sensor_10&quot;, 1547718205L, 38.1) ) ); DataStream&lt;Integer&gt; intDataStream = env.fromElements(1, 2, 3, 4, 5, 6, 7, 8, 9); // 3. 打印，参数为数据流的名称，可选 sensorDataStream.print(&quot;sensorDataName&quot;); intDataStream.print(&quot;intDataStreamName&quot;); // 4. 执行任务，参数为Job的名称，可选 env.execute(&quot;JobName&quot;); &#125;&#125; 输出结果： 123456789101112131415161718SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.intDataStreamName:1&gt; 8intDataStreamName:5&gt; 4intDataStreamName:4&gt; 3sensorDataName:5&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;sensorDataName:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;intDataStreamName:3&gt; 2sensorDataName:4&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;intDataStreamName:2&gt; 1intDataStreamName:2&gt; 9intDataStreamName:7&gt; 6intDataStreamName:6&gt; 5sensorDataName:6&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;intDataStreamName:8&gt; 7Process finished with exit code 0 从文件读取数据 代码实现： 12345678910111213141516171819/** * @author XiSun * @Date 2021/4/28 22:10 */public class SourceTest2_File &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据：单线程读取文件，按顺序逐行读取，如果不设置，则多线程读取，文件内容会乱序 DataStream&lt;String&gt; dataStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.打印，多线程 dataStream.print(); // 4.执行任务 env.execute(); &#125;&#125; sensor.txt 文件内容： 1234567sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718207,36.3sensor_1,1547718209,32.8sensor_1,1547718212,37.1 输出结果 (多线程打印，每次输出结果都会不同)： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.4&gt; sensor_1,1547718207,36.31&gt; sensor_6,1547718201,15.43&gt; sensor_10,1547718205,38.16&gt; sensor_1,1547718212,37.12&gt; sensor_7,1547718202,6.75&gt; sensor_1,1547718209,32.88&gt; sensor_1,1547718199,35.8Process finished with exit code 0 设置 dataStream.print().setParallelism(1);，单线程打印的输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.sensor_1,1547718199,35.8sensor_6,1547718201,15.4sensor_7,1547718202,6.7sensor_10,1547718205,38.1sensor_1,1547718207,36.3sensor_1,1547718209,32.8sensor_1,1547718212,37.1Process finished with exit code 0 从 Kafka 消息队列读取数据 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 代码实现： 12345678910111213141516171819202122232425262728/** * @author XiSun * @Date 2021/4/28 22:15 */public class SourceTest3_Kafka &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.创建Kafka消费者 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;); properties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;); properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(&quot;sensor&quot;, new SimpleStringSchema(), properties); // 3.从Kafka读取数据 DataStream&lt;String&gt; dataStream = env.addSource(consumer); // 4.打印 dataStream.print(); // 5.执行任务 env.execute(); &#125;&#125; 自定义 Source 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * @author XiSun * @Date 2021/4/28 22:25 */public class SourceTest4_UDF &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// env.setParallelism(1); // 2.从自定义Source读取数据 DataStream&lt;SensorReading&gt; dataStream = env.addSource(new MySensorSource()); // 3.打印 dataStream.print(); // 4.执行任务 env.execute(); &#125; // 实现自定义的SourceFunction，随机生成传感器数据 public static class MySensorSource implements SourceFunction&lt;SensorReading&gt; &#123; // 定义一个标识位，用来控制数据的产生 private boolean running = true; @Override public void run(SourceContext&lt;SensorReading&gt; ctx) throws Exception &#123; // 定义一个随机数发生器 Random random = new Random(); // 设置10个传感器的初始温度 HashMap&lt;String, Double&gt; sensorTempMap = new HashMap&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; sensorTempMap.put(&quot;sensor_&quot; + (i + 1), 60 + random.nextGaussian() * 20); &#125; while (running) &#123; for (String sensorId : sensorTempMap.keySet()) &#123; // 在当前温度基础上随机波动 Double newtemp = sensorTempMap.get(sensorId) + random.nextGaussian(); sensorTempMap.put(sensorId, newtemp); ctx.collect(new SensorReading(sensorId, System.currentTimeMillis(), newtemp)); &#125; // 控制输出频率 Thread.sleep(1000L); &#125; &#125; @Override public void cancel() &#123; running = false; &#125; &#125;&#125; 输出结果 (程序会一直输出下去)： 12345678910111213141516171819SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.2&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1619670947828, temperature=57.523519020755195&#125;6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1619670947828, temperature=60.16683595604395&#125;6&gt; SensorReading&#123;id=&#x27;sensor_9&#x27;, timestamp=1619670947831, temperature=42.125026316308286&#125;4&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1619670947826, temperature=82.58226594512607&#125;5&gt; SensorReading&#123;id=&#x27;sensor_4&#x27;, timestamp=1619670947828, temperature=78.73909616880852&#125;4&gt; SensorReading&#123;id=&#x27;sensor_5&#x27;, timestamp=1619670947831, temperature=26.71490359887942&#125;5&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1619670947831, temperature=85.09026456845346&#125;1&gt; SensorReading&#123;id=&#x27;sensor_2&#x27;, timestamp=1619670947828, temperature=64.90731492147165&#125;3&gt; SensorReading&#123;id=&#x27;sensor_3&#x27;, timestamp=1619670947822, temperature=31.96909359527708&#125;3&gt; SensorReading&#123;id=&#x27;sensor_8&#x27;, timestamp=1619670947831, temperature=86.34172370619932&#125;4&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1619670948831, temperature=60.71931724451548&#125;6&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1619670948831, temperature=57.36109139383153&#125;3&gt; SensorReading&#123;id=&#x27;sensor_4&#x27;, timestamp=1619670948831, temperature=78.1152626762054&#125;5&gt; SensorReading&#123;id=&#x27;sensor_2&#x27;, timestamp=1619670948831, temperature=63.90599072985537&#125;2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1619670948831, temperature=82.1517010383502&#125;... Transform基本转换算子 map、flatMap、filter 通常被统一称为基本转换算子 (简单转换算子)。 map： flatMap： filter： 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * @author XiSun * @Date 2021/4/28 10:39 */public class TransformTest1_Base &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.map，把String转换成长度输出 DataStream&lt;Integer&gt; mapStream = inputStream.map(new MapFunction&lt;String, Integer&gt;() &#123; @Override public Integer map(String value) throws Exception &#123; return value.length(); &#125; &#125;); // 4.flatmap，按逗号分割字符串 DataStream&lt;String&gt; flatMapStream = inputStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; String[] fields = value.split(&quot;,&quot;); for (String field : fields) &#123; out.collect(field); &#125; &#125; &#125;); // 5.filter，筛选&quot;sensor_1&quot;开头的id对应的数据 DataStream&lt;String&gt; filterStream = inputStream.filter(new FilterFunction&lt;String&gt;() &#123; @Override public boolean filter(String value) throws Exception &#123; return value.startsWith(&quot;sensor_1&quot;); &#125; &#125;); // 6.打印 mapStream.print(&quot;map&quot;); flatMapStream.print(&quot;flatMap&quot;); filterStream.print(&quot;filter&quot;); // 7.执行任务 env.execute(); &#125;&#125; 输出结果： 1234567891011121314151617181920212223242526272829303132333435363738SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.map:5&gt; 24map:5&gt; 24flatMap:4&gt; sensor_1flatMap:4&gt; 1547718199flatMap:4&gt; 35.8flatMap:4&gt; sensor_1flatMap:4&gt; 1547718212flatMap:4&gt; 37.1flatMap:6&gt; sensor_7flatMap:6&gt; 1547718202flatMap:6&gt; 6.7map:6&gt; 24filter:3&gt; sensor_1,1547718199,35.8filter:3&gt; sensor_1,1547718212,37.1filter:6&gt; sensor_10,1547718205,38.1map:4&gt; 24map:1&gt; 23map:3&gt; 24map:2&gt; 25filter:1&gt; sensor_1,1547718207,36.3filter:2&gt; sensor_1,1547718209,32.8flatMap:5&gt; sensor_6flatMap:5&gt; 1547718201flatMap:5&gt; 15.4flatMap:3&gt; sensor_1flatMap:3&gt; 1547718209flatMap:3&gt; 32.8flatMap:2&gt; sensor_1flatMap:2&gt; 1547718207flatMap:2&gt; 36.3flatMap:1&gt; sensor_10flatMap:1&gt; 1547718205flatMap:1&gt; 38.1Process finished with exit code 0 聚合操作算子 DataStream 里没有 reduce 和 sum 这类聚合操作的方法，因为 Flink 设计中，所有数据必须先分组才能做聚合操作。 先 keyBy 得到 KeyedStream，然后调用其 reduce、sum 等聚合操作方法。(先分组后聚合) 常见的聚合操作算子主要有： keyBy 滚动聚合算子 Rolling Aggregation reduce keyBy： DataStream —&gt; KeyedStream：逻辑地将一个流拆分成不相交的分组，每个分组包含具有相同 key 的元素，在内部以 hash 的形式实现的。 keyBy 会重新分组。相同的 key 一定在同一个分组，而不同的 key 可能会在一个分组，因为是通过 hash 原理实现的，可能存在取模操作。 keyBy 可以按照元组的位置，或者对象的属性名分组，在 Flink 新版本中，有一些方法被弃用，可以用其他的方法替换。 12345678910111213141516171819202122public &lt;K&gt; KeyedStream&lt;T, K&gt; keyBy(KeySelector&lt;T, K&gt; key) &#123; Preconditions.checkNotNull(key); return new KeyedStream(this, (KeySelector)this.clean(key));&#125;public &lt;K&gt; KeyedStream&lt;T, K&gt; keyBy(KeySelector&lt;T, K&gt; key, TypeInformation&lt;K&gt; keyType) &#123; Preconditions.checkNotNull(key); Preconditions.checkNotNull(keyType); return new KeyedStream(this, (KeySelector)this.clean(key), keyType);&#125;/** @deprecated */@Deprecatedpublic KeyedStream&lt;T, Tuple&gt; keyBy(int... fields) &#123; return !(this.getType() instanceof BasicArrayTypeInfo) &amp;&amp; !(this.getType() instanceof PrimitiveArrayTypeInfo) ? this.keyBy((Keys)(new ExpressionKeys(fields, this.getType()))) : this.keyBy((KeySelector)KeySelectorUtil.getSelectorForArray(fields, this.getType()));&#125;/** @deprecated */@Deprecatedpublic KeyedStream&lt;T, Tuple&gt; keyBy(String... fields) &#123; return this.keyBy((Keys)(new ExpressionKeys(fields, this.getType())));&#125; 滚动聚合算子 (Rolling Aggregation)： 这些算子可以针对 KeyedStream 的每一个支流做聚合，包括： sum() min() max() minBy() maxBy() min()、max() 和 minBy()、maxBy() 的区别在于：前者每次输出时，只有作为参数比较的字段会更新，其他字段不变；而后者除了作为参数比较的字段会更新，其他的字段会一起更新。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @author XiSun * @Date 2021/4/30 9:44 */public class TransformTest2_RollingAggregation &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.按照SensorReading对象的id分组 // 方式一：直接以属性名作为参数，此方法已弃用 /*KeyedStream&lt;SensorReading, Tuple&gt; keyedStream = dataStream.keyBy(&quot;id&quot;);*/ // 方式二：以KeySelector作为参数 /*KeyedStream&lt;SensorReading, String&gt; keyedStream = dataStream.keyBy(new KeySelector&lt;SensorReading, String&gt;() &#123; @Override public String getKey(SensorReading sensorReading) throws Exception &#123; return sensorReading.getId(); &#125; &#125;);*/ // 方式三：方式二的Lambda表达式版 KeyedStream&lt;SensorReading, String&gt; keyedStream = dataStream.keyBy(SensorReading::getId); // 5.滚动聚合，取当前最大的温度值，可以输入对象的属性名，或者元组里面的位置 // DataStream&lt;SensorReading&gt; resultStream = keyedStream.max(&quot;temperature&quot;); DataStream&lt;SensorReading&gt; resultStream = keyedStream.maxBy(&quot;temperature&quot;); // 6.打印 resultStream.print(&quot;result&quot;); // 7.执行任务 env.execute(); &#125;&#125; max() 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.result:4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;result:2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=37.1&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=37.1&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=37.1&#125;Process finished with exit code 0 因为是滚动更新，对于每一个分组，每次来一条数据时，都会输出一次历史最大值，所以有的数据才会出现多次。 sensor_7 和 sensor_10 各属于一个分组 (线程2 和线程 4)，但各只有一条数据。sensor_6 和 sensor_1 在同一个分组 (线程 3)，sensor_6 只有一条数据，对于 sensor_1，有四条数据，temperature 最大值为 37.1，输出了四次，但每次只更新了 temperature 的值，其他字段的值没有更新。 maxBy() 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;result:2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;result:4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;result:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;Process finished with exit code 0 对于 sensor_1 的四条数据，每次输出时，除了更新 temperature 的值，其他字段的值也一起更新，但保留时间戳仍是当前 temperature 最大值对应的时间戳，而这个时间戳可能不是实时的值，比如第 9 行，其时间戳应该是 1547718209 (32.8 度的时间戳)，而不是 1547718207。 reduce： reduce，归约，适用于更加一般化的聚合操作场景。 KeyedStream —&gt; DataStream：一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。(返回值类型与传入类型一致，不能改变) 在前面 Rolling Aggregation 的前提下，对需求进行修改。获取同组历史温度最高的传感器信息，并要求实时更新其时间戳信息。 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @author XiSun * @Date 2021/4/30 16:57 */public class TransformTest3_Reduce &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度为1，能更好的体验效果，sensor.txt从上到下时间戳是递增的 env.setParallelism(1); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.按照SensorReading对象的id分组 KeyedStream&lt;SensorReading, String&gt; keyedStream = dataStream.keyBy(SensorReading::getId); // 5.reduce聚合，取每个分组最大的温度值，并更新为当前最新的时间戳 SingleOutputStreamOperator&lt;SensorReading&gt; resultStream = keyedStream.reduce(new ReduceFunction&lt;SensorReading&gt;() &#123; // curSensor：上次聚合的结果，newSensor：当前的元素 @Override public SensorReading reduce(SensorReading curSensor, SensorReading newSensor) throws Exception &#123; return new SensorReading(curSensor.getId(), newSensor.getTimestamp(), Math.max(curSensor.getTemperature(), newSensor.getTemperature())); &#125; &#125;); // 6.打印 resultStream.print(&quot;result&quot;); // 7.执行任务 env.execute(); &#125;&#125; 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.result&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;result&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;result&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;result&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;result&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;result&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=36.3&#125;result&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;Process finished with exit code 0 对于 sensor_1 的四条数据，从第 8 行开始，每次输出时，temperature 都是当前历史温度的最高值，而时间戳也在实时更新。 多流转换算子 多流转换算子一般包括： split 和 select (Filink 1.12.1 版本被移除) connect 和 coMap union split 和 select： split： DataStream —&gt; SplitStream：根据某些特征把一个 DataStream 拆分成两个或者多个 DataStream。 select： SplitStream —&gt; DataStream：从一个 SplitStream 中获取一个或者多个 DataStream。 split 和 select 需要结合使用。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author XiSun * @Date 2021/5/2 11:21 */public class TransformTest4_MultipleStreams &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.分流 SplitStream&lt;SensorReading&gt; splitStream = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading sensorReading) &#123; // 可以获得多个分流，此处按温度是否超过30℃设置了两个分流 return (sensorReading.getTemperature() &gt; 30) ? Collections.singletonList(&quot;high&quot;) : Collections.singletonList(&quot;low&quot;); &#125; &#125;); // 5.获取分流 DataStream&lt;SensorReading&gt; highTempStream = splitStream.select(&quot;high&quot;); DataStream&lt;SensorReading&gt; lowTempStream = splitStream.select(&quot;low&quot;); DataStream&lt;SensorReading&gt; allTempStream = splitStream.select(&quot;high&quot;, &quot;low&quot;); // 6.打印 highTempStream.print(&quot;high&quot;); lowTempStream.print(&quot;low&quot;); allTempStream.print(&quot;all&quot;); // 7.执行任务 env.execute(); &#125;&#125; 输出结果： 12345678910111213141516171819SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.all:2&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;all:5&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;high:5&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;all:6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;high:3&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;all:3&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;all:4&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;high:4&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;all:1&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;low:1&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;high:6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;all:6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;high:6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;low:2&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;Process finished with exit code 0 connect 和 coMap/coFlatMap： connect： DataStream，DataStream —&gt; ConnectedStreams：连接两个保持他们类型的数据流，两个数据流被 Connect 之后，只是被放在了同一个流中，其内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。 coMap/coFlatMap： ConnectedStreams —&gt; DataStream：作用于 ConnectedStreams 上，功能与 map 和 flatMap 一样，对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。 connect 和 coMap/coFlatMap 需要结合使用。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * @author XiSun * @Date 2021/5/2 11:21 */public class TransformTest4_MultipleStreams &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.分流 SplitStream&lt;SensorReading&gt; splitStream = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading sensorReading) &#123; // 可以获得多个分流，此处按温度是否超过30℃设置了两个分流 return (sensorReading.getTemperature() &gt; 30) ? Collections.singletonList(&quot;high&quot;) : Collections.singletonList(&quot;low&quot;); &#125; &#125;); // 5.获取分流 DataStream&lt;SensorReading&gt; highTempStream = splitStream.select(&quot;high&quot;); DataStream&lt;SensorReading&gt; lowTempStream = splitStream.select(&quot;low&quot;); DataStream&lt;SensorReading&gt; allTempStream = splitStream.select(&quot;high&quot;, &quot;low&quot;); // 6.合流connect，将高温流转换成二元组类型，再与低温流连接合并之后，输出状态信息 DataStream&lt;Tuple2&lt;String, Double&gt;&gt; warningStream = highTempStream.map(new MapFunction&lt;SensorReading, Tuple2&lt;String, Double&gt;&gt;() &#123; @Override public Tuple2&lt;String, Double&gt; map(SensorReading sensorReading) throws Exception &#123; return new Tuple2&lt;&gt;(sensorReading.getId(), sensorReading.getTemperature()); &#125; &#125;); ConnectedStreams&lt;Tuple2&lt;String, Double&gt;, SensorReading&gt; connectedStreams = warningStream.connect(lowTempStream); SingleOutputStreamOperator&lt;Object&gt; resultStream = connectedStreams.map(new CoMapFunction&lt;Tuple2&lt;String, Double&gt;, SensorReading, Object&gt;() &#123; @Override public Object map1(Tuple2&lt;String, Double&gt; stringDoubleTuple2) throws Exception &#123; return new Tuple3&lt;&gt;(stringDoubleTuple2.f0, stringDoubleTuple2.f1, &quot;high temperature warning&quot;); &#125; @Override public Object map2(SensorReading sensorReading) throws Exception &#123; return new Tuple3&lt;&gt;(sensorReading.getId(), sensorReading.getTemperature(), &quot;normal temperature&quot;); &#125; &#125;); // 7.打印 resultStream.print(); // 8.执行任务 env.execute(); &#125;&#125; 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.3&gt; (sensor_1,35.8,high temperature warning)2&gt; (sensor_1,32.8,high temperature warning)1&gt; (sensor_1,36.3,high temperature warning)4&gt; (sensor_6,15.4,normal temperature)3&gt; (sensor_1,37.1,high temperature warning)5&gt; (sensor_7,6.7,normal temperature)6&gt; (sensor_10,38.1,high temperature warning)Process finished with exit code 0 union： DataStream —&gt; DataStream：对两个或者两个以上的 DataStream 进行 union 操作，产生一个包含所有 DataStream 元素的新 DataStream。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * @author XiSun * @Date 2021/5/2 11:21 */public class TransformTest4_MultipleStreams &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.分流 SplitStream&lt;SensorReading&gt; splitStream = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading sensorReading) &#123; // 可以获得多个分流，此处按温度是否超过30℃设置了两个分流 return (sensorReading.getTemperature() &gt; 30) ? Collections.singletonList(&quot;high&quot;) : Collections.singletonList(&quot;low&quot;); &#125; &#125;); // 5.获取分流 DataStream&lt;SensorReading&gt; highTempStream = splitStream.select(&quot;high&quot;); DataStream&lt;SensorReading&gt; lowTempStream = splitStream.select(&quot;low&quot;); DataStream&lt;SensorReading&gt; allTempStream = splitStream.select(&quot;high&quot;, &quot;low&quot;); // 6.联合两个分流 DataStream&lt;SensorReading&gt; unionStream = highTempStream.union(lowTempStream); // 7.打印 unionStream.print(); // 8.执行任务 env.execute(); &#125;&#125; 输出结果： 123456789101112SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.6&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;5&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;Process finished with exit code 0 connect 与 union 区别： 执行 union 操作的流的类型必须是一样，connect 可以不一样，可以在之后的 coMap 中再调整为一样的类型。 connect 只能合并两个流，union 可以合并多个流。 算子转换 在 Flink 中，Transformation 算子就是将一个或多个 DataStream 转换为新的 DataStream，可以将多个转换组合成复杂的数据流拓扑。 如上图所示，DataStream 会由不同的 Transformation 操作，转换、过滤、聚合成其他不同的流，从而完成我们的业务要求。 支持的数据类型 Flink 流应用程序处理的是以数据对象表示的事件流。所以在 Flink 内部，我们需要能够处理这些对象。它们需要被序列化和反序列化，以便通过网络传送它们；或者从状态后端、检查点和保存点读取它们。为了有效地做到这一点，Flink 需要明确知道应用程序所处理的数据类型。Flink 使用类型信息的概念来表示数据类型，并为每个数据类型生成特定的序列化器、反序列化器和比较器。 Flink 还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息，从而获得序列化器和反序列化器。但是，在某些情况下，例如 lambda 函数或泛型类型，需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。 Flink 支持 Java 和 Scala 中所有常见数据类型。使用最广泛的类型有以下几种。 基础数据类型 Flink 支持所有的 Java 和 Scala 基础数据类型，Int，Double，Long，String，… 12DataStream&lt;Integer&gt; numberStream = env.fromElements(1, 2, 3, 4);numberStream.map(data -&gt; data * 2); Java 和 和 Scala 元组 (Tuples) Java 不像 Scala 天生支持元组 Tuple 类型，Java 的元组类型由 Flink 的包提供，默认提供 Tuple0 ~ Tuple25。 123DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; personStream = env.fromElements( new Tuple2&lt;&gt;(&quot;Adam&quot;, 17), new Tuple2&lt;&gt;(&quot;Sarah&quot;, 23));personStream.filter(p -&gt; p.f1 &gt; 18); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293@Publicpublic abstract class Tuple implements Serializable &#123; private static final long serialVersionUID = 1L; public static final int MAX_ARITY = 25; private static final Class&lt;?&gt;[] CLASSES = new Class[]&#123;Tuple0.class, Tuple1.class, Tuple2.class, Tuple3.class, Tuple4.class, Tuple5.class, Tuple6.class, Tuple7.class, Tuple8.class, Tuple9.class, Tuple10.class, Tuple11.class, Tuple12.class, Tuple13.class, Tuple14.class, Tuple15.class, Tuple16.class, Tuple17.class, Tuple18.class, Tuple19.class, Tuple20.class, Tuple21.class, Tuple22.class, Tuple23.class, Tuple24.class, Tuple25.class&#125;; public Tuple() &#123; &#125; public abstract &lt;T&gt; T getField(int var1); public &lt;T&gt; T getFieldNotNull(int pos) &#123; T field = this.getField(pos); if (field != null) &#123; return field; &#125; else &#123; throw new NullFieldException(pos); &#125; &#125; public abstract &lt;T&gt; void setField(T var1, int var2); public abstract int getArity(); public abstract &lt;T extends Tuple&gt; T copy(); public static Class&lt;? extends Tuple&gt; getTupleClass(int arity) &#123; if (arity &gt;= 0 &amp;&amp; arity &lt;= 25) &#123; return CLASSES[arity]; &#125; else &#123; throw new IllegalArgumentException(&quot;The tuple arity must be in [0, 25].&quot;); &#125; &#125; public static Tuple newInstance(int arity) &#123; switch(arity) &#123; case 0: return Tuple0.INSTANCE; case 1: return new Tuple1(); case 2: return new Tuple2(); case 3: return new Tuple3(); case 4: return new Tuple4(); case 5: return new Tuple5(); case 6: return new Tuple6(); case 7: return new Tuple7(); case 8: return new Tuple8(); case 9: return new Tuple9(); case 10: return new Tuple10(); case 11: return new Tuple11(); case 12: return new Tuple12(); case 13: return new Tuple13(); case 14: return new Tuple14(); case 15: return new Tuple15(); case 16: return new Tuple16(); case 17: return new Tuple17(); case 18: return new Tuple18(); case 19: return new Tuple19(); case 20: return new Tuple20(); case 21: return new Tuple21(); case 22: return new Tuple22(); case 23: return new Tuple23(); case 24: return new Tuple24(); case 25: return new Tuple25(); default: throw new IllegalArgumentException(&quot;The tuple arity must be in [0, 25].&quot;); &#125; &#125;&#125; Scala 样例类 (case classes)123case class Person(name: String, age: Int)val persons: DataStream[Person] = env.fromElements(Person(&quot;Adam&quot;, 17), Person(&quot;Sarah&quot;, 23))persons.filter(p =&gt; p.age &gt; 18) Java 简单对象 (POJO) 要求必须提供无参构造函数。 要求成员变量都是 public，或者 private 的提供 getter、setter 方法。 123456789101112public class Person &#123; public String name; public int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125;&#125; 1234DataStream Person &gt; persons = env.fromElements( new Person(&quot; Alex&quot;, 42), new Person(&quot; Wendy&quot;, 23)); 其它 (Arrays，Lists，Maps，Enums，等等) Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的，比如 Java 的 ArrayList，HashMap，Enum 等等。 实现 UDF 函数——更细粒度的控制流函数类 (Function Classes) Flink 暴露了所有 udf 函数的接口 (实现方式为接口或者抽象类)。例如 MapFunction，FilterFunction，ProcessFunction 等等。 下面的例子，实现了 FilterFunction 接口： 1DataStream&lt;String&gt; flinkTweets = tweets.filter(new FlinkFilter()); 123456public static class FlinkFilter implements FilterFunction&lt;String&gt; &#123; @Override public boolean filter(String value) throws Exception &#123; return value.contains(&quot;flink&quot;); &#125;&#125; 还可以将函数实现成匿名类： 123456DataStream&lt;String&gt; flinkTweets = tweets.filter(new FilterFunction&lt;String&gt;() &#123; @Override public boolean filter(String value) throws Exception &#123; return value.contains(&quot;flink&quot;); &#125;&#125;); 需要 filter 的字符串 “flink” 可以当作参数传进去： 123DataStream&lt;String&gt; tweets = env.readTextFile(&quot;INPUT_FILE &quot;);DataStream&lt;String&gt; flinkTweets = tweets.filter(new KeyWordFilter(&quot;flink&quot;)); 123456789101112public static class KeyWordFilter implements FilterFunction&lt;String&gt; &#123; private String keyWord; KeyWordFilter(String keyWord) &#123; this.keyWord = keyWord; &#125; @Override public boolean filter(String value) throws Exception &#123; return value.contains(this.keyWord); &#125;&#125; 匿名函数123DataStream&lt;String&gt; tweets = env.readTextFile(&quot;INPUT_FILE&quot;);DataStream&lt;String&gt; flinkTweets = tweets.filter(tweet -&gt; tweet.contains(&quot;flink&quot;)); 富函数 (Rich Functions) 富函数是 DataStream API 提供的一个函数类的接口，所有 Flink 函数类都有其 Rich 版本。 它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。 RichMapFunction RichFlatMapFunction RichFilterFunction … Rich Function 有一个生命周期的概念。典型的生命周期方法有： open() 是 rich function 的初始化方法，当一个算子例如 map 或者 filter 被调用之前，open() 会被调用。 close() 是生命周期中的最后一个调用的方法，做一些清理工作。 getRuntimeContext() 提供了函数的 RuntimeContext 的一些信息，例如函数执行的并行度，任务的名字，以及 state 状态。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author XiSun * @Date 2021/5/5 14:33 */public class TransformTest5_RichFunction &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; resultStream = dataStream.map(new MyMapper()); // 4.打印 resultStream.print(); // 5.执行任务 env.execute(); &#125; // 传统的Function不能获取上下文信息，只能处理当前数据，不能和其他数据交互 public static class MyMapper0 implements MapFunction&lt;SensorReading, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public Tuple2&lt;String, Integer&gt; map(SensorReading value) throws Exception &#123; return new Tuple2&lt;&gt;(value.getId(), value.getId().length()); &#125; &#125; // 实现自定义富函数类(RichMapFunction是一个abstract类) public static class MyMapper extends RichMapFunction&lt;SensorReading, Tuple2&lt;Integer, String&gt;&gt; &#123; @Override public Tuple2&lt;Integer, String&gt; map(SensorReading value) throws Exception &#123; // getRuntimeContext().getState(); return new Tuple2&lt;&gt;(getRuntimeContext().getIndexOfThisSubtask() + 1, value.getId()); &#125; @Override public void open(Configuration parameters) throws Exception &#123; // 初始化工作，一般是定义状态，或者建立数据库连接 System.out.println(&quot;open&quot;); &#125; @Override public void close() throws Exception &#123; // 一般是关闭连接和清空状态的收尾操作 System.out.println(&quot;close&quot;); &#125; &#125;&#125; 输出结果： 1234567891011121314151617181920SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.openopenopenopen2&gt; (2,sensor_7)2&gt; (2,sensor_1)3&gt; (3,sensor_10)1&gt; (1,sensor_6)1&gt; (1,sensor_1)closeclose4&gt; (4,sensor_1)4&gt; (4,sensor_1)closecloseProcess finished with exit code 0 由于设置了执行环境 env 的并行度为 4，所以有 4 个 slot 执行自定义的 RichFunction，输出 4 次 open 和 close。 数据重分区操作 在多并行度的情况下，Flink 对数据的分配方式有多种： 常用的分配方式有： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are broadcasted * to every parallel instance of the next operation. * * @return The DataStream with broadcast partitioning set. */public DataStream&lt;T&gt; broadcast() &#123; return setConnectionType(new BroadcastPartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are broadcasted * to every parallel instance of the next operation. In addition, it implicitly as many &#123;@link * org.apache.flink.api.common.state.BroadcastState broadcast states&#125; as the specified * descriptors which can be used to store the element of the stream. * * @param broadcastStateDescriptors the descriptors of the broadcast states to create. * @return A &#123;@link BroadcastStream&#125; which can be used in the &#123;@link #connect(BroadcastStream)&#125; * to create a &#123;@link BroadcastConnectedStream&#125; for further processing of the elements. */@PublicEvolvingpublic BroadcastStream&lt;T&gt; broadcast( final MapStateDescriptor&lt;?, ?&gt;... broadcastStateDescriptors) &#123; Preconditions.checkNotNull(broadcastStateDescriptors); final DataStream&lt;T&gt; broadcastStream = setConnectionType(new BroadcastPartitioner&lt;&gt;()); return new BroadcastStream&lt;&gt;(environment, broadcastStream, broadcastStateDescriptors);&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are shuffled * uniformly randomly to the next operation. * * @return The DataStream with shuffle partitioning set. */@PublicEvolvingpublic DataStream&lt;T&gt; shuffle() &#123; return setConnectionType(new ShufflePartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are forwarded to * the local subtask of the next operation. * * @return The DataStream with forward partitioning set. */public DataStream&lt;T&gt; forward() &#123; return setConnectionType(new ForwardPartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are distributed * evenly to instances of the next operation in a round-robin fashion. * * @return The DataStream with rebalance partitioning set. */public DataStream&lt;T&gt; rebalance() &#123; return setConnectionType(new RebalancePartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are distributed * evenly to a subset of instances of the next operation in a round-robin fashion. * * &lt;p&gt;The subset of downstream operations to which the upstream operation sends elements depends * on the degree of parallelism of both the upstream and downstream operation. For example, if * the upstream operation has parallelism 2 and the downstream operation has parallelism 4, then * one upstream operation would distribute elements to two downstream operations while the other * upstream operation would distribute to the other two downstream operations. If, on the other * hand, the downstream operation has parallelism 2 while the upstream operation has parallelism * 4 then two upstream operations will distribute to one downstream operation while the other * two upstream operations will distribute to the other downstream operations. * * &lt;p&gt;In cases where the different parallelisms are not multiples of each other one or several * downstream operations will have a differing number of inputs from upstream operations. * * @return The DataStream with rescale partitioning set. */@PublicEvolvingpublic DataStream&lt;T&gt; rescale() &#123; return setConnectionType(new RescalePartitioner&lt;T&gt;());&#125;/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output values all go to the first * instance of the next processing operator. Use this setting with care since it might cause a * serious performance bottleneck in the application. * * @return The DataStream with shuffle partitioning set. */@PublicEvolvingpublic DataStream&lt;T&gt; global() &#123; return setConnectionType(new GlobalPartitioner&lt;T&gt;());&#125; 默认情况下，使用的分配方式是 rebalance 策略，即轮询。 DataStream 类中，partitionCustom(...) 用于自定义重分区。 代码实现： 123456789101112131415161718192021222324252627282930313233343536/** * @author XiSun * @Date 2021/5/5 17:39 */public class TransformTest6_Partition &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;);// SingleOutputStreamOperator // 4.SingleOutputStreamOperator多并行度时，默认分配方式是rebalance，即轮询方式分配 dataStream.print(&quot;rebalance&quot;); // 5.shuffle (并非批处理中的获取一批后才打乱，这里每次获取到直接打乱且分区) DataStream&lt;String&gt; shuffleStream = inputStream.shuffle(); shuffleStream.print(&quot;shuffle&quot;); // 6.keyBy (按Hash，然后取模) dataStream.keyBy(SensorReading::getId).print(&quot;keyBy&quot;); // 7.global (直接发送给第一个分区，少数特殊情况才用) dataStream.global().print(&quot;global&quot;); env.execute(); &#125;&#125; 输出结果： 123456789101112131415161718192021222324252627282930313233SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.shuffle:2&gt; sensor_1,1547718199,35.8shuffle:3&gt; sensor_1,1547718207,36.3shuffle:3&gt; sensor_1,1547718212,37.1rebalance:2&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;shuffle:4&gt; sensor_6,1547718201,15.4shuffle:1&gt; sensor_7,1547718202,6.7shuffle:4&gt; sensor_10,1547718205,38.1rebalance:2&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;rebalance:3&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;rebalance:4&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;rebalance:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;rebalance:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;shuffle:4&gt; sensor_1,1547718209,32.8rebalance:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;keyBy:2&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;keyBy:4&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_10&#x27;, timestamp=1547718205, temperature=38.1&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_6&#x27;, timestamp=1547718201, temperature=15.4&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=32.8&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718199, temperature=35.8&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_7&#x27;, timestamp=1547718202, temperature=6.7&#125;global:1&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718207, temperature=36.3&#125;keyBy:3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718212, temperature=37.1&#125;Process finished with exit code 0 Sink Flink 没有类似于 spark 中的 foreach 方法，让用户进行迭代的操作。所有对外的输出操作都要利用 Sink 完成，最后通过类似如下的方式，完成整个任务的最终输出操作： 1stream.addSink(new MySink(xxxx)) Flink 官方提供了一部分框架的 Sink。除此以外，需要用户自定义实现 Sink。 地址：https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/datastream/overview/ Kafka 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * @author XiSun * @Date 2021/5/6 12:18 */public class SinkTest1_Kafka &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.创建Kafka消费者 Properties consumerProperties = new Properties(); consumerProperties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); consumerProperties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;); consumerProperties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;); consumerProperties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); consumerProperties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(&quot;sensor&quot;, new SimpleStringSchema(), consumerProperties); // 3.从Kafka读取数据 DataStream&lt;String&gt; inputStream = env.addSource(consumer); // 4.序列化从Kafka中读取的数据 DataStream&lt;String&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])).toString(); &#125;); // 5.创建Kafka生产者 Properties producerProperties = new Properties(); producerProperties.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); producerProperties.put(&quot;group.id&quot;, &quot;producer-group&quot;); producerProperties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); producerProperties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.ByteArraySerializer&quot;); FlinkKafkaProducer&lt;String&gt; producer = new FlinkKafkaProducer&lt;&gt;(&quot;sinkTest&quot;, new SimpleStringSchema(), producerProperties); // 6.将数据写入Kafka dataStream.addSink(producer); // 7.执行任务 env.execute(); &#125; Redis 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * @author XiSun * @Date 2021/5/6 12:35 */public class SinkTest2_Redis &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.定义jedis连接配置(我这里连接的是docker的redis) FlinkJedisPoolConfig redisConfig = new FlinkJedisPoolConfig.Builder() .setHost(&quot;localhost&quot;) .setPort(6379) .setPassword(&quot;123456&quot;) .setDatabase(0) .build(); // 6.将数据写入Redis dataStream.addSink(new RedisSink&lt;&gt;(redisConfig, new MyRedisMapper())); // 7.执行任务 env.execute(); &#125; // 5.自定义RedisMapper public static class MyRedisMapper implements RedisMapper&lt;SensorReading&gt; &#123; // 定义保存数据到Redis的命令，存成哈希表：hset sensor_temp id temperature @Override public RedisCommandDescription getCommandDescription() &#123; return new RedisCommandDescription(RedisCommand.HSET, &quot;sensor_temp&quot;); &#125; @Override public String getKeyFromData(SensorReading sensorReading) &#123; return sensorReading.getId(); &#125; @Override public String getValueFromData(SensorReading sensorReading) &#123; return sensorReading.getTemperature().toString(); &#125; &#125;&#125; 查看 Redis 数据： 123456789localhost:0&gt;hgetall sensor_temp1) &quot;sensor_1&quot;2) &quot;37.1&quot;3) &quot;sensor_6&quot;4) &quot;15.4&quot;5) &quot;sensor_7&quot;6) &quot;6.7&quot;7) &quot;sensor_10&quot;8) &quot;38.1&quot; Elasticsearch 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch7_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @author XiSun * @Date 2021/5/6 12:50 */public class SinkTest3_Es &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.定义es的连接配置 List&lt;HttpHost&gt; httpHosts = new ArrayList&lt;&gt;();// org.apache.http.HttpHost; httpHosts.add(new HttpHost(&quot;localhost&quot;, 9200)); // 6.将数据写入es dataStream.addSink(new ElasticsearchSink.Builder&lt;&gt;(httpHosts, new MyEsSinkFunction()).build()); // 7.执行任务 env.execute(); &#125; // 5.实现自定义的ES写入操作 public static class MyEsSinkFunction implements ElasticsearchSinkFunction&lt;SensorReading&gt; &#123; @Override public void open() throws Exception &#123; &#125; @Override public void close() throws Exception &#123; &#125; @Override public void process(SensorReading sensorReading, RuntimeContext runtimeContext, RequestIndexer requestIndexer) &#123; // 定义写入的数据source HashMap&lt;String, String&gt; dataSource = new HashMap&lt;&gt;(5); dataSource.put(&quot;id&quot;, sensorReading.getId()); dataSource.put(&quot;temp&quot;, sensorReading.getTemperature().toString()); dataSource.put(&quot;ts&quot;, sensorReading.getTimestamp().toString()); // 创建请求，作为向es发起的写入命令(ES7统一type就是_doc，不再允许指定type) IndexRequest indexRequest = Requests.indexRequest() .index(&quot;sensor&quot;) .source(dataSource); // 用index发送请求 requestIndexer.add(indexRequest); &#125; &#125;&#125; 查看 ElasticSearch 数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697$ curl &quot;localhost:9200/sensor/_search?pretty&quot;&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 7, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;jciyWXcBiXrGJa12kSQt&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;35.8&quot;, &quot;id&quot; : &quot;sensor_1&quot;, &quot;ts&quot; : &quot;1547718199&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;jsiyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;15.4&quot;, &quot;id&quot; : &quot;sensor_6&quot;, &quot;ts&quot; : &quot;1547718201&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;j8iyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;6.7&quot;, &quot;id&quot; : &quot;sensor_7&quot;, &quot;ts&quot; : &quot;1547718202&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;kMiyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;38.1&quot;, &quot;id&quot; : &quot;sensor_10&quot;, &quot;ts&quot; : &quot;1547718205&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;kciyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;36.3&quot;, &quot;id&quot; : &quot;sensor_1&quot;, &quot;ts&quot; : &quot;1547718207&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;ksiyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;32.8&quot;, &quot;id&quot; : &quot;sensor_1&quot;, &quot;ts&quot; : &quot;1547718209&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;sensor&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;k8iyWXcBiXrGJa12kSQu&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;temp&quot; : &quot;37.1&quot;, &quot;id&quot; : &quot;sensor_1&quot;, &quot;ts&quot; : &quot;1547718212&quot; &#125; &#125; ] &#125;&#125; JDBC 自定义 Sink 以 MySQL 为例，添加 MySQL 连接依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.19&lt;/version&gt;&lt;/dependency&gt; 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * @author XiSun * @Date 2021/5/6 13:02 */public class SinkTest4_Jdbc &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从文件读取数据，单线程读取 DataStream&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;).setParallelism(1); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 使用之前编写的随机变动温度的SourceFunction来生成数据，数据一直生成 /*DataStream&lt;SensorReading&gt; dataStream = env.addSource(new SourceTest4_UDF.MySensorSource());*/ // 4.将数据写入MySQL dataStream.addSink(new MyJdbcSink()); // 6.执行任务 env.execute(); &#125; // 5.实现自定义的SinkFunction public static class MyJdbcSink extends RichSinkFunction&lt;SensorReading&gt; &#123; // 声明连接和预编译语句 Connection connection = null; PreparedStatement insertStmt = null; PreparedStatement updateStmt = null; @Override public void open(Configuration parameters) throws Exception &#123; // 创建连接 connection = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/flink_test?useUnicode=true&amp;&quot; + &quot;serverTimezone=Asia/Shanghai&amp;characterEncoding=UTF-8&amp;useSSL=false&quot;, &quot;root&quot;, &quot;example&quot;); // 创建预编译语句，有占位符，可传入参数 insertStmt = connection.prepareStatement(&quot;insert into sensor_temp (id, temp) values (?, ?)&quot;); updateStmt = connection.prepareStatement(&quot;update sensor_temp set temp = ? where id = ?&quot;); &#125; // 每来一条数据，调用连接，执行sql @Override public void invoke(SensorReading sensorReading, Context context) throws Exception &#123; // 直接执行更新语句，如果没有更新那么就插入 updateStmt.setDouble(1, sensorReading.getTemperature()); updateStmt.setString(2, sensorReading.getId()); updateStmt.execute(); if (updateStmt.getUpdateCount() == 0) &#123; insertStmt.setString(1, sensorReading.getId()); insertStmt.setDouble(2, sensorReading.getTemperature()); insertStmt.execute(); &#125; &#125; @Override public void close() throws Exception &#123; insertStmt.close(); updateStmt.close(); connection.close(); &#125; &#125;&#125; 查看 MySQL 数据： 123456789101112131415161718192021222324252627282930313233mysql&gt; SELECT * FROM sensor_temp;+-----------+--------------------+| id | temp |+-----------+--------------------+| sensor_3 | 20.489172407885917 || sensor_10 | 73.01289164711463 || sensor_4 | 43.402500895809744 || sensor_1 | 6.894772325662007 || sensor_2 | 101.79309911751122 || sensor_7 | 63.070612021580324 || sensor_8 | 63.82606628090501 || sensor_5 | 57.67115738487047 || sensor_6 | 50.84442627975055 || sensor_9 | 52.58400793021675 |+-----------+--------------------+10 rows in set (0.00 sec)mysql&gt; SELECT * FROM sensor_temp;+-----------+--------------------+| id | temp |+-----------+--------------------+| sensor_3 | 19.498209543035923 || sensor_10 | 71.92981963197121 || sensor_4 | 43.566017489470426 || sensor_1 | 6.378208186786803 || sensor_2 | 101.71010087830145 || sensor_7 | 62.11402602179431 || sensor_8 | 64.33196455020062 || sensor_5 | 56.39071692662006 || sensor_6 | 48.952784757264894 || sensor_9 | 52.078086096436685 |+-----------+--------------------+10 rows in set (0.00 sec) Flink 中的 WindowWindow 概述 Streaming 流式计算是一种被设计用于处理无限数据集的数据处理引擎，而无限数据集是指一种不断增长的本质上无限的数据集，而 Window 是一种切割无限数据为有限块进行处理的手段。 Window 是无限数据流处理的核心，Window 将一个无限的 stream 拆分成有限大小的 “buckets” 桶，我们可以在这些桶上做计算操作。 Window 类型 时间窗口 (Time Window)：按照时间生成 Window。 滚动时间窗口 (Tumbling Windows) 滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口中，滚动窗口有一个固定的大小，并且不会出现重叠。 原理：依据固定的窗口长度对数据进行切片。 特点：时间对齐，窗口长度固定，没有重叠。 适用场景：适合做 BI 统计等 (做每个时间段的聚合计算)。 例如，如果指定了一个 5 分钟大小的滚动窗口，窗口的创建如下图所示： 滑动时间窗口 (Sliding Windows) 滑动窗口是固定窗口的更广义的一种形式。滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率。因此，滑动窗口如果滑动参数小于窗口大小的话，窗口是可以重叠的，在这种情况下元素会被分配到多个窗口中。 原理：滑动窗口由固定的窗口长度和滑动间隔组成。 特点：时间对齐，窗口长度固定，可以有重叠。 适用场景：对最近一个时间段内的统计 (比如求某接口最近 5 min 的失败率来决定是否要报警)。 例如，你有 10 分钟的窗口和 5 分钟的滑动，那么每个窗口中 5 分钟的窗口里包含着上个 10 分钟产生的数据，如下图所示： 会话窗口 (Session Windows) session 窗口分配器通过 session 活动来对元素进行分组，session 窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，当它在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关闭。一个 session 窗口通过一个 session 间隔来配置，这个 session 间隔定义了非活跃周期的长度，当这个非活跃周期产生，那么当前的 session 将关闭并且后续的元素将被分配到新的 session 窗口中去。 由一系列事件组合一个指定时间长度的 timeout 间隙组成，类似于 web 应用的 session，也就是一段时间没有接收到新数据就会生成新的窗口。 特点：时间无对齐。 计数窗口 (Count Window)：按照指定的数据条数生成一个 Window，与时间无关。 滚动计数窗口 滑动计数窗口 Window API概述 Flink 使用 window() 来定义一个窗口，然后基于这个 Window 去做一些聚合或者其他处理操作。 window() 是最基础的定义窗口的方法。 window() 必须在 keyBy 之后才能使用。 DataStream 的 windowAll() 类似数据传输分区的 global 操作，这个操作是 non-parallel 的 (并行度强行为 1)，所有的数据都会被传递到同一个算子 operator 上，官方建议如果非必要就不要用这个 API。 window() 之后需要有一个窗口函数。 一个完整的窗口操作参考如下： 123456DataStream&lt;Tuple2&lt;String, Double&gt;&gt; minTempPerWindowStream = datastream ---&gt; 数据流 .map(new MyMapper()) .keyBy(data -&gt; data.f0) ---&gt; 分组 .timeWindow(Time.seconds(15)) ---&gt; 开窗 .minBy(1); ---&gt; 窗口函数 window() 需要接收一个输入参数：WindowAssigner (窗口分配器)。 1234567891011121314151617/** * Windows this data stream to a &#123;@code WindowedStream&#125;, which evaluates windows over a key * grouped stream. Elements are put into windows by a &#123;@link WindowAssigner&#125;. The grouping of * elements is done both by key and by window. * * &lt;p&gt;A &#123;@link org.apache.flink.streaming.api.windowing.triggers.Trigger&#125; can be defined to * specify when windows are evaluated. However, &#123;@code WindowAssigners&#125; have a default &#123;@code * Trigger&#125; that is used if a &#123;@code Trigger&#125; is not specified. * * @param assigner The &#123;@code WindowAssigner&#125; that assigns elements to windows. * @return The trigger windows data stream. */@PublicEvolvingpublic &lt;W extends Window&gt; WindowedStream&lt;T, KEY, W&gt; window( WindowAssigner&lt;? super T, W&gt; assigner) &#123; return new WindowedStream&lt;&gt;(this, assigner);&#125; WindowAssigner 是一个抽象类，负责将每条输入的数据分发到正确的 Window 中。 WindowAssigner 的实现类位于 org.apache.flink.streaming.api.windowing.assigners 包下： 说明：这些实现类的构造方法多是 protected 或 privated 的，需要通过类中的静态方法如 of() 或 withGap() 来获取一个实例。 1234dataStream .keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) .minBy(1); 归纳起来，Flink 提供了四种类型通用的 WindowAssigner： 滚动窗口 (tumbling window) 滑动窗口 (sliding window) 会话窗口 (session window) 全局窗口 (global window) 除了 .window()，Flink 提供了更加简单的 .timeWindow() 和 .countWindow() 方法，用于定义时间窗口和计数窗口。 创建不同类型的窗口 Flink 创建窗口的方法有多种，实际使用时，按需求创建。 滚动时间窗口 (tumbling time window)：当时间达到窗口大小时，就会触发窗口的执行。 .window(TumblingProcessingTimeWindows.of(Time.seconds(10))) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .timeWindow(Time.seconds(15))，Flink 1.12.1 版本已弃用。 123456789101112131415161718192021/** * Windows this &#123;@code KeyedStream&#125; into tumbling time windows. * * &lt;p&gt;This is a shortcut for either &#123;@code .window(TumblingEventTimeWindows.of(size))&#125; or &#123;@code * .window(TumblingProcessingTimeWindows.of(size))&#125; depending on the time characteristic set * using &#123;@link * org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#setStreamTimeCharacteristic(org.apache.flink.streaming.api.TimeCharacteristic)&#125; * * @param size The size of the window. * @deprecated Please use &#123;@link #window(WindowAssigner)&#125; with either &#123;@link * TumblingEventTimeWindows&#125; or &#123;@link TumblingProcessingTimeWindows&#125;. For more information, * see the deprecation notice on &#123;@link TimeCharacteristic&#125; */@Deprecatedpublic WindowedStream&lt;T, KEY, TimeWindow&gt; timeWindow(Time size) &#123; if (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123; return window(TumblingProcessingTimeWindows.of(size)); &#125; else &#123; return window(TumblingEventTimeWindows.of(size)); &#125;&#125; 滑动时间窗口 (sliding time window)：两个参数，前者是 window_size，后者是 sliding_size。每隔 sliding_size 计算输出结果一次，每一次计算的 window 范围是 window_size 内的所有元素。 .window(TumblingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .window(TumblingEventTimeWindows.of(Time.seconds(10), Time.seconds(5s))) .timeWindow(Time.seconds(15), Time.seconds(5))，Flink 1.12.1 版本已弃用。 123456789101112131415161718192021/** * Windows this &#123;@code KeyedStream&#125; into sliding time windows. * * &lt;p&gt;This is a shortcut for either &#123;@code .window(SlidingEventTimeWindows.of(size, slide))&#125; or * &#123;@code .window(SlidingProcessingTimeWindows.of(size, slide))&#125; depending on the time * characteristic set using &#123;@link * org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#setStreamTimeCharacteristic(org.apache.flink.streaming.api.TimeCharacteristic)&#125; * * @param size The size of the window. * @deprecated Please use &#123;@link #window(WindowAssigner)&#125; with either &#123;@link * SlidingEventTimeWindows&#125; or &#123;@link SlidingProcessingTimeWindows&#125;. For more information, * see the deprecation notice on &#123;@link TimeCharacteristic&#125; */@Deprecatedpublic WindowedStream&lt;T, KEY, TimeWindow&gt; timeWindow(Time size, Time slide) &#123; if (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123; return window(SlidingProcessingTimeWindows.of(size, slide)); &#125; else &#123; return window(SlidingEventTimeWindows.of(size, slide)); &#125;&#125; 会话窗口 (session window) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) 滚动计数窗口 (tumbling count window)：当元素数量达到窗口大小时，就会触发窗口的执行。 .countWindow(5) 滑动计数窗口 (sliding count window)：两个参数，前者是 window_size，后者是 sliding_size。每隔 sliding_size 计算输出结果一次，每一次计算的 window 范围是 window_size 内的所有元素。 .countWindow(10, 2) 窗口函数 (window function) window function 定义了要对窗口中收集的数据做的计算操作，主要分为两类。 增量聚合函数 (incremental aggregation functions) 每条数据到来就进行计算，保持一个简单的状态。(来一条处理一条，但是不输出，到窗口临界位置才输出) 典型的增量聚合函数有 ReduceFunction，AggregateFunction。 ReduceFunction： 源码： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Base interface for Reduce functions. Reduce functions combine groups of elements to a single * value, by taking always two elements and combining them into one. Reduce functions may be used on * entire data sets, or on grouped data sets. In the latter case, each group is reduced * individually. * * &lt;p&gt;For a reduce functions that work on an entire group at the same time (such as the * MapReduce/Hadoop-style reduce), see &#123;@link GroupReduceFunction&#125;. In the general case, * ReduceFunctions are considered faster, because they allow the system to use more efficient * execution strategies. * * &lt;p&gt;The basic syntax for using a grouped ReduceFunction is as follows: * * &lt;pre&gt;&#123;@code * DataSet&lt;X&gt; input = ...; * * DataSet&lt;X&gt; result = input.groupBy(&lt;key-definition&gt;).reduce(new MyReduceFunction()); * &#125;&lt;/pre&gt; * * &lt;p&gt;Like all functions, the ReduceFunction needs to be serializable, as defined in &#123;@link * java.io.Serializable&#125;. * * @param &lt;T&gt; Type of the elements that this function processes. */@Public@FunctionalInterfacepublic interface ReduceFunction&lt;T&gt; extends Function, Serializable &#123; /** * The core method of ReduceFunction, combining two values into one value of the same type. The * reduce function is consecutively applied to all values of a group until only a single value * remains. * * @param value1 The first value to combine. * @param value2 The second value to combine. * @return The combined value of both input values. * @throws Exception This method may throw exceptions. Throwing an exception will cause the * operation to fail and may trigger recovery. */ T reduce(T value1, T value2) throws Exception;&#125; 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839/** * @author XiSun * @Date 2021/5/7 12:50 */public class WindowTest1_TimeWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，时间窗口，增量聚合函数 DataStream&lt;SensorReading&gt; resultStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) // 归约 .reduce(new ReduceFunction&lt;SensorReading&gt;() &#123; @Override public SensorReading reduce(SensorReading value1, SensorReading value2) throws Exception &#123; return new SensorReading(value1.getId(), value2.getTimestamp(), Math.max(value1.getTemperature(), value2.getTemperature())); &#125; &#125;); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125;&#125; 输出结果： 12345xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718212,37.1sensor_1,1547718199,35.8sensor_1,1547718209,32.8... 12345log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; SensorReading&#123;id=&#x27;sensor_1&#x27;, timestamp=1547718209, temperature=37.1&#125;... AggregateFunction： 源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137/** * The &#123;@code AggregateFunction&#125; is a flexible aggregation function, characterized by the following * features: * * &lt;ul&gt; * &lt;li&gt;The aggregates may use different types for input values, intermediate aggregates, and * result type, to support a wide range of aggregation types. * &lt;li&gt;Support for distributive aggregations: Different intermediate aggregates can be merged * together, to allow for pre-aggregation/final-aggregation optimizations. * &lt;/ul&gt; * * &lt;p&gt;The &#123;@code AggregateFunction&#125;&#x27;s intermediate aggregate (in-progress aggregation state) is * called the &lt;i&gt;accumulator&lt;/i&gt;. Values are added to the accumulator, and final aggregates are * obtained by finalizing the accumulator state. This supports aggregation functions where the * intermediate state needs to be different than the aggregated values and the final result type, * such as for example &lt;i&gt;average&lt;/i&gt; (which typically keeps a count and sum). Merging intermediate * aggregates (partial aggregates) means merging the accumulators. * * &lt;p&gt;The AggregationFunction itself is stateless. To allow a single AggregationFunction instance to * maintain multiple aggregates (such as one aggregate per key), the AggregationFunction creates a * new accumulator whenever a new aggregation is started. * * &lt;p&gt;Aggregation functions must be &#123;@link Serializable&#125; because they are sent around between * distributed processes during distributed execution. * * &lt;h1&gt;Example: Average and Weighted Average&lt;/h1&gt; * * &lt;pre&gt;&#123;@code * // the accumulator, which holds the state of the in-flight aggregate * public class AverageAccumulator &#123; * long count; * long sum; * &#125; * * // implementation of an aggregation function for an &#x27;average&#x27; * public class Average implements AggregateFunction&lt;Integer, AverageAccumulator, Double&gt; &#123; * * public AverageAccumulator createAccumulator() &#123; * return new AverageAccumulator(); * &#125; * * public AverageAccumulator merge(AverageAccumulator a, AverageAccumulator b) &#123; * a.count += b.count; * a.sum += b.sum; * return a; * &#125; * * public AverageAccumulator add(Integer value, AverageAccumulator acc) &#123; * acc.sum += value; * acc.count++; * return acc; * &#125; * * public Double getResult(AverageAccumulator acc) &#123; * return acc.sum / (double) acc.count; * &#125; * &#125; * * // implementation of a weighted average * // this reuses the same accumulator type as the aggregate function for &#x27;average&#x27; * public class WeightedAverage implements AggregateFunction&lt;Datum, AverageAccumulator, Double&gt; &#123; * * public AverageAccumulator createAccumulator() &#123; * return new AverageAccumulator(); * &#125; * * public AverageAccumulator merge(AverageAccumulator a, AverageAccumulator b) &#123; * a.count += b.count; * a.sum += b.sum; * return a; * &#125; * * public AverageAccumulator add(Datum value, AverageAccumulator acc) &#123; * acc.count += value.getWeight(); * acc.sum += value.getValue(); * return acc; * &#125; * * public Double getResult(AverageAccumulator acc) &#123; * return acc.sum / (double) acc.count; * &#125; * &#125; * &#125;&lt;/pre&gt; * * @param &lt;IN&gt; The type of the values that are aggregated (input values) ---&gt; 聚合值的类型(输入值) * @param &lt;ACC&gt; The type of the accumulator (intermediate aggregate state). ---&gt; 累加器的类型(中间聚合状态) * @param &lt;OUT&gt; The type of the aggregated result ---&gt; 聚合结果的类型 */@PublicEvolvingpublic interface AggregateFunction&lt;IN, ACC, OUT&gt; extends Function, Serializable &#123; /** * Creates a new accumulator, starting a new aggregate. * * &lt;p&gt;The new accumulator is typically meaningless unless a value is added via &#123;@link * #add(Object, Object)&#125;. * * &lt;p&gt;The accumulator is the state of a running aggregation. When a program has multiple * aggregates in progress (such as per key and window), the state (per key and window) is the * size of the accumulator. * * @return A new accumulator, corresponding to an empty aggregate. */ ACC createAccumulator(); /** * Adds the given input value to the given accumulator, returning the new accumulator value. * * &lt;p&gt;For efficiency, the input accumulator may be modified and returned. * * @param value The value to add * @param accumulator The accumulator to add the value to * @return The accumulator with the updated state */ ACC add(IN value, ACC accumulator); /** * Gets the result of the aggregation from the accumulator. * * @param accumulator The accumulator of the aggregation * @return The final aggregation result. */ OUT getResult(ACC accumulator); /** * Merges two accumulators, returning an accumulator with the merged state. * * &lt;p&gt;This function may reuse any of the given accumulators as the target for the merge and * return that. The assumption is that the given accumulators will not be used any more after * having been passed to this function. * * @param a An accumulator to merge * @param b Another accumulator to merge * @return The accumulator with the merged state */ ACC merge(ACC a, ACC b);&#125; 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @author XiSun * @Date 2021/5/7 12:50 */public class WindowTest1_TimeWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，时间窗口，增量聚合函数 DataStream&lt;Integer&gt; resultStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) // 统计每个分组下数据的个数，中间聚合状态的类型和最终输出的类型是一致的 .aggregate(new AggregateFunction&lt;SensorReading, Integer, Integer&gt;() &#123; // 创建一个累加器 @Override public Integer createAccumulator() &#123; // 初始值，从0开始 return 0; &#125; // 来一条数据后，该怎么累加 @Override public Integer add(SensorReading value, Integer accumulator) &#123; // 累加器基础上+1 return accumulator + 1; &#125; // 返回最终的处理结果 @Override public Integer getResult(Integer accumulator) &#123; // 就是返回累加器 return accumulator; &#125; // merge方法一般在session window中使用，可能会存在一些合并的操作 // 不存在分区合并，因为当前处理的都是keyBy之后的 @Override public Integer merge(Integer a, Integer b) &#123; // 为防止意外，将两个状态a和b相加 return a + b; &#125; &#125;); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125;&#125; 输出结果： 1234567xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718199,35.8sensor_1,1547718207,36.3sensor_1,1547718209,32.8sensor_1,1547718212,37.1sensor_10,1547718205,38.1... 1234567log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; 22&gt; 13&gt; 2... 全窗口函数 (full window functions) 先把窗口所有数据收集起来，等到计算的时候再遍历所有数据。(来一条存放一条，到窗口临界位置才遍历且计算、输出) 典型的全窗口函数有 ProcessWindowFunction，WindowFunction。 ProcessWindowFunction： 源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * Base abstract class for functions that are evaluated over keyed (grouped) windows using a context * for retrieving extra information. * * @param &lt;IN&gt; The type of the input value. * @param &lt;OUT&gt; The type of the output value. * @param &lt;KEY&gt; The type of the key. * @param &lt;W&gt; The type of &#123;@code Window&#125; that this window function can be applied on. */@PublicEvolvingpublic abstract class ProcessWindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends AbstractRichFunction &#123; private static final long serialVersionUID = 1L; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public abstract void process( KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out) throws Exception; /** * Deletes any state in the &#123;@code Context&#125; when the Window expires (the watermark passes its * &#123;@code maxTimestamp&#125; + &#123;@code allowedLateness&#125;). * * @param context The context to which the window is being evaluated * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public void clear(Context context) throws Exception &#123;&#125; /** The context holding window metadata. */ public abstract class Context implements java.io.Serializable &#123; /** Returns the window that is being evaluated. */ public abstract W window(); /** Returns the current processing time. */ public abstract long currentProcessingTime(); /** Returns the current event-time watermark. */ public abstract long currentWatermark(); /** * State accessor for per-key and per-window state. * * &lt;p&gt;&lt;b&gt;NOTE:&lt;/b&gt;If you use per-window state you have to ensure that you clean it up by * implementing &#123;@link ProcessWindowFunction#clear(Context)&#125;. */ public abstract KeyedStateStore windowState(); /** State accessor for per-key global state. */ public abstract KeyedStateStore globalState(); /** * Emits a record to the side output identified by the &#123;@link OutputTag&#125;. * * @param outputTag the &#123;@code OutputTag&#125; that identifies the side output to emit to. * @param value The record to emit. */ public abstract &lt;X&gt; void output(OutputTag&lt;X&gt; outputTag, X value); &#125;&#125; 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author XiSun * @Date 2021/5/7 12:50 */public class WindowTest1_TimeWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，时间窗口，全窗口函数 DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; resultStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) // 统计每个分组下数据的个数 .process(new ProcessWindowFunction&lt;SensorReading, Tuple3&lt;String, Long, Integer&gt;, String, TimeWindow&gt;() &#123; @Override public void process(String key, Context context, Iterable&lt;SensorReading&gt; elements, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out) throws Exception &#123; // 把elements转换为List，然后其长度就是当前分组下数据的个数 Integer count = IteratorUtils.toList(elements.iterator()).size(); // 输出一个三元组：key，窗口结束时间，数据个数 out.collect(new Tuple3&lt;&gt;(key, context.window().getEnd(), count)); &#125; &#125;); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125;&#125; 输出结果： 123456xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718199,35.8sensor_7,1547718202,6.7sensor_1,1547718209,32.8sensor_1,1547718212,37.1 ---&gt; 此数据的输入与前面相同的key的时间间隔，超出15s，在下一个窗口处理... 1234567log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.4&gt; (sensor_7,1620451335000,1)3&gt; (sensor_1,1620451335000,2)3&gt; (sensor_1,1620451365000,1)... WindowFunction： 源码： 12345678910111213141516171819202122/** * Base interface for functions that are evaluated over keyed (grouped) windows. * * @param &lt;IN&gt; The type of the input value. * @param &lt;OUT&gt; The type of the output value. * @param &lt;KEY&gt; The type of the key. ---&gt; 分组的key * @param &lt;W&gt; The type of &#123;@code Window&#125; that this window function can be applied on. */@Publicpublic interface WindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends Function, Serializable &#123; /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param window The window that is being evaluated. * @param input The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ void apply(KEY key, W window, Iterable&lt;IN&gt; input, Collector&lt;OUT&gt; out) throws Exception;&#125; 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @author XiSun * @Date 2021/5/7 12:50 */public class WindowTest1_TimeWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，时间窗口，全窗口函数 DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; resultStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15))) // 统计每个分组下数据的个数 .apply(new WindowFunction&lt;SensorReading, Tuple3&lt;String, Long, Integer&gt;, String, TimeWindow&gt;() &#123; /* input：当前输入的所有的数据 out：当前输出的数据 */ @Override public void apply(String key, TimeWindow window, Iterable&lt;SensorReading&gt; input, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out) throws Exception &#123; // 把input转换为List，然后其长度就是当前分组下数据的个数 Integer count = IteratorUtils.toList(input.iterator()).size(); // 输出一个三元组：key，窗口结束时间，数据个数 out.collect(new Tuple3&lt;&gt;(key, window.getEnd(), count)); &#125; &#125;); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125;&#125; 输出结果： 123456xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718199,35.8sensor_7,1547718202,6.7sensor_1,1547718209,32.8sensor_1,1547718212,37.1 ---&gt; 此数据的输入与前面相同的key的时间间隔，超出15s，在下一个窗口处理... 1234567log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; (sensor_1,1620450990000,2)4&gt; (sensor_7,1620450990000,1)3&gt; (sensor_1,1620451095000,1)... 前面的例子是以时间窗口写的，下面以计数窗口为例。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * @author XiSun * @Date 2021/5/8 13:21 */public class WindowTest2_CountWindow &#123; public static void main(String[] args) throws Exception &#123; // 1.创建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); // 2.从Scoket文本流读取数据，对于开窗，从本地读取数据时，耗时很短，可能窗口的临界点还没到，程序就结束了，也就看不到效果 DataStream&lt;String&gt; inputStream = env.socketTextStream(&quot;localhost&quot;, 7777); // 3.将文件内容转换成SensorReading对象 DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(&quot;,&quot;); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); &#125;); // 4.开窗测试，技术窗口，增量聚合函数 DataStream&lt;Double&gt; resultStream = dataStream.keyBy(SensorReading::getId) // 4个数开一个窗口，隔两个数滑动一次 .countWindow(4, 2) // 计算窗口内数据温度的平均值 .aggregate(new MyAvgTemp()); // 5.打印 resultStream.print(); // 6.执行任务 env.execute(); &#125; public static class MyAvgTemp implements AggregateFunction&lt;SensorReading, Tuple2&lt;Double, Integer&gt;, Double&gt; &#123; @Override public Tuple2&lt;Double, Integer&gt; createAccumulator() &#123; return new Tuple2&lt;&gt;(0.0, 0); &#125; @Override public Tuple2&lt;Double, Integer&gt; add(SensorReading value, Tuple2&lt;Double, Integer&gt; accumulator) &#123; // 每来一条数据，把温度值加到二元组的第一个元素上，二元组第二个元素自增1 return new Tuple2&lt;&gt;(accumulator.f0 + value.getTemperature(), accumulator.f1 + 1); &#125; @Override public Double getResult(Tuple2&lt;Double, Integer&gt; accumulator) &#123; // 返回所有数据温度的平均值 return accumulator.f0 / accumulator.f1; &#125; @Override public Tuple2&lt;Double, Integer&gt; merge(Tuple2&lt;Double, Integer&gt; a, Tuple2&lt;Double, Integer&gt; b) &#123; return new Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1); &#125; &#125;&#125; 输出结果： 123456789101112xisun@DESKTOP-OM8IACS:/mnt/c/WINDOWS/system32$ nc -tl 7777sensor_1,1547718199,1sensor_1,1547718199,2sensor_1,1547718199,3sensor_1,1547718199,4sensor_1,1547718199,5sensor_1,1547718199,6sensor_1,1547718199,7sensor_1,1547718199,8sensor_1,1547718199,9sensor_1,1547718199,10... 123456789log4j:WARN No appenders could be found for logger (org.apache.flink.api.java.ClosureCleaner).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.3&gt; 1.5 ---&gt; (1+2)/23&gt; 2.5 ---&gt; (1+2+3+4)/43&gt; 4.5 ---&gt; (3+4+5+6)/43&gt; 6.5 ---&gt; (5+6+7+8)/43&gt; 8.5 ---&gt; (7+8+9+10)/4... 滑动的距离是 2，因此前两个数计算一次平均值，后两个数来时，与前面两个数组成一个完整窗口 4 个数，计算一次平均值，后面都是 4 个数计算一次平均值。 其他可选 API .trigger()：触发器，定义 window 什么时候关闭，触发计算并输出结果。一般不使用。 .evictor()：移除器，定义移除某些数据的逻辑。一般不使用。 .allowedLateness()：允许处理迟到的数据。 .sideOutputLateData()：将迟到的数据放入侧输出流。 .getSideOutput()：获取侧输出流。 实例： 1234567891011121314OutputTag&lt;SensorReading&gt; outputTag = new OutputTag&lt;SensorReading&gt;(&quot;late&quot;) &#123; &#125;; SingleOutputStreamOperator&lt;SensorReading&gt; sumStream = dataStream.keyBy(SensorReading::getId) .window(TumblingProcessingTimeWindows.of(Time.seconds(15)))// .trigger()// .evictor() // 允许1分钟内的迟到数据&lt;=比如数据产生时间在窗口范围内，但是要处理的时候已经超过窗口时间了 .allowedLateness(Time.minutes(1)) // 侧输出流，迟到超过1分钟的数据，收集于此 .sideOutputLateData(outputTag) .sum(&quot;temperature&quot;); sumStream.getSideOutput(outputTag).print(&quot;late&quot;); Window API 总览 时间语义和 WartermarkFlink 中的时间语义 Event Time：事件创建的时间。 Event Time 是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事件时间戳。 Ingestion Time：数据进入Flink 的时间。 Processing Time：执行操作算子的本地系统时间，与机器相关。 哪种时间语义更重要 不同的时间语义有不同的应用场合。 我们往往更关心事件时间 (Event Time)。 这里假设玩游戏，两分钟内如果过 5 关就有奖励。用户坐地铁玩游戏，进入隧道前已经过 3 关，在隧道中又过了 5 关。但是信号不好，后 5 关通关的信息，等到出隧道的时候 (8:23:20) 才正式到达服务器。 在这个应用场合下，如果为了用户体验，则不应该使用 Processing Time，而是应该按照 Event Time 处理信息，保证用户获得游戏奖励。 Event Time 可以从日志数据的时间戳 (timestamp) 中提取： 2017-11-02 18:37:15.624 INFO Fail over to rm 在代码中设置 Event Time 在 Flink 的流式处理中，绝大部分的业务都会使用 Event Time，一般只在 Event Time 无法使用时，才会被迫使用 Processing Time 或者 Ingestion Time。 如果要使用 Event Time，那么需要引入 Event Time 的时间属性，引入方式如下所示： 123StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 从调用时刻开始给env创建的每一个stream追加时间特征env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 本文参考https://www.bilibili.com/video/BV1qy4y1q728 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"}]},{"title":"Spring 之 WebFlux","slug":"spring-webflux","date":"2021-04-23T07:35:11.000Z","updated":"2021-04-25T07:28:18.074Z","comments":true,"path":"2021/04/23/spring-webflux/","link":"","permalink":"http://example.com/2021/04/23/spring-webflux/","excerpt":"","text":"Spring WebFlux 介绍 官方文档：https://docs.spring.io/spring-framework/docs/5.2.7.RELEASE/spring-framework-reference/web-reactive.html#spring-webflux Spring WebFlux 是 Spring5 添加的新模块，用于 web 开发，功能和 Spring MVC 类似的，底层实现不同。 Spring WebFlux 是契合使用响应式编程而出现的框架。 传统的 web 框架，比如 Spring MVC、Struts2 等，是基于 Servlet 容器运行的。Spring WebFlux 是一种异步非阻塞的框架，异步非阻塞的框架在 Servlet3.1 以后才支持，其核心是基于 Reactor 的相关 API 实现的。 异步非阻塞： 异步和同步针对调用者：调用者发送请求，如果等着对方回应之后才去做其他事情就是同步，如果发送请求之后不等着对方回应就去做其他事情就是异步。 阻塞和非阻塞针对被调用者：被调用者收到请求时，如果做完请求任务之后才给出反馈就是阻塞，如果收到请求之后马上给出反馈，然后再去做任务就是非阻塞。 阻塞需要等待，非阻塞不需要等待。 Spring WebFlux 的特点： 非阻塞式：能够在有限的资源下，提高系统的吞吐量和伸缩性，从而处理更多的请求。Spring WebFlux 是以 Reactor 为基础来实现的响应式编程框架。 函数式编程：Spring5 框架基于 Java8，Spring Webflux 能够使用 Java8 的函数式编程方式来实现路由请求。 Spring WebFlux 和 Spring MVC 的对比： 两个框架都可以使用注解方式操作，也都可以运行在 Tomcat 等容器中。 Spring MVC 采用命令式编程，Spring WebFlux 采用异步响应式编程。 响应式编程概述 响应式编程是一种面向数据流和变化传播的编程范式。这意味着可以在编程语言中很方便地表达静态或动态的数据流，而相关的计算模型会自动将变化的值通过数据流进行传播。 例如，对于 a = b + c 这个表达式的处理，在命令式编程中，会先计算 b + c 的结果，再把此结果赋值给变量 a，因此 b，c 两值的变化不会对变量 a 产生影响。但在响应式编程中，变量 a 的值会随时跟随 b，c 的变化而变化。 电子表格程序就是响应式编程的一个例子。单元格可以包含字面值或类似 “= B1 + C1” 的公式，而包含公式的单元格的值会依据其他单元格的值的变化而变化。 Java8 及其之前版本的实现方式： 本质上使用的是观察者设计模式。 Java8 提供的观察者模式的两个类 Observer 和 Observable： 12345678910111213141516171819202122public class ObserverDemo extends Observable &#123; public static void main(String[] args) &#123; ObserverDemo observer = new ObserverDemo(); // 添加观察者 observer.addObserver(new Observer() &#123; @Override public void update(Observable o, Object arg) &#123; System.out.println(&quot;发生了变化&quot;); &#125; &#125;); observer.addObserver(new Observer() &#123; @Override public void update(Observable o, Object arg) &#123; System.out.println(&quot;收到被观察者通知，准备改变&quot;); &#125; &#125;); observer.setChanged();// 监控数据是否发生变化 observer.notifyObservers();// 通知 &#125;&#125; Java9 及之后的版本，使用 Flow 类替换了 Observer 和 Observable。 1234567891011121314151617181920212223242526272829303132public class Test &#123; public static void main(String[] args) &#123; Flow.Publisher&lt;String&gt; publisher = subscriber -&gt; &#123; subscriber.onNext(&quot;1&quot;);// 1 subscriber.onNext(&quot;2&quot;); subscriber.onError(new RuntimeException(&quot;出错&quot;));// 2 // subscriber.onComplete(); &#125;; publisher.subscribe(new Flow.Subscriber&lt;&gt;() &#123; @Override public void onSubscribe(Flow.Subscription subscription) &#123; subscription.cancel(); &#125; @Override public void onNext(String item) &#123; System.out.println(item); &#125; @Override public void onError(Throwable throwable) &#123; System.out.println(&quot;出错了&quot;); &#125; @Override public void onComplete() &#123; System.out.println(&quot;publish complete&quot;); &#125; &#125;); &#125;&#125; Reactor 实现。 响应式编程操作中，都需要满足 Reactive 规范，Reactor 即为这样的一个框架，WebFlux 的核心即是使用 Reactor 实现的。 Reactor 有两个核心类，Mono 和 Flux ，这两个类都实现了 Publisher 接口，提供了丰富的操作符。 Flux 对象实现发布者时，返回 N 个元素；Mono 实现发布者时，返回 0 或者 1 个元素。 Flux 和 Mono 都是数据流的发布者，使用 Flux 和 Mono 都可以发出三种数据信号：元素值、错误信号、完成信号。 错误信号和完成信号都代表终止信号，终止信号用于告诉订阅者数据流结束了。 错误信号在终止数据流的同时，会把错误信息传递给订阅者。 错误信号和完成信号不能共存。 如果没有发送任何元素值，而是直接发送错误信号或者完成信号，表示是空数据流。 如果既没有错误信号，也没有完成信号，表示是无限数据流。 代码演示 Flux 和 和 Mono： 第一步：引入依赖。 12345&lt;dependency&gt; &lt;groupId&gt;io.projectreactor&lt;/groupId&gt; &lt;artifactId&gt;reactor-core&lt;/artifactId&gt; &lt;version&gt;3.3.9.RELEASE&lt;/version&gt;&lt;/dependency&gt; 第二步：声明数据流，有以下几种方式。 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; // just方法直接声明数据流，此时没有订阅，数据是不会输出的 Flux.just(1, 2, 3, 4, 5); Mono.just(1); // 其他方法声明数据流 Integer[] arr = &#123;1, 2, 3, 4, 5&#125;; Flux.fromArray(arr);// 来自数组 List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(5); Flux.fromIterable(list);// 来自集合 Stream&lt;Integer&gt; stream = list.stream(); Flux.fromStream(stream);// 来自流 &#125;&#125; 第三步：订阅。调用 just() 或者其他方法只是声明数据流，数据流并没有发出，只有进行订阅之后才会触发数据流，不订阅什么都不会发生。 1234567891011public class Test &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(5); list.add(1); list.add(2); list.add(3); list.add(4); list.add(5); Flux.fromIterable(list).subscribe(System.out::print); &#125;&#125; 常用操作符： 对数据流进行一道道操作，称为操作符，比如工厂流水线。 **map()**：将数据流中的每一个元素，按一定的规则映射为新元素。 **flatmap()**：将数据流中的每一个元素，按一定的规则转换成流，然后再把所有的流合并为一个整体的流。 **filter()**：将数据流中的元素，按一定的规则进行筛选。 **zip()**：将数据流中的元素，按一定的规则进行压缩。 Spring WebFlux 的执行流程和核心 API Spring WebFlux 基于 Reactor，默认使用的容器是 Netty，Netty 是一个高性能的异步非阻塞的 NIO 框架。 BIO：阻塞方式。 NIO：非阻塞方式。 Channel：通道；Register：注册；Selector：选择器。 Spring WebFlux 执行过程和 Spring MVC 相似。 Spring MVC 的核心控制器是 DispatcherServlet，Spring WebFlux 的核心控制器是 DispatcherHandler，DispatcherHandler 实现了 WebHandler 接口，重写了 handle()： 12345678910public interface WebHandler &#123; /** * Handle the web server exchange. * @param exchange the current server exchange * @return &#123;@code Mono&lt;Void&gt;&#125; to indicate when request handling is complete */ Mono&lt;Void&gt; handle(ServerWebExchange exchange);&#125; 123456789101112@Overridepublic Mono&lt;Void&gt; handle(ServerWebExchange exchange) &#123; if (this.handlerMappings == null) &#123; return createNotFoundError(); &#125; return Flux.fromIterable(this.handlerMappings) .concatMap(mapping -&gt; mapping.getHandler(exchange)) .next() .switchIfEmpty(createNotFoundError()) .flatMap(handler -&gt; invokeHandler(exchange, handler)) .flatMap(result -&gt; handleResult(exchange, result));&#125; exchange：放 http 请求响应的信息。 mapping.getHandler(exchange)：根据 http 请求地址获得其对应的 handlerMapping。 invokeHandler(exchange, handler)：调用具体的业务方法处理 http 请求。 handleResult(exchange, result))：返回处理的结果。 Spring WebFlux 除了 DispatcherHandler 组件外，还有其他几个重要的组件： DispatcherHandler：负责请求的处理。 HandlerMapping：负责查询请求对应的处理的方法。 HandlerAdapter：负责请求处理的实际的业务。 HandlerResultHandler：负责响应结果的处理。 Spring WebFlux 实现函数式编程，依赖于两个接口：RouterFunction (负责路由处理) 和 HandlerFunction (负责处理函数)。 Spring WebFlux 实现 Spring MVC 方式实现，是同步阻塞的方式，基于 Spring MVC + Servlet + Tomcat。 Spring WebFlux 方式实现，是异步非阻塞的方式，基于 Spring WebFlux + Reactor + Netty。 基于注解编程模型 第一步：创建 Spring Boot 工程，引入 Spring WebFlux 依赖。 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;cn.xisun.spring.webflux&lt;/groupId&gt; &lt;artifactId&gt;xisun-webflux&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;xisun-webflux&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二步：打开 application.properties 配置文件，配置启动端口号。 1server.port=8081 第三步：创建包和相关类。 entity 层： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * @author XiSun * @Date 2021/4/24 10:58 */public class User &#123; private String name; private String gender; private Integer age; public User() &#123; &#125; public User(String name, String gender, Integer age) &#123; this.name = name; this.gender = gender; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (!Objects.equals(name, user.name)) &#123; return false; &#125; if (!Objects.equals(gender, user.gender)) &#123; return false; &#125; return Objects.equals(age, user.age); &#125; @Override public int hashCode() &#123; int result = name != null ? name.hashCode() : 0; result = 31 * result + (gender != null ? gender.hashCode() : 0); result = 31 * result + (age != null ? age.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, gender=&#x27;&quot; + gender + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; dao 层： 12345678910public interface UserDao &#123; // 根据id查询用户 User getUserById(int id); // 查询所有用户 List&lt;User&gt; getAllUser(); // 添加用户 String saveUser(User user);&#125; 123456789101112131415161718192021222324252627282930313233@Repositorypublic class UserDaoImpl implements UserDao &#123; // 创建map集合存储数据，代替从数据库查询 private final Map&lt;Integer, User&gt; users = new HashMap&lt;&gt;(); &#123; this.users.put(1, new User(&quot;Lucy&quot;, &quot;male&quot;, 20)); this.users.put(2, new User(&quot;Mary&quot;, &quot;female&quot;, 30)); this.users.put(3, new User(&quot;Jack&quot;, &quot;male&quot;, 50)); &#125; @Override public User getUserById(int id) &#123; System.out.println(&quot;dao: &quot; + id); return this.users.get(id); &#125; @Override public List&lt;User&gt; getAllUser() &#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(5); Collection&lt;User&gt; values = this.users.values(); userList.addAll(values); return userList; &#125; @Override public String saveUser(User user) &#123; int id = this.users.size() + 1; this.users.put(id, user); System.out.println(this.users); return &quot;success&quot;; &#125;&#125; service 层： 12345678910public interface UserService &#123; // 根据id查询用户 Mono&lt;User&gt; getUserById(int id); // 查询所有用户 Flux&lt;User&gt; getAllUser(); // 添加用户 Mono&lt;String&gt; saveUser(Mono&lt;User&gt; user);&#125; 123456789101112131415161718192021222324@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private UserDao userDao; @Override public Mono&lt;User&gt; getUserById(int id) &#123; System.out.println(&quot;service: &quot; + id); User user = userDao.getUserById(id); return Mono.just(user); &#125; @Override public Flux&lt;User&gt; getAllUser() &#123; List&lt;User&gt; allUser = userDao.getAllUser(); return Flux.fromIterable(allUser); &#125; @Override public Mono&lt;String&gt; saveUser(Mono&lt;User&gt; userMono) &#123; // return userMono.doOnNext(person -&gt; userDao.saveUser(person)).thenEmpty(Mono.empty());// 返回 Mono&lt;Void&gt; return userMono.map(user -&gt; userDao.saveUser(user)); &#125;&#125; controller 层： 12345678910111213141516171819202122232425@RestControllerpublic class UserController &#123; @Autowired private UserService userService; // 根据id查询用户 @GetMapping(&quot;/getUserById/&#123;id&#125;&quot;) public Mono&lt;User&gt; getUserById(@PathVariable int id) &#123; System.out.println(&quot;controller: &quot; + id); return userService.getUserById(id); &#125; // 查询所有用户 @GetMapping(&quot;/getAllUser&quot;) public Flux&lt;User&gt; getAllUser() &#123; return userService.getAllUser(); &#125; // 添加用户 @PostMapping(&quot;/saveUserMessage&quot;) public Mono&lt;String&gt; saveUser(@RequestBody User user) &#123; System.out.println(&quot;save user: &quot; + user); return userService.saveUser(Mono.just(user)); &#125;&#125; main 方法： 123456@SpringBootApplicationpublic class XisunWebfluxApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(XisunWebfluxApplication.class, args); &#125;&#125; 整体结构： 测试： 12345678910111213 . ____ _ __ _ _ /\\\\ / ___&#x27;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | &#x27;_ | &#x27;_| | &#x27;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) &#x27; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.4.5)2021-04-24 18:54:21.418 INFO 4836 --- [ main] c.x.s.w.x.XisunWebfluxApplication : Starting XisunWebfluxApplication using Java 1.8.0_222 on DESKTOP-OM8IACS with PID 4836 (D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-webflux\\target\\classes started by Ziyoo in D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-webflux)2021-04-24 18:54:21.423 INFO 4836 --- [ main] c.x.s.w.x.XisunWebfluxApplication : No active profile set, falling back to default profiles: default2021-04-24 18:54:22.719 INFO 4836 --- [ main] o.s.b.web.embedded.netty.NettyWebServer : Netty started on port 80812021-04-24 18:54:22.730 INFO 4836 --- [ main] c.x.s.w.x.XisunWebfluxApplication : Started XisunWebfluxApplication in 2.007 seconds (JVM running for 3.003) 基于函数式编程模型 在使用函数式编程模型操作的时候，需要自己初始化服务器。 基于函数式编程模型操作的时候，有两个核心接口：RouterFunction (实现路由功能，将请求转发给对应的 handler) 和 HandlerFunction (处理请求并生成响应的函数)。基于函数式编程模型的核心任务就是定义这两个函数式接口的实现，并且启动需要的服务器。 Spring WebFlux 请求和响应不再是 ServletRequest 和 ServletResponse，而是 ServerRequest 和 ServerResponse。 第一步：创建 Spring Boot 工程，引入 Spring WebFlux 依赖。 第二步：打开 application.properties 配置文件，配置启动端口号。 第三步：创建包和相关类。 entity 层： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class User &#123; private String name; private String gender; private Integer age; public User() &#123; &#125; public User(String name, String gender, Integer age) &#123; this.name = name; this.gender = gender; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (!Objects.equals(name, user.name)) &#123; return false; &#125; if (!Objects.equals(gender, user.gender)) &#123; return false; &#125; return Objects.equals(age, user.age); &#125; @Override public int hashCode() &#123; int result = name != null ? name.hashCode() : 0; result = 31 * result + (gender != null ? gender.hashCode() : 0); result = 31 * result + (age != null ? age.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, gender=&#x27;&quot; + gender + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; dao 层： 1234567public interface UserDao &#123; User getUserById(int id); List&lt;User&gt; getAllUser(); String saveUser(User user);&#125; 1234567891011121314151617181920212223242526272829303132public class UserDaoImpl implements UserDao &#123; // 创建map集合存储数据，代替从数据库查询 private final Map&lt;Integer, User&gt; users = new HashMap&lt;&gt;(); &#123; this.users.put(1, new User(&quot;Lucy&quot;, &quot;male&quot;, 20)); this.users.put(2, new User(&quot;Mary&quot;, &quot;female&quot;, 30)); this.users.put(3, new User(&quot;Jack&quot;, &quot;male&quot;, 50)); &#125; @Override public User getUserById(int id) &#123; System.out.println(&quot;dao: &quot; + id); return this.users.get(id); &#125; @Override public List&lt;User&gt; getAllUser() &#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(5); Collection&lt;User&gt; values = this.users.values(); userList.addAll(values); return userList; &#125; @Override public String saveUser(User user) &#123; int id = this.users.size() + 1; this.users.put(id, user); System.out.println(this.users); return &quot;success&quot;; &#125;&#125; service 层： 12345678910public interface UserService &#123; // 根据id查询用户 Mono&lt;User&gt; getUserById(int id); // 查询所有用户 Flux&lt;User&gt; getAllUser(); // 添加用户 Mono&lt;Void&gt; saveUser(Mono&lt;User&gt; user);&#125; 1234567891011121314151617181920212223242526272829public class UserServiceImpl implements UserService &#123; private UserDao userDao; public UserServiceImpl() &#123; &#125; public UserServiceImpl(UserDao userDao) &#123; this.userDao = userDao; &#125; @Override public Mono&lt;User&gt; getUserById(int id) &#123; System.out.println(&quot;service: &quot; + id); User user = userDao.getUserById(id); return Mono.just(user); &#125; @Override public Flux&lt;User&gt; getAllUser() &#123; List&lt;User&gt; allUser = userDao.getAllUser(); return Flux.fromIterable(allUser); &#125; @Override public Mono&lt;Void&gt; saveUser(Mono&lt;User&gt; userMono) &#123; // return userMono.map(user -&gt; userDao.saveUser(user)); return userMono.doOnNext(person -&gt; userDao.saveUser(person)).thenEmpty(Mono.empty());// 返回 Mono&lt;Void&gt; &#125;&#125; 创建 Handler (具体实现方法)： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class UserHandler &#123; private final UserService userService; public UserHandle(UserService userService) &#123; this.userService = userService; &#125; // 根据id查询用户 public Mono&lt;ServerResponse&gt; getUserById(ServerRequest request) &#123; // 获取路径中的id值，返回的是String int userId = Integer.parseInt(request.pathVariable(&quot;id&quot;)); // 可能查询不到用户，进行空值处理 Mono&lt;ServerResponse&gt; notFound = ServerResponse.notFound().build(); // 调用userService的方法查询用户 Mono&lt;User&gt; userMono = userService.getUserById(userId); // 把userMono进行转换，返回Mono&lt;ServerResponse&gt; return userMono.flatMap(user -&gt; ServerResponse .ok() .contentType(MediaType.APPLICATION_JSON) .body(BodyInserters.fromObject(user)) .switchIfEmpty(notFound)); &#125; // 查询所有用户，ServerRequest参数即使不用，也要添加，否则在Server中会找不到这个方法 public Mono&lt;ServerResponse&gt; getAllUsers(ServerRequest request) &#123; // 调用userService的方法查询所有用户 Flux&lt;User&gt; userFlux = userService.getAllUser(); return ServerResponse .ok() .contentType(MediaType.APPLICATION_JSON) .body(userFlux, User.class); &#125; // 添加用户 public Mono&lt;ServerResponse&gt; saveUser(ServerRequest request) &#123; // 从请求中拿到user对象 Mono&lt;User&gt; userMono = request.bodyToMono(User.class); return ServerResponse .ok() .build(userService.saveUser(userMono)); &#125;&#125; 第四步：初始化服务器，编写 Router。 创建路由，创建服务器完成适配。 12345678910111213141516171819202122232425262728293031323334353637public class Server &#123; // 1.创建Router路由 public RouterFunction&lt;ServerResponse&gt; routingFunction() &#123; // 创建hanler对象(@Repository这些注解无效，需手动注入dao和service，是否有其他方法？) UserDaoImpl userDao = new UserDaoImpl(); UserService userService = new UserServiceImpl(userDao); UserHandler handler = new UserHandler(userService); // 设置路由 return RouterFunctions .route(RequestPredicates.GET(&quot;/getUserById/&#123;id&#125;&quot;) .and(RequestPredicates.accept(MediaType.APPLICATION_JSON)), handler::getUserById) .andRoute(RequestPredicates.GET(&quot;/getAllUser&quot;) .and(RequestPredicates.accept(MediaType.APPLICATION_JSON)), handler::getAllUser) .andRoute(RequestPredicates.POST(&quot;/saveUserMessage&quot;) .and(RequestPredicates.accept(MediaType.APPLICATION_JSON)), handler::saveUser); &#125; // 2.创建服务器完成适配 public void createReactorServer() &#123; // 路由和handler适配 RouterFunction&lt;ServerResponse&gt; route = routingFunction(); HttpHandler httpHandler = RouterFunctions.toHttpHandler(route); ReactorHttpHandlerAdapter adapter = new ReactorHttpHandlerAdapter(httpHandler); // 创建服务器 HttpServer httpServer = HttpServer.create(); httpServer.handle(adapter).bindNow(); &#125; // 3.最终调用 public static void main(String[] args) throws Exception &#123; Server server = new Server(); server.createReactorServer(); System.out.println(&quot;enter to exit&quot;); System.in.read(); &#125; 最终调用：启动 main 方法，并在网页上输入地址进行测试。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657D:\\java\\jdk\\jdk8u222-b10\\bin\\java.exe &quot;-javaagent:D:\\Program Files\\IntelliJ IDEA 2020.1.2\\lib\\idea_rt.jar=11755:D:\\Program Files\\IntelliJ IDEA 2020.1.2\\bin&quot; -Dfile.encoding=UTF-8 -classpath D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\charsets.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\access-bridge-64.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\cldrdata.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\dnsns.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\jaccess.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\localedata.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\nashorn.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\sunec.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\sunjce_provider.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\sunmscapi.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\sunpkcs11.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\ext\\zipfs.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\jce.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\jsse.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\management-agent.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\resources.jar;D:\\java\\jdk\\jdk8u222-b10\\jre\\lib\\rt.jar;D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-webflux\\target\\classes;D:\\java\\maven-repo\\org\\springframework\\boot\\spring-boot-starter-webflux\\2.2.5.RELEASE\\spring-boot-starter-webflux-2.2.5.RELEASE.jar;D:\\java\\maven-repo\\org\\springframework\\boot\\spring-boot-starter\\2.4.5\\spring-boot-starter-2.4.5.jar;D:\\java\\maven-repo\\org\\springframework\\boot\\spring-boot\\2.4.5\\spring-boot-2.4.5.jar;D:\\java\\maven-repo\\org\\springframework\\spring-context\\5.3.6\\spring-context-5.3.6.jar;D:\\java\\maven-repo\\org\\springframework\\spring-aop\\5.3.6\\spring-aop-5.3.6.jar;D:\\java\\maven-repo\\org\\springframework\\spring-expression\\5.3.6\\spring-expression-5.3.6.jar;D:\\java\\maven-repo\\org\\springframework\\boot\\spring-boot-autoconfigure\\2.4.5\\spring-boot-autoconfigure-2.4.5.jar;D:\\java\\maven-repo\\org\\springframework\\boot\\spring-boot-starter-logging\\2.4.5\\spring-boot-starter-logging-2.4.5.jar;D:\\java\\maven-repo\\ch\\qos\\logback\\logback-classic\\1.2.3\\logback-classic-1.2.3.jar;D:\\java\\maven-repo\\ch\\qos\\logback\\logback-core\\1.2.3\\logback-core-1.2.3.jar;D:\\java\\maven-repo\\org\\apache\\logging\\log4j\\log4j-to-slf4j\\2.13.3\\log4j-to-slf4j-2.13.3.jar;D:\\java\\maven-repo\\org\\apache\\logging\\log4j\\log4j-api\\2.13.3\\log4j-api-2.13.3.jar;D:\\java\\maven-repo\\org\\slf4j\\jul-to-slf4j\\1.7.30\\jul-to-slf4j-1.7.30.jar;D:\\java\\maven-repo\\jakarta\\annotation\\jakarta.annotation-api\\1.3.5\\jakarta.annotation-api-1.3.5.jar;D:\\java\\maven-repo\\org\\yaml\\snakeyaml\\1.27\\snakeyaml-1.27.jar;D:\\java\\maven-repo\\org\\springframework\\boot\\spring-boot-starter-json\\2.4.5\\spring-boot-starter-json-2.4.5.jar;D:\\java\\maven-repo\\com\\fasterxml\\jackson\\core\\jackson-databind\\2.11.4\\jackson-databind-2.11.4.jar;D:\\java\\maven-repo\\com\\fasterxml\\jackson\\core\\jackson-annotations\\2.11.4\\jackson-annotations-2.11.4.jar;D:\\java\\maven-repo\\com\\fasterxml\\jackson\\core\\jackson-core\\2.11.4\\jackson-core-2.11.4.jar;D:\\java\\maven-repo\\com\\fasterxml\\jackson\\datatype\\jackson-datatype-jdk8\\2.11.4\\jackson-datatype-jdk8-2.11.4.jar;D:\\java\\maven-repo\\com\\fasterxml\\jackson\\datatype\\jackson-datatype-jsr310\\2.11.4\\jackson-datatype-jsr310-2.11.4.jar;D:\\java\\maven-repo\\com\\fasterxml\\jackson\\module\\jackson-module-parameter-names\\2.11.4\\jackson-module-parameter-names-2.11.4.jar;D:\\java\\maven-repo\\org\\springframework\\boot\\spring-boot-starter-reactor-netty\\2.4.5\\spring-boot-starter-reactor-netty-2.4.5.jar;D:\\java\\maven-repo\\io\\projectreactor\\netty\\reactor-netty-http\\1.0.6\\reactor-netty-http-1.0.6.jar;D:\\java\\maven-repo\\io\\netty\\netty-codec-http\\4.1.63.Final\\netty-codec-http-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-common\\4.1.63.Final\\netty-common-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-buffer\\4.1.63.Final\\netty-buffer-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-transport\\4.1.63.Final\\netty-transport-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-codec\\4.1.63.Final\\netty-codec-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-handler\\4.1.63.Final\\netty-handler-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-codec-http2\\4.1.63.Final\\netty-codec-http2-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-resolver-dns\\4.1.63.Final\\netty-resolver-dns-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-resolver\\4.1.63.Final\\netty-resolver-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-codec-dns\\4.1.63.Final\\netty-codec-dns-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-resolver-dns-native-macos\\4.1.63.Final\\netty-resolver-dns-native-macos-4.1.63.Final-osx-x86_64.jar;D:\\java\\maven-repo\\io\\netty\\netty-transport-native-unix-common\\4.1.63.Final\\netty-transport-native-unix-common-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-transport-native-epoll\\4.1.63.Final\\netty-transport-native-epoll-4.1.63.Final-linux-x86_64.jar;D:\\java\\maven-repo\\io\\projectreactor\\netty\\reactor-netty-core\\1.0.6\\reactor-netty-core-1.0.6.jar;D:\\java\\maven-repo\\io\\netty\\netty-handler-proxy\\4.1.63.Final\\netty-handler-proxy-4.1.63.Final.jar;D:\\java\\maven-repo\\io\\netty\\netty-codec-socks\\4.1.63.Final\\netty-codec-socks-4.1.63.Final.jar;D:\\java\\maven-repo\\org\\springframework\\boot\\spring-boot-starter-validation\\2.4.5\\spring-boot-starter-validation-2.4.5.jar;D:\\java\\maven-repo\\org\\glassfish\\jakarta.el\\3.0.3\\jakarta.el-3.0.3.jar;D:\\java\\maven-repo\\org\\hibernate\\validator\\hibernate-validator\\6.1.7.Final\\hibernate-validator-6.1.7.Final.jar;D:\\java\\maven-repo\\jakarta\\validation\\jakarta.validation-api\\2.0.2\\jakarta.validation-api-2.0.2.jar;D:\\java\\maven-repo\\org\\jboss\\logging\\jboss-logging\\3.4.1.Final\\jboss-logging-3.4.1.Final.jar;D:\\java\\maven-repo\\com\\fasterxml\\classmate\\1.5.1\\classmate-1.5.1.jar;D:\\java\\maven-repo\\org\\springframework\\spring-web\\5.3.6\\spring-web-5.3.6.jar;D:\\java\\maven-repo\\org\\springframework\\spring-beans\\5.3.6\\spring-beans-5.3.6.jar;D:\\java\\maven-repo\\org\\springframework\\spring-webflux\\5.3.6\\spring-webflux-5.3.6.jar;D:\\java\\maven-repo\\io\\projectreactor\\reactor-core\\3.4.5\\reactor-core-3.4.5.jar;D:\\java\\maven-repo\\org\\reactivestreams\\reactive-streams\\1.0.3\\reactive-streams-1.0.3.jar;D:\\java\\maven-repo\\org\\synchronoss\\cloud\\nio-multipart-parser\\1.1.0\\nio-multipart-parser-1.1.0.jar;D:\\java\\maven-repo\\org\\slf4j\\slf4j-api\\1.7.30\\slf4j-api-1.7.30.jar;D:\\java\\maven-repo\\org\\synchronoss\\cloud\\nio-stream-storage\\1.1.3\\nio-stream-storage-1.1.3.jar;D:\\java\\maven-repo\\org\\springframework\\spring-core\\5.3.6\\spring-core-5.3.6.jar;D:\\java\\maven-repo\\org\\springframework\\spring-jcl\\5.3.6\\spring-jcl-5.3.6.jar cn.xisun.spring.webflux.xisunwebflux.Server09:45:43.306 [main] DEBUG reactor.util.Loggers - Using Slf4j logging framework09:45:43.726 [main] DEBUG org.springframework.web.server.adapter.HttpWebHandlerAdapter - enableLoggingRequestDetails=&#x27;false&#x27;: form data and headers will be masked to prevent unsafe logging of potentially sensitive data09:45:43.785 [main] DEBUG io.netty.util.internal.logging.InternalLoggerFactory - Using SLF4J as the default logging framework09:45:43.786 [main] DEBUG io.netty.util.internal.PlatformDependent - Platform: Windows09:45:43.792 [main] DEBUG io.netty.util.internal.PlatformDependent0 - -Dio.netty.noUnsafe: false09:45:43.792 [main] DEBUG io.netty.util.internal.PlatformDependent0 - Java version: 809:45:43.794 [main] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.theUnsafe: available09:45:43.796 [main] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.copyMemory: available09:45:43.799 [main] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Buffer.address: available09:45:43.800 [main] DEBUG io.netty.util.internal.PlatformDependent0 - direct buffer constructor: available09:45:43.802 [main] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Bits.unaligned: available, true09:45:43.802 [main] DEBUG io.netty.util.internal.PlatformDependent0 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java909:45:43.802 [main] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.DirectByteBuffer.&lt;init&gt;(long, int): available09:45:43.802 [main] DEBUG io.netty.util.internal.PlatformDependent - sun.misc.Unsafe: available09:45:43.804 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.tmpdir: C:\\Users\\Ziyoo\\AppData\\Local\\Temp (java.io.tmpdir)09:45:43.805 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.bitMode: 64 (sun.arch.data.model)09:45:43.808 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.maxDirectMemory: 1653604352 bytes09:45:43.809 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.uninitializedArrayAllocationThreshold: -109:45:43.810 [main] DEBUG io.netty.util.internal.CleanerJava6 - java.nio.ByteBuffer.cleaner(): available09:45:43.811 [main] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.noPreferDirect: false09:45:43.873 [main] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.level: simple09:45:43.873 [main] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.targetRecords: 409:45:43.911 [main] DEBUG reactor.netty.tcp.TcpResources - [http] resources will use the default LoopResources: DefaultLoopResources &#123;prefix=reactor-http, daemon=true, selectCount=8, workerCount=8&#125;09:45:43.911 [main] DEBUG reactor.netty.tcp.TcpResources - [http] resources will use the default ConnectionProvider: reactor.netty.resources.DefaultPooledConnectionProvider@5552768b09:45:43.913 [main] DEBUG reactor.netty.resources.DefaultLoopIOUring - Default io_uring support : false09:45:44.117 [main] DEBUG reactor.netty.resources.DefaultLoopEpoll - Default Epoll support : false09:45:44.118 [main] DEBUG reactor.netty.resources.DefaultLoopKQueue - Default KQueue support : false09:45:44.125 [main] DEBUG io.netty.channel.MultithreadEventLoopGroup - -Dio.netty.eventLoopThreads: 1609:45:44.154 [main] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 102409:45:44.154 [main] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 409609:45:44.161 [main] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.noKeySetOptimization: false09:45:44.161 [main] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.selectorAutoRebuildThreshold: 51209:45:44.170 [main] DEBUG io.netty.util.internal.PlatformDependent - org.jctools-core.MpscChunkedArrayQueue: available09:45:44.218 [main] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.processId: 7052 (auto-detected)09:45:44.221 [main] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false09:45:44.221 [main] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false09:45:44.317 [main] DEBUG io.netty.util.NetUtilInitializations - Loopback interface: lo (Software Loopback Interface 1, 127.0.0.1)09:45:44.318 [main] DEBUG io.netty.util.NetUtil - Failed to get SOMAXCONN from sysctl and file \\proc\\sys\\net\\core\\somaxconn. Default: 20009:45:44.432 [main] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)09:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numHeapArenas: 1609:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numDirectArenas: 1609:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.pageSize: 819209:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxOrder: 1109:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.chunkSize: 1677721609:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.smallCacheSize: 25609:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.normalCacheSize: 6409:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedBufferCapacity: 3276809:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimInterval: 819209:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimIntervalMillis: 009:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.useCacheForAllThreads: true09:45:44.457 [main] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 102309:45:44.466 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled09:45:44.466 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 009:45:44.466 [main] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 1638409:45:44.589 [reactor-http-nio-1] DEBUG reactor.netty.transport.ServerTransport - [id:ae5a7227, L:/0:0:0:0:0:0:0:0:11779] Bound new serverenter to exit 除了上面的调用方式，也可以使用 WebClient 调用，这个不需要在浏览器中输入地址，可以直接在本地进行模拟测试： 123456789101112131415161718public class Client &#123; public static void main(String[] args) &#123; // 先启动Server，查看端口，然后调用服务器的地址 WebClient webClient = WebClient.create(&quot;http://127.0.0.1:12009&quot;); // 根据id查询 String id = &quot;1&quot;; User user = webClient.get().uri(&quot;/getUserById/&#123;id&#125;&quot;, id) .accept(MediaType.APPLICATION_JSON).retrieve().bodyToMono(User.class).block(); System.out.println(user); // 查询所有 Flux&lt;User&gt; users = webClient.get().uri(&quot;/getAllUser&quot;) .accept(MediaType.APPLICATION_JSON).retrieve().bodyToFlux(User.class); // 打印每一个User的名字 users.map(User::getName).buffer().doOnNext(System.out::println).blockFirst(); &#125;&#125; 说明：需要先启动 Server，然后查询端口号，设置 WebClient 的地址，然后启动 Client，即可在控制台查询相应操作的输出结果。 本文参考https://www.bilibili.com/video/BV1Vf4y127N5 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"}]},{"title":"tool-idea","slug":"tool-idea","date":"2021-04-13T12:38:59.000Z","updated":"2021-04-13T12:40:09.688Z","comments":true,"path":"2021/04/13/tool-idea/","link":"","permalink":"http://example.com/2021/04/13/tool-idea/","excerpt":"","text":"IDEA 快捷键ctrl + H：查看类的继承层级关系","categories":[],"tags":[{"name":"tool","slug":"tool","permalink":"http://example.com/tags/tool/"}]},{"title":"Spring5 入门","slug":"spring-base","date":"2021-04-13T05:09:15.000Z","updated":"2021-04-25T06:47:27.660Z","comments":true,"path":"2021/04/13/spring-base/","link":"","permalink":"http://example.com/2021/04/13/spring-base/","excerpt":"","text":"Spring 框架概述 Spring 官方文档： 全部版本：https://docs.spring.io/spring-framework/docs/ 5.2.7.RELEASE：https://docs.spring.io/spring-framework/docs/5.2.7.RELEASE/spring-framework-reference/ Spring Framework 5 中文文档：https://cntofu.com/book/95/index.html Spring 是轻量级的开源的 JavaEE 框架。 Spring 可以解决企业应用开发的复杂性。 Spring 有两个核心部分：IOC 和 Aop。 IOC：Inversion of Control，即控制反转。是面向对象编程中的一种设计原则，可以用来降低计算机代码之间的耦合度，其中最常见的方式叫做依赖注入 (Dependency Injection，简称 DI)。Spring 就是采用依赖注入的方式，为我们管理容器中的 bean 实例对象。 Aop：Aspect Oriented Programming，即面向切面。可以在不修改源代码的前提下，通过预编译方式和运行期间动态代理方式实现对原有代码的增强 (添加新功能)。 Spring 特点： 方便解耦，简化开发。 Aop 编程支持。 方便程序测试。 方便和其他框架进行整合。 方便进行事务操作。 降低 API 开发难度。 Spring 官网：https://spring.io/。各版本源码下载地址：https://repo.spring.io/release/org/springframework/spring/。 Spring 模块： Spring 入门案例 创建 Maven 工程。 引入依赖：spring-beans、spring-context、spring-core、spring-expression，另外，Spring 还需依赖 commons-logging 实现日志功能。 123456789101112131415&lt;!-- Spring核心依赖 --&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 这个依赖好像不需要 --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 引入 spring-context 依赖时，会一并将其他几个依赖引入： 创建实体类 1234567891011121314151617181920212223242526272829303132333435363738package cn.xisun.spring.pojo;public class Student &#123; private Integer stuId; private String stuName; public Student() &#123; &#125; public Student(Integer stuId, String stuName) &#123; this.stuId = stuId; this.stuName = stuName; &#125; public Integer getStuId() &#123; return stuId; &#125; public String getStuName() &#123; return stuName; &#125; public void setStuId(Integer stuId) &#123; this.stuId = stuId; &#125; public void setStuName(String stuName) &#123; this.stuName = stuName; &#125; @Override public String toString() &#123; return &quot;Student&#123;&quot; + &quot;stuId=&quot; + stuId + &quot;, stuName=&#x27;&quot; + stuName + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 编写 Spring 配置文件：Spring 配置文件使用 xml 格式。 在 resources 包下点击鼠标右键，选择【New】–&gt;【XML Configuration File】–&gt;【Spring Config】，输入配置文件名 (自定义) 创建。注：resource 包下的配置文件在执行时会被拷贝至类路径的根目录。 在配置文件中添加如下配置：创建 Student 对象的实例，并注入属性的默认值。 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 使用bean元素定义一个由IOC容器创建的对象 --&gt; &lt;!-- id属性指定用于引用bean实例的标识 --&gt; &lt;!-- class属性指定用于创建bean的全类名 --&gt; &lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.pojo.Student&quot;&gt; &lt;!-- 使用property子元素为bean的属性赋值 --&gt; &lt;property name=&quot;studentId&quot; value=&quot;007&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 编写测试代码： 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象 Student student = iocContainer.getBean(&quot;student&quot;, Student.class); // 3.打印bean System.out.println(student); &#125;&#125; 输出结果： 测试说明：Spring 在创建 IOC 容器时，就已经完成了 bean 的创建和属性的赋值。 IOC IOC 反转控制的思想： 在应用程序中的组件需要获取资源时，传统的方式是组件主动的从容器中获取所需要的资源，在这样的模式下开发人员往往需要知道在具体容器中特定资源的获取方式。比如 ClassA 中需要用到 ClassB 的对象，一般情况下，需要在 ClassA 的代码中显式的 new 一个 ClassB 的对象。 反转控制的思想完全颠覆了应用程序组件获取资源的传统方式：反转了资源的获取方向 — 改由容器主动的将资源推送给需要的组件，开发人员不需要知道容器是如何创建资源对象的，只需要提供接收资源的方式即可。采用依赖注入技术之后，ClassA 的代码只需要定义一个私有的 ClassB 对象，不需要直接 new 来获得这个对象，而是通过相关的容器控制程序来将 ClassB 对象在外部 new 出来并注入到 A 类里的引用中。而具体获取的方法、对象被获取时的状态由配置文件 (如 XML) 来指定。 DI 依赖注入：可以将 DI 看作是 IOC 的一种实现方式 — 即组件以一些预先定义好的方式 (例如 setter()) 接受来自于容器的资源注入。相对于 IOC 而言，这种表述更直接：IOC 容器在 Spring 中的实现。 IOC 底层原理：xml 解析，工厂模式，反射。 图解： 代码演示： 原始方式：自己 new 对象，再通过 setter 方法注入器属性值。—&gt; 代码耦合度极高。 123Student stu = new Student();stu.setStuId(7);stu.setStuName(&quot;Tom&quot;); 进阶方式：通过工厂创建对象。—&gt; 可以降低代码的耦合度，不需要自己 new 对象，但仍需要手动去获取和管理 bean。 12345&lt;!-- 1.先通过xml配置文件配置bean的属性 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.pojo.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;007&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt;&lt;/bean&gt; 12345678910111213// 2.再通过工厂模式 + 反射的方法创建该对象的实例，并注入属性值public class StudentFactory &#123; public static Student getStudent()&#123; String className = ...;// 通过xml解析获取全类名 String[] fieldNames = ..;// 通过xml解析获取字段名 String[] fieldValues = ...;// 通过xml解析获取字段值 Class clazz = Class.forName(className);// 通过反射创建对象实例 for (int i = 0; i &lt; fieldNames.length; i++) &#123; // 依次为字段赋值 &#125; return clazz;// 返回创建的实例对象 &#125;&#125; 最终方式：通过 Spring IOC 管理 bean。—&gt; bean 的创建与它们之间的依赖关系完全交给 Spring IOC 容器去管理，代码耦合程度极大降低。 12345&lt;!-- 1.先通过xml配置文件配置bean的属性 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.pojo.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;007&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt;&lt;/bean&gt; 12// 2.再通过iocContainer.getbean(&quot;beanId&quot;, 类.class)方法或者@Autowire方式获取beanStudent student = iocContainer.getBean(&quot;student&quot;, Student.class); IOC 思想基于 IOC 容器完成，IOC 容器底层就是对象工厂。 Spring 提供了 IOC 容器的两种实现方式 (两个接口)： 在通过 IOC 容器读取 bean 的实例之前，需要先将 IOC 容器本身实例化。 BeanFactory：IOC 容器的基本实现，是 Spring 内部的使用接口。面向 Spring 本身，不提供给开发人员使用。 BeanFactory 加载配置文件的时候，不会创建对象，而是在使用对象的时候才去创建。 ApplicationContext：BeanFactory 的子接口，提供了更多功能。面向 Spring 的使用者，一般由开发人员进行使用。几乎所有场合都使用 ApplicationContext 而不是底层的 BeanFactory。 ApplicationContext 加载配置文件的时候，就会把配置文件中配置的对象进行创建。(在服务启动的时候，就把加载对象等耗时的工作全部完成，而不是在用到的时候才创建，这对于 web 项目等的使用者，会有比较好的效果，因为一般项目部署到服务器启动后，都尽量不再关闭。) BeanFactory 接口的实现类： ConfigurableApplicationContext 接口： ApplicationContext 的子接口，包含一些扩展方法。 refresh() 和 close() 让 ApplicationContext 具有启动、关闭和刷新上下文的能力。 ApplicationContext 接口的主要实现类： FileSystemXmlApplicationContext：对应文件系统中的 xml 格式的配置文件。(xml 配置文件的绝对路径) 12ApplicationContext iocContainer = new FileSystemXmlApplicationContext( &quot;D:\\\\JetBrainsWorkSpace\\\\IDEAProjects\\\\xisun-projects\\\\xisun-spring\\\\src\\\\main\\\\resources\\\\spring.xml&quot;); ClassPathXmlApplicationContext：对应类路径下的 xml 格式的配置文件。(xml 配置文件的相对路径) 1ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); WebApplicationContext 接口： ApplicationContext 的子接口，扩展了 ApplicationContext，是专门为 Web 应用准备的，它允许从相对于 Web 根目录的路径中装载配置文件完成初始化。 需要引入 spring-web 依赖： 123456&lt;!-- Spring Web依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; IOC 操作 Bean 管理： Bean 管理指的是两个操作： Spring 创建对象。—&gt; 实例化 Spirng 注入属性。—&gt; 初始化 Bean 管理操作有两种方式： 基于 xml 配置文件方式实现。 基于注解方式实现。 Bean 对象的三种获取方式 (定义在 beanFactory 接口中)： Object getbean(String name) throws beansException;：通过 bean name 获取 bean 实例。 1Student student = (Student) iocContainer.getBean(&quot;student&quot;); &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException;：通过 bean class 获取 bean 实例。 1Student student1 = iocContainer.getBean(Student.class); **&lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException;**：通过 bean name 和 bean class 获取 bean 实例。 1Student student = iocContainer.getBean(&quot;student&quot;, Student.class); 基于 xml 配置文件方式实现 Bean 管理： 基于 xml 方式创建对象： 12&lt;!-- 配置Student对象 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.pojo.Student&quot;&gt;&lt;/bean&gt; 在 Spring 配置文件中，使用 bean 标签，标签里面添加对应属性，就可以实现对象创建。 在 bean 标签有很多属性，常用的属性： id 属性：bean 实例的唯一标识。 class 属性：bean 的全类名。 创建对象时候，默认执行无参数构造方法完成对象创建。 基于 xml 方式注入属性： DI：依赖注入，就是注入属性。 第一种注入方式：通过 bean 的 setter 方法注入属性值。 创建类，定义属性，创建属性对应的 setter 方法。 123456789101112public class Book &#123; private String bookName; private String bookAuthor; public void setBookName(String bookName) &#123; this.bookName = bookName; &#125; public void setBookAuthor(String bookAuthor) &#123; this.bookAuthor = bookAuthor; &#125;&#125; 在 Spring 配置文件配置对象创建，配置属性注入。 123456789&lt;!-- 配置Book对象 --&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;!-- 使用property完成属性注入： name：类里面属性名称 value：向属性注入的值 --&gt; &lt;property name=&quot;bookName&quot; value=&quot;论语&quot;/&gt; &lt;property name=&quot;bookAuthor&quot; value=&quot;孔子&quot;/&gt;&lt;/bean&gt; 通过 &lt;property&gt; 标签指定属性名，Spring 会帮我们找到该属性对应的 setter 方法，并注入其属性值。 第二种注入方式：通过 bean 的有参数构造方法注入属性值。 创建类，定义属性，创建属性对应的有参数构造方法。 123456789public class Orders &#123; private String orderName; private String address; public Orders(String orderName, String address) &#123; this.orderName = orderName; this.address = address; &#125;&#125; 在 Spring 配置文件配置对象创建，配置属性注入。 12345&lt;!-- 配置Orders对象 --&gt;&lt;bean id=&quot;orders&quot; class=&quot;cn.xisun.spring.pojo.Orders&quot;&gt; &lt;constructor-arg name=&quot;orderName&quot; value=&quot;computer&quot;/&gt; &lt;constructor-arg name=&quot;address&quot; value=&quot;China&quot;/&gt;&lt;/bean&gt; 通过 constructor-arg 标签为对象的属性赋值，通过 name 指定属性名，value 指定属性值。 第三种注入方式：通过 p 名称空间注入属性值。 为了简化 xml 文件的配置，越来越多的 xml 文件采用属性而非子元素配置信息。Spring 从 2.5 版本开始引入了一个新的 p 命名空间，可以通过 &lt;bean&gt; 元素属性的方式配置 Bean 的属性。使用 p 命名空间后，基于 xml 的配置方式将进一步简化。 添加 p 名称空间在配置文件中。 12345&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; 通过 p 名称空间注入属性值，也是调用 bean 的 setter 方法设置属性值的。 12&lt;!-- 配置Book对象 --&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot; p:bookName=&quot;论语&quot; p:bookAuthor=&quot;孔子&quot;/&gt; xml 注入其他类型属性： 字面量 null 值。 1234567&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name=&quot;bookName&quot; value=&quot;无名&quot;/&gt; &lt;!-- null值--&gt; &lt;property name=&quot;bookAuthor&quot;&gt; &lt;null/&gt; &lt;/property&gt;&lt;/bean&gt; 效果：Book{bookName=’无名’, bookAuthor=’null’} 属性值包含特殊符号。 12345678910&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name=&quot;bookName&quot; value=&quot;春秋&quot;/&gt; &lt;property name=&quot;bookAuthor&quot;&gt; &lt;!-- 方式一：将特殊字符进行转义，比如：&lt;&gt;转义为&amp;lt; &amp;gt; --&gt; &lt;!--&lt;value&gt;&amp;lt;相传是孔子&amp;gt;&lt;/value&gt;--&gt; &lt;!-- 方式二：把带特殊符号内容写到CDATA中 --&gt; &lt;value&gt;&lt;![CDATA[&lt;相传是孔子&gt;]]&gt;&lt;/value&gt; &lt;/property&gt;&lt;/bean&gt; 效果：Book{bookName=’春秋’, bookAuthor=’&lt;相传是孔子&gt;’} 注入属性 — 外部 bean。 创建两个类： 12345public class UserDao &#123; public void update()&#123; &#125;&#125; 123456789101112public class UserService &#123; private UserDao userDao; public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125; public void add() &#123; System.out.println(&quot;service add...............&quot;); userDao.update(); &#125;&#125; 在 Spring 配置文件中进行配置。 123456789&lt;bean id=&quot;userService&quot; class=&quot;cn.xisun.spring.pojo.UserService&quot;&gt; &lt;!-- 注入userDao对象： name属性：类里面属性名称 ref属性：配置userDao对象的bean标签的id值 --&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;userDao&quot; class=&quot;cn.xisun.spring.pojo.UserDao&quot;/&gt; 注入属性 — 内部 bean。 当 bean 实例仅仅给一个特定的属性使用时，可以将其声明为内部 bean。内部 bean 声明直接包含在 &lt;property&gt; 或 &lt;constructor-arg&gt; 元素里，不需要设置任何 id 或 name 属性，内部 bean 不能使用在任何其他地方。 一对多关系：部门和员工，一个部门有多个员工，一个员工属于一个部门，部门是一，员工是多。 1234567891011121314public class Department &#123; private String depName; public void setDepName(String depName) &#123; this.depName = depName; &#125; @Override public String toString() &#123; return &quot;Department&#123;&quot; + &quot;depName=&#x27;&quot; + depName + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617181920212223242526public class Employee &#123; private String name; private String gender; private Department dep; public void setName(String name) &#123; this.name = name; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public void setDep(Department dep) &#123; this.dep = dep; &#125; @Override public String toString() &#123; return &quot;Employee&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, gender=&#x27;&quot; + gender + &#x27;\\&#x27;&#x27; + &quot;, dep=&quot; + dep + &#x27;&#125;&#x27;; &#125;&#125; 在 spring 配置文件中进行配置。 12345678910&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.pojo.Employee&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;gender&quot; value=&quot;male&quot;/&gt; &lt;property name=&quot;dep&quot;&gt; &lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.pojo.Department&quot;&gt; &lt;property name=&quot;depName&quot; value=&quot;IT&quot;/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 注入属性 — 级联赋值。 写法一： 12345678910&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.pojo.Employee&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;gender&quot; value=&quot;male&quot;/&gt; &lt;!-- 级联赋值写法一 --&gt; &lt;property name=&quot;dep&quot; ref=&quot;department&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.pojo.Department&quot;&gt; &lt;property name=&quot;depName&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt; 写法二：注意，必须要在 Employee 类中添加 dep 属性的 getter 方法，否则会报错。 1234567891011&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.pojo.Employee&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;gender&quot; value=&quot;male&quot;/&gt; &lt;!-- 级联赋值写法二 --&gt; &lt;property name=&quot;dep&quot; ref=&quot;department&quot;/&gt; &lt;property name=&quot;dep.depName&quot; value=&quot;editorial&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.pojo.Department&quot;&gt; &lt;property name=&quot;depName&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt; xml 注入集合属性：数组类型、List 类型、Map 类型、Set 类型。 在 Spring 中可以通过一组内置的 xml 标签来配置集合属性，比如：&lt;array&gt;、&lt;list&gt;、&lt;set&gt;、&lt;map&gt;、&lt;props&gt;，并且可以用过引入 util 名称空间来提取集合类型的 bean。 创建类，定义数组、list、map、set 类型属性，生成对应 setter 方法。 123456789101112131415161718192021222324252627public class CollectionExample &#123; private String[] array; private List&lt;String&gt; list; private Map&lt;String, String&gt; map; private Set&lt;String&gt; set; private Properties properties; public void setArray(String[] array) &#123; this.array = array; &#125; public void setList(List&lt;String&gt; list) &#123; this.list = list; &#125; public void setMap(Map&lt;String, String&gt; map) &#123; this.map = map; &#125; public void setSet(Set&lt;String&gt; set) &#123; this.set = set; &#125; public void setProperties(Properties properties) &#123; this.properties = properties; &#125;&#125; 在 Spring 配置文件进行配置。 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;bean id=&quot;collectionExample&quot; class=&quot;cn.xisun.spring.pojo.CollectionExample&quot;&gt; &lt;!-- 数组类型属性注入 --&gt; &lt;property name=&quot;array&quot;&gt; &lt;array value-type=&quot;java.lang.String&quot;&gt; &lt;value&gt;Java&lt;/value&gt; &lt;value&gt;数据库&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!-- List类型属性注入 --&gt; &lt;property name=&quot;list&quot;&gt; &lt;list value-type=&quot;java.lang.String&quot;&gt; &lt;value&gt;张三&lt;/value&gt; &lt;value&gt;李四&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- Map类型属性注入 --&gt; &lt;property name=&quot;map&quot;&gt; &lt;map key-type=&quot;java.lang.String&quot; value-type=&quot;java.lang.String&quot;&gt; &lt;entry key=&quot;JAVA&quot; value=&quot;java&quot;/&gt; &lt;entry key=&quot;PYTHON&quot; value=&quot;python&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- Set类型属性注入 --&gt; &lt;property name=&quot;set&quot;&gt; &lt;list value-type=&quot;java.lang.String&quot;&gt; &lt;value&gt;MySQL&lt;/value&gt; &lt;value&gt;Redis&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- Properties类型属性注入 --&gt; &lt;property name=&quot;properties&quot;&gt; &lt;props value-type=&quot;java.lang.String&quot;&gt; &lt;prop key=&quot;SPRING&quot;&gt;spring&lt;/prop&gt; &lt;prop key=&quot;JVM&quot;&gt;jvm&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 在集合里面设置对象类型值。 1234567public class Course &#123; private String name; public void setName(String name) &#123; this.name = name; &#125;&#125; 1234567public class Stu &#123; private List&lt;Course&gt; coursesist; public void setCoursesist(List&lt;Course&gt; coursesist) &#123; this.coursesist = coursesist; &#125;&#125; 1234567891011121314151617&lt;!-- 1.创建多个course对象 --&gt;&lt;bean id=&quot;course1&quot; class=&quot;cn.xisun.spring.pojo.Course&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Spring&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;course2&quot; class=&quot;cn.xisun.spring.pojo.Course&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;SpringMVC&quot;/&gt;&lt;/bean&gt;&lt;!-- 2.注入list集合类型，值是对象 --&gt;&lt;bean id=&quot;stu&quot; class=&quot;cn.xisun.spring.pojo.Stu&quot;&gt; &lt;property name=&quot;coursesist&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;course1&quot;/&gt; &lt;ref bean=&quot;course2&quot;/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 把集合注入部分提取出来作为公共部分。 1234567891011121314public class Book &#123; private List&lt;String&gt; bookList; public void setBookList(List&lt;String&gt; bookList) &#123; this.bookList = bookList; &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;bookList=&quot; + bookList + &#x27;&#125;&#x27;; &#125;&#125; 在 Spring 配置文件中引入名称空间 util。 1234567&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd&quot;&gt; 使用 util 标签完成 list 集合注入提取。 1234567891011 &lt;!-- 1.提取 list 集合类型属性注入 --&gt; &lt;util:list id=&quot;bookList&quot;&gt; &lt;value&gt;论语&lt;/value&gt; &lt;value&gt;孟子&lt;/value&gt; &lt;value&gt;大学&lt;/value&gt; &lt;/util:list&gt; &lt;!-- 2.注入list集合类型，值是对象 --&gt; &lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name=&quot;bookList&quot; ref=&quot;bookList&quot;/&gt;&lt;/bean&gt; Map 和 Set 参考 List 的写法。 Spring 中 bean 的类型： Spring 内置了两种类型的 bean ，一种是普通 bean ，另外一种是工厂 bean (FactoryBean)。 普通 bean：在配置文件中定义的 bean 类型与返回类型一致。 工厂 bean：在配置文件定义 bean 类型可以和返回类型不一样。 第一步：创建类，让这个类作为工厂 bean，要求实现接口 FactoryBean 接口。 FactoryBean 接口中有如下三个方法：getObject() 负责将创建好的 bean 实例返回给 IOC 容器；getObjectType() 负责返回工厂生产的 bean 类型；isSingleton() 用于指示该 bean 实例是否为单例，默认是单例 bean。 12345678910111213public interface FactoryBean&lt;T&gt; &#123; String OBJECT_TYPE_ATTRIBUTE = &quot;factoryBeanObjectType&quot;; @Nullable T getObject() throws Exception; @Nullable Class&lt;?&gt; getObjectType(); default boolean isSingleton() &#123; return true; &#125;&#125; 第二步：实现接口里面的方法，在实现的方法中定义返回的 bean 类型。 1234567891011121314151617181920public class Book &#123; private String name; private String author; public void setName(String name) &#123; this.name = name; &#125; public void setAuthor(String author) &#123; this.author = author; &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, author=&#x27;&quot; + author + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617public class MyBean implements FactoryBean&lt;Book&gt; &#123; // 在getObject()方法中定义返回的bean类型 @Override public Book getObject() throws Exception &#123; return new Book(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Book.class; &#125; @Override public boolean isSingleton() &#123; return false; &#125;&#125; 第三步：在 Spring 配置文件中进行配置并测试，注意获取 bean 的时候要使用工厂 bean 返回的那个 bean 的类型。 1&lt;bean id=&quot;myBean&quot; class=&quot;cn.xisun.spring.pojo.MyBean&quot;&gt;&lt;/bean&gt; 123456789101112public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Book book = iocContainer.getBean(&quot;myBean&quot;, Book.class); // 3.打印bean System.out.println(book); &#125;&#125; Spring 中 bean 的作用域： 默认情况下，Spring 只为每个在 IOC 容器里声明的 bean 创建唯一一个实例 (单例对象)，整个 IOC 容器范围内都能共享该实例：所有后续的 getBean() 调用和 bean 引用都将返回这个唯一的 bean 实例。该作用域被称为 singleton，它是所有 bean 的默认作用域。 在 Spring 中，可以在 &lt;bean&gt; 元素的 scope 属性里设置 bean 的作用域，以决定这个 bean 是单实例的还是多实例的。scope 属性值有四个： singleton：在 Spring IOC 容器中仅存在一个 bean 实例，bean 以单实例的方式存在。默认值。 1234&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;平凡的世界&quot;/&gt; &lt;property name=&quot;author&quot; value=&quot;路遥&quot;/&gt;&lt;/bean&gt; 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Book book = iocContainer.getBean(&quot;book&quot;, Book.class); Book book1 = iocContainer.getBean(&quot;book&quot;, Book.class); // 3.打印bean System.out.println(book == book1); &#125;&#125; 输出结果是 true，说明 book 和 book1 的地址一样，二者指向同一个对象。 prototype：每次调用 getBean() 时都会返回一个新的实例，bean 以多实例的方式存在。 1234&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot; scope=&quot;prototype&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;平凡的世界&quot;/&gt; &lt;property name=&quot;author&quot; value=&quot;路遥&quot;/&gt;&lt;/bean&gt; 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Book book = iocContainer.getBean(&quot;book&quot;, Book.class); Book book1 = iocContainer.getBean(&quot;book&quot;, Book.class); // 3.打印bean System.out.println(book == book1); &#125;&#125; 输出结果是 false，说明 book 和 book1 的地址不一样，二者指向不同的对象。 设置 scope 值是 singleton 时候，加载 Spring 配置文件时候就会创建单实例对象；设置 scope 值是 prototype 时候，不是在加载 Spring 配置文件的时候创建对象，而是在调用 getBean() 时创建多实例对象。 request 和 session 不常用。 Spring 中 bean 的生命周期： 生命周期：对象从创建到销毁的过程，是这个对象的生命周期。 Spring IOC 容器可以管理 bean 的生命周期，Spring 允许在 bean 生命周期内特定的时间点执行指定的任务。Spring IOC 容器对 bean 的生命周期进行管理的过程： 1. 通过构造器或工厂方法创建 bean 实例。 2. 为 bean 的属性设置值和对其他 bean 的引用。 3. 调用 bean 的初始化方法 (需要创建和配置初始化的方法)。 4. 获取 bean 实例并使用。 5. 当容器关闭时，调用 bean 的销毁方法 (需要创建和配置销毁的方法)。 代码演示： 1234567891011121314151617181920212223242526272829public class Book &#123; private String name; public Book() &#123; System.out.println(&quot;第一步：执行无参数构造方法创建bean实例&quot;); &#125; public void setName(String name) &#123; System.out.println(&quot;第二步：调用setter方法设置属性值&quot;); this.name = name; &#125; // 创建执行的初始化的方法 public void initMethod()&#123; System.out.println(&quot;第三步：执行初始化的方法&quot;); &#125; // 创建执行的销毁的方法 public void destroyMethod()&#123; System.out.println(&quot;第五步：执行销毁的方法&quot;); &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 123&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;平凡的世界&quot;/&gt;&lt;/bean&gt; 在 &lt;bean&gt; 标签中指定 book 实例的 init-method 属性 (初始化方法) 和 destroy-method 属性 (销毁方法)。 123456789101112131415161718192021222324public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 System.out.println(&quot;第四步：获取创建的bean实例对象&quot;); Book book = iocContainer.getBean(&quot;book&quot;, Book.class); // 3.打印bean System.out.println(book); // 手动销毁bean的实例，会调用Book中定义的destroyMethod()，前提：在Spring配置文件中bean标签配置了destroy-method // ApplicationContext接口没有close()，需要它的子接口或实现类才能调用 ((ClassPathXmlApplicationContext)iocContainer).close(); &#125;&#125;输出结果：第一步：执行无参数构造方法创建bean实例第二步：调用setter方法设置属性值第三步：执行初始化的方法第四步：获取创建的bean实例对象Book&#123;name=&#x27;平凡的世界&#x27;&#125;第五步：执行销毁的方法 注意：要手动关闭 IOC 容器才会执行 destroy-method 方法。 Spring 中可以设置 bean 后置处理器： bean 后置处理器允许在调用初始化方法前后对bean进行额外的处理。 bean 后置处理器对 IOC 容器里的所有 bean 实例逐一处理，而非单一实例。其典型应用是：检查 bean 属性的正确性或根据特定的标准更改 bean 的属性。 定义 bean 后置处理器时需要实现接口：org.springframework.beans.factory.config.BeanPostProcessor。在 bean 的初始化方法被调用前后，Spring 将把每个 bean 实例分别传递给上述接口的以下两个方法： postProcessBeforeInitialization(Object, String) postProcessAfterInitialization(Object, String) 添加 BeanPostProcessor 后的 bean 生命周期： 1. 通过构造器或工厂方法创建 bean 实例。 2. 为 bean 的属性设置值和对其他 bean 的引用。 3. 将 bean 实例传递给 bean 后置处理器的 postProcessBeforeInitialization()。 4. 调用 bean 的初始化方法 (需要创建和配置初始化的方法)。 5. 将 bean 实例传递给 bean 后置处理器的 postProcessAfterInitialization()。 6. 获取 bean 实例并使用。 7. 当容器关闭时，调用 bean 的销毁方法 (需要创建和配置销毁的方法)。 代码演示： 12345678910111213141516/** * 自定义bean后置处理器 */public class MyBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;第三步：执行初始化方法之前，执行postProcessBeforeInitialization方法&quot;); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;第五步：执行初始化方法之后，执行postProcessAfterInitialization方法&quot;); return bean; &#125;&#125; 1234567891011121314151617181920212223242526272829public class Book &#123; private String name; public Book() &#123; System.out.println(&quot;第一步：执行无参数构造方法创建bean实例&quot;); &#125; public void setName(String name) &#123; System.out.println(&quot;第二步：调用setter方法设置属性值&quot;); this.name = name; &#125; // 创建执行的初始化的方法 public void initMethod()&#123; System.out.println(&quot;第四步：执行初始化的方法&quot;); &#125; // 创建执行的销毁的方法 public void destroyMethod()&#123; System.out.println(&quot;第七步：执行销毁的方法&quot;); &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 123456&lt;!-- 配置后置处理器，适用于配置的所有的bean --&gt;&lt;bean id=&quot;myBeanPostProcessor&quot; class=&quot;cn.xisun.spring.bean.MyBeanPostProcessor&quot;/&gt;&lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;平凡的世界&quot;/&gt;&lt;/bean&gt; 1234567891011121314151617181920212223242526public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 System.out.println(&quot;第六步：获取创建的bean实例对象&quot;); Book book = iocContainer.getBean(&quot;book&quot;, Book.class); // 3.打印bean System.out.println(book); // 手动销毁bean的实例，会调用Book中定义的destroyMethod()，前提：在Spring配置文件中bean标签配置了destroy-method // ApplicationContext接口没有close()，需要它的子接口或实现类才能调用 ((ClassPathXmlApplicationContext)iocContainer).close(); &#125;&#125;输出结果：第一步：执行无参数构造方法创建bean实例第二步：调用setter方法设置属性值第三步：执行初始化方法之前，执行postProcessBeforeInitialization方法第四步：执行初始化的方法第五步：执行初始化方法之后，执行postProcessAfterInitialization方法第六步：获取创建的bean实例对象Book&#123;name=&#x27;平凡的世界&#x27;&#125;第七步：执行销毁的方法 Spring 中 bean 的自动装配： 手动装配：在 &lt;bean&gt; 标签中，以 value 或 ref 的方式明确指定属性值都是手动装配。 自动装配：根据指定的装配规则 (属性名称或者属性类型)，不需要明确指定，Spring 自动将匹配的属性值注入 bean 中。 自动装配的装配模式： 根据类型自动装配 (byType)：将类型匹配的 bean 作为属性注入到另一个 bean 中。若 IOC 容器中有多个与目标 bean 类型一致的 bean，Spring 将无法判定哪个 bean 最合适该属性，继而不能执行自动装配。 123456789101112131415&lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.pojo.Department&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt;&lt;!-- 不能出现两个Department类型的bean --&gt;&lt;!--&lt;bean id=&quot;department1&quot; class=&quot;cn.xisun.spring.pojo.Department&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt;--&gt;&lt;!-- 通过bean标签属性autowire，实现自动装配。 autowire 属性常用两个值： byName：根据属性名称注入，要求注入值bean的id值和类对应的属性名称一样。 byType：根据属性类型注入，要求配置文件中只能有一个与目标bean类型一致的bean。--&gt;&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.pojo.Employee&quot; autowire=&quot;byType&quot;/&gt; 根据名称自动装配 (byName)：必须将目标 bean 的名称和属性名设置的完全相同。 1234567891011&lt;bean id=&quot;department&quot; class=&quot;cn.xisun.spring.pojo.Department&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;IT&quot;/&gt;&lt;/bean&gt;&lt;!-- 通过bean标签属性autowire，实现自动装配。 autowire 属性常用两个值： byName：根据属性名称注入，要求注入值bean的id值和类对应的属性名称一样。 byType：根据属性类型注入，要求配置文件中只能有一个与目标bean类型一致的bean。--&gt;&lt;bean id=&quot;employee&quot; class=&quot;cn.xisun.spring.pojo.Employee&quot; autowire=&quot;byName&quot;/&gt; 根据构造器自动装配 (constructor)：当 bean 中存在多个构造器时，此种自动装配方式将会很复杂。不推荐使用。 相对于使用注解的方式实现的自动装配，在 xml 配置文件中进行的自动装配略显笨拙，在项目中更多的是使用注解的方式实现。 代码演示： 1234567891011121314public class Department &#123; private String name; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Department&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617181920public class Employee &#123; private String name; private Department department; public void setName(String name) &#123; this.name = name; &#125; public void setDepartment(Department department) &#123; this.department = department; &#125; @Override public String toString() &#123; return &quot;Employee&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, department=&quot; + department + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Employee employee = iocContainer.getBean(&quot;employee&quot;, Employee.class); // 3.打印bean System.out.println(employee); &#125;&#125;输出结果：Employee&#123;name=&#x27;null&#x27;, department=Department&#123;name=&#x27;IT&#x27;&#125;&#125; Spring 中 bean 配置信息的继承： Spring 允许继承 bean 的配置，被继承的 bean 称为父 bean，继承这个父 bean 的 bean 称为子 bean。子 bean 可以从父 bean 中继承配置，包括 bean 的属性配置，子 bean 也可以覆盖从父 bean 继承过来的配置。 父 bean 可以作为配置模板，也可以作为 bean 实例。若只想把父 bean 作为模板，可以设置 &lt;bean&gt; 的 abstract 属性为 true，这样 Spring 将不会实例化这个 bean。 创建实体类： 1234567891011121314151617181920212223242526public class Book &#123; private String name; private String author; private String era; public void setName(String name) &#123; this.name = name; &#125; public void setAuthor(String author) &#123; this.author = author; &#125; public void setEra(String era) &#123; this.era = era; &#125; @Override public String toString() &#123; return &quot;Book&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, author=&#x27;&quot; + author + &#x27;\\&#x27;&#x27; + &quot;, era=&#x27;&quot; + era + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617- 不使用继承配置 bean： &#96;&#96;&#96;xml &lt;bean id&#x3D;&quot;book1&quot; class&#x3D;&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;论语&quot;&#x2F;&gt; &lt;!-- 以下都是重复的属性 --&gt; &lt;property name&#x3D;&quot;author&quot; value&#x3D;&quot;孔子&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;era&quot; value&#x3D;&quot;春秋末期&quot;&#x2F;&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;book2&quot; class&#x3D;&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;春秋&quot;&#x2F;&gt; &lt;!-- 以下都是重复的属性 --&gt; &lt;property name&#x3D;&quot;author&quot; value&#x3D;&quot;孔子&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;era&quot; value&#x3D;&quot;春秋末期&quot;&#x2F;&gt; &lt;&#x2F;bean&gt; 1234567891011121314151617 &gt; book1 和 book2 两个 bean 的 author 和 era 两个属性的值相同，像上面的配置会有点冗余。 - 使用配置信息的继承配置 bean： &#96;&#96;&#96;xml &lt;bean id&#x3D;&quot;book1&quot; class&#x3D;&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;论语&quot;&#x2F;&gt; &lt;!-- 以下都是重复的属性 --&gt; &lt;property name&#x3D;&quot;author&quot; value&#x3D;&quot;孔子&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;era&quot; value&#x3D;&quot;春秋末期&quot;&#x2F;&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;book2&quot; parent&#x3D;&quot;book1&quot;&gt; &lt;!-- 重写不同值的属性即可 --&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;春秋&quot;&#x2F;&gt; &lt;&#x2F;bean&gt; 1 代码演示： 1234567891011121314151617public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 Book book1 = iocContainer.getBean(&quot;book1&quot;, Book.class); Book book2 = iocContainer.getBean(&quot;book2&quot;, Book.class); // 3.打印bean System.out.println(book1); System.out.println(book2); &#125;&#125;输出结果：Book&#123;name=&#x27;论语&#x27;, author=&#x27;孔子&#x27;, era=&#x27;春秋末期&#x27;&#125;Book&#123;name=&#x27;春秋&#x27;, author=&#x27;孔子&#x27;, era=&#x27;春秋末期&#x27;&#125; Spring 中 bean 之间的依赖： 有的时候创建一个 bean 的时候需要保证另外一个 bean 也被创建，这时我们称前面的 bean 对后面的 bean 有依赖。例如：要求创建 Student 对象的时候必须创建 Book。这里需要注意的是依赖关系不等于引用关系，Student 即使依赖 Book 也可以不引用它。 12345678910&lt;!-- 一定要创建一个book对象，否则student对象无法创建 --&gt; &lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.pojo.Student&quot; depends-on=&quot;book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;论语&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;book&quot; class=&quot;cn.xisun.spring.pojo.Book&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;论语&quot;/&gt; &lt;property name=&quot;author&quot; value=&quot;孔子&quot;/&gt; &lt;property name=&quot;era&quot; value=&quot;春秋末期&quot;/&gt; &lt;/bean&gt; Spring 引入外部 Properties 文件： 当 bean 的配置信息逐渐增多时，查找和修改一些 bean 的配置信息就变得愈加困难。这时可以将一部分信息提取到 bean 配置文件的外部，以 properties 格式的属性文件保存起来，同时在 bea n的配置文件中引用 properties 属性文件中的内容，从而实现一部分属性值在发生变化时仅修改 properties 属性文件即可。这种技术多用于连接数据库的基本信息的配置。 引入 druid 依赖和 mysql-connector-java 驱动依赖： 12345678910111213&lt;!-- druid连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.20&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.19&lt;/version&gt;&lt;/dependency&gt; 在 Spring 配置文件中直接配置数据库连接信息： 1234567&lt;!-- 直接配置数据库连接池 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/userDb&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;&lt;/bean&gt; 在 Spring 配置文件中引入外部 properties 文件中单独存放的数据库连接信息： 在类路径下创建 jdbc.properties 数据库配置文件： 1234prop.driverClass=com.mysql.cj.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/userDbprop.userName=rootprop.password=root 在 Spring 配置文件中引入 context 名称空间： 1234567&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; 通过 &lt;context:property-placeholder&gt; 标签中的 location 属性来制定配置文件的路径，classpath: 表示该配置文件位于类路径下，并通过 $&#123;prop.userName&#125; 的方式来取出配置文件中的属性值。 12345678910&lt;!-- 引用外部属性文件来配置数据库连接池 --&gt;&lt;!-- 指定properties属性文件的位置，classpath:xxx表示属性文件位于类路径下 --&gt;&lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt;&lt;!-- 从properties属性文件中引入属性值 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;prop.driverClass&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;prop.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;prop.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;prop.password&#125;&quot;/&gt;&lt;/bean&gt; 代码演示： 123456789101112131415161718192021222324public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 DataSource dataSource = iocContainer.getBean(&quot;dataSource&quot;, DataSource.class); // 3.打印bean System.out.println(dataSource); &#125;&#125;输出结果：&#123; CreateTime:&quot;2021-04-15 15:36:05&quot;, ActiveCount:0, PoolingCount:0, CreateCount:0, DestroyCount:0, CloseCount:0, ConnectCount:0, Connections:[ ]&#125; 基于注解方式实现 Bean 管理： 什么是注解： 注解是代码特殊标记，格式：@注解名称(属性名称=属性值, 属性名称=属性值...)。 使用注解的时候，注解作用在类上面、方法上面、属性上面。 相对于 xml 方式而言，通过注解的方式配置 bean 更加简洁和优雅，而且和 MVC 组件化开发的理念十分契合，是开发中常用的使用方式。 Spring 中用于标识 bean 的四个注解： **@Component**：普通组件，用于标识一个受 Spring IOC 容器管理的组件。 **@Respository**：持久化层组件，用于标识一个受 Spring IOC 容器管理的持久化层组件。 **@Service**：业务逻辑层组件，用于标识一个受 Spring IOC 容器管理的业务逻辑层组件。 **@Controller**：表述层控制器组件，用于标识一个受 Spring IOC 容器管理的表述层控制器组件。 事实上 Spring 并没有能力识别一个组件到底是不是它所标记的类型，即使将 @Respository 注解用在一个非持久化层组件上面，也不会产生任何错误，所以 @Respository、@Service、@Controller 这几个注解仅仅是为了让开发人员自己明确当前的组件扮演的角色。 组件命名规则： 默认情况：使用组件的简单类名首字母小写后得到的字符串作为 bean 的 id。 也可以使用四个组件注解的 value 属性指定 bean 的 id。 开启 Spring 注解方式的整体流程： 第一步：引入 spring-aop 依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 第二步：引入 context 名称空间。 1234567&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; 第三步：开启组件扫描。 123456&lt;!-- 开启组件扫描： 1.如果扫描多个包，多个包间使用逗号隔开。 2.扫描包的上层目录。--&gt;&lt;context:component-scan base-package=&quot;cn.xisun.spring&quot;/&gt; 第四步：创建类，在类上面添加创建对象注解。 12345678910package cn.xisun.spring.service;import org.springframework.stereotype.Service;@Servicepublic class UserService &#123; public void add() &#123; System.out.println(&quot;user service add ......&quot;); &#125;&#125; 第五步：获取和使用 bean。 12345678910111213141516public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 UserService userService = iocContainer.getBean(&quot;userService&quot;, UserService.class); // 3.打印bean System.out.println(userService); userService.add(); &#125;&#125;输出结果：cn.xisun.spring.service.UserService@8e0379duser service add ...... 开启组件扫描的注意事项： base-package 属性指定一个需要扫描的基类包，Spring 容器将会扫描这个基类包及其子包中的所有类。 当需要扫描多个包时可以使用逗号分隔，或者指定这多个包的上层包。 如果仅希望扫描特定的类而非基包下的所有类，可使用 resource-pattern 属性过滤特定的类，示例： 12&lt;!-- resource-pattern：只扫描cn.xisun.spring包下的dao子包下的所有类。 --&gt;&lt;context:component-scan base-package=&quot;cn.xisun.spring&quot; resource-pattern=&quot;dao/*.class&quot;/&gt; 使用 resource-pattern 并不能提供完善的功能，所有我们得使用过滤子元素的方法。 **&lt;context:include-filter&gt;**：表示要包含的目标类。注意：通常需要与 use-default-filters 属性配合使用才能够达到 “仅包含某些组件” 这样的效果。即：通过将 use-default-filters 属性设置为 false，禁用默认过滤器，然后扫描的就只是 &lt;context:include-filter&gt; 中的规则指定的组件了。 123456789&lt;!-- 示例1： use-default-filters=&quot;false&quot;：表示现在不使用默认filter，而是使用自己配置filter。 context:include-filter：用于设置需要扫描哪些内容(这里配置扫描Repository、Service和Controller注解)--&gt; &lt;context:component-scan base-package=&quot;cn.xisun.spring&quot; use-default-filters=&quot;false&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Repository&quot;/&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Service&quot;/&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; **&lt;context:exclude-filter&gt;**：表示要排除在外的目标类。 1234&lt;!-- 示例2：下面配置扫描包所有内容context:exclude-filter，设置哪些内容不进行扫描(这里排除Controller注解) --&gt;&lt;context:component-scan base-package=&quot;cn.xisun.spring&quot;&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt;&lt;/context:component-scan&gt; 一个 component-scan 标签下可以有多个 include-filter 和 exclude-filter。 include-filter 和 exclude-filter 的 type 属性所支持的类型如下表： 在这些类型当中，除了 custom 外，aspectj 的过滤功能最强大，它能轻易的实现其他类别的过滤规则。 基于注解方式实现属性注入： 项目中组件装配时，Controller 组件中往往需要用到 Service 组件的实例，Service 组件中往往需要用到 Repository 组件的实例。Spring 可以通过注解的方式帮我们实现属性的装配。 在指定要扫描的包时，&lt;context:component-scan&gt; 元素会自动注册一个 bean 的后置处理器 AutowiredAnnotationBeanPostProcessor 的实例。该后置处理器可以自动装配标记了@Autowired、@Resource 或 @Inject 注解的属性。这就是组件扫描的原理。 @Autowired 根据属性类型实现自动装配。 构造器、普通字段 (即使是非 public)、一切具有参数的方法都可以应用 @Autowired 注解。 默认情况下，所有使用 @Autowired 注解的属性都需要被设置。当 Spring 找不到匹配的 bean 装配属性时，会抛出异常。 若某一属性允许不被设置，可以设置 @Autowired 注解的 required 属性为 false。 默认情况下，当 IOC 容器里存在多个类型兼容的 bean 时，Spring 会尝试匹配 bean 的 id 值是否与变量名相同，如果相同则进行装配。如果 bean 的 id 值不相同，通过类型的自动装配将无法工作。此时可以在 @Qualifier 注解里提供 bean 的名称。Spring 甚至允许在方法的形参上标注 @Qualifiter 注解以指定注入 bean 的名称。 @Autowired 注解也可以应用在数组类型的属性上，此时 Spring 将会把所有匹配的 bean 进行自动装配。 @Autowired 注解也可以应用在集合属性上，此时 Spring 读取该集合的类型信息，然后自动装配所有与之兼容的 bean。 @Autowired 注解用在 java.util.Map上时，若该 Map 的键值为 String，那么 Spring 将自动装配与值类型兼容的 bean 作为值，并以 bean 的 id 值作为键。 @Autowired 注解使用过程： 第一步 把 service 和 dao 对象创建，在 service 和 dao 类添加创建对象注解。 第二步 在 service 注入 dao 对象，在 service 类添加 dao 类型属性，在属性上面使用注解 123public interface UserDao &#123; public void add();&#125; 1234567@Repositorypublic class UserDaoImpl implements UserDao &#123; @Override public void add() &#123; System.out.println(&quot;dao add ......&quot;); &#125;&#125; 1234567891011@Servicepublic class UserService &#123; // 定义dao类型属性，添加注入属性注解，不需要添加setter方法 @Autowired private UserDao userDao; public void add() &#123; System.out.println(&quot;user service add ......&quot;); userDao.add(); &#125;&#125; 1234567891011121314151617public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 UserService userService = iocContainer.getBean(&quot;userService&quot;, UserService.class); // 3.打印bean System.out.println(userService); userService.add(); &#125;&#125;输出结果：cn.xisun.spring.service.UserService@161b062auser service add ......dao add ...... @Qualifier 根据属性名称实现自动装配。 @Qualifier 注解需要和上面 @Autowired 注解一起使用。 如果存在多个类型相同的 bean，可以为每个 bean 单独命名，然后根据名称使用 @Qualifier 注解指定需要注入的 bean。 @Qualifier 注解使用过程： 123public interface UserDao &#123; public void add();&#125; 1234567@Repository(value = &quot;userDaoImpl1&quot;)public class UserDaoImpl implements UserDao &#123; @Override public void add() &#123; System.out.println(&quot;dao add ......&quot;); &#125;&#125; 1234567891011@Servicepublic class UserService &#123; @Autowired @Qualifier(value = &quot;userDaoImpl1&quot;)// 需要与指定的bean的value相同，否则会找不到 private UserDao userDao; public void add() &#123; System.out.println(&quot;user service add ......&quot;); userDao.add(); &#125;&#125; 1234567891011121314151617public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置文件，创建IOC容器对象 ApplicationContext iocContainer = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 UserService userService = iocContainer.getBean(&quot;userService&quot;, UserService.class); // 3.打印bean System.out.println(userService); userService.add(); &#125;&#125;输出结果：cn.xisun.spring.service.UserService@3ee0fea4user service add ......dao add ...... @Resource 可以根据类型注入，也可以根据名称注入。@Resource 注解要求提供一个 bean 名称的属性，若该属性为空，则自动采用标注处的变量或方法名作为 bean 的名称。 @Resource 是 JDK 提供的注解，应该尽量使用 Spring 提供的注解。 @Resource 注解使用说明： 123// @Resource // 根据类型进行注入@Resource(name = &quot;userDaoImpl1&quot;) // 根据bean名称进行注入 private UserDao userDao; @Value 注入普通属性的值。 @Value 注解使用说明： 123456789101112131415@Servicepublic class UserService &#123; @Autowired @Qualifier(value = &quot;userDaoImpl1&quot;) private UserDao userDao; @Value(value = &quot;Tom&quot;) private String name;// @Value注解为name属性注入了一个值Tom public void add() &#123; System.out.println(&quot;name is: &quot; + this.name);// name is: Tom System.out.println(&quot;user service add ......&quot;); userDao.add(); &#125;&#125; 完全注解开发： 第一步：创建 SpringConfig 配置类，代替之前的 xml 配置文件。 1234@Configuration@ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;)public class SpringConfig &#123;&#125; @Configuration：标识这是一个配置类。 @ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;)：配置组件扫描。 第二步：编写测试类，通过 new 一个 AnnotationConfigApplicationContext 对象创建 IOC 容器对象。其他与前面的相同。 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; // 1.加载Spring配置类，创建IOC容器对象 ApplicationContext iocContainer = new AnnotationConfigApplicationContext(SpringConfig.class); // 2.根据id值获取配置文件中的bean实例对象，要求使用返回的bean的类型 UserService userService = iocContainer.getBean(&quot;userService&quot;, UserService.class); // 3.打印bean System.out.println(userService); userService.add(); &#125;&#125; SqEL 表达式语言： SpEL 的全称是 Spring Expression Language，即 Spring 表达式语言，简称 SpEL，支持运行时查询并可以操作对象图，和 JSP 页面上的 EL 表达式、Struts2 中用到的 OGNL 表达式一样，SpEL 根据 JavaBean 风格的 getXxx()、setXxx() 方法定义的属性访问对象图，完全符合我们熟悉的操作习惯。 基本语法： SpEL 使用 #&#123;…&#125;作为定界符，所有在大框号中的字符都将被认为是 SpEL 表达式。 字面量： 整数：&lt;property name=&quot;count&quot; value=&quot;#&#123;5&#125;&quot;/&gt; 小数：&lt;property name=&quot;frequency&quot; value=&quot;#&#123;89.7&#125;&quot;/&gt; 科学计数法：&lt;property name=&quot;capacity&quot; value=&quot;#&#123;1e4&#125;&quot;/&gt; String 类型的字面量可以使用单引号或者双引号作为字符串的定界符号： &lt;property name=&quot;name&quot; value=&quot;#&#123;&#39;xisun&#39;&#125;&quot;/&gt; &lt;property name=&#39;name&#39; value=&#39;#&#123;&quot;xisun&quot;&#125;&#39;/&gt; Boolean：&lt;property name=&quot;enabled&quot; value=&quot;#&#123;false&#125;&quot;/&gt; 引用其他 bean： 在 &lt;bean&gt; 标签的 value 属性中通过 #&#123;对象名&#125; 引用其他 bean，注意：不能使用 ref 属性。 1234567891011&lt;!-- 引用其他bean --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.entity.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;233&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;computer&quot; value=&quot;#&#123;computer&#125;&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;computer&quot; class=&quot;cn.xisun.spring.entity.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;666&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;HP&quot;/&gt;&lt;/bean&gt; 引用其他 bean 的属性: 在 &lt;property&gt; 标签中通过 #&#123;对象名.属性名&#125; 引用其他 bean 的属性。 12345678910111213141516&lt;!-- 引用其他bean的属性 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.entity.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;233&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt; &lt;property name=&quot;computer&quot; &gt; &lt;bean class=&quot;cn.xisun.spring.entity.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;#&#123;computer.computerId&#125;&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;#&#123;computer.computerName&#125;&quot;/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;computer&quot; class=&quot;cn.xisun.spring.entity.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;666&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;HP&quot;/&gt;&lt;/bean&gt; 调用非静态方法： 通过 #&#123;对象名.方法名&#125; 调用对象的非静态方法。 12345678910111213141516&lt;!-- 调用非静态方法 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.entity.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;233&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Oneby&quot;/&gt; &lt;property name=&quot;computer&quot;&gt; &lt;bean class=&quot;cn.xisun.spring.entity.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;#&#123;computer.getComputerId()&#125;&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;#&#123;computer.getComputerName()&#125;&quot;/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;computer&quot; class=&quot;cn.xisun.spring.entity.Computer&quot;&gt; &lt;property name=&quot;computerId&quot; value=&quot;666&quot;/&gt; &lt;property name=&quot;computerName&quot; value=&quot;HP&quot;/&gt;&lt;/bean&gt; 调用静态方法： 通过 T(静态类路径).方法名 调用静态方法。举例：定义获取随机整数的方法，随机整数的范围为 [start, end]。 12345public class MathUtil &#123; public static int getRandomInt(int start, int end) &#123; return (int) (Math.random() * (end - start + 1) + start); &#125;&#125; 12345&lt;!-- 调用静态方法 --&gt;&lt;bean id=&quot;student&quot; class=&quot;cn.xisun.spring.entity.Student&quot;&gt; &lt;property name=&quot;studentId&quot; value=&quot;#&#123;T(cn.xisun.spring.util.MathUtil).getRandomInt(0, 255)&#125;&quot;/&gt; &lt;property name=&quot;studentName&quot; value=&quot;Tom&quot;/&gt;&lt;/bean&gt; Spring 中配置文件的整合： Spring 允许通过 &lt;import&gt; 将多个配置文件引入到一个文件中，进行配置文件的集成。这样在启动 Spring 容器时，仅需要指定这个合并好的配置文件就可以。 import 元素的 resource 属性支持 Spring 的标准的路径资源： AOP AOP 概述： AOP (Aspect-Oriented Programming，面向切面编程)：是一种新的方法论，是对传统 OOP (Object-Oriented Programming，面向对象编程) 的补充。 AOP 编程操作的主要对象是切面 (aspect)，而切面模块化横切关注点。 在应用 AOP 编程时，仍然需要定义公共功能，但可以明确的定义这个功能应用在哪里，以什么方式应用，并且不必修改受影响的类。这样一来横切关注点就被模块化到特殊的类里 — 这样的类我们通常称之为 “切面”。 AOP 的好处：每个事物逻辑位于一个位置，代码不分散，便于维护和升级；业务模块更简洁，只包含核心业务代码。以上面的计算器案例说明： 通俗的说：AOP 是面向切面 (方面) 编程，利用 AOP 可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。即：可在不通过修改源代码方式，在主干功能里面添加新功能。 AOP 底层原理： AOP 底层使用动态代理。 第一种：有接口情况，使用 JDK 动态代理。 创建接口实现类代理对象，增强类的方法。 数学计算器要求：① 执行加减乘除运算；② 日志增强：在程序执行期间追踪正在发生的活动；③ 验证增强：希望计算器只能处理正数的运算。 数学计算器的常规实现代码 (这里为了简便形参类型设置为 int)： 123456789101112/** * 计算器接口 */public interface ArithmeticCalculator &#123; Integer add(int i, int j); Integer subtract(int i, int j); Integer multiply(int i, int j); Integer div(int i, int j);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 常规方法实现类 */public class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public Integer add(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method add() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i + j; System.out.println(&quot;The method add() ends with [&quot; + result + &quot;]&quot;); return result; &#125; @Override public Integer subtract(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method subtract() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i - j; System.out.println(&quot;The method subtract() ends with [&quot; + result + &quot;]&quot;); return result; &#125; @Override public Integer multiply(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method multiply() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i * j; System.out.println(&quot;The method multiply() ends with [&quot; + result + &quot;]&quot;); return result; &#125; @Override public Integer div(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method div() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i / j; System.out.println(&quot;The method div() ends with [&quot; + result + &quot;]&quot;); return result; &#125;&#125; 存在的问题一：代码混乱。越来越多的非业务需求 (日志和验证等) 加入后，原有的业务方法急剧膨胀。每个方法在处理核心逻辑的同时还必须兼顾其他多个关注点。 存在的问题二：代码分散。以日志需求为例，只是为了满足这个单一需求，就不得不在多个模块 (方法) 里多次重复相同的日志代码。如果日志需求发生变化，必须修改所有模块。 使用 JDK 动态代理改进： 123456789101112/** * 计算器接口 */public interface ArithmeticCalculator &#123; Integer add(int i, int j); Integer subtract(int i, int j); Integer multiply(int i, int j); Integer div(int i, int j);&#125; 12345678910111213141516171819202122232425262728/** * ArithmeticCalculator实现类，只做计算的核心功能 */public class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public Integer add(int i, int j) &#123; System.out.println(&quot;add 核心方法&quot;); return i + j; &#125; @Override public Integer subtract(int i, int j) &#123; System.out.println(&quot;subtract 核心方法&quot;); return i - j; &#125; @Override public Integer multiply(int i, int j) &#123; System.out.println(&quot;multiply 核心方法&quot;); return i * j; &#125; @Override public Integer div(int i, int j) &#123; System.out.println(&quot;div 核心方法&quot;); return i / j; &#125;&#125; 12345678910111213141516171819202122232425/** * 日志处理器：在计算的过程中添加日志记录 */public class ArithmeticCalculatorLoggingHandler implements InvocationHandler &#123; private Object obj; public ArithmeticCalculatorLoggingHandler(Object obj) &#123; this.obj = obj; &#125; // 重写invoke()，增加日志处理 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;The method &quot; + method.getName() + &quot;() begins with &quot; + Arrays.toString(args)); Object result = method.invoke(obj, args); System.out.println(&quot;The method &quot; + method.getName() + &quot;() ends with [&quot; + result + &quot;]&quot;); return result; &#125; // 创建当前代理的代理对象 public static Object createProxy(Object obj) &#123; ArithmeticCalculatorLoggingHandler handler = new ArithmeticCalculatorLoggingHandler(obj); return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), handler); &#125;&#125; 12345678910111213141516171819202122232425262728293031/** * 验证处理器：在计算之前对参数进行验证 */public class ArithmeticCalculatorValidationHandler implements InvocationHandler &#123; private Object obj; public ArithmeticCalculatorValidationHandler(Object obj) &#123; this.obj = obj; &#125; // 重写invoke()，增加验证处理 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; for (Object arg : args) &#123; validate((int) arg); &#125; return method.invoke(obj, args); &#125; private void validate(int number) &#123; if (number &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + number); &#125; &#125; // 创建当前代理的代理对象 public static Object createProxy(Object obj) &#123; ArithmeticCalculatorValidationHandler handler = new ArithmeticCalculatorValidationHandler(obj); return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), handler); &#125;&#125; 12345678910// 测试方法public class SpringTest &#123; public static void main(String[] args) &#123; // 两级增强：普通计算 ---&gt; 日志增强 ---&gt; 验证增强 ArithmeticCalculator calculator = (ArithmeticCalculator) ArithmeticCalculatorValidationHandler.createProxy( ArithmeticCalculatorLoggingHandler.createProxy(new ArithmeticCalculatorImpl())); int addResult = calculator.add(-1, 2); System.out.println(&quot;result: &quot; + addResult); &#125;&#125; 第二种：没有接口情况，使用 CGLIB 动态代理。 创建子类的代理对象，增强类的方法。 数学计算器要求：① 执行加减乘除运算；② 日志增强：在程序执行期间追踪正在发生的活动；③ 验证增强：希望计算器只能处理正数的运算。 数学计算器的常规实现代码 (这里为了简便形参类型设置为 int)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 常规方法实现类 */public class ArithmeticCalculator &#123; public Integer add(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method add() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i + j; System.out.println(&quot;The method add() ends with [&quot; + result + &quot;]&quot;); return result; &#125; public Integer subtract(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method subtract() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i - j; System.out.println(&quot;The method subtract() ends with [&quot; + result + &quot;]&quot;); return result; &#125; public Integer multiply(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method multiply() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i * j; System.out.println(&quot;The method multiply() ends with [&quot; + result + &quot;]&quot;); return result; &#125; public Integer div(int i, int j) &#123; if (i &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + i); &#125; if (j &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + j); &#125; System.out.println(&quot;The method div() begins with [&quot; + i + &quot;, &quot; + j + &quot;]&quot;); int result = i / j; System.out.println(&quot;The method div() ends with [&quot; + result + &quot;]&quot;); return result; &#125;&#125; 使用 CGLIB 动态代理改进： 123456789101112131415161718192021public class ArithmeticCalculator &#123; public Integer add(int i, int j) &#123; System.out.println(&quot;add 核心方法&quot;); return i + j; &#125; public Integer subtract(int i, int j) &#123; System.out.println(&quot;subtract 核心方法&quot;); return i - j; &#125; public Integer multiply(int i, int j) &#123; System.out.println(&quot;multiply 核心方法&quot;); return i * j; &#125; public Integer div(int i, int j) &#123; System.out.println(&quot;div 核心方法&quot;); return i / j; &#125;&#125; 1234567891011121314151617181920/** * 日志拦截器：在计算的过程中添加日志记录 */public class ArithmeticCalculatorLoggingInterceptor implements MethodInterceptor &#123; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; System.out.println(&quot;The method &quot; + method.getName() + &quot;() begins with &quot; + Arrays.toString(args)); Object result = methodProxy.invokeSuper(obj, args); System.out.println(&quot;The method &quot; + method.getName() + &quot;() ends with [&quot; + result + &quot;]&quot;); return result; &#125; public static Object createProxy(Object obj) &#123; Enhancer enhancer = new Enhancer(); enhancer.setClassLoader(obj.getClass().getClassLoader()); enhancer.setSuperclass(obj.getClass()); enhancer.setCallback(new ArithmeticCalculatorLoggingInterceptor()); return enhancer.create(); &#125;&#125; 1234567891011121314151617181920212223242526/** * 验证处理器：在计算之前对参数进行验证 */public class ArithmeticCalculatorValidationInterceptor implements MethodInterceptor &#123; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; for (Object arg : args) &#123; validate((int) arg); &#125; return methodProxy.invokeSuper(obj, args); &#125; private void validate(int number) &#123; if (number &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + number); &#125; &#125; public static Object createProxy(Object obj) &#123; Enhancer enhancer = new Enhancer(); enhancer.setClassLoader(obj.getClass().getClassLoader()); enhancer.setSuperclass(obj.getClass()); enhancer.setCallback(new ArithmeticCalculatorValidationInterceptor()); return enhancer.create(); &#125;&#125; 12345678910// 测试方法public class SpringTest &#123; public static void main(String[] args) &#123; // 日志增强 ArithmeticCalculator arithmeticCalculator = (ArithmeticCalculator) ArithmeticCalculatorLoggingInterceptor .createProxy(new ArithmeticCalculator()); Integer addResult = arithmeticCalculator.add(-1, 2); System.out.println(addResult); &#125;&#125; CGLIB 不支持类嵌套增强，如果需要多个多个嵌套增强，需要其他方法实现，此处不涉及。 AOP 相关术语： 连接点 (JoinPoint)**：类里面可以被增强的方法被称为连接点。**就是 Spring 允许使用通知的地方，基本每个方法的前、后 (两者都有也行)，或抛出异常时都可以是连接点，Spring 只支持方法连接点。 切入点 (Pointcut)**：实际被真正增强的方法，称为切入点。**在上面说的连接点的基础上，来定义切入点，假设一个类里，有 15 个方法，那就可能有几十个连接点，但不一定需要在所有方法附近都使用通知，而是只想让其中的几个方法使用通知。则在调用这几个方法之前，之后或者抛出异常时，利用切入点来定义这几个方法，让切入点来筛选连接点，选中那几个需要使用通知的方法。 通知 (Advice)**：实际增强的逻辑部分，也就是想要的功能，比如上面说的日志处理、验证处理等。**事先定义好，然后在想用的地方用一下。通知的类型：前置通知、最终通知、后置通知、异常通知、环绕通知。 前置通知 (Before Advice)：在切入点选择的连接点处的方法之前执行的通知，该通知不影响正常程序执行流程 (除非该通知抛出异常，该异常将中断当前方法链的执行而返回)。 最终通知 (After Advice)：在切入点选择的连接点处的方法之后执行的通知 (无论方法执行是否成功都会被调用)。 后置通知 (After returning Advice)：在切入点选择的连接点处的方法正常执行完毕时执行的通知，必须是连接点处的方法没抛出任何异常正常返回时才调用。 异常通知 (After throwing Advice)：在切入点选择的连接点处的方法抛出异常返回时执行的通知，必须是连接点处的方法抛出任何异常返回时才调用异常通知。 环绕通知 (Around Advices)：环绕着在切入点选择的连接点处的方法所执行的通知，环绕通知可以在方法调用之前和之后自定义任何行为，并且可以决定是否执行连接点处的方法、替换返回值、抛出异常等等。 切面 (Aspect)**：把通知应用到切入点的过程 (是动作)。**切面是通知和切入点的结合，也就是说，没连接点什么事情，连接点是为了好理解切入点而提出来的概念。 **引入 (introduction)**：允许我们向现有的类添加新方法属性，也就是把切面 (即新方法属性：通知定义的) 用到目标类中。 **目标 (target)**：引入中所提到的目标类，也就是要被通知的对象，即真正的业务逻辑，他可以在毫不知情的情况下，被织入切面。而自己专注于业务本身的逻辑。 **代理 (proxy)**：怎么实现整套 AOP 机制的，都是通过代理。 **织入 (weaving)**：把切面应用到目标对象来创建新的代理对象的过程。有 3 种方式，Spring 采用的是运行时。 切入点表达式： 切入点表达式作用：表明对哪个类里面的哪个方法进行增强。 语法结构： execution([权限修饰符] [返回类型] [类全类名] [方法名称]([参数列表]) )。 权限修饰符一般使用 * 替代；返回类型可以省略；参数列表使用 .. 代替。 举例 1：对 cn.xisun.spring.dao.UserDao 类里面的 add() 进行增强。 execution(* cn.xisun.spring.dao.UserDao.add(..)) 举例 2：对 cn.xisun.spring.dao.UserDao 类里面的所有的方法进行增强。 execution(* cn.xisun.spring.dao.UserDao.*(..)) 举例 3：对 cn.xisun.spring.dao 包里面所有类，类里面所有方法进行增强。 execution(* cn.xisun.spring.dao.*.*(..)) 举例 4：对 cn.xisun.spring.dao.UserDao 类里面返回 double 类型的方法进行增强。 execution(* double cn.xisun.spring.dao.UserDao.*(..)) 举例 5：对 cn.xisun.spring.dao.UserDao 类里面第一个参数为 double 类型的方法进行增强。 execution(* cn.xisun.spring.dao.UserDao.*(double, ..)) 举例 6：对 cn.xisun.spring.dao.UserDao 类里面里面的 add() 或 div() 进行增强。 execution(* cn.xisun.spring.dao.UserDao.add(..)) || execution(* cn.xisun.spring.dao.UserDap.div(..)) 在 AspectJ 中，切入点表达式可以通过 &amp;&amp;、||、! 等操作符结合起来。 实现 AOP 操作的准备工作： Spring 框架一般都是基于 AspectJ 实现 AOP 操作： AspectJ 不是 Spring 组成部分，它是 Java 社区里最完整最流行的 AOP 框架。在 Spring 2.0 以上版本中，可以使用基于 AspectJ 注解或基于 xml 配置的 AOP。 基于 AspectJ 实现 AOP 操作： 基于注解方式实现 (常用)。 基于 xml 配置文件实现。 引入 AOP 和 AspectJ 的相关依赖： 123456789101112131415161718192021222324252627282930&lt;!-- Spring AOP和AspectJ相关依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;5.1.10.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.9.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;aopalliance&lt;/groupId&gt; &lt;artifactId&gt;aopalliance&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;net.sourceforge.cglib&lt;/groupId&gt; &lt;artifactId&gt;com.springsource.net.sf.cglib&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 基于注解方式实现 AOP 操作： 第一步：编写 Spring 配置文件，引入 context 和 aop 名称空间，并开启组件扫描，指明包路径，以及开启自动代理功能。 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=&quot;cn.xisun.spring.aop&quot;/&gt; &lt;!-- 开启Aspect生成代理对象--&gt; &lt;!-- 被增强类有接口，需指定proxy-target-class为true，如果没有接口，不需要指定这个参数 --&gt; &lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt;&lt;/beans&gt; 第二步：被增强类 (目标类) 的定义。添加 @Component 注解。 123456789public interface ArithmeticCalculator &#123; int add(int i, int j); int subtract(int i, int j); int multiply(int i, int j); int div(int i, int j);&#125; 1234567891011121314151617181920212223242526272829/** * 需要被增强的类 */@Componentpublic class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public Integer add(int i, int j) &#123; System.out.println(&quot;add 核心方法&quot;); return i + j; &#125; @Override public Integer subtract(int i, int j) &#123; System.out.println(&quot;subtract 核心方法&quot;); return i - j; &#125; @Override public Integer multiply(int i, int j) &#123; System.out.println(&quot;multiply 核心方法&quot;); return i * j; &#125; @Override public Integer div(int i, int j) &#123; System.out.println(&quot;div 核心方法&quot;); return i / j; &#125;&#125; 第三步：增强类 (切面类) 的定义。在增强类上添加 @Component 和 @Aspect 注解；在增强类里面，在作为通知的方法上面添加对应的通知类型注解，并使用切入点表达式配置需要增强的方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 日志增强 */@Component@Aspect@Order(1)public class ArithmeticCalculatorLoggingAspect &#123; // 相同的切入点抽取 @Pointcut(value = &quot;execution(* cn.xisun.spring.aop.ArithmeticCalculatorImpl.*(..))&quot;) public void pointSame() &#123; &#125; @Before(value = &quot;pointSame()&quot;) public void before() &#123; System.out.println(&quot;@Before 前置通知&quot;); &#125; @AfterReturning(value = &quot;pointSame()&quot;) public void afterReturning() &#123; System.out.println(&quot;@AfterReturning 后置通知&quot;); &#125; @After(value = &quot;pointSame()&quot;) public void after() &#123; System.out.println(&quot;@After 最终通知&quot;); &#125; @AfterThrowing(value = &quot;pointSame()&quot;) public void afterThrowing() &#123; System.out.println(&quot;@AfterThrowing 异常通知&quot;); &#125; @Around(value = &quot;execution(* cn.xisun.spring.aop.ArithmeticCalculatorImpl.add(..))&quot;) public Object around(ProceedingJoinPoint proceedingJoinPoint) &#123; System.out.println(&quot;@Around 环绕通知之前&quot;); // 被增强的方法执行，proceed是该方法的返回结果，如果原方法为void，则proceed为null Object proceed = null; try &#123; proceed = proceedingJoinPoint.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; System.out.println(&quot;@Around 环绕通知之后&quot;); return proceed; &#125;&#125; 前置通知、后置通知、异常通知和最终通知，可以额外接受一个 JoinPoint 参数，用来获取目标对象和目标方法相关信息，但是一定要保证这个参数是第一个参数。在环绕通知中必须显式的通过调用 ProceedingJoinPoint 来执行目标方法，否则目标方法不会执行。 123456789101112131415161718192021222324/** * 验证增强 */@Component@Aspect@Order(0)public class ArithmeticCalculatorValidationAspect &#123; @Before(value = &quot;execution(* cn.xisun.spring.aop.ArithmeticCalculatorImpl.*(..))&quot;) public void before(JoinPoint joinPoint) &#123; System.out.println(&quot;验证方法开始执行&quot;); Class&lt;?&gt; clazz = joinPoint.getTarget().getClass();// 当前执行的方法所属的类 String name = joinPoint.getSignature().getName();// 当前执行的方法名 Object[] args = joinPoint.getArgs();// 当前执行的方法的参数 for (Object arg : args) &#123; validate((int) arg); &#125; &#125; private void validate(int number) &#123; if (number &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + number); &#125; &#125;&#125; 第四步：测试方法。 12345678910111213141516171819public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); ArithmeticCalculator arithmeticCalculatorImpl = context.getBean(&quot;arithmeticCalculatorImpl&quot;, ArithmeticCalculatorImpl.class); Integer addResult = arithmeticCalculatorImpl.add(1, 2); System.out.println(&quot;计算结果：&quot; + addResult); &#125;&#125;输出结果：Spring 测试版本：5.2.7.RELEASE验证方法开始执行@Around 环绕通知之前@Before 前置通知add 核心方法@AfterReturning 后置通知@After 最终通知@Around 环绕通知之后计算结果：3 进阶操作： 1. 相同的切入点抽取： 在编写 AspectJ 切面时，可以直接在通知注解中书写切入点表达式。但同一个切点表达式可能会在多个通知中重复出现。此时，在 AspectJ 切面中，可以通过 @Pointcut 注解将一个重复的切入点声明成简单的方法，该切入点的方法体通常是空的。 切入点方法的访问权限控制符同时也控制着这个切入点的可见性。如果切入点要在多个切面中共用，最好将它们集中在一个公共的类中。在这种情况下，它们必须被声明为 public。在引入这个切入点时，必须将类名也包括在内。如果类没有与这个切面放在同一个包中，还必须包含包名。 比如，前面的日志增强类，各个通知的切入点表达式主要是 execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.*(..))，可以把它单独抽取出来： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 日志增强 */@Component@Aspectpublic class LoggingAspect implements CutAspect &#123; // 相同的切入点抽取 @Pointcut(value = &quot;execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.*(..))&quot;) public void pointSame() &#123; &#125; @Override @Before(value = &quot;pointSame()&quot;) public void before() &#123; System.out.println(&quot;@Before 前置通知&quot;); &#125; @Override @AfterReturning(value = &quot;pointSame()&quot;) public void afterReturning() &#123; System.out.println(&quot;@AfterReturning 后置通知&quot;); &#125; @Override @After(value = &quot;pointSame()&quot;) public void after() &#123; System.out.println(&quot;@After 最终通知&quot;); &#125; @Override @AfterThrowing(value = &quot;pointSame()&quot;) public void afterThrowing() &#123; System.out.println(&quot;@AfterThrowing 异常通知&quot;); &#125; @Override @Around(value = &quot;execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.add(..))&quot;) public Object around(ProceedingJoinPoint proceedingJoinPoint) &#123; System.out.println(&quot;proceedingJoinPoint: &quot; + proceedingJoinPoint); System.out.println(&quot;@Around 环绕通知之前&quot;); // 被增强的方法执行，proceed是该方法的返回结果，如果原方法为void，则proceed为null Object proceed = null; try &#123; proceed = proceedingJoinPoint.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; System.out.println(&quot;@Around 环绕通知之后&quot;); return proceed; &#125;&#125; 2. 指定切面的优先级： 在同一个连接点上应用不止一个切面时，除非明确指定，否则它们的优先级是不确定的。切面的优先级可以通过实现 Ordered 接口或利用 @Order(数值类型值) 注解指定。 若是实现 Ordered 接口，getOrder() 方法的返回值越小，优先级越高。 若是使用 @Order(数值类型值) 注解，数字类型值越小，优先级越高。 123456789@Component@Aspect@Order(1)public class ArithmeticCalculatorLoggingAspect implements CutAspect &#123;&#125; @Component@Aspect@Order(0)public class ArithmeticCalculatorValidationAspect implements CutAspect &#123;&#125; 完全使用注解方式实现 AOP 开发: 第一步：创建配置类，替代 xml 配置文件。其他操作，与基于注解方式实现 AOP 操作相同。 12345@Configuration@ComponentScan(basePackages = &#123;&quot;cn.xisun.spring.aop&quot;&#125;)@EnableAspectJAutoProxy(proxyTargetClass = true)public class SpringAopConfig &#123;&#125; @Configuration：表示这是一个配置类。 @ComponentScan(basePackages = &#123;&quot;cn.xisun.spring.aop&quot;&#125;)：配置包扫描路径为 cn.xisun.spring.aop。 @EnableAspectJAutoProxy(proxyTargetClass = true)：表示开启 AOP 自动代理。如果被增强类有接口，需指定 proxy-target-class 为 true，如果被增强类没有接口，不需要指定这个参数。 第二步：编写测试代码。 12345678910111213141516171819public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new AnnotationConfigApplicationContext(SpringAopConfig.class); ArithmeticCalculator arithmeticCalculatorImpl = context.getBean(&quot;arithmeticCalculatorImpl&quot;, ArithmeticCalculatorImpl.class); Integer addResult = arithmeticCalculatorImpl.add(1, 2); System.out.println(&quot;计算结果：&quot; + addResult); &#125;&#125;输出结果：Spring 测试版本：5.2.7.RELEASE验证方法开始执行@Around 环绕通知之前@Before 前置通知add 核心方法@AfterReturning 后置通知@After 最终通知@Around 环绕通知之后计算结果：3 基于 xml 配置文件实现 AOP 操作 (了解，不建议深究)： 除了使用 AspectJ 注解声明切面，Spring 也支持在 bean 配置文件中声明切面。这种声明是通过 AOP 名称空间中的 xml 元素完成的。 正常情况下，基于注解的声明要优先于基于 xml 的声明，尽可能不使用基于 xml 的声明。通过 AspectJ 注解，切面可以与 AspectJ 兼容，而基于 xml 的配置则是 Spring 专有的。由于 AspectJ 得到越来越多的 AOP 框架支持，因此以注解风格编写的切面将会有更多重用的机会。 具体步骤： 第一步：编写 Spring 配置文件，引入 aop 名称空间，并开启自动代理功能。 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 开启Aspect生成代理对象--&gt; &lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt;&lt;/beans&gt; 第二步：定义增强类和被增强类。 123456789public interface ArithmeticCalculator &#123; Integer add(int i, int j); Integer subtract(int i, int j); Integer multiply(int i, int j); Integer div(int i, int j);&#125; 12345678910111213141516171819202122232425262728/** * 需要被增强的类 */public class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public Integer add(int i, int j) &#123; System.out.println(&quot;add 核心方法&quot;); return i + j; &#125; @Override public Integer subtract(int i, int j) &#123; System.out.println(&quot;subtract 核心方法&quot;); return i - j; &#125; @Override public Integer multiply(int i, int j) &#123; System.out.println(&quot;multiply 核心方法&quot;); return i * j; &#125; @Override public Integer div(int i, int j) &#123; System.out.println(&quot;div 核心方法&quot;); return i / j; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344public interface CutAspect &#123; /** * 前置通知：在方法执行前执行 */ default void before() &#123; &#125; default void before(JoinPoint joinPoint) &#123; &#125; /** * 后置通知 */ default void afterReturning() &#123; &#125; default void afterReturning(JoinPoint joinPoint) &#123; &#125; /** * 异常通知 */ default void afterThrowing() &#123; &#125; default void afterThrowing(JoinPoint joinPoint) &#123; &#125; /** * 环绕通知 */ default Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123; return proceedingJoinPoint.proceed(); &#125; /** * 最终通知 */ default void after() &#123; &#125; default void after(JoinPoint joinPoint) &#123; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738/** * 日志增强 */public class LoggingAspect implements CutAspect &#123; @Override public void before() &#123; System.out.println(&quot;@Before 前置通知&quot;); &#125; @Override public void afterReturning() &#123; System.out.println(&quot;@AfterReturning 后置通知&quot;); &#125; @Override public void after() &#123; System.out.println(&quot;@After 最终通知&quot;); &#125; @Override public void afterThrowing() &#123; System.out.println(&quot;@AfterThrowing 异常通知&quot;); &#125; @Override public Object around(ProceedingJoinPoint proceedingJoinPoint) &#123; System.out.println(&quot;@Around 环绕通知之前&quot;); // 被增强的方法执行，proceed是该方法的返回结果，如果原方法为void，则proceed为null Object proceed = null; try &#123; proceed = proceedingJoinPoint.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; System.out.println(&quot;@Around 环绕通知之后&quot;); return proceed; &#125;&#125; 123456789101112131415161718192021/** * 验证增强 */public class ArithmeticCalculatorValidationAspect implements CutAspect &#123; @Override public void before(JoinPoint joinPoint) &#123; System.out.println(&quot;验证方法开始执行&quot;); Class&lt;?&gt; clazz = joinPoint.getTarget().getClass();// 当前执行的方法所属的类 String name = joinPoint.getSignature().getName();// 当前执行的方法名 Object[] args = joinPoint.getArgs();// 当前执行的方法的参数 for (Object arg : args) &#123; validate((int) arg); &#125; &#125; private void validate(int number) &#123; if (number &lt;= 0) &#123; throw new IllegalArgumentException(&quot;positive numbers only: &quot; + number); &#125; &#125;&#125; 第三步：在 Spring 配置文件中配置两个类的对象。 123&lt;!-- 配置增强类LoggingAspect和被增强类ArithmeticCalculatorImpl的对象 --&gt;&lt;bean id=&quot;arithmeticCalculatorImpl&quot; class=&quot;cn.xisun.spring.dao.ArithmeticCalculatorImpl&quot;/&gt;&lt;bean id=&quot;loggingAspect&quot; class=&quot;cn.xisun.spring.dao.LoggingAspect&quot;/&gt; 第四步：配置切入点和切面。 12345678910111213141516&lt;!-- 配置aop切入点 --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点表达式 --&gt; &lt;aop:pointcut id=&quot;add&quot; expression=&quot;execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.add(..))&quot;/&gt; &lt;aop:pointcut id=&quot;all&quot; expression=&quot;execution(* cn.xisun.spring.dao.ArithmeticCalculatorImpl.*(..))&quot;/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:aspect ref=&quot;loggingAspect&quot;&gt; &lt;!-- 配置通知的类型，以及对应的切入点 --&gt; &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:after-returning method=&quot;afterReturning&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;all&quot;/&gt; &lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;add&quot;/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 在 bean 配置文件中，所有的 Spring AOP 配置都必须定义在 &lt;aop:config&gt; 元素内部。对于每个切面而言，都要创建一个 &lt;aop:aspect&gt; 元素来为具体的切面实现引用后端 bean 实例。切面 bean 必须有一个标识符，供 &lt;aop:aspect&gt; 元素引用。 切入点： 切入点使用 &lt;aop:pointcut&gt; 元素声明。 切入点必须定义在 &lt;aop:aspect&gt; 元素下，或者直接定义在 &lt;aop:config&gt; 元素下。 切入点定义在 &lt;aop:aspect&gt; 元素下时：只对当前切面有效。 切入点定义在 &lt;aop:config&gt; 元素下：对所有切面都有效。 基于 xml 的 AOP 配置不允许在切入点表达式中用名称引用其他切入点。 通知： 在 aop 名称空间中，每种通知类型都对应一个特定的 xml 元素。 通知元素需要使用 &lt;pointcut-ref&gt; 来引用切入点，或用 &lt;pointcut&gt; 直接嵌入切入点表达式。 method 属性指定切面类中通知方法的名称。 xml 在配置带参数的通知时，有部分细节未搞清楚，ArithmeticCalculatorValidationAspect 配置不成功，不做探讨了。 第五步：测试方法。 123456789public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); ArithmeticCalculator arithmeticCalculatorImpl = context.getBean(&quot;arithmeticCalculatorImpl&quot;, ArithmeticCalculatorImpl.class); Integer add = arithmeticCalculatorImpl.add(7, 2); System.out.println(&quot;计算结果：&quot; + add); &#125;&#125; JdbcTemplate 为了使 JDBC 更加易于使用，Spring 在 JDBC API 上定义了一个抽象层，以此建立一个 JDBC 存取框架。 作为 Spring JDBC 框架的核心，JDBC 模板的设计目的是为不同类型的 JDBC 操作提供模板方法，通过这种方式，可以在尽可能保留灵活性的情况下，将数据库存取的工作量降到最低。 可以将 Spring 的 JdbcTemplate 看作是一个小型的轻量级持久化层框架，和我们之前使用过的 DBUtils 风格非常接近。 第一步：引入 jdbc 和 mysql 的相关依赖。 1234567891011121314151617181920212223242526272829303132333435&lt;!-- Spring jdbc相关依赖--&gt;&lt;!-- spring-jdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- spring-tx: 事务相关 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- spring-orm: 整合Mybatis等框架需要 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- druid连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.20&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.19&lt;/version&gt;&lt;/dependency&gt; 第二步：开启组件扫描。 12&lt;!-- 开启组件扫描 --&gt;&lt;context:component-scan base-package=&quot;cn.xisun.spring&quot;/&gt; 第三步：配置数据库连接池。 12345678&lt;!-- 配置数据库连接池 --&gt;&lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;prop.driverClass&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;prop.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;prop.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;prop.password&#125;&quot;/&gt;&lt;/bean&gt; 第四步：配置 JdbcTemplate 对象，注入 DataSource。 12345&lt;!-- 配置JdbcTemplate对象 --&gt;&lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt; 第五步：创建 dao 类，在 dao 注入 jdbcTemplate 对象；创建 service 类，在 service 类注入 dao 对象。 12public interface UserDao &#123;&#125; 12345678/** * dao类 */@Repositorypublic class UserDaoImpl implements UserDao &#123; @Autowired private JdbcTemplate jdbcTemplate;// 注入JdbcTemplate&#125; 12345678/** * service类 */@Servicepublic class UserService &#123; @Autowired private UserDao userDao;// 注入dao&#125; JdbcTemplate 操作数据库 — 添加、修改、删除。 创建对应数据库表的实体类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class User &#123; private String userId; private String userName; private String userStatus; public User() &#123; &#125; public User(String userId, String userName, String userStatus) &#123; this.userId = userId; this.userName = userName; this.userStatus = userStatus; &#125; public String getUserId() &#123; return userId; &#125; public void setUserId(String userId) &#123; this.userId = userId; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getUserStatus() &#123; return userStatus; &#125; public void setUserStatus(String userStatus) &#123; this.userStatus = userStatus; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (!Objects.equals(userId, user.userId)) &#123; return false; &#125; if (!Objects.equals(userName, user.userName)) &#123; return false; &#125; return Objects.equals(userStatus, user.userStatus); &#125; @Override public int hashCode() &#123; int result = userId != null ? userId.hashCode() : 0; result = 31 * result + (userName != null ? userName.hashCode() : 0); result = 31 * result + (userStatus != null ? userStatus.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;userId=&#x27;&quot; + userId + &#x27;\\&#x27;&#x27; + &quot;, userName=&#x27;&quot; + userName + &#x27;\\&#x27;&#x27; + &quot;, userStatus=&#x27;&quot; + userStatus + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 在 dao 中调用 JdbcTemplate 对象里面的 update() 进行数据库添加、修改和删除操作。 1234567public interface UserDao &#123; void add(User user); void update(User user); void delete(String userId);&#125; 12345678910111213141516171819202122232425262728293031@Repositorypublic class UserDaoImpl implements UserDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public void add(User user) &#123; // 1.创建sql语句 String sql = &quot;insert into t_user values(?, ?, ?)&quot;; // 2.设置参数 Object[] args = &#123;user.getUserId(), user.getUserName(), user.getUserStatus()&#125;; // 3.调用方法实现 int update = jdbcTemplate.update(sql, args); System.out.println(update); &#125; @Override public void update(User user) &#123; String sql = &quot;update t_user set user_name = ?, user_status = ? where user_id = ?&quot;; Object[] args = &#123;user.getUserName(), user.getUserStatus(), user.getUserId()&#125;; int update = jdbcTemplate.update(sql, args); System.out.println(update); &#125; @Override public void delete(String userId) &#123; String sql = &quot;delete from t_user where user_id = ?&quot;; int update = jdbcTemplate.update(sql, userId); System.out.println(update); &#125;&#125; 1234567891011121314151617@Servicepublic class UserService &#123; @Autowired private UserDao userDao; public void addUser(User user) &#123; userDao.add(user); &#125; public void updateUser(User user) &#123; userDao.update(user); &#125; public void deleteUser(String userId) &#123; userDao.delete(userId); &#125;&#125; 测试方法，执行 service 类相应方法实现添加、修改和删除操作。 1234567891011121314151617181920public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); User user = new User(); user.setUserId(&quot;1000&quot;); user.setUserName(&quot;Tom&quot;); user.setUserStatus(&quot;ok&quot;); // 添加 userService.addUser(user); // 修改 user.setUserStatus(&quot;ng&quot;); userService.updateUser(user); // 删除 userService.deleteUser(&quot;1000&quot;); &#125;&#125; JdbcTemplate 操作数据库 — 查询返回某个值、查询返回对象、查询返回集合。 在 dao 中调用 JdbcTemplate 对象里面的 query() 和 queryForObject() 进行数据库相应查询操作。 12345678public interface UserDao &#123; Integer selectCount(); User findUser(String userId); List&lt;User&gt; findAllUser();&#125; 1234567891011121314151617181920212223@Repositorypublic class UserDaoImpl implements UserDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public Integer selectCount() &#123; String sql = &quot;select count(*) from t_user&quot;; return jdbcTemplate.queryForObject(sql, Integer.class); &#125; @Override public User findUser(String userId) &#123; String sql = &quot;select * from t_user where user_id = ?&quot;; return jdbcTemplate.queryForObject(sql, new BeanPropertyRowMapper&lt;&gt;(User.class), userId); &#125; @Override public List&lt;User&gt; findAllUser() &#123; String sql = &quot;select * from t_user&quot;; return jdbcTemplate.query(sql, new BeanPropertyRowMapper&lt;&gt;(User.class)); &#125;&#125; RowMapper 是一个函数式接口，其中只有一个方法：T mapRow(ResultSet rs, int rowNum) throws SQLException，该方法的具体作用是将查询得到的每行数据映射到 ResultSet 中。 BeanPropertyRowMapper 类实现了 RowMapper 接口，其功能是：将查询得到的结果集的值，注入到对象属性中。 1234567891011121314151617@Servicepublic class UserService &#123; @Autowired private UserDao userDao; public Integer selectCount() &#123; return userDao.selectCount(); &#125; public User findUser(String userId) &#123; return userDao.findUser(userId); &#125; public List&lt;User&gt; findAllUser() &#123; return userDao.findAllUser(); &#125;&#125; 测试方法，执行 service 类相应方法实现查询返回某个值、查询返回对象和查询返回集合操作。 12345678910111213141516171819public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); // 查询返回某个值 Integer number = userService.selectCount(); System.out.println(number); // 查询返回对象 User user = userService.findUser(&quot;1001&quot;); System.out.println(user); // 查询返回集合 List&lt;User&gt; allUser = userService.findAllUser(); System.out.println(allUser); &#125;&#125; JdbcTemplate 操作数据库 — 批量添加、修改和删除操作。 在 dao 中调用 JdbcTemplate 对象里面的 batchUpdate() 进行数据库批量添加、修改和删除操作。 12345678public interface UserDao &#123; void batchAddUser(List&lt;Object[]&gt; batchArgs); void batchUpdateUser(List&lt;Object[]&gt; batchArgs); void batchDeleteUser(List&lt;Object[]&gt; batchArgs);&#125; 1234567891011121314151617181920212223242526@Repositorypublic class UserDaoImpl implements UserDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public void batchAddUser(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;insert into t_user values(?, ?, ?)&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); &#125; @Override public void batchUpdateUser(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;update t_user set user_name = ?, user_status = ? where user_id = ?&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); &#125; @Override public void batchDeleteUser(List&lt;Object[]&gt; batchArgs) &#123; String sql = &quot;delete from t_user where user_id = ?&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); &#125;&#125; 1234567891011121314151617@Servicepublic class UserService &#123; @Autowired private UserDao userDao; public void batchAddUser(List&lt;Object[]&gt; batchArgs) &#123; userDao.batchAddUser(batchArgs); &#125; public void batchUpdateUser(List&lt;Object[]&gt; batchArgs) &#123; userDao.batchUpdateUser(batchArgs); &#125; public void batchDeleteUser(List&lt;Object[]&gt; batchArgs) &#123; userDao.batchDeleteUser(batchArgs); &#125;&#125; 测试方法，执行 service 类相应方法实现批量添加、修改和删除操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); // 批量添加 List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;(); // 方式一 List&lt;User&gt; list = new ArrayList&lt;&gt;(10); list.add(new User(&quot;1001&quot;, &quot;Tom&quot;, &quot;ok&quot;)); list.add(new User(&quot;1002&quot;, &quot;Jerry&quot;, &quot;ok&quot;)); list.add(new User(&quot;1003&quot;, &quot;Mike&quot;, &quot;ok&quot;)); for (User user : list) &#123; batchArgs.add(new Object[]&#123;user.getUserId(), user.getUserName(), user.getUserStatus()&#125;); &#125; // 方式二 /*Object[] o1 = &#123;&quot;1001&quot;, &quot;Tom&quot;, &quot;ok&quot;&#125;; Object[] o2 = &#123;&quot;1002&quot;, &quot;Jerry&quot;, &quot;ok&quot;&#125;; Object[] o3 = &#123;&quot;1003&quot;, &quot;Mike&quot;, &quot;ok&quot;&#125;; batchArgs.add(o1); batchArgs.add(o2); batchArgs.add(o3);*/ userService.batchAddUser(batchArgs); // 批量修改 List&lt;Object[]&gt; batchArgs1 = new ArrayList&lt;&gt;(); Object[] o4 = &#123;&quot;1001&quot;, &quot;Tom&quot;, &quot;ng&quot;&#125;; Object[] o5 = &#123;&quot;1002&quot;, &quot;Jerry&quot;, &quot;ng&quot;&#125;; Object[] o6 = &#123;&quot;1003&quot;, &quot;Mike&quot;, &quot;ng&quot;&#125;; batchArgs1.add(o4); batchArgs1.add(o5); batchArgs1.add(o6); userService.batchUpdateUser(batchArgs1); // 批量删除 List&lt;Object[]&gt; batchArgs2 = new ArrayList&lt;&gt;(); Object[] o7 = &#123;&quot;1002&quot;&#125;; Object[] o8 = &#123;&quot;1003&quot;&#125;; batchArgs2.add(o7); batchArgs2.add(o8); userService.batchDeleteUser(batchArgs2); &#125;&#125; 事务操作 事务是数据库操作的最基本单元，是一组由于逻辑上紧密关联而合并成一个整体 (工作单元) 的多个数据库操作，这些操作要么都执行成功，如果有一个失败所有操作都失败。典型应用场景：银行转账。 事务的四个特性 (ACID)： 原子性 (atomicity)：原子的本意是不可再分，事务的原子性表现为一个事务中涉及到的多个操作在逻辑上缺一不可。事务的原子性要求事务中的所有操作要么都执行，要么都不执行。 一致性 (consistency)：一致指的是数据的一致，具体是指：所有数据都处于满足业务规则的一致性状态。一致性原则要求：一个事务中不管涉及到多少个操作，都必须保证事务执行之前数据是正确的，事务执行之后数据仍然是正确的。如果一个事务在执行的过程中，其中某一个或某几个操作失败了，则必须将其他所有操作撤销，将数据恢复到事务执行之前的状态，这就是回滚。 隔离性 (isolation)：在应用程序实际运行过程中，事务往往是并发执行的，所以很有可能有许多事务同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏。隔离性原则要求多个事务在并发执行过程中不会互相干扰。 持久性 (durability)：持久性原则要求事务执行完成后，对数据的修改永久的保存下来，不会因各种系统错误或其他意外情况而受到影响。通常情况下，事务对数据的修改应该被写入到持久化存储器中。 事务管理一般添加到 JavaEE 三层结构里面的 Service 层 (业务逻辑层)。 事务管理操作有两种方式： 编程式事务管理： 执行步骤 — 使用原生的 JDBC API 进行事务管理： 获取数据库连接Connection对象 取消事务的自动提交 执行操作 正常完成操作时手动提交事务 执行失败时回滚事务 关闭相关资源 使用原生的 JDBC API 实现事务管理是所有事务管理方式的基石，同时也是最典型的编程式事务管理。编程式事务管理需要将事务管理代码嵌入到业务方法中来控制事务的提交和回滚。在使用编程的方式管理事务时，必须在每个事务操作中包含额外的事务管理代码。相对于核心业务而言，事务管理的代码显然属于非核心业务，如果多个模块都使用同样模式的代码进行事务管理，显然会造成较大程度的代码冗余。 声明式事务管理： 大多数情况下声明式事务比编程式事务管理更好：它将事务管理代码从业务方法中分离出来，以声明的方式来实现事务管理。事务管理代码的固定模式作为一种横切关注点，可以通过 AOP 方法模块化，进而借助 Spring AOP 框架实现声明式事务管理。 Spring 既支持编程式事务管理，也支持声明式事务管理。 Spring 进行声明式事务管理，底层使用 AOP 原理。 Spring 在不同的事务管理 API 之上定义了一个抽象层，通过配置的方式使其生效，从而让应用程序开发人员不必了解事务管理 API 的底层实现细节，就可以使用 Spring 的事务管理机制。 Spring 的事务管理器： Spring 的核心事务管理抽象是 PlatformTransactionManager。它为事务管理封装了一组独立于技术的方法。无论使用 Spring 的哪种事务管理策略 (编程式或声明式)，事务管理器都是必须的。 DataSourceTransactionManager：在应用程序中只需要处理一个数据源，而且通过 JDBC 存取。 JtaTransactionManager：在 JavaEE 应用服务器上用 JTA (Java Transaction API) 进行事务管理。 HibernateTransactionManager：用 Hibernate 框架存取数据库。 事务管理器可以以普通的 bean 的形式声明在 Spring IOC 容器中。 Spring 声明式事务管理的两种实现方式： 基于 xml 配置文件方式 基于注解方式 (常用) Spring 基于注解实现声明式事务管理： 第一步：引入 jdbc 和 mysql 的相关依赖、开启组件扫描、配置数据库连接池、配置 JdbcTemplate 对象，注入 DataSource。具体操作见 JdbcTemplate。 第二步：在 Spring 配置文件中，配置事务管理器并注入数据源。 12345&lt;!-- 配置事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt; 事务管理器的名字一定要叫 transactionManager，不然会抛异常。 第三步：在 Spring 配置文件中，引入 tx 名称空间并开启事务注解。 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt;&lt;/beans&gt; 12&lt;!-- 开启事务注解 --&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; 第四步：创建 dao 类，在 dao 注入 jdbcTemplate 对象；创建 service 类，在 service 类注入 dao 对象。具体操作见 JdbcTemplate。 第五步：在需要进行事务控制的方法或类上添加 @Transactional 注解。 如果把 @Transactional 注解添加类上面，则这个类里面所有的方法都添加事务。 如果把 @Transactional 注解添加方法上面，则为这个方法添加事务。 代码一览： 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt; &lt;!-- 开启组件扫描 --&gt; &lt;context:component-scan base-package=&quot;cn.xisun.spring.dao,cn.xisun.spring.service&quot;/&gt; &lt;!-- 配置数据库连接池 --&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;prop.driverClass&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;prop.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;prop.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;prop.password&#125;&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置JdbcTemplate对象 --&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 开启事务注解 --&gt; &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt;&lt;/beans&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class Account &#123; private Integer accountId; private String accountName; private Integer accountBalance; public Account() &#123; &#125; public Account(Integer accountId, String accountName, Integer accountBalance) &#123; this.accountId = accountId; this.accountName = accountName; this.accountBalance = accountBalance; &#125; public Integer getAccountId() &#123; return accountId; &#125; public void setAccountId(Integer accountId) &#123; this.accountId = accountId; &#125; public String getAccountName() &#123; return accountName; &#125; public void setAccountName(String accountName) &#123; this.accountName = accountName; &#125; public Integer getAccountBalance() &#123; return accountBalance; &#125; public void setAccountBalance(Integer accountBalance) &#123; this.accountBalance = accountBalance; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; Account account = (Account) o; if (!Objects.equals(accountId, account.accountId)) &#123; return false; &#125; if (!Objects.equals(accountName, account.accountName)) &#123; return false; &#125; return Objects.equals(accountBalance, account.accountBalance); &#125; @Override public int hashCode() &#123; int result = accountId != null ? accountId.hashCode() : 0; result = 31 * result + (accountName != null ? accountName.hashCode() : 0); result = 31 * result + (accountBalance != null ? accountBalance.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;Account&#123;&quot; + &quot;accountId=&quot; + accountId + &quot;, accountName=&#x27;&quot; + accountName + &#x27;\\&#x27;&#x27; + &quot;, accountBalance=&quot; + accountBalance + &#x27;&#125;&#x27;; &#125;&#125; 123456789public interface AccountDao &#123; void reduceMoney(); void addMoney(); // 上面两个方法可以合并 int tranfer(String accountName, int money);&#125; 123456789101112131415161718192021222324252627282930313233@Repositorypublic class AccountDaoImpl implements AccountDao &#123; @Autowired private JdbcTemplate jdbcTemplate; // lucy少钱 @Override public void reduceMoney() &#123; String sql = &quot;update t_account set account_balance = account_balance - ? where account_name = ?&quot;; jdbcTemplate.update(sql, 100, &quot;lucy&quot;); &#125; // mary多钱 @Override public void addMoney() &#123; String sql = &quot;update t_account set account_balance = account_balance + ? where account_name = ?&quot;; jdbcTemplate.update(sql, 100, &quot;mary&quot;); &#125; // 上面两个方法可以合并 @Override public int tranfer(String accountName, int money) &#123; // 创建 SQL 语句 String sql = &quot;update t_account set account_balance = account_balance - ? where account_name = ?&quot;; // SQL 语句参数 Object[] args = &#123;money, accountName&#125;; // 执行 SQL 语句 int insertRows = jdbcTemplate.update(sql, args); return insertRows; &#125;&#125; 123456789101112131415161718192021@Service@Transactionalpublic class AccountService &#123; @Autowired private AccountDao accountDao; // 转账的方法一 public void accountMoney() &#123; // lucy 少 100 accountDao.reduceMoney(); // mary 多 100 accountDao.addMoney(); &#125; // 转账的方法二 public void transfer(String srcAccountName, String destAccountName, int money) &#123; accountDao.tranfer(srcAccountName, money); accountDao.tranfer(destAccountName, -money); System.out.println(srcAccountName + &quot; 向 &quot; + destAccountName + &quot; 转账 &quot; + money + &quot; 元&quot;); &#125;&#125; 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); AccountService accountService = context.getBean(&quot;accountService&quot;, AccountService.class); // 测试方法一 accountService.accountMoney(); // 测试方法二 accountService.transfer(&quot;lucy&quot;, &quot;mary&quot;, 100); &#125;&#125; Spring 声明式事务管理参数配置： @Transactional 注解里面可以配置事务的相关参数。 123456789101112131415161718192021222324252627@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Transactional &#123; @AliasFor(&quot;transactionManager&quot;) String value() default &quot;&quot;; @AliasFor(&quot;value&quot;) String transactionManager() default &quot;&quot;; Propagation propagation() default Propagation.REQUIRED; Isolation isolation() default Isolation.DEFAULT; int timeout() default -1; boolean readOnly() default false; Class&lt;? extends Throwable&gt;[] rollbackFor() default &#123;&#125;; String[] rollbackForClassName() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] noRollbackFor() default &#123;&#125;; String[] noRollbackForClassName() default &#123;&#125;;&#125; propagation：事务传播行为。 对数据库表数据进行变化的操作叫事务方法。当一个事务方法被另一个事务方法调用时，必须指定事务应该如何传播。 事务的传播行为可以由传播属性指定，Spring 中定义了 7 种传播行为： REQUIRED 和 REQUIRED_NEW 是常用的两种事务传播行为。REQUIRED 是默认的事务传播行为。 REQUIRED 和 REQUIRED_NEW 的区别示例如下： Spring 中，可以通过指定 @Transactional 注解的 propagation 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 propagation 属性值，设置事务传播行为： 1@Transactional(propagation = Propagation.REQUIRES_NEW) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; propagation=&quot;REQUIRES_NEW&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; isolation：事务隔离级别。 事务的特性之一是隔离性，能够使得多事务在执行过程中，不会互相干扰。 但是，如果不考虑事务的隔离性，会产生三个读的问题：脏读、不可重复读、幻 (虚) 读。 脏读：一个未提交的事务读取到另一个事务未提交的数据。通俗点说：事务 A 更新了数据，但事务 A 还未提交，数据就被事务 B 读取了。 不可重复读：一个未提交的事务读取到另一个已提交事务修改的数据。通俗点说：一个事务中多次读取一个数据的结果不一致。 幻 (虚) 读：一个未提交的事务读取到另一个已提交事务新增的数据。通俗点说：一个事务多次读取同一个条件的数据时，数据的总条目不一致。 举例说明，假设现在有两个事务：Transaction01 和 Transaction02 并发执行。 ① 脏读： [1] Transaction01 将某条记录的 AGE 值从 20 修改为 30，但还未提交。 [2] Transaction02 读取了 Transaction01 更新后的值：30。 [3] Transaction01 回滚，AGE 值恢复到了 20。 [4] Transaction02 读取到的 30 就是一个无效的值。 ② 不可重复读： [1] Transaction01 读取了 AGE 值为 20。 [2] Transaction02 将 AGE 值修改为 30 并提交。 [3] Transaction01 再次读取 AGE 值为 30，和第一次读取不一致。 ③ 幻 (虚) 读： [1] Transaction01 读取了 STUDENT 表中的一部分数据。 [2] Transaction02 向 STUDENT 表中插入了新的行。 [3] Transaction01 同一条件下再次读取 STUDENT 表时，多出了一些行。 通过设置事务隔离级别，解决读问题： 数据库系统必须具有隔离并发运行各个事务的能力，使它们不会相互影响，避免各种并发问题。一个事务与其他事务隔离的程度称为隔离级别。SQL标准中规定了多种事务隔离级别，不同隔离级别对应不同的干扰程度，隔离级别越高，数据一致性就越好，但并发性越弱。 各个隔离级别解决并发问题的能力： 隔离级别 脏读 不可重复读 幻 (虚) 读 READ UNCOMMITTED (读未提交) 有 有 有 READ COMMITTED (读已提交) 无 有 有 REPEATABLE READ (可重复读) 无 无 有 SERIALIZABLE (串行化) 无 无 无 各种数据库产品对事务隔离级别的支持程度： 隔离级别 Oracle MySQL READ UNCOMMITTED × √ READ COMMITTED √ √ REPEATABLE READ × √ (默认) SERIALIZABLE √ √ Spring 中，可以通过指定 @Transactional 注解的 isolation 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 isolation 属性值，设置事务隔离级别： 1@Transactional(isolation = Isolation.REPEATABLE_READ) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; isolation=&quot;REPEATABLE_READ&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; timeout：事务超时时间。 事务需要在一定时间内进行提交，如果不提交则进行回滚。 默认值是 -1，设置时间以秒为单位。 Spring 中，可以通过指定 @Transactional 注解的 timeout 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 timeout 属性值，设置事务超时时间： 1@Transactional(timeout = 20) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; timeout=&quot;20&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; readOnly：事务是否只读。 读：查询操作，写：添加、修改、删除操作。 由于事务可以在行和表上获得锁，因此长事务会占用资源，并对整体性能产生影响。如果一个事物只读取数据但不做修改，数据库引擎可以对这个事务进行优化。 readOnly 默认值为 false，表示可以查询，也可以添加、修改和删除。 若设置 readOnly 值是 true，表示这个事务只读取数据但不更新数据, 这样可以帮助数据库引擎优化事务。 Spring 中，可以通过指定 @Transactional 注解的 readOnly 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 read-only 属性值，设置事务超时时间： 1@Transactional(readOnly = true) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; read-only=&quot;true&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; rollbackFor：事务回滚触发条件。 设置出现哪些异常时，必须进行事务回滚，可以为多个。 Spring 中，可以通过指定 @Transactional 注解的 rollbackFor 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 rollback-for 属性值，设置事务超时时间： 1@Transactional(rollbackFor = &#123;IOException.class, SQLException.class&#125;) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; rollback-for=&quot;java.io.IOException, java.sql.SQLException&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; noRollbackFor：事务不回滚触发条件。 设置出现哪些异常时，不进行事务回滚，可以为多个。 Spring 中，可以通过指定 @Transactional 注解的 noRollbackFor 属性的值，或者在 xml 文件中通过 &lt;tx:method&gt; 元素的 no-rollback-for 属性值，设置事务超时时间： 1@Transactional(noRollbackFor = &#123;ArithmeticException.class&#125;) 12345&lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;accountMoney&quot; no-rollback-for=&quot;java.lang.ArithmeticException&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; Spring 基于 xml 配置文件实现声明式事务管理 (了解)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 配置数据库连接池 --&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;prop.driverClass&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;prop.url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;prop.userName&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;prop.password&#125;&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置JdbcTemplate对象 --&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 注入数据源dataSource --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置通知 --&gt; &lt;tx:advice id=&quot;accountService&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;!-- 配置事务参数 --&gt; &lt;tx:attributes&gt; &lt;!-- 指定哪种规则的方法上面添加事务 --&gt; &lt;tx:method name=&quot;accountMoney&quot; propagation=&quot;REQUIRED&quot; no-rollback-for=&quot;java.lang.ArithmeticException&quot;/&gt; &lt;!-- 下面的配置含义是account开头的方法 --&gt; &lt;!--&lt;tx:method name=&quot;account*&quot;/&gt;--&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 配置切入点和切面 --&gt; &lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* cn.xisun.spring.service.AccountService.*(..))&quot;/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref=&quot;accountService&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;/aop:config&gt;&lt;/beans&gt; Spring 基于完全注解实现声明式事务管理： 方式一： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Configuration@ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;)@EnableTransactionManagementpublic class SpringConfig &#123; /** * 创建数据库连接池 * 从jdbc.properties配置文件中获取数据库连接信息 * * Bean注解：该注解只能写在方法上，表明使用此方法创建一个对象，并且放入Spring容器。 * name属性：给当前@Bean注解方法创建的对象指定一个名称(即bean的id)，默认bean的名称就是其方法名。 * * @return 向IOC容器注入一个name为dataSource的bean */ @Bean(name = &quot;dataSource&quot;) public DataSource createDataSource() &#123; Properties pros = new Properties(); try (InputStream resource = this.getClass().getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;)) &#123; pros.load(resource); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(pros.getProperty(&quot;prop.driverClass&quot;)); dataSource.setUrl(pros.getProperty(&quot;prop.url&quot;)); dataSource.setUsername(pros.getProperty(&quot;prop.userName&quot;)); dataSource.setPassword(pros.getProperty(&quot;prop.password&quot;)); return dataSource; &#125; /** * 创建JdbcTemplate对象 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为jdbcTemplate的bean */ @Bean(name = &quot;jdbcTemplate&quot;) public JdbcTemplate createJdbcTemplate(DataSource dataSource) &#123; JdbcTemplate jdbcTemplate = new JdbcTemplate(); jdbcTemplate.setDataSource(dataSource); return jdbcTemplate; &#125; /** * 创建事务管理器 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为dataSourceTransactionManager的bean */ @Bean(name = &quot;dataSourceTransactionManager&quot;) public DataSourceTransactionManager createDataSourceTransactionManager(DataSource dataSource) &#123; DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); dataSourceTransactionManager.setDataSource(dataSource); return dataSourceTransactionManager; &#125;&#125; @Configuration：标识这是一个配置类。 @ComponentScan(basePackages = &quot;cn.xisun.spring&quot;)：配置包扫描路径。 @EnableTransactionManagement：开启注解事务管理。 方式二： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class JdbcConfig &#123; /** * 创建数据库连接池 * 从jdbc.properties配置文件中获取数据库连接信息 * * Bean注解：该注解只能写在方法上，表明使用此方法创建一个对象，并且放入Spring容器。 * name属性：给当前@Bean注解方法创建的对象指定一个名称(即bean的id)，默认bean的名称就是其方法名。 * * @return 向IOC容器注入一个name为dataSource的bean */ @Bean(name = &quot;dataSource&quot;) public DataSource createDataSource() &#123; Properties pros = new Properties(); try (InputStream resource = this.getClass().getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;)) &#123; pros.load(resource); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(pros.getProperty(&quot;prop.driverClass&quot;)); dataSource.setUrl(pros.getProperty(&quot;prop.url&quot;)); dataSource.setUsername(pros.getProperty(&quot;prop.userName&quot;)); dataSource.setPassword(pros.getProperty(&quot;prop.password&quot;)); return dataSource; &#125; /** * 创建JdbcTemplate对象 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为jdbcTemplate的bean */ @Bean(name = &quot;jdbcTemplate&quot;) public JdbcTemplate createJdbcTemplate(DataSource dataSource) &#123; JdbcTemplate jdbcTemplate = new JdbcTemplate(); jdbcTemplate.setDataSource(dataSource); return jdbcTemplate; &#125; /** * 创建事务管理器 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为dataSourceTransactionManager的bean */ @Bean(name = &quot;dataSourceTransactionManager&quot;) public DataSourceTransactionManager createDataSourceTransactionManager(DataSource dataSource) &#123; DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); dataSourceTransactionManager.setDataSource(dataSource); return dataSourceTransactionManager; &#125;&#125; 1234567@Configuration @ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;) @EnableTransactionManagement @Import(JdbcConfig.class) public class SpringConfig &#123; &#125; @Import(JdbcConfig.class)：引入 JdbcConfig.class 配置文件。 方式三： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Configuration @ComponentScan(basePackages = &#123;&quot;cn.xisun.spring&quot;&#125;) @EnableTransactionManagement @PropertySource(value = &quot;classpath:jdbc.properties&quot;) public class SpringConfig &#123; @Value(&quot;$&#123;prop.driverClass&#125;&quot;) private String driverClass; @Value(&quot;$&#123;prop.url&#125;&quot;) private String url; @Value(&quot;$&#123;prop.userName&#125;&quot;) private String userName; @Value(&quot;$&#123;prop.password&#125;&quot;) private String password; /** * 创建数据库连接池 * 从jdbc.properties配置文件中获取数据库连接信息 * * Bean注解：该注解只能写在方法上，表明使用此方法创建一个对象，并且放入Spring容器。 * name属性：给当前@Bean注解方法创建的对象指定一个名称(即bean的id)，默认bean的名称就是其方法名。 * * @return 向IOC容器注入一个name为dataSource的bean */ @Bean(name = &quot;dataSource&quot;) public DataSource createDataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(driverClass); dataSource.setUrl(url); dataSource.setUsername(userName); dataSource.setPassword(password); return dataSource; &#125; /** * 创建JdbcTemplate对象 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为jdbcTemplate的bean */ @Bean(name = &quot;jdbcTemplate&quot;) public JdbcTemplate createJdbcTemplate(DataSource dataSource) &#123; JdbcTemplate jdbcTemplate = new JdbcTemplate(); jdbcTemplate.setDataSource(dataSource); return jdbcTemplate; &#125; /** * 创建事务管理器 * * @param dataSource 根据类型匹配从IOC容器中找到DataSource的对象，也就是createDataSource()返回的对象 * @return 向IOC容器注入一个name为dataSourceTransactionManager的bean */ @Bean(name = &quot;dataSourceTransactionManager&quot;) public DataSourceTransactionManager createDataSourceTransactionManager(DataSource dataSource) &#123; DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); dataSourceTransactionManager.setDataSource(dataSource); return dataSourceTransactionManager; &#125; &#125; @PropertySource(value = &quot;classpath:jdbc.properties&quot;)：标识 properties 配置文件的路径。 @Value：给当前属性赋值，取值来源于读取的 jdbc.properties 配置文件中的内容。 测试方法： 12345678910111213public class SpringTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;Spring 测试版本：&quot; + SpringVersion.getVersion()); ApplicationContext context = new AnnotationConfigApplicationContext(SpringConfig.class); AccountService accountService = context.getBean(&quot;accountService&quot;, AccountService.class); // 测试方法一 accountService.accountMoney(); // 测试方法二 accountService.transfer(&quot;lucy&quot;, &quot;mary&quot;, 100); &#125; &#125; Spring5 框架部分新功能 整个 Spring5 框架的代码基于 JDK 8，运行时兼容 JDK 9，许多不建议使用的类和方法在代码库中被删除。 Spring5 框架自带了通用的日志封装。 Spring5 已经移除 Log4jConfigListener，官方建议使用 Log4j2。 Spring5 框架整合 Log4j2： 第一步：引入 jar 包。 第二步：创建 log4j2.xml 配置文件，名称只能是这个。 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!-- 日志级别以及优先级排序: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL --&gt;&lt;!-- Configuration后面的status用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，可以看到log4j2内部各种详细输出 --&gt;&lt;configuration status=&quot;INFO&quot;&gt; &lt;!-- 先定义所有的appender --&gt; &lt;appenders&gt; &lt;!-- 输出日志信息到控制台 --&gt; &lt;console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!-- 控制日志输出的格式 --&gt; &lt;PatternLayout pattern=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n&quot;/&gt; &lt;/console&gt; &lt;/appenders&gt; &lt;!-- 然后定义logger，只有定义了logger并引入的appender，appender才会生效 --&gt; &lt;!-- root：用于指定项目的根日志，如果没有单独指定Logger，则会使用root作为默认的日志输出 --&gt; &lt;loggers&gt; &lt;root level=&quot;info&quot;&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;/loggers&gt;&lt;/configuration&gt; Spring5 框架核心容器支持 @Nullable 注解。 @Nullable 注解可以使用在方法上面，属性上面，参数上面，表示方法返回值可以为空，属性值可以为空，参数值可以为空。 123// 使用在方法上面，表示方法返回值可以为空@NullableString getId(); 123// 使用在属性上面，表示属性值可以为空@Nullableprivate String bookName; 1234// 使用在参数上面，表示参数值可以为空public &lt;T&gt; void registerBean(@Nullable String beanName, Class&lt;T&gt; beanClass, @Nullable Supplier&lt;T&gt; supplier, BeanDefinitionCustomizer... customizers) &#123; this.reader.registerBean(beanClass, beanName, supplier, customizers);&#125; Spring5 核心容器支持函数式风格 GenericApplicationContext。 1234567891011121314public class SpringTest &#123; public static void main(String[] args) &#123; // 1.创建GenericApplicationContext对象 GenericApplicationContext context = new GenericApplicationContext(); // 2.调用context的方法注册对象 context.refresh();// 清空context中的内容 // context.registerBean(Account.class, Account::new);// 方式一：注册bean context.registerBean(&quot;account&quot;, Account.class, Account::new);// 方式二：注册bean // 3.获取在Spring中注册的对象 // Account account = (Account) context.getBean(&quot;cn.xisun.spring.entity.Account&quot;);// 方式一：获取bean Account account = (Account) context.getBean(&quot;account&quot;);// 方式二：获取bean System.out.println(account); &#125;&#125; Spring5 支持整合 JUnit5。 Spring5 整合 JUnit4。 第一步：引入 Spring 相关针对测试依赖。 第二步：创建测试类，使用注解方式完成。 1234567891011@RunWith(SpringJUnit4ClassRunner. class) //单元测试框架@ContextConfiguration( &quot;classpath:bean1.xml&quot;) //加载配置文件public class JTest4 &#123; @Autowired private UserService userService; @Test public void test1() &#123; userService.accountMoney(); &#125;&#125; Spring5 整合 JUnit5。 第一步：引入 JUnit5 的依赖。 12345678910111213&lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt; &lt;version&gt;5.6.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.2.7.RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 第二步：创建测试类，使用注解 @ExtendWith 和 @ContextConfiguration 完成。 1234567891011121314151617181920212223242526package cn.xisun.spring.entity;import cn.xisun.spring.service.AccountService;import org.junit.jupiter.api.Test;import org.junit.jupiter.api.extension.ExtendWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit.jupiter.SpringExtension;import static org.junit.jupiter.api.Assertions.*;/** * @author XiSun * @Date 2021/4/23 14:03 */@ExtendWith(SpringExtension.class)@ContextConfiguration(&quot;classpath:spring.xml&quot;)class AccountTest &#123; @Autowired private AccountService accountService; @Test void test() &#123; accountService.transfer(&quot;Tom&quot;, &quot;Jerry&quot;, 100); &#125;&#125; 第三步：使用一个复合注解 @SpringJUnitConfig 替代上面两个注解完成整合。 1234567891011121314151617181920212223package cn.xisun.spring.entity;import cn.xisun.spring.service.AccountService;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.junit.jupiter.SpringJUnitConfig;import static org.junit.jupiter.api.Assertions.*;/** * @author XiSun * @Date 2021/4/23 14:03 */@SpringJUnitConfig(locations = &quot;classpath:spring.xml&quot;)class AccountTest &#123; @Autowired private AccountService accountService; @Test void test() &#123; accountService.transfer(&quot;Tom&quot;, &quot;Jerry&quot;, 100); &#125;&#125; 本文参考https://www.bilibili.com/video/BV1Vf4y127N5 https://blog.csdn.net/oneby1314/article/details/114259893 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"}]},{"title":"Java 新特性","slug":"java-newfeature","date":"2021-04-09T08:16:06.000Z","updated":"2021-04-13T01:01:56.012Z","comments":true,"path":"2021/04/09/java-newfeature/","link":"","permalink":"http://example.com/2021/04/09/java-newfeature/","excerpt":"","text":"Java 8 的新特性简介 Java 8 (又称为 jdk 1.8) 是 Java 语言开发的一个主要版本。Java 8 是 oracle 公司于 2014 年 3 月发布，可以看成是自 Java 5 以来最具革命性的版本。Java 8 为 Java 语言、编译器、类库、开发工具与 JVM 带来了大量新特性。 Java 8 新特性一览： 速度更快。 代码更少 (增加了新的语法：Lambda 表达式)。 强大的 Stream API。 便于并行。 最大化减少空指针异常：Optional。 Nashorn 引擎，允许在 JVM上运行 JS 应用。 并行流和串行流： 并行流就是把一个内容分成多个数据块，并用不同的线程分别处理每个数据块的流。相比较串行的流，并行的流可以很大程度上提高程序的执行效率。 Java 8 中将并行进行了优化，我们可以很容易的对数据进行并行操作。Stream API 可以声明性地通过 parallel() 与 sequential() 在并行流与顺序流之间进行切换。 Lambda 表达式 Lambda 是一个匿名函数，我们可以把 Lambda 表达式理解为是一段可以传递的代码 (将代码像数据一样进行传递)。使用它可以写出更简洁、更灵活的代码。作为一种更紧凑的代码风格，使 Java 的语言表达能力得到了提升。 Lambda 表达式：在 Java 8 语言中引入的一种新的语法元素和操作符。这个操作符为 “-&gt;”，该操作符被称为 Lambda 操作符或箭头操作符。它将 Lambda 分为两个部分： 左侧：指定了 Lambda 表达式需要的参数列表。 右侧：指定了 Lambda 体，是抽象方法的实现逻辑，也即 Lambda 表达式要执行的功能。 语法格式： 类型推断：上述 Lambda 表达式中的参数类型都是由编译器推断得出的。Lambda 表达式中无需指定类型，程序依然可以编译，这是因为 javac 根据程序的上下文，在后台推断出了参数的类型。Lambda 表达式的类型依赖于上下文环境，是由编译器推断出来的。这就是所谓的 **”类型推断”**。 123456789101112131415161718192021222324public class LambdaTest &#123; // 语法格式三：数据类型可以省略，因为可由编译器推断得出，称为&quot;类型推断&quot; @Test public void test3() &#123; Consumer&lt;String&gt; con1 = (String s) -&gt; &#123; System.out.println(s); &#125;; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con2 = (s) -&gt; &#123; System.out.println(s); &#125;; con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125; @Test public void test4() &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();// 类型推断，ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); int[] arr = &#123;1, 2, 3&#125;;// 类型推断，int[] arr = new int[]&#123;1, 2, 3&#125;; &#125;&#125; Lambda 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143/** * Lambda表达式的使用 * * 1.举例: (o1,o2) -&gt; Integer.compare(o1,o2); * 2.格式: * -&gt;: lambda操作符或箭头操作符 * -&gt;左边：lambda形参列表(其实就是接口中的抽象方法的形参列表) * -&gt;右边：lambda体(其实就是重写的抽象方法的方法体) * * 3.Lambda表达式的使用: (分为6种情况介绍) * * 总结: * -&gt;左边: lambda形参列表的参数类型可以省略(类型推断)；如果lambda形参列表只有一个参数，其一对()也可以省略，其他情况不能省略 * -&gt;右边: lambda体应该使用一对&#123;&#125;包裹；如果lambda体只有一条执行语句(也可能是return语句)，省略这一对&#123;&#125;和return关键字 * * 4.Lambda表达式的本质: 作为函数式接口的实例 * * 5.如果一个接口中，只声明了一个抽象方法，则此接口就称为函数式接口。我们可以在一个接口上使用@FunctionalInterface注解， * 这样做可以检查它是否是一个函数式接口。 * * 6.所有以前用匿名实现类表示的现在都可以用Lambda表达式来写 */public class LambdaTest &#123; // 语法格式一：无参，无返回值 @Test public void test1() &#123; Runnable r1 = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;我爱北京天安门&quot;); &#125; &#125;; r1.run(); System.out.println(&quot;***********************&quot;); Runnable r2 = () -&gt; &#123; System.out.println(&quot;我爱北京故宫&quot;); &#125; r2.run(); &#125; // 语法格式二：Lambda需要一个参数，但是没有返回值。 @Test public void test2() &#123; Consumer&lt;String&gt; con = new Consumer&lt;String&gt;() &#123; @Override public void accept(String s) &#123; System.out.println(s); &#125; &#125;; con.accept(&quot;谎言和誓言的区别是什么？&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con1 = (String s) -&gt; &#123; System.out.println(s); &#125; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125; // 语法格式三：数据类型可以省略，因为可由编译器推断得出，称为&quot;类型推断&quot; @Test public void test3() &#123; Consumer&lt;String&gt; con1 = (String s) -&gt; &#123; System.out.println(s); &#125;; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con2 = (s) -&gt; &#123; System.out.println(s); &#125;; con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125; // 语法格式四：Lambda若只需要一个参数时，参数的小括号可以省略 @Test public void test4() &#123; Consumer&lt;String&gt; con1 = (s) -&gt; &#123; System.out.println(s); &#125;; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*******************&quot;); Consumer&lt;String&gt; con2 = s -&gt; &#123; System.out.println(s); &#125;; con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125; // 语法格式五：Lambda需要两个或以上的参数，多条执行语句，并且可以有返回值 @Test public void test5() &#123; Comparator&lt;Integer&gt; com1 = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; System.out.println(o1); System.out.println(o2); return o1.compareTo(o2); &#125; &#125;; System.out.println(com1.compare(12, 21)); System.out.println(&quot;*****************************&quot;); Comparator&lt;Integer&gt; com2 = (o1, o2) -&gt; &#123; System.out.println(o1); System.out.println(o2); return o1.compareTo(o2); &#125;; System.out.println(com2.compare(12, 6)); &#125; // 语法格式六：当Lambda体只有一条语句时，return与大括号若有，都可以省略 @Test public void test6() &#123; Comparator&lt;Integer&gt; com1 = (o1, o2) -&gt; &#123; return o1.compareTo(o2); &#125;; System.out.println(com1.compare(12, 6)); System.out.println(&quot;*****************************&quot;); Comparator&lt;Integer&gt; com2 = (o1, o2) -&gt; o1.compareTo(o2); System.out.println(com2.compare(12, 21)); &#125; @Test public void test7() &#123; Consumer&lt;String&gt; con1 = s -&gt; &#123; System.out.println(s); &#125;; con1.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); System.out.println(&quot;*****************************&quot;); Consumer&lt;String&gt; con2 = s -&gt; System.out.println(s); con2.accept(&quot;一个是听得人当真了，一个是说的人当真了&quot;); &#125;&#125; 函数式 (Functional) 接口 什么是函数式 (Functional) 接口： 只包含一个抽象方法的接口，称为函数式接口。 你可以通过 Lambda 表达式来创建该接口的对象。(若 Lambda 表达式抛出一个受检异常 (即：非运行时异常)，那么该异常需要在目标接口的抽象方法上进行声明。) 我们可以在一个接口上使用 @FunctionalInterface 注解，这样做可以检查它是否是一个函数式接口。同时 javadoc 也会包含一条声明，说明这个接口是一个函数式接口。 在 java.util.function 包下定义了 Java 8 的丰富的函数式接口。 如何理解函数式接口： Java 从诞生日起就是一直倡导 “一切皆对象”，在 Java 里面面向对象 (OOP) 编程是一切。但是随着 python、scala 等语言的兴起和新技术的挑战，Java 不得不做出调整以便支持更加广泛的技术要求，也即 Java 不但可以支持 OOP 还可以支持 OOF (面向函数编程)。 在函数式编程语言当中，函数被当做一等公民对待。在将函数作为一等公民的编程语言中，Lambda 表达式的类型是函数。但是在 Java 8 中，有所不同。在 Java 8 中，Lambda 表达式是对象，而不是函数，它们必须依附于一类特别的对象类型——函数式接口。 简单的说，在 Java 8 中，Lambda 表达式就是一个函数式接口的实例。这就是 Lambda 表达式和函数式接口的关系。也就是说，只要一个对象是函数式接口的实例，那么该对象就可以用 Lambda 表达式来表示。 所有以前用匿名实现类表示的现在都可以用 Lambda 表达式来写。 函数式接口举例： 自定义函数式接口： 函数式接口中不使用泛型： 函数式接口中使用泛型： 作为参数传递 Lambda 表达式： Java 内置四大核心函数式接口： 其他接口： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * java内置的4大核心函数式接口： * * 消费型接口 Consumer&lt;T&gt; void accept(T t) * 供给型接口 Supplier&lt;T&gt; T get() * 函数型接口 Function&lt;T,R&gt; R apply(T t) * 断定型接口 Predicate&lt;T&gt; boolean test(T t) */public class LambdaTest &#123; // 作为参数传递Lambda表达式 // happyTime()：将参数1传给函数式接口con，Consumer函数式接口包含唯一方法accept() public void happyTime(double money, Consumer&lt;Double&gt; con) &#123; con.accept(money); &#125; @Test public void test1() &#123; happyTime(500, new Consumer&lt;Double&gt;() &#123; @Override public void accept(Double aDouble) &#123;// 重写accept() System.out.println(&quot;学习太累了，去天上人间买了瓶矿泉水，价格为：&quot; + aDouble); &#125; &#125;); System.out.println(&quot;********************&quot;); happyTime(400, money -&gt; System.out.println(&quot;学习太累了，去天上人间喝了口水，价格为：&quot; + money)); &#125; // filterString()：根据给定的规则，过滤集合中的字符串。此规则由Predicate的方法决定 // Predicate函数式接口包含唯一方法test() public List&lt;String&gt; filterString(List&lt;String&gt; list, Predicate&lt;String&gt; pre) &#123; ArrayList&lt;String&gt; filterList = new ArrayList&lt;&gt;(); // 过滤list中的每一个元素，通过Predicate实例test()验证的，添加到filterList中并返回 for (String s : list) &#123; if (pre.test(s)) &#123; filterList.add(s); &#125; &#125; return filterList; &#125; @Test public void test2() &#123; List&lt;String&gt; list = Arrays.asList(&quot;北京&quot;, &quot;南京&quot;, &quot;天津&quot;, &quot;东京&quot;, &quot;西京&quot;, &quot;普京&quot;); List&lt;String&gt; filterStrs = filterString(list, new Predicate&lt;String&gt;() &#123; @Override public boolean test(String s) &#123;// 重写test() return s.contains(&quot;京&quot;); &#125; &#125;); System.out.println(filterStrs); System.out.println(&quot;********************&quot;); List&lt;String&gt; filterStrs1 = filterString(list, s -&gt; s.contains(&quot;京&quot;)); System.out.println(filterStrs1); &#125;&#125; 方法引用与构造器引用 方法引用 (Method References)： 当要传递给 Lambda 体的操作，已经有实现的方法了，可以使用方法引用！ 方法引用可以看做是 Lambda 表达式深层次的表达。换句话说，方法引用就是 Lambda 表达式，也就是函数式接口的一个实例，通过方法的名字来指向一个方法，可以认为是 Lambda 表达式的一个语法糖。 要求：实现接口的抽象方法的参数列表和返回值类型，必须与方法引用的方法的参数列表和返回值类型保持一致！(针对情况一和情况二) ClassName :: methodName：当函数式接口方法的第一个参数是方法引用的方法的调用者，并且第二个参数是方法引用的方法的参数 (或无参数/返回值类型) 时使用。(针对情况三) 格式：使用操作符 “::” 将类 (或对象)与方法名分隔开来。 方法引用有如下三种主要使用情况： 对象 :: 实例方法名 类 :: 静态方法名 类 :: 实例方法 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class Employee &#123; private int id; private String name; private int age; private double salary; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public double getSalary() &#123; return salary; &#125; public void setSalary(double salary) &#123; this.salary = salary; &#125; public Employee() &#123; System.out.println(&quot;Employee().....&quot;); &#125; public Employee(int id) &#123; this.id = id; System.out.println(&quot;Employee(int id).....&quot;); &#125; public Employee(int id, String name) &#123; this.id = id; this.name = name; &#125; public Employee(int id, String name, int age, double salary) &#123; this.id = id; this.name = name; this.age = age; this.salary = salary; &#125; @Override public String toString() &#123; return &quot;Employee&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, salary=&quot; + salary + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Employee employee = (Employee) o; if (id != employee.id) return false; if (age != employee.age) return false; if (Double.compare(employee.salary, salary) != 0) return false; return name != null ? name.equals(employee.name) : employee.name == null; &#125; @Override public int hashCode() &#123; int result; long temp; result = id; result = 31 * result + (name != null ? name.hashCode() : 0); result = 31 * result + age; temp = Double.doubleToLongBits(salary); result = 31 * result + (int) (temp ^ (temp &gt;&gt;&gt; 32)); return result; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155/** * 方法引用的使用 * * 1.使用情境：当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用！ * * 2.方法引用，本质上就是Lambda表达式，而Lambda表达式作为函数式接口的实例。所以 * 方法引用，也是函数式接口的实例。 * * 3. 使用格式： 类(或对象) :: 方法名 * * 4. 具体分为如下的三种情况： * 情况1 对象 :: 非静态方法 * 情况2 类 :: 静态方法 * * 情况3 类 :: 非静态方法 * * 5. 方法引用使用的要求：要求接口中的抽象方法的形参列表和返回值类型与方法引用的方法的 * 形参列表和返回值类型相同！（针对于情况1和情况2） */public class MethodRefTest &#123; // 情况一：对象 :: 实例方法 // Consumer中的void accept(T t) // PrintStream中的void println(T t) @Test public void test1() &#123; // System.out.println(str)这个方法体，在PrintStream中已经存在实现的方法 Consumer&lt;String&gt; con1 = str -&gt; System.out.println(str); con1.accept(&quot;北京&quot;); System.out.println(&quot;*******************&quot;); PrintStream ps = System.out;// 利用System.out的对象，调用其println()方法 Consumer&lt;String&gt; con2 = ps::println; con2.accept(&quot;beijing&quot;); &#125; // Supplier中的T get() // Employee中的String getName() @Test public void test2() &#123; Employee emp = new Employee(1001, &quot;Tom&quot;, 23, 5600); // emp.getName()这个方法体，对应的就是emp对象的getName()方法 Supplier&lt;String&gt; sup1 = () -&gt; emp.getName(); System.out.println(sup1.get());// 返回emp对象的name System.out.println(&quot;*******************&quot;); Supplier&lt;String&gt; sup2 = emp::getName; System.out.println(sup2.get()); &#125; // 情况二：类 :: 静态方法 // Comparator中的int compare(T t1,T t2) // Integer中的int compare(T t1,T t2) @Test public void test3() &#123; Comparator&lt;Integer&gt; com1 = (t1, t2) -&gt; Integer.compare(t1, t2); System.out.println(com1.compare(12, 21)); System.out.println(&quot;*******************&quot;); Comparator&lt;Integer&gt; com2 = Integer::compare; System.out.println(com2.compare(12, 3)); &#125; // Function中的R apply(T t) // Math中的Long round(Double d) @Test public void test4() &#123; Function&lt;Double, Long&gt; func = new Function&lt;Double, Long&gt;() &#123; @Override public Long apply(Double d) &#123; return Math.round(d); &#125; &#125;; System.out.println(&quot;*******************&quot;); Function&lt;Double, Long&gt; func1 = d -&gt; Math.round(d);// lambda表达式 System.out.println(func1.apply(12.3)); System.out.println(&quot;*******************&quot;); Function&lt;Double, Long&gt; func2 = Math::round;// 方法引用 System.out.println(func2.apply(12.6)); &#125; // 情况三：类 :: 实例方法 (有难度) // Comparator中的int comapre(T t1,T t2) // String中的int t1.compareTo(t2) @Test public void test5() &#123; Comparator&lt;String&gt; com1 = (s1, s2) -&gt; s1.compareTo(s2); System.out.println(com1.compare(&quot;abc&quot;, &quot;abd&quot;)); System.out.println(&quot;*******************&quot;); Comparator&lt;String&gt; com2 = String::compareTo; System.out.println(com2.compare(&quot;abd&quot;, &quot;abm&quot;)); &#125; // BiPredicate中的boolean test(T t1, T t2); // String中的boolean t1.equals(t2) @Test public void test6() &#123; // 原始写法 BiPredicate&lt;String, String&gt; pre = new BiPredicate&lt;String, String&gt;() &#123; @Override public boolean test(String s1, String s2) &#123; return s1.equals(s2); &#125; &#125;; System.out.println(pre.test(&quot;abc&quot;, &quot;abc&quot;)); System.out.println(&quot;*******************&quot;); // lambda表达式：lambda体是参数1调用一个方法，参数2是那个方法的入参 BiPredicate&lt;String, String&gt; pre1 = (s1, s2) -&gt; s1.equals(s2); System.out.println(pre1.test(&quot;abc&quot;, &quot;abc&quot;)); System.out.println(&quot;*******************&quot;); // 方法引用：String类的equals()符合上述lambda体的功能 BiPredicate&lt;String, String&gt; pre2 = String::equals; System.out.println(pre2.test(&quot;abc&quot;, &quot;abd&quot;)); &#125; // Function中的R apply(T t) // Employee中的String getName(); @Test public void test7() &#123; Employee employee = new Employee(1001, &quot;Jerry&quot;, 23, 6000); // 原始写法：lambda体是参数1调用一个方法，返回一个参数2类型的值 Function&lt;Employee, String&gt; func = new Function&lt;Employee, String&gt;() &#123; @Override public String apply(Employee employee) &#123; return employee.getName(); &#125; &#125;; System.out.println(&quot;*******************&quot;); // lambda表达式：Employee类的getName()符合上述lambda体的功能 Function&lt;Employee, String&gt; func1 = e -&gt; e.getName(); System.out.println(func1.apply(employee)); System.out.println(&quot;*******************&quot;); // 方法引用 Function&lt;Employee, String&gt; func2 = Employee::getName; System.out.println(func2.apply(employee)); &#125;&#125; 构造器引用： 格式：ClassName :: new 与函数式接口相结合，自动与函数式接口中方法兼容。可以把构造器引用赋值给定义的方法，要求构造器参数列表要与接口中抽象方法的参数列表一致，且方法的返回值即为构造器对应类的对象。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * 一、构造器引用 * 和方法引用类似，函数式接口的抽象方法的形参列表和构造器的形参列表一致。 * 抽象方法的返回值类型即为构造器所属的类的类型 */public class ConstructorRefTest &#123; // 构造器引用 // Supplier中的T get() // Employee的空参构造器：Employee() @Test public void test1() &#123; // 原始写法 Supplier&lt;Employee&gt; sup = new Supplier&lt;Employee&gt;() &#123; @Override public Employee get() &#123; return new Employee(); &#125; &#125;; System.out.println(sup.get()); System.out.println(&quot;*******************&quot;); // Lambda表达式 Supplier&lt;Employee&gt; sup1 = () -&gt; new Employee(); System.out.println(sup1.get()); System.out.println(&quot;*******************&quot;); // 方法引用：Employee的无参构造器符合上述Lambda体 Supplier&lt;Employee&gt; sup2 = Employee::new; System.out.println(sup2.get()); &#125; // Function中的R apply(T t) @Test public void test2() &#123; // 原始写法 Function&lt;Integer, Employee&gt; func = new Function&lt;Integer, Employee&gt;() &#123; @Override public Employee apply(Integer id) &#123; return new Employee(id); &#125; &#125;; Employee employee = func.apply(1000); System.out.println(employee); System.out.println(&quot;*******************&quot;); // Lambda表达式 Function&lt;Integer, Employee&gt; func1 = id -&gt; new Employee(id); Employee employee1 = func1.apply(1001); System.out.println(employee1); System.out.println(&quot;*******************&quot;); // 方法引用：Employee的带id的有参构造器符合上述Lambda体 Function&lt;Integer, Employee&gt; func2 = Employee::new; Employee employee2 = func2.apply(1002); System.out.println(employee2); &#125; // BiFunction中的R apply(T t,U u) @Test public void test3() &#123; // 原始写法 BiFunction&lt;Integer, String, Employee&gt; func = new BiFunction&lt;Integer, String, Employee&gt;() &#123; @Override public Employee apply(Integer id, String name) &#123; return new Employee(id, name); &#125; &#125;; System.out.println(func.apply(1000, &quot;Tom&quot;)); System.out.println(&quot;*******************&quot;); // Lambda表达式 BiFunction&lt;Integer, String, Employee&gt; func1 = (id, name) -&gt; new Employee(id, name); System.out.println(func1.apply(1001, &quot;Tom&quot;)); System.out.println(&quot;*******************&quot;); // 方法引用：Employee的带id和name的有参构造器符合上述Lambda体 BiFunction&lt;Integer, String, Employee&gt; func2 = Employee::new; System.out.println(func2.apply(1002, &quot;Tom&quot;)); &#125;&#125; 数组引用： 格式：type[] :: new 可以把数组看做是一个特殊的类，则写法与构造器引用一致。 实例： 12345678910111213141516171819202122232425262728293031323334/** * 二、数组引用 * 大家可以把数组看做是一个特殊的类，则写法与构造器引用一致。 */public class ConstructorRefTest &#123; // 数组引用 // Function中的R apply(T t) @Test public void test4() &#123; // 原始写法 Function&lt;Integer, String[]&gt; func = new Function&lt;Integer, String[]&gt;() &#123; @Override public String[] apply(Integer length) &#123; return new String[length]; &#125; &#125;; String[] arr = func.apply(1); System.out.println(Arrays.toString(arr)); System.out.println(&quot;*******************&quot;); // Lambda表达式 Function&lt;Integer, String[]&gt; func1 = length -&gt; new String[length]; String[] arr1 = func1.apply(5); System.out.println(Arrays.toString(arr1)); System.out.println(&quot;*******************&quot;); // 方法引用 Function&lt;Integer, String[]&gt; func2 = String[]::new; String[] arr2 = func2.apply(10); System.out.println(Arrays.toString(arr2)); &#125;&#125; 强大的 Stream API Java 8 中有两大最为重要的改变。第一个是 Lambda 表达式；另外一个则是 Stream API。 Stream API (java.util.stream) 把真正的函数式编程风格引入到 Java 中。这是目前为止对 Java 类库最好的补充，因为 Stream API 可以极大提供 Java 程序员的生产力，让程序员写出高效率、干净、简洁的代码。 Stream 是 Java 8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。 使用 Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简言之，Stream API 提供了一种高效且易于使用的处理数据的方式。 为什么要使用 Stream API： 实际开发中，项目中多数数据源都来自于 Mysql，Oracle 等。但现在数据源可以更多了，有 MongDB，Redis 等，而这些 NoSQL 的数据就需要 Java 层面去处理。 Stream 和 Collection 集合的区别：Collection 是一种静态的内存数据结构，而 Stream 是有关计算的。前者是主要面向内存，存储在内存中，后者主要是面向 CPU，通过 CPU 实现计算。 Stream 就是一个数据渠道，用于操作数据源 (集合、数组等) 所生成的元素序列。”集合讲的是数据，Stream 讲的是计算！” Stream 的特性： Stream 自己不会存储元素。 Stream 不会改变源对象。相反，他们会返回一个持有结果的新 Stream。 Stream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行。 Stream 操作的三个步骤： 1 - 创建 Stream 一个数据源 (如：集合、数组)，获取一个流。 2 - 中间操作 一个中间操作链，对数据源的数据进行处理。 3 - 终止操作 (终端操作) 一旦执行终止操作，就执行中间操作链，并产生结果。之后，不会再被使用。 步骤一：Stream 的四种创建方式。 方式一：通过集合 Java 8 中的 Collection 接口被扩展，提供了两个获取流的方法： **default Stream&lt;E&gt; stream()**：返回一个顺序流。 **default Stream&lt;E&gt; parallelStream()**：返回一个并行流。 方式二：通过数组 Java 8 中的 Arrays 类的静态方法 stream() 可以获取数组流： **static &lt;T&gt; Stream&lt;T&gt; stream(T[] array)**：返回一个特殊对象数组的流。 重载形式，能够处理对应基本类型的数组： public static IntStream stream(int[] array)：返回一个 int 数组的流。 public static LongStream stream(long[] array)：返回一个 long 数组的流。 public static DoubleStream stream(double[] array)：返回一个 double 数组的流。 方式三：通过 Stream 类的 of() 可以调用 Stream 类静态方法 of()，通过显示值创建一个流。它可以接收任意数量的参数。 **public static&lt;T&gt; Stream&lt;T&gt; of(T... values)**：返回一个流。 方式四：创建无限流 可以使用静态方法 Stream.iterate() 和 Stream.generate() 这两种方式，创建无限流。 迭代：public static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f) 生成：public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) 实例： 12345678910111213141516/** * 提供用于测试的数据 */public class EmployeeData &#123; public static List&lt;Employee&gt; getEmployees() &#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); list.add(new Employee(1001, &quot;马1&quot;, 34, 6000.38)); list.add(new Employee(1002, &quot;马2&quot;, 12, 9876.12)); list.add(new Employee(1003, &quot;刘&quot;, 33, 3000.82)); list.add(new Employee(1004, &quot;雷&quot;, 26, 7657.37)); list.add(new Employee(1005, &quot;李&quot;, 65, 5555.32)); list.add(new Employee(1006, &quot;比&quot;, 42, 9500.43)); list.add(new Employee(1007, &quot;任&quot;, 26, 4333.32)); return list; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 1. Stream关注的是对数据的运算，与CPU打交道 * 集合关注的是数据的存储，与内存打交道 * * 2. * ① Stream 自己不会存储元素。 * ② Stream 不会改变源对象。相反，他们会返回一个持有结果的新Stream。 * ③ Stream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行 * * 3.Stream 执行流程 * ① Stream的实例化 * ② 一系列的中间操作(过滤、映射、...) * ③ 终止操作 * * 4.说明： * 4.1 一个中间操作链，对数据源的数据进行处理 * 4.2 一旦执行终止操作，就执行中间操作链，并产生结果。之后，不会再被使用 * * 测试Stream的实例化 */public class StreamAPITest &#123; // 创建Stream方式一：通过集合 @Test public void test1() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // 方法一： // default Stream&lt;E&gt; stream() : 返回一个顺序流 Stream&lt;Employee&gt; stream = employees.stream(); // 方法二： // default Stream&lt;E&gt; parallelStream() : 返回一个并行流 Stream&lt;Employee&gt; parallelStream = employees.parallelStream(); &#125; // 创建Stream方式二：通过数组 @Test public void test2() &#123; int[] arr = new int[]&#123;1, 2, 3, 4, 5, 6&#125;; // 调用Arrays类的static &lt;T&gt; Stream&lt;T&gt; stream(T[] array): 返回一个流 IntStream stream = Arrays.stream(arr); Employee e1 = new Employee(1001, &quot;Tom&quot;); Employee e2 = new Employee(1002, &quot;Jerry&quot;); Employee[] arr1 = new Employee[]&#123;e1, e2&#125;; Stream&lt;Employee&gt; stream1 = Arrays.stream(arr1); &#125; // 创建Stream方式三：通过Stream的of() @Test public void test3() &#123; Stream&lt;Integer&gt; stream = Stream.of(1, 2, 3, 4, 5, 6); Stream&lt;String&gt; stringStream = Stream.of(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;); &#125; // 创建Stream方式四：创建无限流 --- 用的比较少 @Test public void test4() &#123; // 迭代 // public static&lt;T &gt; Stream &lt; T &gt; iterate( final T seed, final UnaryOperator&lt;T&gt; f) // 遍历前10个偶数 Stream.iterate(0, t -&gt; t + 2).limit(10).forEach(System.out::println);// 从0开始，后一个数是前一个数+2 // 生成 // public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) // 遍历前10个随机数 Stream.generate(Math::random).limit(10).forEach(System.out::println); &#125;&#125; 步骤二：Stream 的中间操作。 多个中间操作可以连接起来形成一个流水线，除非流水线上触发终止操作，否则中间操作不会执行任何的处理！而在终止操作时一次性全部处理，这称为 “惰性求值”。 操作 1 - 筛选与切片： 操作 2 - 映射： 操作 3 - 排序： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126/** * 测试Stream的中间操作 */public class StreamAPITest &#123; // 1-筛选与切片 @Test public void test1() &#123; List&lt;Employee&gt; list = EmployeeData.getEmployees(); // filter(Predicate p) --- 接收Lambda，从流中排除某些元素。 // 练习：查询员工表中薪资大于7000的员工信息 list.stream().filter(e -&gt; e.getSalary() &gt; 7000).forEach(System.out::println); System.out.println(&quot;************************&quot;); // limit(n) --- 截断流，使其元素不超过给定数量n。 // 练习：打印员工表中前三名的员工信息 list.stream().limit(3).forEach(System.out::println);// 前一个流已经关闭，必须重新建一个流 System.out.println(&quot;************************&quot;); // skip(n) --- 跳过元素，返回一个扔掉了前n个元素的流。若流中元素不足n个，则返回一个空流。与limit(n)互补。 // 练习：跳过员工表中前三名的员工信息，然后打印之后的每个员工的信息 list.stream().skip(3).forEach(System.out::println); System.out.println(&quot;************************&quot;); // distinct() --- 筛选，通过流所生成元素的hashCode()和equals()去除重复元素 list.add(new Employee(1010, &quot;刘强东&quot;, 40, 8000)); list.add(new Employee(1010, &quot;刘强东&quot;, 41, 8000)); list.add(new Employee(1010, &quot;刘强东&quot;, 40, 8000)); list.add(new Employee(1010, &quot;刘强东&quot;, 40, 8000)); list.add(new Employee(1010, &quot;刘强东&quot;, 40, 8000)); // System.out.println(list); list.stream().distinct().forEach(System.out::println); &#125; // 2-映射 @Test public void test2() &#123; // map(Function f) --- 接收一个函数作为参数，将元素转换成其他形式或提取信息， // 该函数会被应用到每个元素上，并将其映射成一个新的元素。 // ---&gt; 类似于List的add()：如果流的每个值转换成新流，则将每个新流作为一个元素组成新的流 // 即类似：[1, [1, 2], 5, [1, 3, 2, 5], 9] // 练习1：将list中的每一个元素变成大写并打印 List&lt;String&gt; list = Arrays.asList(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;); // list.stream().map(str -&gt; str.toUpperCase()).forEach(System.out::println); list.stream().map(String::toUpperCase).forEach(System.out::println); System.out.println(); // 练习2：获取员工姓名长度大于3的员工的姓名。 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;String&gt; namesStream = employees.stream().map(Employee::getName); namesStream.filter(name -&gt; name.length() &gt; 3).forEach(System.out::println); System.out.println(); // 练习3： Stream&lt;Stream&lt;Character&gt;&gt; streamStream = list.stream().map(StreamAPITest::fromStringToStream); // streamStream.forEach(System.out::println); // 体会下下面的写法与上面写法的区别 streamStream.forEach(s -&gt; &#123; s.forEach(System.out::println); &#125;); System.out.println(&quot;************************&quot;); // flatMap(Function f) --- 接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。 // ---&gt; 似于List的addAll()：如果流的每个值转换成新流，则将每个新流的值组合连接成一个流 // 即类似：[1, 1, 2, 5, 1, 3, 2, 5, 9] Stream&lt;Character&gt; characterStream = list.stream().flatMap(StreamAPITest::fromStringToStream); characterStream.forEach(System.out::println); &#125; // 将字符串中的多个字符构成的集合转换为对应的Stream的实例 public static Stream&lt;Character&gt; fromStringToStream(String str) &#123;// 如：aa---&gt;返回两个字符a组成的集合对应的流 ArrayList&lt;Character&gt; list = new ArrayList&lt;&gt;(); for (Character c : str.toCharArray()) &#123; list.add(c); &#125; return list.stream(); &#125; // 对比map()和flatmap()的区别 @Test public void test3() &#123; ArrayList list1 = new ArrayList(); list1.add(1); list1.add(2); list1.add(3); ArrayList list2 = new ArrayList(); list2.add(4); list2.add(5); list2.add(6); list1.add(list2);// [1, 2, 3, [4, 5, 6]] list1.addAll(list2);// [1, 2, 3, 4, 5, 6] System.out.println(list1); &#125; // 3-排序 @Test public void test4() &#123; // sorted() --- 自然排序 List&lt;Integer&gt; list = Arrays.asList(12, 43, 65, 34, 87, 0, -98, 7); list.stream().sorted().forEach(System.out::println); // 抛异常，原因: Employee没有实现Comparable接口 // List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // employees.stream().sorted().forEach(System.out::println); // sorted(Comparator com) --- 定制排序 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); employees.stream().sorted((e1, e2) -&gt; &#123; int ageValue = Integer.compare(e1.getAge(), e2.getAge());// 先按年龄 if (ageValue != 0) &#123; return ageValue; &#125; else &#123; return -Double.compare(e1.getSalary(), e2.getSalary());// 再按薪水 &#125; &#125;).forEach(System.out::println); &#125;&#125; 步骤三：Stream 的终止操作。 终端操作会从流的流水线生成结果。其结果可以是任何不是流的值，例如：List、Integer，甚至是 void。 流进行了终止操作后，不能再次使用。 操作 1 - 匹配与查找： 操作 2 - 归约： map 和 reduce 的连接通常称为 map-reduce 模式，因 Google 用它来进行网络搜索而出名。 map 是一对一映射，由 n 到 n；reduce 是多对一归约，由 n 到 1。 操作 3 - 收集： Collector 接口中方法的实现决定了如何对流执行收集的操作，如收集到 List、Set、Map 等。 Collectors 实用类提供了很多静态方法，可以方便地创建常见收集器实例 (Collector 实例)，具体方法与实例如下表： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 测试Stream的终止操作 */public class StreamAPITest &#123; // 1-匹配与查找 @Test public void test1() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // allMatch(Predicate p) --- 检查是否匹配所有元素。 // 练习：是否所有的员工的年龄都大于18 boolean allMatch = employees.stream().allMatch(e -&gt; e.getAge() &gt; 18); System.out.println(allMatch); // anyMatch(Predicate p) --- 检查是否至少匹配一个元素。 // 练习：是否存在员工的工资大于10000 boolean anyMatch = employees.stream().anyMatch(e -&gt; e.getSalary() &gt; 10000); System.out.println(anyMatch); // noneMatch(Predicate p) ---- 检查是否没有匹配的元素。如果有，返回false // 练习：是否存在员工姓&quot;雷&quot; boolean noneMatch = employees.stream().noneMatch(e -&gt; e.getName().startsWith(&quot;雷&quot;)); System.out.println(noneMatch); // findFirst() --- 返回第一个元素 Optional&lt;Employee&gt; employee = employees.stream().findFirst(); System.out.println(employee); // findAny() --- 返回当前流中的任意元素 Optional&lt;Employee&gt; employee1 = employees.parallelStream().findAny(); System.out.println(employee1); &#125; @Test public void test2() &#123; List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // count --- 返回流中元素的总个数 // 练习：返回工资高于5000的员工个数 long count = employees.stream().filter(e -&gt; e.getSalary() &gt; 5000).count(); System.out.println(count); // max(Comparator c) --- 返回流中最大值 // 练习：返回最高的工资 Stream&lt;Double&gt; salaryStream = employees.stream().map(Employee::getSalary); Optional&lt;Double&gt; maxSalary = salaryStream.max(Double::compare); System.out.println(maxSalary); // min(Comparator c) --- 返回流中最小值 // 练习：返回最低工资的员工 Optional&lt;Employee&gt; employee = employees.stream().min((e1, e2) -&gt; Double.compare(e1.getSalary(), e2.getSalary())); System.out.println(employee); System.out.println(&quot;************************&quot;); // forEach(Consumer c) --- 内部迭代 employees.stream().forEach(System.out::println); // 外部迭代 Iterator&lt;Employee&gt; iterator = employees.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; // 使用集合的遍历操作方法 employees.forEach(System.out::println); &#125; // 2-归约 @Test public void test3() &#123; // reduce(T identity, BinaryOperator) --- 可以将流中元素反复结合起来，得到一个值。返回T // 练习1：计算1-10的自然数的和 List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); Integer sum = list.stream().reduce(0, Integer::sum);// 有一个初始值，在初始值基础上操作 System.out.println(sum); // reduce(BinaryOperator) --- 可以将流中元素反复结合起来，得到一个值。返回Optional&lt;T&gt; // 练习2：计算公司所有员工工资的总和 List&lt;Employee&gt; employees = EmployeeData.getEmployees(); Stream&lt;Double&gt; salaryStream = employees.stream().map(Employee::getSalary); Optional&lt;Double&gt; sumMoney = salaryStream.reduce((d1, d2) -&gt; d1 + d2); // Optional&lt;Double&gt; sumMoney = salaryStream.reduce(Double::sum);// 方法引用 // Double sumMoney = salaryStream.reduce(0.0, Double::sum);// 也可以计算工资总和 System.out.println(sumMoney.get()); &#125; // 3-收集 @Test public void test4() &#123; // collect(Collector c) --- 将流转换为其他形式。接收一个Collector接口的实现，用于给Stream中元素做汇总的方法 // 练习：查找工资大于6000的员工，结果返回为一个List或Set List&lt;Employee&gt; employees = EmployeeData.getEmployees(); // 返回List List&lt;Employee&gt; employeeList = employees.stream().filter(e -&gt; e.getSalary() &gt; 6000).collect(Collectors.toList()); employeeList.forEach(System.out::println); System.out.println(&quot;************************&quot;); // 返回Set Set&lt;Employee&gt; employeeSet = employees.stream().filter(e -&gt; e.getSalary() &gt; 6000).collect(Collectors.toSet()); employeeSet.forEach(System.out::println); &#125;&#125; Optional 类 到目前为止，臭名昭著的空指针异常是导致 Java 应用程序失败的最常见原因。以前，为了解决空指针异常，Google 公司著名的 Guava 项目引入了 Optional 类，Guava 通过使用检查空值的方式来防止代码污染，它鼓励程序员写更干净的代码。受到 Google Guava 的启发，Optional 类已经成为 Java 8 类库的一部分。 Optional&lt;T&gt; 类 (java.util.Optional) 是一个容器类，它可以保存类型 T 的值，代表这个值存在。或者仅仅保存 null，表示这个值不存在。原来用 null 表示一个值不存在，现在 Optional 可以更好的表达这个概念。并且可以避免空指针异常。 Optional 类的 Javadoc 描述如下：这是一个可以为 null 的容器对象。如果值存在则 isPresent() 会返回 true，调用 get() 会返回该对象。 Optional 类提供了很多有用的方法，这样我们就不用显式进行空值检测。 创建 Optional 类对象的方法： **Optional.of(T t)**：创建一个 Optional 实例，t 必须非空。否则，报 NullPointerException。 123456789public class OptionalTest &#123; @Test public void test() &#123; Optional&lt;Employee&gt; opt = Optional.of(new Employee(&quot;张三&quot;, 8888)); // 判断opt中员工对象是否满足条件，如果满足就保留，否则返回空 Optional&lt;Employee&gt; emp = opt.filter(e -&gt; e.getSalary() &gt; 10000); System.out.println(emp); &#125;&#125; 12345678910111213public class OptionalTest &#123; @Test public void test() &#123; Optional&lt;Employee&gt; opt = Optional.of(new Employee(&quot;张三&quot;, 8888)); // 如果opt中员工对象不为空，就涨薪10% Optional&lt;Employee&gt; emp = opt.map(e -&gt; &#123; e.setSalary(e.getSalary() % 1.1); return e; &#125;); System.out.println(emp); &#125;&#125; Optional.empty()：创建一个空的 Optional 实例 **Optional.ofNullable(T t)**：创建一个 Optional 实例，t 可以为 null。 判断 Optional 容器中是否包含对象： **boolean isPresent()**：判断是否包含对象 void ifPresent(Consumer&lt;? super T&gt; consumer)：如果有值，就执行 Consumer 接口的实现代码，并且该值会作为参数传给它。 123456789public class OptionalTest &#123; @Test public void test() &#123; Boy b = new Boy(&quot;张三&quot;); Optional&lt;Girl&gt; opt = Optional.ofNullable(b.getGrilFriend()); // 如果女朋友存在就打印女朋友的信息 opt.ifPresent(System.out::println); &#125;&#125; 获取 Optional 容器的对象： **T get()**：如果调用对象包含值，返回该值，否则抛异常。可以对应于 Optional.of(T t) 一起使用。 **T orElse(T other)**：如果有值则将其返回，否则返回指定的 other 对象。可以对应于 Optional.ofNullable(T t) 一起使用。 12345678910public class OptionalTest &#123; @Test public void test() &#123; Boy b = new Boy(&quot;张三&quot;); Optional&lt;Girl&gt; opt = Optional.ofNullable(b.getGrilFriend()); // 如果有女朋友就返回他的女朋友，否则只能欣赏“嫦娥”了 Girl girl = opt.orElse(new Girl(&quot;嫦娥&quot;)); System.out.println(&quot;他的女朋友是：&quot; + girl.getName()); &#125;&#125; T orElseGet(Supplier&lt;? extends T&gt; other)：如果有值则将其返回，否则返回由 Supplier 接口实现提供的对象。 T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier)：如果有值则将其返回，否则抛出由 Supplier 接口实现提供的异常。 实例： 12345678910111213141516171819202122232425public class Boy &#123; private Girl girl; public Girl getGirl() &#123; return girl; &#125; public void setGirl(Girl girl) &#123; this.girl = girl; &#125; public Boy() &#123; &#125; public Boy(Girl girl) &#123; this.girl = girl; &#125; @Override public String toString() &#123; return &quot;Boy&#123;&quot; + &quot;girl=&quot; + girl + &#x27;&#125;&#x27;; &#125;&#125; 12345678910111213141516171819202122232425public class Girl &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Girl() &#123; &#125; public Girl(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;Girl&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * Optional类：为了在程序中避免出现空指针异常而创建的。 * * 常用的方法：ofNullable(T t) * orElse(T t) */public class OptionalTest &#123; /* Optional.of(T t): 创建一个Optional实例，t必须非空。否则，报NullPointerException Optional.empty(): 创建一个空的Optional实例 Optional.ofNullable(T t): t可以为null */ @Test public void test1() &#123; Girl girl = new Girl(); // girl = null; // of(T t): 保证t是非空的 Optional&lt;Girl&gt; optionalGirl = Optional.of(girl); &#125; @Test public void test2() &#123; Girl girl = new Girl(); // girl = null; // ofNullable(T t): t可以为null Optional&lt;Girl&gt; optionalGirl = Optional.ofNullable(girl); System.out.println(optionalGirl); // orElse(T t1): 如果当前的Optional内部封装的t是非空的，则返回内部的t。 // 如果内部的t是空的，则返回orElse()方法中的参数t1。 Girl girl1 = optionalGirl.orElse(new Girl(&quot;赵&quot;)); System.out.println(girl1); &#125; @Test public void test3() &#123; Boy boy = new Boy(); boy = null; String girlName = getGirlName(boy); // String girlName = getGirlName1(boy);// 不会出现NullPointerException System.out.println(girlName); &#125; @Test public void test4() &#123; Boy boy = null; boy = new Boy(); boy = new Boy(new Girl(&quot;苍&quot;)); String girlName = getGirlName2(boy); System.out.println(girlName); &#125; // 未优化代码，容易出现NullPointerException public String getGirlName(Boy boy) &#123; return boy.getGirl().getName(); &#125; // 优化以后的getGirlName(): public String getGirlName1(Boy boy) &#123; if (boy != null) &#123; Girl girl = boy.getGirl(); if (girl != null) &#123; return girl.getName(); &#125; &#125; return null; &#125; // 使用Optional类优化的getGirlName() public String getGirlName2(Boy boy) &#123; // boy可能为空 Optional&lt;Boy&gt; boyOptional = Optional.ofNullable(boy); // 此时的boy1一定非空 Boy boy1 = boyOptional.orElse(new Boy(new Girl(&quot;迪&quot;))); // girl可能为空 Girl girl = boy1.getGirl(); Optional&lt;Girl&gt; girlOptional = Optional.ofNullable(girl); // 此时的girl1一定非空 Girl girl1 = girlOptional.orElse(new Girl(&quot;古&quot;)); return girl1.getName(); &#125;&#125; Java 9 的新特性Java 10 的新特性Java 11 的新特性本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 反射机制","slug":"java-reflection","date":"2021-04-07T05:23:13.000Z","updated":"2021-04-09T08:08:10.654Z","comments":true,"path":"2021/04/07/java-reflection/","link":"","permalink":"http://example.com/2021/04/07/java-reflection/","excerpt":"","text":"Java 反射机制概述 Reflection (反射) 是被视为动态语言的关键，反射机制允许程序在执行期借助于 Reflection API 取得任何类的内部信息，并能直接操作任意对象的内部属性及方法。 动态语言：是一类在运行时可以改变其结构的语言。例如新的函数、对象、甚至代码可以被引进，已有的函数可以被删除或是其他结构上的变化。通俗点说就是在运行时代码可以根据某些条件改变自身结构。主要动态语言：Object-C、C#、JavaScript、PHP、Python、Erlang。 静态语言：与动态语言相对应的，运行时结构不可变的语言就是静态语言。如 Java、C、C++。 Java 不是动态语言，但 Java 可以称之为 “准动态语言”。即 Java 有一定的动态性，我们可以利用反射机制、字节码操作获得类似动态语言的特性。Java 的动态性让编程的时候更加灵活。 加载完类之后，在堆内存的方法区中就产生了一个 Class 类型的对象 (一个类只有一个 Class 对象)，这个对象就包含了完整的类的结构信息。我们可以通过这个对象看到类的结构。这个对象就像一面镜子，透过这个镜子看到类的结构，所以，我们形象的称之为： 反射。 Java 反射机制提供的功能： 在运行时判断任意一个对象所属的类。 在运行时构造任意一个类的对象。 在运行时判断任意一个类所具有的成员变量和方法。 在运行时获取泛型信息。 在运行时调用任意一个对象的成员变量和方法。 在运行时处理注解。 生成动态代理。 反射相关的主要 API： java.lang.Class：代表一 个类。 java.lang.reflect.Method：代表类的方法。 java.lang.reflect.Field：代表类的成员变量。 java.lang.reflect.Constructor：代表类的构造器。 理解 Class 类并获取 Class 类的实例 在 Object 类中定义了以下的方法，此方法将被所有子类继承： public final Class getClass() 以上的方法返回值的类型是一个 Class 类，此类是 Java 反射的源头，实际上所谓反射从程序的运行结果来看也很好理解，即：可以通过对象反射求出类的名称。 对象照镜子后可以得到的信息：某个类的属性、方法和构造器、某个类到底实现了哪些接口。对于每个类而言，JRE 都为其保留一个不变的 Class 类型的对象。一个 Class 对象包含了特定某个结构 (class/interface/enum/annotation/primitive type/void/[]) 的有关信息。 Class 本身也是一个类。 Class 对象只能由系统建立对象。 一个加载的类在 JVM 中只会有一个 Class 实例。 一个 Class 对象对应的是一个加载到 JVM 中的一个 .class 文件。 每个类的实例都会记得自己是由哪个 Class 实例所生成。 通过 Class 可以完整地得到一个类中的所有被加载的结构。 Class 类是 Reflection 的根源，针对任何你想动态加载、运行的类，唯有先获得相应的 Class 对象。 Class 类的常用方法： static Class forName(String name)：返回指定类名 name 的 Class 对象。 Object newInstance()：调用缺省构造函数，返回该 Class 对象的一个实例。 getName()：返回此 Class 对象所表示的实体 (类、接口、数组类、基本类型或 void) 名称。 Class getSuperClass()：返回当前 Class 对象的父类的 Class 对象。 Class [] getInterfaces()：获取当前 Class 对象的接口。 ClassLoader getClassLoader()：返回该类的类加载器。 Class getSuperclass()：返回表示此 Class 所表示的实体的超类的 Class。 Constructor[] getConstructors()：返回一个包含某些 Constructor 对象的数组。 Field[] getDeclaredFields()：返回 Field 对象的一个数组。 Method getMethod(String name,Class … paramTypes)：返回一个 Method 对象，此对象的形参类型为 paramType。 反射实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package cn.xisun.java.base.file;public class Person &#123; private String name; public int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public Person() &#123; System.out.println(&quot;Person()&quot;); &#125; private Person(String name) &#123; this.name = name; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public void show() &#123; System.out.println(&quot;你好，我是一个人&quot;); &#125; private String showNation(String nation) &#123; System.out.println(&quot;我的国籍是：&quot; + nation); return nation; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class ReflectionTest &#123; /* 反射之前，对于Person的操作 */ @Test public void test1() &#123; // 1.创建Person类的对象 Person p1 = new Person(&quot;Tom&quot;, 12); // 2.通过对象，调用其内部的属性、方法 p1.age = 10; System.out.println(p1.toString()); p1.show(); // 3.在Person类外部，不可以通过Person类的对象调用其内部私有结构。---封装性的限制 // 比如：name、showNation()以及私有的构造器 &#125; /* 反射之后，对于Person的操作 */ @Test public void test2() throws Exception &#123; Class clazz = Person.class; // 1.通过反射，创建Person类的对象 Constructor cons = clazz.getConstructor(String.class, int.class); Object obj = cons.newInstance(&quot;Tom&quot;, 12); Person p = (Person) obj; System.out.println(p.toString());// Person&#123;name=&#x27;Tom&#x27;, age=12&#125; // 2.通过反射，调用对象指定的属性、方法 // 2.1 调用属性 Field age = clazz.getDeclaredField(&quot;age&quot;); age.set(p, 10); System.out.println(p.toString());// Person&#123;name=&#x27;Tom&#x27;, age=10&#125; // 2.2 调用方法 Method show = clazz.getDeclaredMethod(&quot;show&quot;); show.invoke(p);// 你好，我是一个人 System.out.println(&quot;*******************************&quot;); // 3.通过反射，可以调用Person类的私有结构的。比如：私有的构造器、方法、属性 // 3.1 调用私有的构造器 Constructor cons1 = clazz.getDeclaredConstructor(String.class); cons1.setAccessible(true); Person p1 = (Person) cons1.newInstance(&quot;Jerry&quot;); System.out.println(p1);// Person&#123;name=&#x27;Jerry&#x27;, age=0&#125; // 3.2 调用私有的属性 Field name = clazz.getDeclaredField(&quot;name&quot;); name.setAccessible(true); name.set(p1, &quot;HanMeimei&quot;); System.out.println(p1);// Person&#123;name=&#x27;HanMeimei&#x27;, age=0&#125; // 3.3 调用私有的方法 Method showNation = clazz.getDeclaredMethod(&quot;showNation&quot;, String.class); showNation.setAccessible(true); String nation = (String) showNation.invoke(p1, &quot;中国&quot;);// 相当于String nation = p1.showNation(&quot;中国&quot;) System.out.println(nation); &#125; // 疑问1：通过直接new的方式或反射的方式都可以调用公共的结构，开发中到底用那个？ // 建议：直接new的方式。 // 什么时候会使用：反射的方式。---&gt; 根据反射的特征：动态性，进行考虑 // 疑问2：反射机制与面向对象中的封装性是不是矛盾的？如何看待两个技术？ // 不矛盾。封装性是给出了一种建议，不应该调用私有的结构，但如果有调用私有结构的需求，则可以通过反射机制做到。&#125; 获取 Class 类的实例的四种方法： 若已知具体的类，则通过类的 class 属性获取，该方法最为安全可靠，程序性能最高。比如：Class clazz = String.class;。 若已知某个类的实例，则调用该实例的 getClass() 获取 Class 对象。比如：Class clazz = &quot;Hello,World!&quot;.getClass();。 若已知一个类的全类名，且该类在类路径下，可通过 Class 类的静态方法 forName() 获取，可能抛出 ClassNotFoundException。比如：Class clazz = Class.forName(&quot;java.lang.String&quot;);。— 最常用，体现了反射的动态性 使用类的加载器 ClassLoader，比如： 12ClassLoader cl = this.getClass().getClassLoader();Class clazz4 = cl.loadClass(&quot;类的全类名&quot;); 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ReflectionTest &#123; /* 关于java.lang.Class类的理解 1.类的加载过程： 程序经过javac.exe命令以后，会生成一个或多个字节码文件(.class结尾)。 接着我们使用java.exe命令对某个字节码文件进行解释运行。相当于将某个字节码文件 加载到内存中。这个过程就称为类的加载。加载到内存中的类，我们称为运行时类，此 运行时类，就作为Class的一个实例。 (万事万物皆对象：一方面，通过对象.xxx的方式调用方法、属性等；另一方面，在反射机制中，类本身也是Class的对象) 2.换句话说，Class的实例就对应着一个运行时类。 3.加载到内存中的运行时类，会缓存一定的时间。在此时间之内，我们可以通过不同的方式 来获取此运行时类。 */ /* 获取Class的实例的方式（前三种方式需要掌握） */ @Test public void test3() throws ClassNotFoundException &#123; // 方式一：调用运行时类的属性：.class Class clazz1 = Person.class; System.out.println(clazz1);// class cn.xisun.java.base.file.Person // 方式二：通过运行时类的对象，调用getClass() Person p1 = new Person(); Class clazz2 = p1.getClass(); System.out.println(clazz2);// class cn.xisun.java.base.file.Person // 方式三：调用Class的静态方法：forName(String classPath) Class clazz3 = Class.forName(&quot;cn.xisun.java.base.file.Person&quot;);// 指明类的全类名 // clazz3 = Class.forName(&quot;java.lang.String&quot;); System.out.println(clazz3);// class cn.xisun.java.base.file.Person System.out.println(clazz1 == clazz2);// true System.out.println(clazz1 == clazz3);// true // 方式四：使用类的加载器：ClassLoader (了解) ClassLoader classLoader = ReflectionTest.class.getClassLoader(); Class clazz4 = classLoader.loadClass(&quot;cn.xisun.java.base.file.Person&quot;); System.out.println(clazz4);// class cn.xisun.java.base.file.Person System.out.println(clazz1 == clazz4);// true &#125;&#125; 哪些类型可以有 Class 对象： class：外部类，成员 (成员内部类，静态内部类)，局部内部类，匿名内部类。 interface：接口。 []：数组. enum：枚举。 annotation：注解 @interface。 primitive type：基本数据类型。 void 实例： 123456789101112131415161718192021222324public class ReflectionTest &#123; /* Class实例可以是哪些结构的说明 */ @Test public void test4() &#123; Class c1 = Object.class; Class c2 = Comparable.class; Class c3 = String[].class; Class c4 = int[][].class;// 二维数组 Class c5 = ElementType.class;// 枚举类 Class c6 = Override.class;// 注解 Class c7 = int.class; Class c8 = void.class; Class c9 = Class.class; int[] a = new int[10]; int[] b = new int[100]; Class c10 = a.getClass(); Class c11 = b.getClass(); // 只要数组的元素类型与维度一样，就是同一个Class System.out.println(c10 == c11);// true &#125;&#125; 类的加载与 ClassLoader 的理解 类的加载过程：当程序主动使用某个类时，如果该类还未被加载到内存中，则系统会通过如下三个步骤来对该类进行初始化： 加载：将 class 文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后生成一个代表这个类的 java.lang.Class 对象，作为方法区中类数据的访问入口 (即引用地址)。所有需要访问和使用类数据的地方，只能通过这个 Class 对象。这个加载的过程需要类加载器参与。 链接：将 Java 类的二进制代码合并到 JVM 的运行状态之中的过程。 验证：确保加载的类信息符合 JVM 规范，例如：以 cafe 开头，没有安全方面的问题。 准备：正式为类变量 (static) 分配内存并设置类变量默认初始值的阶段，这些内存都将在方法区中进行分配。 解析：虚拟机常量池内的符号引用 (常量名) 替换为直接引用 (地址) 的过程。 初始化： 执行类构造器 &lt;clinit&gt;() 方法的过程。类构造器 &lt;clinit&gt;() 方法是由编译期自动收集类中所有类变量的赋值动作和静态代码块中的语句合并产生的。(类构造器是构造类信息的，不是构造该类对象的构造器) 当初始化一个类的时候，如果发现其父类还没有进行初始化，则需要先触发其父类的初始化。 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境中被正确加锁和同步。 代码图示： 什么时候会发生类的初始化： 类的主动引用 (一定会发生类的初始化)： 当虚拟机启动，先初始化 main 方法所在的类。 new 一个类的对象。 调用类的静态成员 (除了 final 常量) 和静态方法。 使用 java.lang.reflect 包的方法对类进行反射调用。 当初始化一个类，如果其父类没有被初始化，则先会初始化它的父类。 类的被动引用 (不会发生类的初始化)： 当访问一个静态域时，只有真正声明这个域的类才会被初始化。 当通过子类引用父类的静态变量，不会导致子类初始化。 通过数组定义类引用，不会触发此类的初始化。 引用常量不会触发此类的初始化 (常量在链接阶段就存入调用类的常量池中了)。 实例： 123456789101112131415161718192021222324252627282930313233343536public class ClassLoadingTest &#123; public static void main(String[] args) throws ClassNotFoundException &#123; // 主动引用：一定会导致A和Father的初始化 A a = new A(); System.out.println(A.m); Class.forName(&quot;cn.xisun.java.base.file.A&quot;); // 被动引用 A[] array = new A[5];// 不会导致A和Father的初始化 System.out.println(A.b);// 只会初始化Father System.out.println(A.M);// 不会导致A和Father的初始化 &#125; static &#123; System.out.println(&quot;main所在的类&quot;); &#125;&#125;class Father &#123; static int b = 2; static &#123; System.out.println(&quot;父类被加载&quot;); &#125;&#125;class A extends Father &#123; static &#123; System.out.println(&quot;子类被加载&quot;); m = 300; &#125; static int m = 100; static final int M = 1;&#125; 类加载器的作用： 类加载的作用：将 class 文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后在堆中生成一个代表这个类的 java.lang.Class 对象，作为方法区中类数据的访问入口。 类缓存：标准的 Java SE 类加载器可以按要求查找类，但一旦某个类被加载到类加载器中，它将维持加载 (缓存) 一段时间。不过 JVM 垃圾回收机制可以回收这些 Class 对象。 JVM 规范定义了如下类型的类的加载器： 引导类加载器 扩展类加载器 系统类加载器 自定义类加载器 实例： 123456789101112131415161718192021222324252627282930313233/** * 了解类的加载器 */public class ClassLoaderTest &#123; @Test public void test1() &#123; // 对于自定义类，使用系统类加载器进行加载 ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); System.out.println(classLoader);// sun.misc.Launcher$AppClassLoader@18b4aac2 // 调用系统类加载器的getParent()：获取扩展类加载器 ClassLoader classLoader1 = classLoader.getParent(); System.out.println(classLoader1);// sun.misc.Launcher$ExtClassLoader@21588809 // 调用扩展类加载器的getParent()：无法获取引导类加载器 // 引导类加载器主要负责加载java的核心类库，无法加载自定义类的。 ClassLoader classLoader2 = classLoader1.getParent(); System.out.println(classLoader2);// null ClassLoader classLoader3 = String.class.getClassLoader(); System.out.println(classLoader3);// null，String的加载器是引导类加载器，无法获取 // 测试当前类由哪个类加载器进行加载 ClassLoader classLoader4 = Class.forName(&quot;cn.xisun.java.base.file.ClassLoaderTest&quot;).getClassLoader(); System.out.println(classLoader4);// sun.misc.Launcher$AppClassLoader@18b4aac2 // 测试JDK提供的Object类由哪个类加载器加载 ClassLoader classLoader5 = Class.forName(&quot;java.lang.Object&quot;).getClassLoader(); System.out.println(classLoader5);// null，Object的加载器是引导类加载器，无法获取 // 关于类加载器的一个主要方法：getResourceAsStream(String str):获取类路径下的指定文件的输入流 InputStream in = this.getClass().getClassLoader().getResourceAsStream(&quot;test.properties&quot;); System.out.println(in); &#125;&#125; 类加载器读取配置文件： 1234567891011121314151617181920212223242526public class ClassLoaderTest &#123; /* Properties：用来读取配置文件。 注意：配置文件的路径问题 */ @Test public void test2() throws Exception &#123; Properties pros = new Properties(); // 读取配置文件的方式一： // 此时的文件默认在当前的module下 /*FileInputStream fis = new FileInputStream(&quot;jdbc1.properties&quot;); // FileInputStream fis = new FileInputStream(&quot;src\\\\jdbc1.properties&quot;);// 等同于方式二 pros.load(fis);*/ // 读取配置文件的方式二：使用ClassLoader // 此时的配置文件默认识别为：当前module的src下 ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); InputStream is = classLoader.getResourceAsStream(&quot;jdbc1.properties&quot;); pros.load(is); String user = pros.getProperty(&quot;user&quot;); String password = pros.getProperty(&quot;password&quot;); System.out.println(&quot;user = &quot; + user + &quot;, password = &quot; + password); &#125;&#125; 创建运行时类的对象 当拿到了运行时类的 Class 对象后，就可以创建该运行时类的对象，这是反射机制应用最多的地方。 通过 Class 对象的 newInstance() 创建： 运行时类必须有一个无参数的构造器。 运行时类的构造器的访问权限需要足够。 12345678910111213141516171819202122/** * 通过反射创建对应的运行时类的对象 */public class NewInstanceTest &#123; @Test public void test1() throws IllegalAccessException, InstantiationException &#123; Class&lt;Person&gt; clazz = Person.class; /* newInstance(): 调用此方法，创建对应的运行时类的对象。内部调用了运行时类的空参的构造器。 要想此方法正常的创建运行时类的对象，要求： 1.运行时类必须提供空参的构造器 2.空参的构造器的访问权限得够。通常，设置为public。 在javabean中要求提供一个public的空参构造器。原因： 1.便于通过反射，创建运行时类的对象 2.便于子类继承此运行时类时，默认调用super()时，保证父类有此构造器 */ Person obj = clazz.newInstance(); System.out.println(obj); &#125;&#125; 通过 Class 对象的 getDeclaredConstructor(Class … parameterTypes) 创建： 先向构造器的形参中传递一个对象数组进去，里面包含了构造器中所需的各个参数。 再通过 Constructor 实例化对象。 体会反射的动态性： 123456789101112131415161718192021222324252627282930313233343536373839public class NewInstanceTest &#123; /* 体会反射的动态性：以下程序只有在运行时，才能确定到底创建哪个对象 */ @Test public void test2() &#123; for (int i = 0; i &lt; 100; i++) &#123; int num = new Random().nextInt(3);// 0,1,2 String classPath = &quot;&quot;; switch (num) &#123; case 0: classPath = &quot;java.util.Date&quot;; break; case 1: classPath = &quot;java.lang.Object&quot;; break; case 2: classPath = &quot;cn.xisun.java.base.file.Person&quot;; break; &#125; try &#123; Object obj = getInstance(classPath); System.out.println(obj); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /* 创建一个指定类的对象。 classPath: 指定类的全类名 */ public Object getInstance(String classPath) throws Exception &#123; Class clazz = Class.forName(classPath); return clazz.newInstance(); &#125;&#125; 获取运行时类的完整结构 类的完整结构： Field 、Method 、Constructor 、Superclass 、Interface 、Annotation。 全部的 Field。 全部的方法。 全部的构造器。 所继承的父类。 实现的全部接口。 注解。 定义 Person 类和相关接口、注解类： 123456789101112public class Creature&lt;T&gt; implements Serializable &#123; private char gender; public double weight; private void breath() &#123; System.out.println(&quot;生物呼吸&quot;); &#125; public void eat() &#123; System.out.println(&quot;生物吃东西&quot;); &#125;&#125; 123public interface MyInterface &#123; void info();&#125; 12345@Target(&#123;TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotation &#123; String value() default &quot;hello&quot;;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@MyAnnotation(value = &quot;hi&quot;)public class Person extends Creature&lt;String&gt; implements Comparable&lt;String&gt;, MyInterface &#123; private String name; int age; public int id; public Person() &#123; &#125; @MyAnnotation(value = &quot;abc&quot;) private Person(String name) &#123; this.name = name; &#125; Person(String name, int age) &#123; this.name = name; this.age = age; &#125; @MyAnnotation private String show(String nation) &#123; System.out.println(&quot;我的国籍是：&quot; + nation); return nation; &#125; public String display(String interests, int age) throws NullPointerException, ClassCastException &#123; return interests + age; &#125; @Override public void info() &#123; System.out.println(&quot;我是一个人&quot;); &#125; @Override public int compareTo(String o) &#123; return 0; &#125; private static void showDesc() &#123; System.out.println(&quot;我是一个可爱的人&quot;); &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, id=&quot; + id + &#x27;&#125;&#x27;; &#125;&#125; 使用反射获得全部的 Field： public Field[] getFields()：返回此 Class 对象所表示的类或接口的 public 的 Field (包括父类)。 public Field[] getDeclaredFields()：返回此 Class 对象所表示的类或接口的全部 Field (不包括父类)。 Field 类中的方法： public int getModifiers()：以整数形式返回此 Field 的修饰符。 public Class&lt;?&gt; getType()：得到 Field 的属性类型。 public String getName()：返回 Field 的名称。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class FieldTest &#123; @Test public void test1() &#123; Class clazz = Person.class; // 获取属性结构 // getFields(): 获取当前运行时类及其父类中声明为public访问权限的属性 Field[] fields = clazz.getFields(); for (Field f : fields) &#123; System.out.println(f); &#125; System.out.println(); // getDeclaredFields(): 获取当前运行时类中声明的所有属性。（不包含父类中声明的属性） Field[] declaredFields = clazz.getDeclaredFields(); for (Field f : declaredFields) &#123; System.out.println(f); &#125; &#125; /* 权限修饰符 数据类型 变量名 */ @Test public void test2() &#123; Class clazz = Person.class; Field[] declaredFields = clazz.getDeclaredFields(); for (Field f : declaredFields) &#123; // 1.权限修饰符 int modifier = f.getModifiers(); System.out.print(Modifier.toString(modifier) + &quot;, &quot;); // 2.数据类型 Class type = f.getType(); System.out.print(type.getName() + &quot;, &quot;); // 3.变量名 String fName = f.getName(); System.out.print(fName); System.out.println(); &#125; &#125;&#125;test1输出结果：public int cn.xisun.java.base.file.Person.idpublic double cn.xisun.java.base.file.Creature.weightprivate java.lang.String cn.xisun.java.base.file.Person.nameint cn.xisun.java.base.file.Person.agepublic int cn.xisun.java.base.file.Person.idtest2输出结果：private, java.lang.String, name, int, agepublic, int, id 使用反射获得全部的 Method： public Method[] getMethods()：返回此 Class 对象所表示的类或接口的 public 的 Method (包括父类)。 public Method[] getDeclaredMethods()：返回此 Class 对象所表示的类或接口的全部 Method (不包括父类)。 Method 类中的方法： public Class&lt;?&gt;[] getParameterTypes()：取得全部的参数。 public int getModifiers()：取得修饰符。 public Class&lt;?&gt; getReturnType()：取得全部的返回值。 public Class&lt;?&gt;[] getExceptionTypes()：取得异常信息。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class MethodTest &#123; @Test public void test1() &#123; Class clazz = Person.class; // getMethods(): 获取当前运行时类及其所有父类中声明为public权限的方法 Method[] methods = clazz.getMethods(); for (Method m : methods) &#123; System.out.println(m); &#125; System.out.println(); // getDeclaredMethods(): 获取当前运行时类中声明的所有方法。（不包含父类中声明的方法） Method[] declaredMethods = clazz.getDeclaredMethods(); for (Method m : declaredMethods) &#123; System.out.println(m); &#125; &#125; /* @Xxxx 权限修饰符 返回值类型 方法名(参数类型1 形参名1, ...) throws XxxException&#123;&#125; */ @Test public void test2() &#123; Class clazz = Person.class; Method[] declaredMethods = clazz.getDeclaredMethods(); for (Method m : declaredMethods) &#123; // 1.获取方法声明的注解 Annotation[] annos = m.getAnnotations(); for (Annotation a : annos) &#123; System.out.print(a + &quot;, &quot;); &#125; // 2.权限修饰符 System.out.print(Modifier.toString(m.getModifiers()) + &quot;, &quot;); // 3.返回值类型 System.out.print(m.getReturnType().getName() + &quot;, &quot;); // 4.方法名 System.out.print(m.getName()); // 5.形参列表 System.out.print(&quot;(&quot;); Class[] parameterTypes = m.getParameterTypes(); if (!(parameterTypes == null &amp;&amp; parameterTypes.length == 0)) &#123; for (int i = 0; i &lt; parameterTypes.length; i++) &#123; if (i == parameterTypes.length - 1) &#123; System.out.print(parameterTypes[i].getName() + &quot; args_&quot; + i); break; &#125; System.out.print(parameterTypes[i].getName() + &quot; args_&quot; + i + &quot;, &quot;); &#125; &#125; System.out.print(&quot;), &quot;); // 6.抛出的异常 Class[] exceptionTypes = m.getExceptionTypes(); if (exceptionTypes.length &gt; 0) &#123; System.out.print(&quot;throws &quot;); for (int i = 0; i &lt; exceptionTypes.length; i++) &#123; if (i == exceptionTypes.length - 1) &#123; System.out.print(exceptionTypes[i].getName()); break; &#125; System.out.print(exceptionTypes[i].getName() + &quot;, &quot;); &#125; &#125; System.out.println(); &#125; &#125;&#125; 使用反射获得全部的构造器： public Constructor&lt;T&gt;[] getConstructors()：返回此 Class 对象所表示的类的所有 public 构造方法 (没有父类)。 public Constructor&lt;T&gt;[] getDeclaredConstructors()：返回此 Class 对象表示的类声明的所有构造方法 (没有父类)。 Constructor 类中的方法： public int getModifiers()：取得修饰符。 public String getName()：取得方法名称。 public Class&lt;?&gt;[] getParameterTypes()：取得参数的类型。 使用反射获得实现的全部接口： public Class&lt;?&gt;[] getInterfaces()：确定此对象所表示的类或接口实现的接口。 使用反射获得所继承的父类 public Class&lt;? Super T&gt; getSuperclass()：返回表示此 Class 所表示的实体 (类、接口、基本类型) 的父类的 Class。 使用反射获得泛型相关： Type getGenericSuperclass()：获取父类泛型类型。 泛型类型：ParameterizedType。 getActualTypeArguments()：获取实际的泛型类型参数数组。 使用反射获得 Annotation 相关 get Annotation(Class&lt;T&gt; annotationClass) getDeclaredAnnotations() 使用反射获得类所在的包： Package getPackage() 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106public class OtherTest &#123; /* 获取构造器结构 */ @Test public void test1() &#123; Class&lt;Person&gt; clazz = Person.class; // getConstructors(): 获取当前运行时类中声明为public的构造器 Constructor&lt;?&gt;[] constructors = clazz.getConstructors(); for (Constructor&lt;?&gt; c : constructors) &#123; System.out.println(c); &#125; System.out.println(); // getDeclaredConstructors(): 获取当前运行时类中声明的所有的构造器 Constructor&lt;?&gt;[] declaredConstructors = clazz.getDeclaredConstructors(); for (Constructor&lt;?&gt; c : declaredConstructors) &#123; System.out.println(c); &#125; &#125; /* 获取运行时类的父类 */ @Test public void test2() &#123; Class&lt;Person&gt; clazz = Person.class; Class&lt;? super Person&gt; superclass = clazz.getSuperclass(); System.out.println(superclass); &#125; /* 获取运行时类的带泛型的父类 */ @Test public void test3() &#123; Class&lt;Person&gt; clazz = Person.class; Type genericSuperclass = clazz.getGenericSuperclass(); System.out.println(genericSuperclass); &#125; /* 获取运行时类的带泛型的父类的泛型 代码：逻辑性代码 vs 功能性代码 */ @Test public void test4() &#123; Class&lt;Person&gt; clazz = Person.class; Type genericSuperclass = clazz.getGenericSuperclass(); ParameterizedType paramType = (ParameterizedType) genericSuperclass; // 获取泛型类型 Type[] actualTypeArguments = paramType.getActualTypeArguments(); System.out.println(actualTypeArguments[0].getTypeName());// 方式一 System.out.println(((Class) actualTypeArguments[0]).getName());// 方式二 &#125; /* 获取运行时类实现的接口 */ @Test public void test5() &#123; Class&lt;Person&gt; clazz = Person.class; Class&lt;?&gt;[] interfaces = clazz.getInterfaces(); for (Class&lt;?&gt; c : interfaces) &#123; System.out.println(c); &#125; System.out.println(); // 获取运行时类的父类实现的接口 Class&lt;?&gt;[] interfaces1 = clazz.getSuperclass().getInterfaces(); for (Class&lt;?&gt; c : interfaces1) &#123; System.out.println(c); &#125; &#125; /* 获取运行时类声明的注解 */ @Test public void test7() &#123; Class&lt;Person&gt; clazz = Person.class; Annotation[] annotations = clazz.getAnnotations(); for (Annotation annos : annotations) &#123; System.out.println(annos); &#125; &#125; /* 获取运行时类所在的包 */ @Test public void test6() &#123; Class&lt;Person&gt; clazz = Person.class; Package pack = clazz.getPackage(); System.out.println(pack); &#125;&#125; 调用运行时类的指定结构 调用指定属性： 在反射机制中，可以直接通过 Field 类操作类中的属性，通过 Field 类提供的 set() 和 get() 就可以完成设置和取得属性内容的操作。 public Field getField(String name)：返回此 Class 对象表示的类或接口的指定的 public 的Field。 **public Field getDeclaredField(String name)**：返回此 Class 对象表示的类或接口的指定的 Field。 在 Field 中： public void set(Object obj,Object value)：设置指定对象 obj 上此 Field 的属性内容。 public Object get(Object obj)：取得指定对象 obj 上此 Field 的属性内容。 调用指定方法： 通过反射，调用类中的方法，通过 Method 类完成。步骤： 通过 Class 类的 getDeclaredMethod(String name,Class…parameterTypes) 取得一个 Method 对象，并设置此方法操作时所需要的参数类型。 之后使用 Object invoke(Object obj, Object[] args) 进行调用，并向方法中传递要设置的 obj 对象的参数信息。 关于 setAccessible(true) 的使用： Method 和 Field、Constructor 对象都有 setAccessible()。 setAccessible() 启动和禁用访问安全检查的开关。 参数值为 true 则指示反射的对象在使用时应该取消 Java 语言访问检查。 提高反射的效率。如果代码中必须用反射，而该句代码需要频繁的被调用，那么请设置为 true。 使得原本无法访问的私有成员也可以访问。 参数值为 false 则指示反射的对象应该实施 Java 语言访问检查。 关于 Object invoke(Object obj, Object … args) 的使用： Object 对应原方法的返回值，若原方法无返回值，此时返回 null。 若原方法为静态方法，则形参 obj 为运行时类本身或者 null。 若原方法形参列表为空，则形参 args 为 null。 若原方法声明为 private，则需要在调用此 invoke() 前，显式调用方法对象的 setAccessible(true)，即可访问 private 的方法。(一般来说，不论调用的是什么权限的方法，都可显示调用方法对象的 setAccessible(true)。) 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126public class ReflectionTest &#123; /* 不需要掌握，因为只能获取public的,通常不采用此方法 */ @Test public void testField() throws Exception &#123; Class&lt;Person&gt; clazz = Person.class; // 创建运行时类的对象 Person p = clazz.newInstance(); // 获取指定的属性：要求运行时类中属性声明为public Field id = clazz.getField(&quot;id&quot;); /* 设置当前属性的值 set(): 参数1：指明设置哪个对象的属性 参数2：将此属性值设置为多少 */ id.set(p, 1001); /* 获取当前属性的值 get(): 参数1：获取哪个对象的当前属性值 */ int pId = (int) id.get(p); System.out.println(pId);// 1001 &#125; /* 如何操作运行时类中的指定的属性 --- 需要掌握 */ @Test public void testField1() throws Exception &#123; Class clazz = Person.class; // 创建运行时类的对象 Person p = (Person) clazz.newInstance(); // 1. getDeclaredField(String fieldName): 获取运行时类中指定变量名的属性 Field name = clazz.getDeclaredField(&quot;name&quot;); // 2.保证当前属性是可访问的 name.setAccessible(true); // 3.获取、设置指定对象的此属性值 name.set(p, &quot;Tom&quot;); System.out.println(name.get(p)); System.out.println(&quot;*************如何调用静态属性*****************&quot;); // public static String national = &quot;中国&quot;; Field national = clazz.getDeclaredField(&quot;national&quot;); national.setAccessible(true); System.out.println(national.get(Person.class));// 中国 &#125; /* 如何操作运行时类中的指定的方法 --- 需要掌握 */ @Test public void testMethod() throws Exception &#123; Class&lt;Person&gt; clazz = Person.class; // 创建运行时类的对象 Person p = clazz.newInstance(); /* 1.获取指定的某个方法 getDeclaredMethod(): 参数1 ：指明获取的方法的名称 参数2：指明获取的方法的形参列表 */ Method show = clazz.getDeclaredMethod(&quot;show&quot;, String.class); // 2.保证当前方法是可访问的 show.setAccessible(true); /* 3.调用方法的invoke(): 参数1：方法的调用者 参数2：给方法形参赋值的实参 invoke()的返回值即为对应类中调用的方法的返回值 */ Object returnValue = show.invoke(p, &quot;CHN&quot;); // String nation = p.show(&quot;CHN&quot;); System.out.println(returnValue);// CHN，返回的returnValue是一个String，可以强转 System.out.println(&quot;*************如何调用静态方法*****************&quot;); // private static void showDesc()&#123;&#125; Method showDesc = clazz.getDeclaredMethod(&quot;showDesc&quot;); showDesc.setAccessible(true); // 如果调用的运行时类中的方法没有返回值，则此invoke()返回null // Object returnVal = showDesc.invoke(null);// 参数写null也可以，因为静态方法的调用者只有类本身 Object returnVal = showDesc.invoke(Person.class);// 静态方法的调用者就是当前类 System.out.println(returnVal);// null &#125; /* 如何调用运行时类中的指定的构造器 --- 不常用 经常调用类的空参构造器创建类的对象：Person p = clazz.newInstance(); */ @Test public void testConstructor() throws Exception &#123; Class&lt;Person&gt; clazz = Person.class; //private Person(String name) /* 1.获取指定的构造器 getDeclaredConstructor(): 参数：指明构造器的参数列表 */ // private Person(String name) &#123;this.name = name;&#125; Constructor&lt;Person&gt; constructor = clazz.getDeclaredConstructor(String.class); // 2.保证此构造器是可访问的 constructor.setAccessible(true); // 3.调用此构造器创建运行时类的对象 Person per = constructor.newInstance(&quot;Tom&quot;); System.out.println(per); &#125;&#125; 反射的应用：动态代理 代理设计模式的原理：使用一个代理将对象包装起来，然后用该代理对象取代原始对象。任何对原始对象的调用都要通过代理。代理对象决定是否以及何时将方法调用转到原始对象上。 代理过程：代理类和被代理类实现共同的接口，重写接口的方法 a。被代理类中，在方法 a 中实现自身需要完成的逻辑。代理类中，提供被代理类的实例，并在方法 a 中，调用该实例对象的方法 a，同时，在代理类的方法 a 中，也可以添加一些不同代理类需要实现的公共的方法。 代理分为静态代理和动态代理。 静态代理的特征是代理类和目标对象的类都是在编译期间确定下来。静态代理不利于程序的扩展。同时，每一个代理类只能为一个接口服务，这样一来程序开发中必然产生过多的代理。 最好可以通过一个代理类完成全部的代理功能。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 静态代理举例 * * 特点：代理类和被代理类在编译期间，就确定下来了。 */interface ClothFactory &#123; void produceCloth();&#125;/** * 被代理类1 */class AntaClothFactory implements ClothFactory &#123; @Override public void produceCloth() &#123; System.out.println(&quot;Anta工厂生产一批运动服&quot;); &#125;&#125;/** * 被代理类2 */class LiningClothFactory implements ClothFactory &#123; @Override public void produceCloth() &#123; System.out.println(&quot;Lining工厂生产一批运动服&quot;); &#125;&#125;/** * 代理类 --- 只能代理实现了ClothFactory这个接口的被代理类，其他类型的被代理类不能使用 */class ProxyClothFactory implements ClothFactory &#123; // 用被代理类对象进行实例化 private ClothFactory factory; public ProxyClothFactory(ClothFactory factory) &#123; this.factory = factory; &#125; @Override public void produceCloth() &#123; System.out.println(&quot;代理类做一些公共的准备工作&quot;); factory.produceCloth();// 此方法由具体的被代理类自己实现 System.out.println(&quot;代理类做一些公共的收尾工作&quot;); &#125;&#125;public class StaticProxyTest &#123; public static void main(String[] args) &#123; // 创建被代理类的对象 ClothFactory anta = new AntaClothFactory(); // 创建代理类的对象 ClothFactory proxyClothFactory = new ProxyClothFactory(anta); proxyClothFactory.produceCloth(); System.out.println(&quot;******************************&quot;); proxyClothFactory = new ProxyClothFactory(new LiningClothFactory()); proxyClothFactory.produceCloth(); &#125;&#125; 动态代理是指客户通过代理类来调用其它对象的方法，并且是在程序运行时根据需要动态创建目标类的代理对象。 动态代理使用场合： 调试 远程方法调用 动态代理相比于静态代理的优点：抽象角色中 (接口) 声明的所有方法都被转移到调用处理器一个集中的方法中处理，这样，我们可以更加灵活和统一的处理众多的方法。 一个动态代理类能做到所有被代理类的工作，在运行时，会根据传入的被代理类的对象，动态的创建一个对应的代理对象。 Java 动态代理相关的 API： Proxy：专门完成代理的操作类，是所有动态代理类的父类。通过此类为一个或多个接口动态地生成实现类。 提供用于创建动态代理类和动态代理对象的静态方法： static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces)：创建一个动态代理类所对应的Class对象 static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h)：直接创建一个动态代理对象 动态代理步骤： 创建一个实现接口 InvocationHandler 的类，它必须实现 invoke()，以完成代理的具体操作： 创建被代理的类以及接口： 通过 Proxy 的静态方法 newProxyInstance(ClassLoader loader, Class[] interfaces, InvocationHandler h) 创建一个 Subject 接口代理： 12345RealSubject target = new RealSubject();// Create a proxy to wrap the original implementationDebugProxy proxy = new DebugProxy(target);// Get a reference to the proxy through the Subject interfaceSubject sub = (Subject) Proxy.newProxyInstance(Subject.class.getClassLoader(),new Class[] &#123; Subject.class &#125;, proxy); 通过 Subject 代理调用 RealSubject 实现类的方法： 12String info = sub.say(&quot;Peter&quot;, 24);System.out.println(info); 实例： 1234567891011121314151617181920212223/** * 被代理类型一 */interface Human &#123; String getBelief(); void eat(String food);&#125;/** * 被代理类 */class SuperMan implements Human &#123; @Override public String getBelief() &#123; return &quot;I believe I can fly!&quot;; &#125; @Override public void eat(String food) &#123; System.out.println(&quot;超人喜欢吃&quot; + food); &#125;&#125; 12345678910111213141516/** * 被代理类型二 */interface ClothFactory &#123; void produceCloth();&#125;/** * 被代理类 */class AntaClothFactory implements ClothFactory &#123; @Override public void produceCloth() &#123; System.out.println(&quot;Anta工厂生产一批运动服&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/*要想实现动态代理，需要解决的问题？问题一：如何根据加载到内存中的被代理类，动态的创建一个代理类及其对象。问题二：当通过代理类的对象调用方法a时，如何动态的去调用被代理类中的同名方法a。 *//** * 动态代理类 */class ProxyFactory &#123; // 调用此方法，返回一个代理类的对象。---&gt; 解决问题一 // 返回的可能是不同类型的代理类对象，因此返回Object，然后根据传参obj的类型，再进行强转 public static Object getProxyInstance(Object obj) &#123;// obj:被代理类的对象 // 创建InvocationHandler接口的实例，并赋值被代理类的对象 MyInvocationHandler handler = new MyInvocationHandler(); handler.bind(obj); /* 参数1：被代理类obj的类加载器 参数2：被代理类obj实现的接口 参数3：实现InvocationHandler接口的handler，包含被代理类执行方法调用的逻辑 */ return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), handler); &#125;&#125;/** * 实现InvocationHandler接口 ---&gt; 解决问题二 */class MyInvocationHandler implements InvocationHandler &#123; // 需要使用被代理类的对象进行赋值 private Object obj; // 赋值操作，也可以通过构造器进行赋值 public void bind(Object obj) &#123; this.obj = obj; &#125; // 当我们通过代理类的对象，调用方法a时，就会自动的调用如下的方法：invoke() // 将被代理类要执行的方法a的功能就声明在invoke()中 // proxy：代理类的对象 // method：代理类和被代理类共同实现的接口中的重写的方法 // args：该重写方法需要传入的参数 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; // method: 即为代理类对象调用的方法，此方法也就作为了被代理类对象要调用的方法 // obj: 被代理类的对象 Object returnValue = method.invoke(obj, args); // 上述方法的返回值returnValue就作为当前类中的invoke()的返回值。 // 实际上也就是被代理类所调用方法的返回值 return returnValue; &#125;&#125;/** * 测试方法 */public class ProxyTest &#123; public static void main(String[] args) &#123; // 被代理类型一 SuperMan superMan = new SuperMan(); // proxyHuman: 代理类的对象 // 在动态代理中，proxyHuman代表的是代理类的对象，不应该被强转为SuperMan，因为SuperMan是被代理类 // 但可以被强转为共同的接口Human。其他类型的代理类和被代理类同理 Human proxyHuman = (Human) ProxyFactory.getProxyInstance(superMan); // 当通过代理类对象调用方法时，会自动的调用被代理类中同名的方法 String belief = proxyHuman.getBelief();// 方法一：getBelief()有返回值 System.out.println(belief); proxyHuman.eat(&quot;四川麻辣烫&quot;);// 方法二：eat()没有返回值 System.out.println(&quot;*****************************&quot;); // 被代理类型二 AntaClothFactory antaClothFactory = new AntaClothFactory();// 创建被代理类 ClothFactory proxyClothFactory = (ClothFactory) ProxyFactory.getProxyInstance(antaClothFactory);// 创建代理类 proxyClothFactory.produceCloth();// 执行方法 &#125;&#125; 动态代理与 AOP (Aspect Orient Programming)： 前面介绍的 Proxy 和 InvocationHandler，很难看出这种动态代理的优势，下面介绍一种更实用的动态代理机制。 改进前： 改进后的说明：代码段 1、代码段 2、代码段 3 和深色代码段分离开了，但代码段 1、2、3 又和一个特定的方法 A 耦合了！最理想的效果是：代码块 1、2、3 既可以执行方法A ，又无须在程序中以硬编码的方式直接调用深色代码的方法。 AOP 实例，参看 ProxyUtil 的定义和使用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/** * 被代理类型一 */interface Human &#123; String getBelief(); void eat(String food);&#125;/** * 被代理类 */class SuperMan implements Human &#123; @Override public String getBelief() &#123; return &quot;I believe I can fly!&quot;; &#125; @Override public void eat(String food) &#123; System.out.println(&quot;超人喜欢吃&quot; + food); &#125;&#125;/** * 被代理类型二 */interface ClothFactory &#123; void produceCloth();&#125;/** * 被代理类 */class AntaClothFactory implements ClothFactory &#123; @Override public void produceCloth() &#123; System.out.println(&quot;Anta工厂生产一批运动服&quot;); &#125;&#125;/** * 不同被代理类都需要执行的通用方法，比如日志等 --- AOP的应用 */class ProxyUtil &#123; public void method1() &#123; System.out.println(&quot;====================通用方法一====================&quot;); &#125; public void method2() &#123; System.out.println(&quot;====================通用方法二====================&quot;); &#125;&#125;/** * 动态代理类 */class ProxyFactory &#123; public static Object getProxyInstance(Object obj) &#123; MyInvocationHandler handler = new MyInvocationHandler(); handler.bind(obj); return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), handler); &#125;&#125;class MyInvocationHandler implements InvocationHandler &#123; private Object obj; public void bind(Object obj) &#123; this.obj = obj; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; ProxyUtil util = new ProxyUtil(); // 执行通用方法一 util.method1(); // 执行被代理类的相应方法 Object returnValue = method.invoke(obj, args); // 执行通用方法二 util.method2(); return returnValue; &#125;&#125;public class ProxyTest &#123; public static void main(String[] args) &#123; // 被代理类型一 SuperMan superMan = new SuperMan(); Human proxyHuman = (Human) ProxyFactory.getProxyInstance(superMan); String belief = proxyHuman.getBelief();// getBelief()有返回值 System.out.println(belief); proxyHuman.eat(&quot;四川麻辣烫&quot;);// eat()没有返回值 System.out.println(&quot;*****************************&quot;); // 被代理类型二 AntaClothFactory antaClothFactory = new AntaClothFactory(); ClothFactory proxyClothFactory = (ClothFactory) ProxyFactory.getProxyInstance(antaClothFactory); proxyClothFactory.produceCloth(); &#125;&#125; 使用 Proxy 生成一个动态代理时，往往并不会凭空产生一个动态代理，这样没有太大的意义。通常都是为指定的目标对象生成动态代理。 这种动态代理在 AOP 中被称为 AOP 代理，AOP 代理可代替目标对象，AOP 代理包含了目标对象的全部方法。但 AOP 代理中的方法与目标对象的方法存在差异：AOP 代理里的方法可以在执行目标方法之前、之后插入一些通用处理。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 网络编程","slug":"java-network","date":"2021-04-05T04:39:24.000Z","updated":"2021-04-09T07:59:26.744Z","comments":true,"path":"2021/04/05/java-network/","link":"","permalink":"http://example.com/2021/04/05/java-network/","excerpt":"","text":"网络编程概述 Java 是 Internet 上的语言，它从语言级上提供了对网络应用程序的支持，程序员能够很容易开发常见的网络应用程序。 Java 提供的网络类库，可以实现无痛的网络连接，联网的底层细节被隐藏在 Java 的本机安装系统里，由 JVM 进行控制。并且 Java 实现了一个跨平台的网络库， 程序员面对的是一个统一的网络编程环境。 计算机网络：把分布在不同地理区域的计算机与专门的外部设备用通信线路互连成一个规模大、功能强的网络系统，从而使众多的计算机可以方便地互相传递信息、共享硬件、软件、数据信息等资源。 网络编程的目的：直接或间接地通过网络协议与其它计算机实现数据交换，进行通讯。 网络编程中有两个主要的问题： 如何准确地定位网络上一台或多台主机，以及定位主机上的特定的应用。 找到主机后如何可靠高效地进行数据传输。 网络通信要素概述 IP 和端口号 网络通信协议 如何实现网络中的主机互相通信： 通信双方地址： IP 端口号 一定的规则 (即：网络通信协议。有两套参考模型)： OSI 参考模型：模型过于理想化，未能在因特网上进行广泛推广。 TCP/IP 参考模型 (或叫 TCP/IP 协议)：事实上的国际标准。 网络中数据传输过程： 通信要素 1：IP 和端口号 IP 地址：Java 中，一个 InetAddress 类的实例对象，就代表一个 IP 地址。 唯一的标识 Internet 上的计算机 (通信实体)。 本地回环地址 (hostAddress)：127.0.0.1，主机名 (hostName)：localhost。 IP 地址分类方式 1：IPV4 和 IPV6。 IPV4：4 个字节组成，4 个 0 ~ 255。大概 42 亿，30 亿都在北美，亚洲 4亿。2011 年初已经用尽。以点分十进制表示，如 192.168.0.1。 IPV6：128 位，16个字节，写成 8 个无符号整数，每个整数用四个十六进制位表示，数之间用冒号 : 分开，如：3ffe:3201:1401:1280:c8ff:fe4d:db39:1984。 IP 地址分类方式 2： 公网地址 (万维网使用) 和私有地址 (局域网使用)。192.168. 开头的就是私有址址，范围为 192.168.0.0 ~ 192.168.255.255，专门为组织机构内部使用。 特点：不易记忆。 端口号：标识正在计算机上运行的进程 (程序)。 不同的进程有不同的端口号。 被规定为一个 16 位的整数 0 ~ 65535。 端口分类： 公认端口：0 ~ 1023。被预先定义的服务通信占用，如：HTTP 占用端口 80，FTP 占用端口 21，Telnet 占用端口 23 等。 注册端口：1024 ~ 49151。分配给用户进程或应用程序，如：Tomcat 占用端口 8080，MySQL 占用端口 3306，Oracle 占用端口 1521 等。 动态/私有端口：49152 ~ 65535。 端口号与 IP 地址的组合得出一个网络套接字：Socket。 InetAddress 类： Internet 上的主机有两种方式表示地址： 域名 (hostName)，如：www.atguigu.com。 IP 地址 (hostAddress)，如：202.108.35.210。 InetAddress 类主要表示 IP 地址，两个子类：Inet4Address、Inet6Address。 InetAddress 类对象含有一个 Internet 主机地址的域名和 IP 地址，如：www.atguigu.com 和 202.108.35.210。 域名容易记忆，当在连接网络时输入一个主机的域名后，域名服务器 (DNS) 负责将域名转化成 IP 地址，这样才能和主机建立连接。这就是域名解析。 域名解析时，先找本机 hosts 文件，确定是否有输入的域名地址，如果没有，再通过 DNS 服务器，找到对应的主机。 InetAddress 类没有提供公共的构造器，而是提供了如下几个静态方法来获取 InetAddress 实例： public static InetAddress getLocalHost() public static InetAddress getByName(String host) InetAddress 类提供了如下几个常用的方法： public String getHostAddress()：返回 IP 地址字符串，以文本表现形式。 public String getHostName()：获取此 IP 地址的主机名。 public boolean isReachable(int timeout)：测试是否可以达到该地址。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 一、网络编程中有两个主要的问题： * 1.如何准确地定位网络上一台或多台主机；定位主机上的特定的应用 * 2.找到主机后如何可靠高效地进行数据传输 * * 二、网络编程中的两个要素： * 1.对应问题一：IP和端口号 * 2.对应问题二：提供网络通信协议：TCP/IP参考模型（应用层、传输层、网络层、物理+数据链路层） * * * 三、通信要素一：IP和端口号 * * 1. IP: 唯一的标识 Internet 上的计算机（通信实体） * 2. 在Java中使用InetAddress类代表IP * 3. IP分类：IPv4 和 IPv6 ; 万维网 和 局域网 * 4. 域名: www.baidu.com www.mi.com www.sina.com www.jd.com www.vip.com * 5. 本地回路地址：127.0.0.1 对应着：localhost * * 6. 如何实例化InetAddress:两个方法：getByName(String host) 、 getLocalHost() * 两个常用方法：getHostName() / getHostAddress() * * 7. 端口号：正在计算机上运行的进程。 * 要求：不同的进程有不同的端口号 * 范围：被规定为一个 16 位的整数 0~65535。 * * 8. 端口号与IP地址的组合得出一个网络套接字：Socket */public class InetAddressTest &#123; public static void main(String[] args) &#123; try &#123; InetAddress inet1 = InetAddress.getByName(&quot;192.168.10.14&quot;); System.out.println(inet1);// /192.168.10.14 InetAddress inet2 = InetAddress.getByName(&quot;www.atguigu.com&quot;); System.out.println(inet2);// www.atguigu.com/58.215.145.131 InetAddress inet3 = InetAddress.getByName(&quot;127.0.0.1&quot;); System.out.println(inet3);// /127.0.0.1 // 获取本地ip InetAddress inet4 = InetAddress.getLocalHost(); System.out.println(inet4); // getHostAddress() System.out.println(inet2.getHostAddress());// 58.215.145.131 // getHostName() System.out.println(inet2.getHostName());// www.atguigu.com // isReachable(int timeout) try &#123; System.out.println(inet2.isReachable(10));// true &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; catch (UnknownHostException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 通信要素 2：网络通信协议 网络通信协议：计算机网络中实现通信必须有一些约定，即通信协议，对速率、传输代码、代码结构、传输控制步骤、出错控制等制定标准。 问题：网络协议太复杂。计算机网络通信涉及内容很多，比如指定源地址和目标地址、加密解密、压缩解压缩、差错控制、流量控制、路由控制等，如何实现如此复杂的网络协议呢？ 通信协议分层的思想：在制定协议时，把复杂成份分解成一些简单的成份，再将它们复合起来。最常用的复合方式是层次方式，即同层间可以通信、上一层可以调用下一层，而与再下一层不发生关系。各层互不影响，利于系统的开发和扩展。 TCP/IP 协议簇： TCP/IP 协议簇以其两个主要协议传输控制协议 (TCP) 和网络互联协议 (IP) 而得名，实际上是一组协议，包括多个具有不同功能且互为关联的协议。 传输层协议中两个非常重要的协议： 传输控制协议 TCP (Transmission Control Protocol)。 用户数据报协议 UDP (User Datagram Protocol)。 网络层的主要协议： 网络互联协议 IP (Internet Protocol)，其支持网间互连的数据通信。 TCP/IP 协议模型从更实用的角度出发，形成了高效的四层体系结构，即物理链路层、IP 层、传输层和应用层。 TCP 协议： 使用 TCP 协议前，须先建立 TCP 连接，形成传输数据通道。 传输前，采用 “三次握手” 方式，点对点通信，是可靠的。 TCP 协议进行通信的两个应用进程：客户端、服务端。 在连接中可进行大数据量的传输。 传输完毕，采用 “四次挥手” 方式，释放已建立的连接，效率相对低。 UDP 协议： 将数据、源、目的封装成数据包，不需要建立连接。 每个数据报的大小限制在 64 K 内。 发送不管对方是否准备好，接收方收到也不确认，故是不可靠的。 可以广播发送。 发送数据结束时无需释放资源，开销小，速度相对快。 TCP 网络编程 Socket 类实现了基于 TCP 协议的网络编程。 Socket 类： 利用套接字 (Socket) 开发网络应用程序早已被广泛的采用，以至于成为事实上的标准。 网络上具有唯一标识的 IP 地址和端口号组合在一起，才能构成唯一能识别的标识符套接字。 通信的两端都要有 Socket，是两台机器间通信的端点。 网络通信其实就是 Socket 间的通信。 Socket 允许程序把网络连接当成一个流，数据在两个 Socket 间通过 IO 传输。 一般主动发起通信的应用程序属客户端，等待通信请求的为服务端。 Socket 类： 流套接字 (stream socket)：使用 TCP 提供可依赖的字节流服务。 数据报套接字 (datagram socket)：使用 UDP 提供 “尽力而为” 的数据报服务。 Socket 类的常用构造器 ： public Socket(InetAddress address,int port)：创建一个流套接字并将其连接到指定 IP 地址的指定端口号。 public Socket(String host,int port)：创建一个流套接字并将其连接到指定主机上的指定端口号。 Socket 类的常用方法： public InputStream getInputStream()：返回此套接字的输入流。可以用于接收网络消息。 public OutputStream getOutputStream()：返回此套接字的输出流。可以用于发送网络消息。 public InetAddress getInetAddress()：返回此套接字连接到的远程 IP 地址；如果尚未连接套接字，则返回 null。 public InetAddress getLocalAddress()：返回此套接字绑定的本地地址，即本端的 IP 地址。 public int getPort()：返回此套接字连接到的远程端口号；如果尚未连接套接字，则返回 0。 public int getLocalPort()：返回此套接字绑定的本地端口，即本端的端口号。如果尚未绑定套接字，则返回 -1。 public void close()：关闭此套接字。套接字被关闭后，便不可在以后的网络连接中使用 (即无法重新连接或重新绑定)。需要创建新的套接字对象。关闭此套接字也将会关闭该套接字的 InputStream 和 OutputStream。 public void shutdownInput()：关闭此套接字的输入流。如果在套接字上调用 shutdownInput() 后再从套接字输入流读取内容，则流将返回 EOF (文件结束符)，即不能再从此套接字的输入流中接收任何数据。 public void shutdownOutput()：关闭此套接字的输出流。对于 TCP 套接字，任何以前写入的数据都将被发送，并且后跟 TCP 的正常连接终止序列。 如果在套接字上调用 shutdownOutput() 后再写入套接字输出流，则该流将抛出 IOException，即不能再通过此套接字的输出流发送任何数据。 Java 语言的基于套接字编程分为服务端编程和客户端编程，其通信模型如图所示： 服务端 Scoket 的工作过程包含以下四个基本的步骤： 调用 ServerSocket(int port)：创建一个服务器端套接字，并绑定到指定端口上。用于监听客户端的请求。 调用 accept()：监听连接请求，如果客户端请求连接，则接受连接，并返回通信套接字对象。 调用该 Socket 类对象的 getOutputStream() 和 getInputStream()：获取输出流和输入流，开始网络数据的发送和接收。 关闭 ServerSocket 和 Socket 对象：客户端访问结束，关闭通信套接字。 客户端 Socket 的工作过程包含以下四个基本的步骤： 创建 Socket：根据指定服务端的 IP 地址或端口号构造 Socket 类对象。若服务器端响应，则建立客户端到服务器的通信线路。若连接失败，会出现异常。 打开连接到 Socket 的输入/输出流：使用 getInputStream() 获得输入流，使用 getOutputStream() 获得输出流，进行数据传输。 按照一定的协议对 Socket 进行读/写操作：通过输入流读取服务器放入通信线路的信息 (但不能读取自己放入通信线路的信息)，通过输出流将信息写入通信线路。 关闭 Socket：断开客户端到服务器的连接，释放通信线路。 服务器建立 ServerSocket 对象： ServerSocket 对象负责等待客户端请求建立套接字连接，类似邮局某个窗口中的业务员。也就是说，服务器必须事先建立一个等待客户请求建立套接字连接的 ServerSocket 对象。 所谓 “接收” 客户的套接字请求，就是 accept() 会返回一个 Socket 对象。 123456789ServerSocket ss = new ServerSocket(9999);Socket s = ss.accept();InputStream in = s.getInputStream();byte[] buf = new byte[1024];int num = in.read(buf);String str = new String(buf,0,num);System.out.println(s.getInetAddress().toString()+”:”+str);s.close();ss.close(); 客户端创建 Socket 对象： 客户端程序可以使用 Socket 类创建对象，创建的同时会自动向服务器方发起连接。Socket 的构造器是： Socket(String host,int port)throws UnknownHostException,IOException：向服务器 (域名为 host，端口号为 port) 发起 TCP 连接，若成功，则创建 Socket 对象，否则抛出异常。 Socket(InetAddress address,int port)throws IOException：根据 InetAddress 对象所表示的 IP 地址以及端口号 port 发起连接。 客户端建立 socketAtClient 对象的过程就是向服务器发出套接字连接请求。 1234Socket s = new Socket(&quot;192.168.40.165&quot;,9999);OutputStream out = s.getOutputStream();out.write(&quot; hello&quot;.getBytes());s.close(); 流程示意图： 实例 1：客户端发送信息给服务端，服务端将数据显示在控制台上。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * 实现TCP的网络编程 * 实例1：客户端发送信息给服务端，服务端将数据显示在控制台上 */public class TCPTest &#123; /* 客户端 */ @Test public void client() &#123; Socket socket = null; OutputStream os = null; try &#123; // 1.创建Socket对象，指明服务器端的ip和端口号 InetAddress inet = InetAddress.getByName(&quot;127.0.0.1&quot;);// 本机 socket = new Socket(inet, 8879); // 2.获取一个输出流，用于输出数据 os = socket.getOutputStream(); // 3.写出数据的操作 os.write(&quot;你好，我是客户端&quot;.getBytes()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭资源 if (os != null) &#123; try &#123; os.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (socket != null) &#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 服务端 */ @Test public void server() &#123; ServerSocket ss = null; Socket socket = null; InputStream is = null; ByteArrayOutputStream baos = null; try &#123; // 1.创建服务器端的ServerSocket，指明自己的端口号 ss = new ServerSocket(8879); // 2.调用accept()表示接收来自于客户端的socket socket = ss.accept(); // 3.获取输入流 is = socket.getInputStream(); // 4.读取输入流中的数据 // 不建议这样写，可能会有乱码(字节流读取中文) /*byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123; String str = new String(buffer, 0, len); System.out.print(str); &#125;*/ baos = new ByteArrayOutputStream(); byte[] buffer = new byte[5]; int len; while ((len = is.read(buffer)) != -1) &#123; // 将输入流中的数据都读到ByteArrayOutputStream中，读完之后再转换 baos.write(buffer, 0, len); &#125; System.out.println(&quot;收到了来自于：&quot; + socket.getInetAddress().getHostAddress() + &quot;的数据&quot;);// 客户端信息 System.out.println(baos.toString());// 客户端发送的数据 &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 5.关闭资源 if (baos != null) &#123; try &#123; baos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (is != null) &#123; try &#123; is.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (socket != null) &#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (ss != null) &#123; try &#123; ss.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 实例 2：客户端发送文件给服务端，服务端将文件保存在本地。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * * 实现TCP的网络编程 * 实例2：客户端发送文件给服务端，服务端将文件保存在本地。 */public class TCPTest &#123; /* 客户端 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void client() throws IOException &#123; // 1.创建Socket对象，指明服务器端的ip和端口号 Socket socket = new Socket(InetAddress.getByName(&quot;127.0.0.1&quot;), 9090); // 2.获取一个输出流，用于输出数据 OutputStream os = socket.getOutputStream(); // 3.创建输入流，可以使用BufferedInputStream包装 FileInputStream fis = new FileInputStream(new File(&quot;beauty.jpg&quot;)); // 4.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = fis.read(buffer)) != -1) &#123; os.write(buffer, 0, len); &#125; // 5.关闭资源 fis.close(); os.close(); socket.close(); &#125; /* 服务端 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void server() throws IOException &#123; // 1.创建服务器端的ServerSocket，指明自己的端口号 ServerSocket ss = new ServerSocket(9090); // 2.调用accept()表示接收来自于客户端的socket Socket socket = ss.accept(); // 3.获取输入流 InputStream is = socket.getInputStream(); // 4.创建输出流，可以使用BufferedOutputStream包装 FileOutputStream fos = new FileOutputStream(new File(&quot;beauty1.jpg&quot;)); // 5.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; // 6.关闭资源 fos.close(); is.close(); socket.close(); ss.close(); &#125;&#125; 实例 3：从客户端发送文件给服务端，服务端保存到本地，然后返回 “发送成功” 给客户端，并关闭相应的连接。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * 实现TCP的网络编程 * 实例3：从客户端发送文件给服务端，服务端保存到本地，然后返回&quot;发送成功&quot;给客户端，并关闭相应的连接。 */public class TCPTest &#123; /* 客户端 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void client() throws IOException &#123; // 1.创建Socket对象，指明服务器端的ip和端口号 Socket socket = new Socket(InetAddress.getByName(&quot;127.0.0.1&quot;), 9090); // 2.获取一个输出流，用于输出数据 OutputStream os = socket.getOutputStream(); // 3.创建输入流，可以使用BufferedInputStream包装 FileInputStream fis = new FileInputStream(new File(&quot;beauty.jpg&quot;)); // 4.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = fis.read(buffer)) != -1) &#123; os.write(buffer, 0, len); &#125; // 关闭数据的输出，表示客服端数据传输已经完成，提醒服务端不必继续等待 // 如果不执行此操作，服务器端会一直阻塞 socket.shutdownOutput(); // 5.接收来自于服务器端的数据，并显示到控制台上 InputStream is = socket.getInputStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte[] bufferr = new byte[20]; int len1; while ((len1 = is.read(buffer)) != -1) &#123; baos.write(buffer, 0, len1); &#125; System.out.println(baos.toString()); // 6.关闭资源 baos.close(); fis.close(); os.close(); socket.close(); &#125; /* 服务端 这里涉及到的异常，应该使用try-catch-finally处理 */ @Test public void server() throws IOException &#123; // 1.创建服务器端的ServerSocket，指明自己的端口号 ServerSocket ss = new ServerSocket(9090); // 2.调用accept()表示接收来自于客户端的socket Socket socket = ss.accept(); // 3.获取输入流 InputStream is = socket.getInputStream(); // 4.创建输出流，可以使用BufferedOutputStream包装 FileOutputStream fos = new FileOutputStream(new File(&quot;beauty1.jpg&quot;)); // 5.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123;// read()是一个阻塞式方法 fos.write(buffer, 0, len); &#125; System.out.println(&quot;图片传输完成&quot;); // 6.服务器端给予客户端反馈 OutputStream os = socket.getOutputStream(); os.write(&quot;你好，客户端，照片已收到！&quot;.getBytes()); // 7.关闭资源 os.close(); fos.close(); is.close(); socket.close(); ss.close(); &#125;&#125; UDP 网络编程 DatagramSocket 类和 DatagramPacket 类实现了基于 UDP 协议的网络编程。 UDP 数据报通过数据报套接字 DatagramSocket 发送和接收，系统不保证 UDP 数据报一定能够安全送到目的地，也不能确定什么时候可以抵达。 DatagramPacket 对象封装了 UDP 数据报，在数据报中包含了发送端的 IP 地址和端口号以及接收端的 IP 地址和端口号。 UDP 协议中每个数据报都给出了完整的地址信息，因此无须建立发送方和接收方的连接。如同发快递包裹一样。 DatagramSocket 类的常用方法： public DatagramSocket(int port)：创建数据报套接字并将其绑定到本地主机上的指定端口。套接字将被绑定到通配符地址，IP 地址由内核来选择。 public DatagramSocket(int port,InetAddress laddr)：创建数据报套接字，将其绑定到指定的本地地址。本地端口必须在 0 到 65535 之间 (包括两者)。如果 IP 地址为 0.0.0.0，套接字将被绑定到通配符地址，IP 地址由内核选择。 public void close()：关闭此数据报套接字。 public void send(DatagramPacket p)：从此套接字发送数据报包。DatagramPacket 包含的信息指示：将要发送的数据、数据长度、远程主机的 IP 地址和远程主机的端口号。 public void receive(DatagramPacket p)：从此套接字接收数据报包。当此方法返回时，DatagramPacket 的缓冲区填充了接收的数据。数据报包也包含发送方的 IP 地址和发送方机器上的端口号。此方法在接收到数据报前一直阻塞。数据报包对象的 length 字段包含所接收信息的长度。如果信息比包的长度长，该信息将被截短。 public InetAddress getLocalAddress()：获取套接字绑定的本地地址。 public int getLocalPort()：返回此套接字绑定的本地主机上的端口号。 public InetAddress getInetAddress()：返回此套接字连接的地址。如果套接字未连接，则返回 null。 public int getPort()：返回此套接字的端口。如果套接字未连接，则返回 -1。 DatagramPacket 类的常用方法： public DatagramPacket(byte[] buf,int length)：构造 DatagramPacket，用来接收长度为 length 的数据包。 length 参数必须小于等于 buf.length()。 public DatagramPacket(byte[] buf,int length,InetAddress address,int port)：构造数据报包，用来将长度为 length 的包发送到指定主机上的指定端口号。length 参数必须小于等于 buf.length()。 public InetAddress getAddress()：返回某台机器的 IP 地址，此数据报将要发往该机器或者是从该机器接收到的。 public int getPort()：返回某台远程主机的端口号，此数据报将要发往该主机或者是从该主机接收到的。 public byte[] getData()：返回数据缓冲区。接收到的或将要发送的数据从缓冲区中的偏移量 offset 处开始，持续 length 长度。 public int getLength()：返回将要发送或接收到的数据的长度。 UDP 网络通信流程： DatagramSocket 与 DatagramPacket。 建立发送端，接收端， 发送端与接收端是两个独立的运行程序。 建立数据包。 调用 Socket 的发送、接收方法。 关闭 Socket。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class UDPTest &#123; /* 发送端 注意：发送端发送数据，是不管接收端能不能收到，为了保证接收端能收到数据，应该先启动接收端。 */ @Test public void sender() &#123; DatagramSocket socket = null; try &#123; socket = new DatagramSocket(); String str = &quot;我是UDP方式发送的数据&quot;; byte[] data = str.getBytes(); InetAddress inet = InetAddress.getLocalHost(); // 封装数据报，发送到本机的9090端口 DatagramPacket packet = new DatagramPacket(data, 0, data.length, inet, 9090); socket.send(packet); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (socket != null) &#123; socket.close(); &#125; &#125; &#125; /* 接收端 注意：在接收端，要指定监听的端口。 */ @Test public void receiver() &#123; DatagramSocket socket = null; try &#123; socket = new DatagramSocket(9090); byte[] buffer = new byte[100]; DatagramPacket packet = new DatagramPacket(buffer, 0, buffer.length); socket.receive(packet); System.out.println(new String(packet.getData(), 0, packet.getLength())); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (socket != null) &#123; socket.close(); &#125; &#125; &#125;&#125; URL 网络编程 URL (Uniform Resource Locator)：统一资源定位符，它表示 Internet 上某一资源的地址。 它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。 通过 URL 我们可以访问 Internet 上的各种网络资源，比如最常见的 www，ftp 站点。浏览器通过解析给定的 URL 可以在网络上查找相应的文件或其他资源。 URL 的基本结构由 5 部分组成：**&lt;传输协议&gt;://&lt; 主机名&gt;:&lt; 端口号&gt;/&lt; 文件名&gt;#片段名?参数列表**。 例如：http://192.168.1.100:8080/helloworld/index.jsp#a?username=shkstart&amp;password=123 #片段名：即锚点，例如看小说，直接定位到章节 参数列表格式：参数名=参数值&amp;参数名=参数值…. 为了表示 URL，java.net 中实现了类 URL。我们可以通过下面的构造器来初始化一个 URL 对象： **public URL (String spec)**：通过一个表示 URL 地址的字符串可以构造一个 URL 对象。例如：URL url = new URL(&quot;http://www. atguigu.com/&quot;);。 **public URL(URL context, String spec)**：通过基 URL 和相对 URL 构造一个 URL 对象。例如：URL downloadUrl = new URL(url, &quot;download.html&quot;);。 public URL(String protocol, String host, String file)：例如：new URL(&quot;http&quot;,&quot;www.atguigu.com&quot;, “download. html&quot;);。 public URL(String protocol, String host, int port, String file)：例如：URL gamelan = new URL(&quot;http&quot;, &quot;www.atguigu.com&quot;, 80, “download.html&quot;);。 URL 类的构造器都声明抛出非运行时异常，必须要对这一异常进行处理，通常使用 try - catch 语句进行捕获。 一个 URL 对象生成后，其属性是不能被改变的，但可以通过它给定的方法来获取这些属性： public String getProtocol()：获取该 URL 的协议名 public String getHost()：获取该 URL 的主机名。 public String getPort()：获取该 URL 的端口号。 public String getPath()：获取该 URL 的文件路径。 public String getFile()：获取该 URL 的文件名。 public String getQuery()：获取该 URL 的查询名。 实例： 1234567891011121314151617181920212223242526272829/** * URL网络编程 * 1.URL：统一资源定位符，对应着互联网的某一资源地址 * 2.格式： * http://localhost:8080/examples/beauty.jpg?username=Tom * 协议 主机名 端口号 资源地址 参数列表 */public class URLTest &#123; public static void main(String[] args) &#123; try &#123; URL url = new URL(&quot;http://localhost:8080/examples/beauty.jpg?username=Tom&quot;); // public String getProtocol(): 获取该URL的协议名 System.out.println(url.getProtocol());// http // public String getHost(): 获取该URL的主机名 System.out.println(url.getHost());// localhost // public String getPort(): 获取该URL的端口号 System.out.println(url.getPort());// 8080 // public String getPath(): 获取该URL的文件路径 System.out.println(url.getPath());// /examples/beauty.jpg // public String getFile(): 获取该URL的文件名 System.out.println(url.getFile());// /examples/beauty.jpg?username=Tom // public String getQuery(): 获取该URL的查询名 System.out.println(url.getQuery());// username=Tom &#125; catch (MalformedURLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; URL 的方法 openStream()：能从网络上读取数据。 若希望输出数据，例如向服务器端的 CGI (公共网关接口 Common Gateway Interface 的简称，是用户浏览器和服务器端的应用程序进行连接的接口) 程序发送一些数据，则必须先与 URL 建立连接，然后才能对其进行读写，此时需要使用 URLConnection 类。 URLConnection：表示到 URL 所引用的远程对象的连接。当与一个 URL 建立连接时，首先要在一个 URL 对象上通过方法 openConnection() 生成对应的 URLConnection 对象。如果连接过程失败，将产生 IOException。比如： 12URL netchinaren = new URL (&quot;http://www.atguigu.com/index.shtml&quot;);URLConnectonn u = netchinaren.openConnection(); 通过 URLConnection 对象获取的输入流和输出流，即可以与现有的 CGI 程序进行交互。 public Object getContent() throws IOException public int getContentLength() public String getContentType() public long getDate() public long getLastModified() public InputStream getInputStream()throws IOException public OutputSteram getOutputStream()throws IOException 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class URLTest &#123; public static void main(String[] args) &#123; HttpURLConnection urlConnection = null; InputStream is = null; FileOutputStream fos = null; try &#123; URL url = new URL(&quot;http://localhost:8080/examples/beauty.jpg&quot;); urlConnection = (HttpURLConnection) url.openConnection(); urlConnection.connect(); is = urlConnection.getInputStream(); fos = new FileOutputStream(&quot;day10\\\\beauty3.jpg&quot;); byte[] buffer = new byte[1024]; int len; while ((len = is.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; System.out.println(&quot;下载完成&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭资源 if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (is != null) &#123; try &#123; is.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (urlConnection != null) &#123; urlConnection.disconnect(); &#125; &#125; &#125;&#125; URI 、URL 和URN的区别： URI，是 uniform resource identifier，统一资源标识符，用来唯一的标识一个资源。而 URL 是 uniform resource locator，统一资源定位符，它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。而 URN，是 uniform resource name，统一资源命名，是通过名字来标识资源，比如 mailto:&#106;&#x61;&#x76;&#x61;&#x2d;&#x6e;&#x65;&#116;&#x40;&#106;&#x61;&#x76;&#x61;&#x2e;&#x73;&#x75;&#110;&#x2e;&#x63;&#x6f;&#x6d;。也就是说，URI 是以一种抽象的，高层次概念定义统一资源标识，而 URL 和 URN 则是具体的资源标识的方式。URL 和 URN 本身也都是一种 URI。 在 Java 的 URI 中，一个 URI 实例可以代表绝对的，也可以是相对的，只要它符合 URI 的语法规则。而 URL 类则不仅符合语义，还包含了定位该资源的信息，因此它不能是相对的。 总结 位于网络中的计算机具有唯一的 IP 地址，这样不同的主机可以互相区分。 客户端－服务器是一种最常见的网络应用程序模型。服务器是一个为其客户端提供某种特定服务的硬件或软件。客户机是一个用户应用程序，用于访问某台服务器提供的服务。端口号是对一个服务的访问场所，它用于区分同一物理计算机上的多个服务。套接字用于连接客户端和服务器，客户端和服务器之间的每个通信会话使用一个不同的套接字。TCP 协议用于实现面向连接的会话。 Java 中有关网络方面的功能都定义在 java.net 程序包中。Java 用 InetAddress 对象表示 IP 地址，该对象里有两个字段：主机名 (String) 和 IP 地址 (int)。 类 Socket 和 ServerSocket 实现了基于 TCP 协议的客户端－服务器程序。Socket 是客户端和服务器之间的一个连接，连接创建的细节被隐藏了。这个连接提供了一个安全的数据传输通道，这是因为 TCP 协议可以解决数据在传送过程中的丢失、损坏、重复、乱序以及网络拥挤等问题，它保证数据可靠的传送。 类 URL 和 URLConnection 提供了最高级网络应用。URL 的网络资源的位置来统一标识 Internet 上各种网络资源。通过 URL 对象可以创建当前应用程序和 URL 表示的网络资源之间的连接，这样当前程序就可以读取网络资源数据，或者把自己的数据传送到网络上去。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的泛型","slug":"java-generic","date":"2021-03-26T09:16:16.000Z","updated":"2021-04-09T08:00:45.251Z","comments":true,"path":"2021/03/26/java-generic/","link":"","permalink":"http://example.com/2021/03/26/java-generic/","excerpt":"","text":"泛型的设计背景集合容器类在设计阶段/声明阶段不能确定这个容器到底实际存的是什么类型的对象，所以在 JDK 1.5 之前只能把元素类型设计为 Object，JDK 1.5 之后使用泛型来解决。因为这个时候除了元素的类型不确定，其他的部分是确定的，例如关于这个元素如何保存，如何管理等是确定的，因此此时把元素的类型设计成一个参数，这个类型参数叫做泛型。Collection&lt;E&gt;，List&lt;E&gt;，ArrayList&lt;E&gt; 中的这个 &lt;E&gt; 就是类型参数，即泛型。 泛型的概念 所谓泛型，就是允许在定义类、接口时通过一个标识表示类中某个属性的类型或者是某个方法的返回值及参数类型。这个类型参数将在使用时（例如，继承或实现这个接口，用这个类型声明变量、创建对象时）确定（即传入实际的类型参数，也称为类型实参）。 从 JDK 1.5 以后，Java 引入了 “参数化类型” (Parameterized type) 的概念，允许在创建集合时指定集合元素的类型，如：List&lt;String&gt;，表明该 List 只能保存字符串类型的对象。 JDK 1.5 改写了集合框架中的全部接口和类，为这些接口、类增加了泛型支持，从而可以在声明集合变量、创建集合对象时传入类型实参。 在实例化集合类时，可以指明具体的泛型类型。指明完以后，在集合类或接口中凡是定义类或接口时，内部结构 (比如：方法、构造器、属性等) 使用到类的泛型的位置，都指定为实例化的泛型类型。比如：add(E e) —&gt; 实例化以后：add(Integer e)。 在实例化集合类时，如果没有指明泛型的类型，默认类型为 java.lang.Object 类型。 泛型的类型必须是类，不能是基本数据类型。需要用到基本数据类型的位置，拿包装类替换。 使用泛型的必要性： 解决元素存储的安全性问题。 解决获取数据元素时，需要类型强制转换的问题。 Java 泛型可以保证如果程序在编译时没有发出警告，运行时就不会产生 ClassCastException 异常。同时，代码更加简洁、健壮。 使用泛型的主要优点是能够在编译时而不是在运行时检测错误 在集合中使用泛型之前的情况： 12345678910111213141516171819 public class Test &#123; public static void main(String[] args) &#123; // 在集合中使用泛型之前的情况： ArrayList list = new ArrayList(); // 需求：存放学生的成绩 list.add(78); list.add(76); list.add(89); list.add(88); // 问题一：类型不安全 // list.add(&quot;Tom&quot;); for (Object score : list) &#123; // 问题二：强转时，可能出现ClassCastException int stuScore = (Integer) score; System.out.println(stuScore); &#125; &#125;&#125; 在集合中使用泛型的情况，以 ArrayList 为例： 123456789101112131415161718192021222324252627 public class Test &#123; public static void main(String[] args) &#123; // ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); // jdk7新特性：类型推断 ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(78); list.add(87); list.add(99); list.add(65); // 编译时，就会进行类型检查，类型不一致时编译不通过，保证数据的安全 // list1.add(&quot;Tom&quot;); // 方式一： for (Integer score : list) &#123; // 避免了强转操作 int stuScore = score; System.out.println(stuScore); &#125; // 方式二： Iterator&lt;Integer&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; int stuScore = iterator.next(); System.out.println(stuScore); &#125; &#125;&#125; 在集合中使用泛型的情况，以 HashMap 为例： 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; // Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); // jdk7新特性：类型推断 Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;Tom&quot;, 87); map.put(&quot;Jerry&quot;, 87); map.put(&quot;Jack&quot;, 67); // map.put(123,&quot;ABC&quot;); // 泛型的嵌套 Set&lt;Map.Entry&lt;String, Integer&gt;&gt; entries = map.entrySet(); Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; iterator = entries.iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; entry = iterator.next(); String key = entry.getKey(); Integer value = entry.getValue(); System.out.println(key + &quot;----&quot; + value); &#125; &#125;&#125; 自定义泛型结构 自定义泛型类和接口： 泛型类和接口的声明：class GenericClass&lt;K, V&gt; 和 interface GenericInterface&lt;T&gt;。其中，K，V，T 不代表值，而是表示类型，可以使用任意字母，常用 T 表示，是 Type 的缩写。 123456789101112131415161718192021public class Person&lt;T&gt; &#123; // 使用T类型定义变量 private T info; // 使用T类型定义一般方法 public T getInfo() &#123; return info; &#125; public void setInfo(T info) &#123; this.info = info; &#125; // 使用T类型定义构造器 public Person() &#123; &#125; public Person(T info) &#123; this.info = info; &#125;&#125; 泛型类和接口可能有多个参数，此时应将多个参数一起放在尖括号内，以逗号隔开。比如：&lt;E1, E2, E3&gt;。 泛型类的构造器如下：public GenericClass()&#123;&#125;。而下面是错误的：public GenericClass&lt;E&gt;()&#123;&#125;。 泛型类的实例化：如果定义的类是带泛型的，在实例化时应该指明类的泛型。如：List&lt;String&gt; strList = new ArrayList&lt;String&gt;();。 泛型如果不指定，将被擦除，泛型对应的类型均按照 Object 处理，但不等价于 Object。经验：泛型要使用一路都用。要不用，一路都不要用。 指定泛型时，不能使用基本数据类型，可以使用包装类替换。 把一个集合中的内容限制为一个特定的数据类型，这就是 Generic 背后的核心思想。 泛型类实例化后，操作原来泛型位置的结构必须与指定的泛型类型一致。 泛型不同的引用不能相互赋值。 尽管在编译时 ArrayList&lt;String&gt; 和 ArrayList&lt;Integer&gt; 是两种类型，但是，在运行时只有一个 ArrayList 被加载到 JVM 中。 如果泛型结构是一个接口或抽象类，则不可创建泛型类的对象。 JDK 7.0 开始，泛型的简化操作：ArrayList&lt;Fruit&gt; flist = new ArrayList&lt;&gt;();，类型推断。 在类/接口上声明的泛型，在本类或本接口中即代表某种类型，可以作为非静态属性的类型、非静态方法的参数类型、非静态方法的返回值类型。但在静态方法中不能使用类的泛型。 1234567891011121314151617public class Order&lt;T&gt; &#123; String orderName; int orderId; // 类的内部结构就可以使用类的泛型 T orderT; public Order(String orderName, int orderId, T orderT) &#123; this.orderName = orderName; this.orderId = orderId; this.orderT = orderT; &#125; // 静态方法中不能使用类的泛型，编译不通过 /*public static void show(T orderT) &#123; System.out.println(orderT); &#125;*/&#125; 异常类不能声明为泛型类。 12// 异常类不能声明为泛型类，编译不通过public class MyException&lt;T&gt; extends Exception &#123;&#125; 12345678public class Order&lt;T&gt; &#123; public void show() &#123; // try-catch结构中不能使用类的泛型，编译不通过 try &#123; &#125; catch (T t) &#123; &#125; &#125;&#125; 不能使用 new E[]，但是可以：E[] elements = (E[])new Object[capacity];。参考 ArrayList 源码中声明：Object[] elementData，而非泛型参数类型数组。 12345678public class Order&lt;T&gt; &#123; public Order() &#123; // 编译不通过 // T[] arr = new T[10]; // 编译通过 T[] arr = (T[]) new Object[10]; &#125;&#125; 父类有泛型，子类可以选择保留泛型也可以选择指定泛型类型： 子类不保留父类的泛型：按需实现。 没有类型 擦除。 具体类型。 子类保留父类的泛型：泛型子类。 全部保留。 部分保留。 子类除了指定或保留父类的泛型，还可以增加自己的泛型。 实例： 子类不增加自己的泛型： 123456789101112131415class Father&lt;T1, T2&gt; &#123;&#125;// 子类不保留父类的泛型：// 1)没有类型 擦除class Son1 extends Father &#123;&#125;// 等价于class Son1 extends Father&lt;Object, Object&gt;// 2)具体类型class Son2 extends Father&lt;Integer, String&gt; &#123;&#125;// 子类保留父类的泛型：// 1)全部保留class Son3&lt;T1, T2&gt; extends Father&lt;T1, T2&gt; &#123;&#125;// 2)部分保留class Son4&lt;T2&gt; extends Father&lt;Integer, T2&gt; &#123;&#125; 子类增加自己的泛型： 123456789101112131415class Father&lt;T1, T2&gt; &#123;&#125;// 子类不保留父类的泛型：// 1)没有类型 擦除class Son1&lt;A, B&gt; extends Father &#123;&#125;//等价于class Son extends Father&lt;Object, Object&gt;// 2)具体类型class Son2&lt;A, B&gt; extends Father&lt;Integer, String&gt; &#123;&#125; // 子类保留父类的泛型// 1)全部保留class Son3&lt;T1, T2, A, B&gt; extends Father&lt;T1, T2&gt; &#123;&#125;// 2)部分保留class Son4&lt;T2, A, B&gt; extends Father&lt;Integer, T2&gt; &#123;&#125; 如果子类在继承带泛型的父类时，指明了泛型类型，则实例化子类对象时，不再需要指明泛型 123public class Order&lt;T&gt; &#123;&#125;public class SubOrder extends Order&lt;Integer&gt; &#123;&#125;// SubOrder: 不是泛型类 如果子类在继承带泛型的父类时，未指明泛型类型，则实例化子类对象时，仍然需要指明泛型。 123public class Order&lt;T&gt; &#123;&#125;public class SubOrder1&lt;T&gt; extends Order&lt;T&gt; &#123;&#125;// SubOrder1&lt;T&gt;: 仍然是泛型类 自定义泛型方法 泛型方法的格式： 泛型方法的参数与类的泛型参数没有任何关系， 换句话说，泛型方法所属的类是不是泛型类都没有关系。泛型方法，可以声明为静态的。原因：泛型参数是在调用方法时确定的，并非在实例化类时确定。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class Order&lt;T&gt; &#123; String orderName; int orderId; // 类的内部结构就可以使用类的泛型 T orderT; // 如下的三个方法都不是泛型方法 public T getOrderT() &#123; return orderT; &#125; public void setOrderT(T orderT) &#123; this.orderT = orderT; &#125; @Override public String toString() &#123; return &quot;Order&#123;&quot; + &quot;orderName=&#x27;&quot; + orderName + &#x27;\\&#x27;&#x27; + &quot;, orderId=&quot; + orderId + &quot;, orderT=&quot; + orderT + &#x27;&#125;&#x27;; &#125; // 泛型方法：在方法中出现了泛型的结构，泛型参数与类的泛型参数没有任何关系。 // 换句话说，泛型方法所属的类是不是泛型类都没有关系。 public &lt;E&gt; List&lt;E&gt; copyFromArrayToList(E[] arr) &#123; ArrayList&lt;E&gt; list = new ArrayList&lt;&gt;(); list.addAll(Arrays.asList(arr)); return list; &#125; // 泛型方法，可以声明为静态的。原因：泛型参数是在调用方法时确定的。并非在实例化类时确定。 public static &lt;T&gt; void fromArrayToCollection(T[] a, Collection&lt;T&gt; c) &#123; Collections.addAll(c, a); System.out.println(c); &#125; public static void main(String[] args) &#123; Order&lt;String&gt; order = new Order&lt;&gt;(); Integer[] arr = new Integer[]&#123;1, 2, 3, 4&#125;; // 泛型方法在调用时，指明泛型参数的类型 List&lt;Integer&gt; list = order.copyFromArrayToList(arr); System.out.println(list);// [1, 2, 3, 4] ArrayList&lt;String&gt; str = new ArrayList&lt;&gt;(); String[] strings = new String[]&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;&#125;; fromArrayToCollection(strings, str);// [A, B, C, D] Object[] ao = new Object[100]; Collection&lt;Object&gt; co = new ArrayList&lt;&gt;(); fromArrayToCollection(ao, co); String[] sa = new String[20]; Collection&lt;String&gt; cs = new ArrayList&lt;&gt;(); fromArrayToCollection(sa, cs); Collection&lt;Double&gt; cd = new ArrayList&lt;&gt;(); // 下面代码中T是Double类，但sa是String类型，编译错误。 // fromArrayToCollection(sa, cd); // 下面代码中T是Object类型，sa是String类型，可以赋值成功。 fromArrayToCollection(sa, co); &#125;&#125; 泛型方法声明泛型时也可以指定上限： 父类： 1234567891011121314151617181920212223242526272829public class DAO&lt;T&gt; &#123;// 不同表的共性操作的DAO，DAO：data(base) access object // 添加一条记录 public void add(T t) &#123;&#125; // 删除一条记录 public boolean remove(int index) &#123; return false; &#125; // 修改一条记录 public void update(int index, T t) &#123;&#125; // 查询一条记录 public T getIndex(int index) &#123; return null; &#125; // 查询多条记录 public List&lt;T&gt; getForList(int index) &#123; return null; &#125; // 泛型方法：因为返回内容在DAO类中无法确定，由子类自己指定 // 举例：获取表中一共有多少条记录？获取最大的员工入职时间？ public &lt;E&gt; E getValue() &#123; return null; &#125;&#125; 子类 StudentDao： 1public class StudentDAO extends DAO&lt;Student&gt; &#123;&#125;// 只能操作Student表的DAO 子类 CustomerDao： 1public class CustomerDAO extends DAO&lt;Customer&gt;&#123;&#125;// 只能操作Customer表的DAO 泛型在继承上的体现 如果 B 是 A 的一个子类型 (子类或者子接口)，而 G 是具有泛型声明的类或接口，则 G&lt;B&gt; 并不是 G&lt;A&gt; 的子类型，二者是并列关系。如果类 A 是类 B 的父类，则 A&lt;G&gt; 是 B&lt;G&gt; 的父类。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test &#123; public static void show(List&lt;Object&gt; list) &#123; &#125; public static void show1(List&lt;String&gt; list) &#123; &#125; public static void main(String[] args) &#123; // 子类对象赋值给父类对象---&gt;编译通过 Object obj = null; String str = null; obj = str;// Object是String的父类 // 子类对象数组赋值给父类对象数组---&gt;编译通过 Object[] arr1 = null; String[] arr2 = null; arr1 = arr2;// Object[]是String[]的父类 // 虽然类A是类B的父类，但是G&lt;A&gt;和G&lt;B&gt;二者不具备子父类关系，二者是并列关系 List&lt;Object&gt; list1 = null; List&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); // 编译不通过此时的list1和list2的类型不具有子父类关系 // list1 = list2; /* 反证法： 假设list1 = list2; list1.add(123);导致混入非String的数据。出错。 */ show(list1); show1(list2); // 补充：类A是类B的父类，则A&lt;G&gt;是B&lt;G&gt;的父类 List&lt;String&gt; list3 = null; AbstractList&lt;String&gt; list4 = null; ArrayList&lt;String&gt; list5 = null; list3 = list5; list4 = list5; &#125;&#125; 通配符的使用 通配符：？。 如果类 A 是类 B 的父类，G&lt;A&gt; 和 G&lt;B&gt; 是没有关系的，二者共同的父类是：G&lt;?&gt;。比如：List&lt;?&gt;，Map&lt;?, ?&gt;。其中，List&lt;?&gt; 是 List&lt;String&gt;、List&lt;Object&gt; 等各种泛型 List 的父类，Map&lt;?, ?&gt; 是各种泛型 Map 的父类。 12345678910111213141516171819202122public class Test &#123; public static void print(List&lt;?&gt; list) &#123; Iterator&lt;?&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; Object obj = iterator.next(); System.out.println(obj); &#125; &#125; public static void main(String[] args) &#123; List&lt;?&gt; list; List&lt;Object&gt; list1 = null; List&lt;String&gt; list2 = null; list = list1; list = list2; // 编译通过 print(list1); print(list2); &#125;&#125; 对于 List&lt;?&gt;，不能向其内部添加元素，因为不知道 List 中存储的元素的类型。 唯一的例外是 null，它是所有类型的成员。 读取 List&lt;?&gt; 中的元素时，永远是安全的，因为不管 List 中元素的真实类型是什么，都是一个 Object。 实例： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) &#123; List&lt;?&gt; list; List&lt;String&gt; list1 = new ArrayList&lt;&gt;(); list1.add(&quot;AA&quot;); list1.add(&quot;BB&quot;); list1.add(&quot;CC&quot;); list = list1; // 添加(写入)：对于List&lt;?&gt;不能向其内部添加元素，因为不知道List中存储的元素的类型 // 除了添加null之外，其他的都无法添加，编译不通过 // list.add(&quot;DD&quot;); // list.add(&#x27;?&#x27;); list.add(null); // 获取(读取)：允许读取List&lt;?&gt;元素，因为读取的元素，不论其类型为什么，其父类都是Object Object o = list.get(0); System.out.println(o); &#125;&#125; 通配符使用的注意事项： 有限制的通配符： 通配符指定上限 extends：使用时指定的类型必须是继承某个类，或者实现某个接口，即 &lt;=。 ? extends A：G&lt;? extends A&gt; 可以作为 G&lt;A&gt; 和 G&lt;B&gt; 的父类，其中 B 是 A 的子类。 通配符指定下限 super：使用时指定的类型不能小于操作的类，即 &gt;=。 ? super A：G&lt;? super A&gt; 可以作为 G&lt;A&gt; 和 G&lt;B&gt; 的父类，其中 B 是 A 的父类。 实例： 1234567891011121314151617181920public class Test &#123; public static void printCollection3(Collection&lt;? extends Person&gt; coll) &#123; // Iterator只能用Iterator&lt;?&gt;或Iterator&lt;? extends Person&gt;.why? Iterator&lt;?&gt; iterator = coll.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125; public static void printCollection4(Collection&lt;? super Person&gt; coll) &#123; // Iterator只能用Iterator&lt;?&gt;或Iterator&lt;? super Person&gt;.why? Iterator&lt;?&gt; iterator = coll.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125;class Person &#123;&#125; 读取和添加元素： 1public class Person &#123;&#125; 1public class Student extends Person &#123;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041public class Test &#123; public static void main(String[] args) &#123; List&lt;? extends Person&gt; list1; List&lt;? super Person&gt; list2; List&lt;Student&gt; list3 = new ArrayList&lt;&gt;(); List&lt;Person&gt; list4 = new ArrayList&lt;&gt;(); List&lt;Object&gt; list5 = new ArrayList&lt;&gt;(); list1 = list3; list1 = list4; // 编译不通过，因为Object &gt; Person // list1 = list5; // 编译不通过，因为Student &lt; Person // list2 = list3; list2 = list4; list2 = list5; // 读取数据：读出的元素定义为最大的类型 list1 = list3; Person p = list1.get(0); // 编译不通过，因为读出来的元素不一定是Student，也可能是Student的父类，但肯定是Person的子类 // Student s = list1.get(0); list2 = list4; Object obj = list2.get(0); // 编译不通过，因为读出来的元素不一定是Person，也可能是Person的父类，但肯定是Object的子类 // Person obj = list2.get(0); // 写入数据： // 编译不通过，因为list1是(-∞,Person]，添加的元素，可能其类型比Person或Student小，因此除了null都不能添加 // list1.add(new Person()); // list1.add(new Student()); list1.add(null); // 编译通过，因为list2是[Person,+∞)，无论是什么元素，都肯定是Person或其父类，那么Person及其子类都能添加 list2.add(new Person()); list2.add(new Student()); &#125;&#125; 泛型嵌套： 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; HashMap&lt;String, ArrayList&lt;Person&gt;&gt; map = new HashMap&lt;&gt;(); ArrayList&lt;Person&gt; list = new ArrayList&lt;Person&gt;(); list.add(new Person(&quot;AA&quot;)); list.add(new Person(&quot;BB&quot;)); list.add(new Person(&quot;ab&quot;)); map.put(&quot;AA&quot;, list); Set&lt;Map.Entry&lt;String, ArrayList&lt;Person&gt;&gt;&gt; entrySet = map.entrySet(); Iterator&lt;Map.Entry&lt;String, ArrayList&lt;Person&gt;&gt;&gt; iterator = entrySet.iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;String, ArrayList&lt;Person&gt;&gt; entry = iterator.next(); String key = entry.getKey(); ArrayList&lt;Person&gt; value = entry.getValue(); System.out.println(&quot;户主：&quot; + key); System.out.println(&quot;家庭成员：&quot; + value); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123// 只有此接口的子类才是表示人的信息interface Info &#123;&#125;// 表示联系方式class Contact implements Info &#123; private String address;// 联系地址 private String telephone;// 联系方式 private String zipcode;// 邮政编码 public Contact(String address, String telephone, String zipcode) &#123; this.address = address; this.telephone = telephone; this.zipcode = zipcode; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public void setTelephone(String telephone) &#123; this.telephone = telephone; &#125; public void setZipcode(String zipcode) &#123; this.zipcode = zipcode; &#125; public String getAddress() &#123; return this.address; &#125; public String getTelephone() &#123; return this.telephone; &#125; public String getZipcode() &#123; return this.zipcode; &#125; @Override public String toString() &#123; return &quot;Contact [address=&quot; + address + &quot;, telephone=&quot; + telephone + &quot;, zipcode=&quot; + zipcode + &quot;]&quot;; &#125;&#125;// 表示个人信息class Introduction implements Info &#123; private String name;// 姓名 private String sex;// 性别 private int age;// 年龄 public Introduction(String name, String sex, int age) &#123; this.name = name; this.sex = sex; this.age = age; &#125; public void setName(String name) &#123; this.name = name; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return this.name; &#125; public String getSex() &#123; return this.sex; &#125; public int getAge() &#123; return this.age; &#125; @Override public String toString() &#123; return &quot;Introduction [name=&quot; + name + &quot;, sex=&quot; + sex + &quot;, age=&quot; + age + &quot;]&quot;; &#125;&#125;class Person&lt;T extends Info&gt; &#123; private T info; // 通过构造器设置信息属性内容 public Person(T info) &#123; this.info = info; &#125; public void setInfo(T info) &#123; this.info = info; &#125; public T getInfo() &#123; return info; &#125; @Override public String toString() &#123; return &quot;Person [info=&quot; + info + &quot;]&quot;; &#125;&#125;public class GenericPerson &#123; public static void main(String args[]) &#123; Person&lt;Contact&gt; per = null;// 声明Person对象 per = new Person&lt;Contact&gt;(new Contact(&quot;北京市&quot;, &quot;01088888888&quot;, &quot;102206&quot;)); System.out.println(per); Person&lt;Introduction&gt; per2 = null;// 声明Person对象 per2 = new Person&lt;Introduction&gt;(new Introduction(&quot;李雷&quot;, &quot;男&quot;, 24)); System.out.println(per2); &#125;&#125; 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的集合","slug":"java-collectionandmap","date":"2021-03-20T06:51:43.000Z","updated":"2021-04-09T08:00:04.931Z","comments":true,"path":"2021/03/20/java-collectionandmap/","link":"","permalink":"http://example.com/2021/03/20/java-collectionandmap/","excerpt":"","text":"java 集合框架概述 面向对象语言是以对象的形式来对事物进行体现，为了方便对多个对象的操作，就需要对对象进行存储。 在 java 语言中，数组 (Array) 和集合都是对多个数据进行存储操作的结构，简称 java 容器。此时的存储，主要指的是内存层面的存储，不涉及到持久化的存储。 数组在内存存储方面的特点： 数组一旦初始化以后，其长度就确定了。 数组一旦定义好，其元素的类型也就确定了。 数组在存储数据方面的弊端： 数组一旦初始化以后，其长度就不可修改，不便于扩展。 数组中提供的属性和方法少，不便于进行添加、删除、插入等操作，且效率不高。 数组中没有现成的属性和方法，去直接获取数组中已存储的元素的个数 (只能直接知道数组的长度)。 数组存储的数据是有序的、可重复的。对于无序、不可重复的需求，不能满足，即数组存储数据的特点比较单一。 java 集合类可以用于存储数量不等的多个对象，还可用于保存具有映射关系的关联数组。 java 集合框架可分为 Collection 和 Map 两种体系： Collection 接口 ：单列集合，用来存储一个一个的对象CD 。 List 接口：存储有序的、可重复的数据。—&gt; “动态” 数组 ArrayList、LinkedList、Vector Set 接口：存储无序的、不可重复的数据。—&gt; 高中 “集合” HashSet、LinkedHashSet、TreeSet Map 接口：双列集合，用来存储具有映射关系 “key - value 对” 的数据。—&gt; 高中 “函数” HashMap、LinkedHashMap、TreeMap、Hashtable、Properties Collection 接口继承树： Map 接口继承树： Collection 接口 Collection 接口是 List、Set 和 Queue 接口的父接口，该接口里定义的方法既可用于操作 Set 集合，也可用于操作 List 和 Queue 集合。 jdk 不提供此接口的任何直接实现，而是提供更具体的子接口实现，如：Set 和 List。 在 jdk 5.0 之前，java 集合会丢失容器中所有对象的数据类型，把所有对象都当成 Object 类型处理；从 jdk 5.0 增加了泛型以后，java 集合可以记住容器中对象的数据类型。 Collection 接口的方法： 添加元素： add(Object obj) addAll(Collection coll) 获取有效元素的个数： int size() 清空集合中的元素： void clear() 是否是空集合： boolean isEmpty() 是否包含某个元素： boolean contains(Object obj)：判断当前集合中是否包含 obj，通过 obj 的 equals() 来判断是否是同一个对象。 向 Collection 的实现类的对象中添加数据 obj 时，要求 obj 所在的类要重写 equals()，否则调用的是 Object 中的 equals()，即 ==。 boolean containsAll(Collection coll)：对两个集合的元素逐个比较，判断 coll 中的所有元素是否都存在于当前集合中，也是通过元素的 equals() 来比较的。 删除： boolean remove(Object obj)：从当前集合中移除 obj，通过 obj 的 equals() 判断是否是要删除的那个元素，只会删除找到的第一个元素。 boolean removeAll(Collection coll)：从当前集合中移除 coll 中的所有元素，即取当前集合的差集。 取两个集合的交集： boolean retainAll(Collection coll)：把交集的结果存在当前集合中，不影响 coll。 集合是否相等： boolean equals(Object obj)：如果返回 true，则 obj 首先得与当前集合类型相同。如果是 List，要求元素个数和顺序一致，如果是 Set，则不考虑顺序。 转成对象数组： Object[] toArray()：将当前集合转换为数组。 拓展：将数组转换为集合，Arrays.asList()，例如：List&lt;String&gt; strings = Arrays.asList(new String[]&#123;&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;&#125;);。使用此方法时的注意事项： 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; // toArray()：集合转换为数组 Object[] objects = collection.toArray(); System.out.println(Arrays.toString(objects)); // 拓展：数组转换为集合 List&lt;String&gt; strings = Arrays.asList(new String[]&#123;&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;&#125;); List&lt;Person&gt; people = Arrays.asList(new Person(), new Person()); List&lt;int[]&gt; ints = Arrays.asList(new int[]&#123;1, 2, 3&#125;); System.out.println(ints.size());// 1，含有一个int[]数组的集合 List&lt;Integer&gt; integers = Arrays.asList(1, 2, 3); System.out.println(integers.size());// 3，含有三个Integer元素的集合 &#125;&#125; 获取集合对象的哈希值： hashCode() 遍历： iterator()：返回 Iterator 接口的实例，即迭代器对象，用于遍历集合的元素。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); // add(Object obj)：将元素obj添加到集合collection中 collection.add(&quot;AA&quot;); collection.add(&quot;bb&quot;); collection.add(123);// 自动装箱 collection.add(new Date()); // size()：获取添加的元素的个数 System.out.println(collection.size());// 4 // addAll(Collection c)：将c集合中的元素添加到当前的集合中 Collection&lt;Object&gt; collection2 = new ArrayList&lt;&gt;(); collection2.add(456); collection2.add(&quot;CC&quot;); collection.addAll(collection2); System.out.println(collection2.size()); // clear()：清空集合中的元素 collection.clear(); // isEmpty()：判断当前集合是否为空 System.out.println(collection.isEmpty());// true // contains(Object obj)：判断当前集合是否包含obj collection.add(new String(&quot;Tom&quot;)); System.out.println(collection.contains(new String(&quot;Tom&quot;)));// true，比较的是内容 collection.add(new Person(&quot;Jerry&quot;, 20)); // true，如果Person未重写equals()，调用的是Object的方法，即==，返回false System.out.println(collection.contains(new Person(&quot;Jerry&quot;, 20))); // containsAll(Collection coll)：判断coll中的所有元素是否都存在于当前集合中 collection.add(123); collection.add(&quot;bb&quot;); System.out.println(collection.containsAll(Arrays.asList(123, &quot;bb&quot;, new Person(&quot;Jerry&quot;, 20))));// true // remove(Object obj)：从当前集合中移除obj System.out.println(collection.remove(new Person(&quot;Jerry&quot;, 20)));// true System.out.println(collection);// [Tom, 123, bb] // removeAll(Collection coll)：从当前集合中移除coll中的所有元素 System.out.println(collection.removeAll(Arrays.asList(&quot;bb&quot;, &quot;BB&quot;)));// true，移除了一个bb System.out.println(collection);// [Tom, 123, bb] // retainAll(Collection coll)：获取当前集合与coll的交集，并返回给当前集合 System.out.println(collection.retainAll(Arrays.asList(123, new Person(&quot;Jerry&quot;, 20), &quot;BB&quot;)));// true System.out.println(collection);// [123] // equals(Object obj)： collection.add(&quot;BB&quot;); System.out.println(collection);// [123, BB] System.out.println(collection.equals(Arrays.asList(123, &quot;BB&quot;)));// true // false，因为collection是List，元素是有序的 System.out.println(collection.equals(Arrays.asList(&quot;BB&quot;, 123))); // hashCode()：返回当前集合的哈希值 System.out.println(collection.hashCode()); // toArray()：集合转换为数组 Object[] objects = collection.toArray(); System.out.println(Arrays.toString(objects)); // 拓展：数组转换为集合 List&lt;String&gt; strings = Arrays.asList(new String[]&#123;&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;&#125;); List&lt;Person&gt; people = Arrays.asList(new Person(), new Person()); List&lt;int[]&gt; ints = Arrays.asList(new int[]&#123;1, 2, 3&#125;); System.out.println(ints.size());// 1，含有一个int[]数组的集合 List&lt;Integer&gt; integers = Arrays.asList(1, 2, 3); System.out.println(integers.size());// 3，含有三个Integer元素的集合 &#125;&#125;class Person &#123; private String name; private int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; Person person = (Person) o; return age == person.age &amp;&amp; Objects.equals(name, person.name); &#125; @Override public int hashCode() &#123; return Objects.hash(name, age); &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; Iterator 迭代器接口 Iterator对象称为迭代器 (设计模式的一种)，主要用于遍历 Collection 集合中的元素。 GOF 给迭代器模式的定义为：提供一种方法访问一个容器 (container) 对象中各个元素，而又不需暴露该对象的内部细节。迭代器模式，就是为容器而生。 Collection 接口继承了 java.lang.Iterable 接口，该接口有一个 iterator()，所有实现了 Collection 接口的集合类都有一个 iterator()，用以返回一个实现了 Iterator 接口的类的对象。 Iterator 仅用于遍历集合，Iterator 本身并不提供承装对象的能力。如果需要创建 Iterator 对象，则必须有一个被迭代的集合。(不适用于 Map) 集合对象每次调用 iterator() 都得到一个全新的迭代器对象，默认游标都在集合的第一个元素之前。 Iterator 接口的方法： 迭代器的执行原理： 在调用 it.next() 之前必须要调用 it.hasNext() 进行检测。若不调用，且下一条记录无效，直接调用 it.next() 会抛出 NoSuchElementException 异常。 Iterator 接口的 remove()： Iterator 可以删除集合的元素，但是是在遍历过程中通过迭代器对象的 remove() 删除的，不是集合对象的 remove()。 如果还未调用 next()，或在上一次调用 next() 之后已经调用了 remove()，则再次调用 remove() 都会抛出 IllegalStateException 异常。 实例： 1234567891011121314151617181920212223242526public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); collection.add(123); collection.add(456); collection.add(new String(&quot;Tom&quot;)); collection.add(false); Iterator&lt;Object&gt; iterator = collection.iterator(); while (iterator.hasNext()) &#123; // iterator.remove();// 游标处于集合的第一个元素之前，java.lang.IllegalStateException Object obj = iterator.next(); if (&quot;Tom&quot;.equals(obj)) &#123;// Tom放在前面，可以防止obj为null时触发空指针异常 iterator.remove(); // iterator.remove();// 游标所处位置的元素已经被remove，在该位置再次调用remove发生异常，java.lang.IllegalStateException &#125; &#125; // 遍历集合 iterator = collection.iterator();// 重新获取collection的迭代器对象，不能使用原来的，因为其游标已经移到集合末尾了 while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125; 实例： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); collection.add(123); collection.add(456); collection.add(new String(&quot;Tom&quot;)); collection.add(false); Iterator&lt;Object&gt; iterator = collection.iterator(); // 遍历 // hasNext()：判断是否还有下一个元素 while (iterator.hasNext()) &#123; // next()：1.指针下移;2.将下移以后集合位置上的元素返回 System.out.println(iterator.next()); &#125; &#125;&#125; 错误写法： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); collection.add(123); collection.add(456); collection.add(new String(&quot;Tom&quot;)); collection.add(false); Iterator&lt;Object&gt; iterator = collection.iterator(); // 错误写法一：间隔的输出集合中的元素，也会出现java.util.NoSuchElementException异常 while (iterator.next()!=null)&#123;// 游标下移一次 System.out.println(iterator.next());// 游标下移两次 &#125; // 错误写法二：死循环 while (collection.iterator().hasNext()) &#123; System.out.println(collection.iterator().hasNext()); &#125; &#125;&#125; 补充： Enumeration 接口是 Iterator 迭代器的古老版本。 123456789public class Test &#123; public static void main(String[] args) &#123; Enumeration stringEnum = new StringTokenizer(&quot;a-b*c-d-e-g&quot;, &quot;-&quot;); while (stringEnum.hasMoreElements()) &#123; Object obj = stringEnum.nextElement(); System.out.println(obj); &#125; &#125;&#125; foreach 循环 jdk 5.0 提供了 foreach 循环迭代访问 Collection 和数组。格式如下： foreach 对 Collection 或数组的遍历操作，不需获取 Collection 和数组的长度，无需使用索引访问元素。 foreach 遍历 Collection 时，其底层仍然是调用 Iterator 来完成操作。 实例： 1234567891011121314151617181920public class Test &#123; public static void main(String[] args) &#123; Collection&lt;Object&gt; collection = new ArrayList&lt;&gt;(); collection.add(123); collection.add(456); collection.add(new String(&quot;Tom&quot;)); collection.add(false); // 遍历集合：for (集合元素的类型 局部变量 : 集合对象)，底层仍然调用了迭代器 for (Object obj : collection) &#123; System.out.println(obj); &#125; int[] arr = new int[]&#123;1, 2, 3, 4, 5, 6&#125;; // 遍历数组：for(数组元素的类型 局部变量 : 数组对象) for (int i : arr) &#123; System.out.println(i); &#125; &#125;&#125; foreach 的使用注意事项： 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; String[] arr = new String[]&#123;&quot;MM&quot;, &quot;MM&quot;, &quot;MM&quot;&#125;; // 方式一：普通for赋值 for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = &quot;GG&quot;;// 能够赋值 &#125; for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i] + &quot; &quot;);// GG GG GG &#125; System.out.println(); arr = new String[]&#123;&quot;MM&quot;, &quot;MM&quot;, &quot;MM&quot;&#125;; // 方式二：增强for循环 for (String s : arr) &#123; s = &quot;GG&quot;;// 不能赋值，因为s是一个局部变量，foreach循环将arr数组的当前值赋给了s，然后循环中s被重新赋值为GG，不会影响到arr数组中的值 &#125; for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i] + &quot; &quot;);// MM MM MM &#125; &#125;&#125; List 接口 鉴于 java 中数组用来存储数据的局限性，我们通常使用 List 替代数组。 List 集合类中元素有序、且可重复，集合中的每个元素都有其对应的顺序索引。 List 容器中的元素都对应一个整数型的序号记载其在容器中的位置，可以根据序号存取容器中的元素。 JDK API 中 List 接口的实现类常用的有：ArrayList、LinkedList 和 Vector。 List 常用方法： List 除了从 Collection 集合继承的方法外，还添加了一些根据索引来操作集合元素的方法。 void add(int index, Object ele)：在 index 位置插入ele 元素。 boolean addAll(int index, Collection eles)：从 index 位置开始将 eles 中的所有元素添加进来。 Object get(int index)：获取指定 index 位置的元素。 int indexOf(Object obj)：返回 obj 在集合中首次出现的位置。 int lastIndexOf(Object obj)：返回 obj 在当前集合中末次出现的位置。 Object remove(int index)：移除指定 index 位置的元素，并返回此元素，区别于 Collection 接口中的 remove(Object obj)。 123456789101112131415public class Test &#123; private static void updateList(List list) &#123; list.remove(2);// 删除索引2 // list.remove(new Integer(2));// 删除对象2 &#125; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(1); list.add(2); list.add(3); updateList(list); System.out.println(list);// [1, 2] &#125;&#125; Object set(int index, Object ele)：设置指定 index 位置的元素为 ele。 List subList(int fromIndex, int toIndex)：返回当前集合从 fromIndex 到 toIndex 位置的子集合，前包后不包，当前集合不发生改变。 总结： 增：add(Object obj) 删：remove(int index) / remove(Object obj) 改：set(int index, Object ele) 查：get(int index) 插：add(int index, Object ele) 长度：size() 遍历：① Iterator 迭代器方式；② 增强 for 循环；③ 普通的循环。 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; ArrayList&lt;Object&gt; list = new ArrayList(); list.add(123); list.add(456); list.add(&quot;AA&quot;); // 方式一：Iterator迭代器方式 Iterator&lt;Object&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; System.out.println(&quot;***************&quot;); // 方式二：增强for循环 for (Object obj : list) &#123; System.out.println(obj); &#125; System.out.println(&quot;***************&quot;); // 方式三：普通for循环 for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125;&#125; 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Test &#123; public static void main(String[] args) &#123; ArrayList&lt;Object&gt; list = new ArrayList(); list.add(123); list.add(456); list.add(&quot;AA&quot;); list.add(456); System.out.println(list);// [123, 456, AA, 456] // void add(int index, Object ele): 在index位置插入ele元素 list.add(1, &quot;BB&quot;); System.out.println(list);// [123, BB, 456, AA, 456] // boolean addAll(int index, Collection eles): 从index位置开始将eles中的所有元素添加进来 List&lt;Integer&gt; list1 = Arrays.asList(1, 2, 3); list.addAll(list1); // list.add(list1);// 这是把list1当作一个元素添加到list中 System.out.println(list);// [123, BB, 456, AA, 456, 1, 2, 3] // Object get(int index): 获取指定index位置的元素 System.out.println(list.get(0));// 123 // int indexOf(Object obj): 返回obj在集合中首次出现的位置。如果不存在，返回-1。 int index = list.indexOf(4567); System.out.println(index);// -1 // int lastIndexOf(Object obj): 返回obj在当前集合中末次出现的位置。如果不存在，返回-1。 System.out.println(list.lastIndexOf(456));// 4 // Object remove(int index): 移除指定index位置的元素，并返回此元素 Object obj = list.remove(0); System.out.println(obj);// 123 System.out.println(list);// [BB, 456, AA, 456, 1, 2, 3] // Object set(int index, Object ele): 设置指定index位置的元素为ele list.set(1, &quot;CC&quot;); System.out.println(list);// [BB, CC, AA, 456, 1, 2, 3] // List subList(int fromIndex, int toIndex): 返回从fromIndex到toIndex位置的左闭右开区间的子集合 List&lt;Object&gt; subList = list.subList(2, 4); System.out.println(subList);// [AA, 456] System.out.println(list);// [BB, CC, AA, 456, 1, 2, 3] &#125;&#125; ArrayList ArrayList 是 List 接口的典型实现类、主要实现类。 本质上，ArrayList 是对象引用的一个 “变长” 数组。 ArrayList 的 JDK 1.8 之前与之后的实现区别？ JDK 1.7：ArrayList 类似于饿汉式，初始化时直接创建一个初始容量为 10 的数组。 JDK 1.8：ArrayList 类似于懒汉式，初始化时创建一个长度为 0 的数组，当添加第一个元素时再创建一个初始容量为 10 的数组。 Arrays.asList(…) 返回的 List 集合，既不是 ArrayList 实例，也不是 Vector 实例。Arrays.asList(…) 返回值是一个固定长度的 List 集合。 源码分析： JDK 7.0： ArrayList list = new ArrayList();，初始化时，底层创建了长度是 10 的 Object[] 数组 elementData。 12345/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. */private transient Object[] elementData; 123456/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this(10);// 初始化时，数组长度为10&#125; 1234567891011121314/** * Constructs an empty list with the specified initial capacity. * * @param initialCapacity the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity * is negative */public ArrayList(int initialCapacity) &#123;// 也可以直接指定ArrayList的容量 super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity];&#125; list.add(123);，等同于 elementData[0] = new Integer(123);。 list.add(11);，每次添加数据前，会验证数组容量，如果此次的添加导致底层 elementData 数组容量不够，则扩容。 123456789101112/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; // add()添加元素之前，先验证数组容量，size即为已添加的元素的数量 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e;// 添加元素到数组的size+1的位置 return true;&#125; 123456private void ensureCapacityInternal(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0)// 如果添加的元素的总数，已经超过了数组的长度，则进行扩容操作 grow(minCapacity);&#125; 默认情况下，扩容为原来的容量的 1.5 倍，同时需要将原有数组中的数据复制到新的数组中。 1234567891011121314151617/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);// 扩容后的新数组，其长度为原数组长度的1.5倍 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);// 将原数组中的元素，复制到新数组中&#125; 结论：建议开发中使用带参的构造器：ArrayList list = new ArrayList(int capacity);，按需求在初始化时就指定 ArrayList 的容量，以尽可能的避免扩容。 JDK 8.0： ArrayList list = new ArrayList();，底层 Object[] 数组 elementData 初始化为 {} (长度为 0 的空数组)，并没有创建长度为 10 的数组。 1234567/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */transient Object[] elementData; // non-private to simplify nested class access 123456/** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; 123456/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;// 初始化时，没有创建长度为10的数组&#125; list.add(123);，第一次调用 add() 时，底层才创建了长度为 10 的数组，并将数据 123 添加到 elementData[0]。 123456789101112/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; // 第一次添加元素，size=0，先初始化数组 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e;// 添加元素到数组的size+1的位置 return true;&#125; 123private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));// 得到数组的长度&#125; 1234567private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 第一次添加元素，elementData为&#123;&#125;，返回数组长度为DEFAULT_CAPACITY，即10 return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;// 不是第一次添加元素，elementData不为&#123;&#125;，直接返回下一个添加元素的数目&#125; 1234567private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0)// 如果添加的元素的总数，已经超过了数组的长度，则进行扩容操作 grow(minCapacity);&#125; 后续的添加和扩容操作与 JDK 7.0 无异。 小结： JDK 7.0 中的 ArrayList 的对象的创建，类似于单例的饿汉式，初始化时直接创建一个初始容量为 10 的数组。 JDK 8.0 中的 ArrayList 的对象的创建，类似于单例的懒汉式，延迟了数组的创建，节省内存。 添加数据时，如果底层的数组需要扩容，均扩容为原来的容量的 1.5 倍，同时将原有数组中的数据复制到新的数组中。 LinkedList 双向链表，内部定义了内部类 Node，作为 LinkedList 中保存数据的基本结构。LinkedList 内部没有声明数组，而是定义了 Node 类型的 first 和 last，用于记录首末元素。 对于频繁的插入或删除元素的操作，建议使用 LinkedList 类，效率较高。 新增方法： void addFirst(Object obj) void addLast(Object obj) Object getFirst() Object getLast() Object removeFirst() Object removeLast() 源码分析： LinkedList list = new LinkedList();，内部声明了 Node 类型的 first 和 last 属性，默认值为 null。 12/** * Pointer to first node. Invariant: (first == null &amp;&amp; last == null) || (first.prev == null &amp;&amp; first.item != null) /transient Node first; /** Pointer to last node. Invariant: (first == null &amp;&amp; last == null) || (last.next == null &amp;&amp; last.item != null) */ transient Node last; 12345678910111213 &#96;&#96;&#96;javaprivate static class Node&lt;E&gt; &#123; E item;&#x2F;&#x2F; 这个就是往LinkedList中添加的数据 Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item &#x3D; element; this.next &#x3D; next; this.prev &#x3D; prev; &#125; &#125; 12345/** * Constructs an empty list. */public LinkedList() &#123;&#125; list.add(123);，将 123 封装到 Node 中，创建了 Node 对象。 123456789101112/** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #addLast&#125;. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; linkLast(e); return true;&#125; 1234567891011121314/** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; Node 的定义体现了 LinkedList 的双向链表的说法，其除了保存数据，还定义了两个变量： prev：变量记录前一个元素的位置。 next：变量记录下一个元素的位置。 Vector Vector 是一个古老的集合，JDK 1.0 就有了。大多数操作与 ArrayList 相同，区别之处在于 Vector 是线程安全的。 在各种 List 中，最好把 ArrayList 作为缺省选择。当插入、删除频繁时，使用 LinkedList。Vector 总是比 ArrayList 慢，所以尽量避免使用。 新增方法： void addElement(Object obj) void insertElementAt(Object obj,int index) void setElementAt(Object obj,int index) void removeElement(Object obj) void removeAllElements() 源码分析： JDK 7.0 和 JDK 8.0 中，通过 new Vector() 构造器创建对象时，底层都创建了长度为 10 的数组。在扩容方面，默认扩容为原来的数组长度的 2 倍。 12345678/** * Constructs an empty vector so that its internal data array * has size &#123;@code 10&#125; and its standard capacity increment is * zero. */public Vector() &#123; this(10);// 初始化长度为10&#125; 1234567891011/** * Constructs an empty vector with the specified initial capacity and * with its capacity increment equal to zero. * * @param initialCapacity the initial capacity of the vector * @throws IllegalArgumentException if the specified initial capacity * is negative */public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125; 123456789101112131415161718/** * Constructs an empty vector with the specified initial capacity and * capacity increment. * * @param initialCapacity the initial capacity of the vector * @param capacityIncrement the amount by which the capacity is * increased when the vector overflows * @throws IllegalArgumentException if the specified initial capacity * is negative */public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity];// 创建长度为10的数组 this.capacityIncrement = capacityIncrement;&#125; add() 添加数据之前，先验证数组容量： 12345678910111213/** * Appends the specified element to the end of this Vector. * * @param e element to be appended to this Vector * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) * @since 1.2 */public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 12345678910111213/** * This implements the unsynchronized semantics of ensureCapacity. * Synchronized methods in this class can internally call this * method for ensuring capacity without incurring the cost of an * extra synchronization. * * @see #ensureCapacity(int) */private void ensureCapacityHelper(int minCapacity) &#123; // overflow-conscious code if (minCapacity - elementData.length &gt; 0)// 数组容量不够，扩容 grow(minCapacity);&#125; 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity);// 扩容到原来数组长度的二倍 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; ArrayList、LinkedL`ist、Vector三者的异同 相同点：三个类都实现了 List 接口，存储数据的特点相同，都是存储有序的、可重复的数据。 不同点： ArrayList：作为 List 接口的主要实现类；线程不安全的，效率高；底层使用 Object[] elementData 存储。 LinkedList：线程不安全的，对于频繁的插入、删除操作，使用此类效率比 ArrayList 高；底层使用双向链表存储。 Vector：作为 List 接口的古老实现类；线程安全的，效率低；底层使用 Object[] elementData 存储。 ArrayList 和 LinkedList 的异同： ArrayList 和 LinkedList 都线程不安全，相对线程安全的 Vector，二者执行效率更高。 ArrayList 底层是实现了基于动态数组的数据结构，LinkedList 底层是实现了基于链表的数据结构。 对于随机访问 get() 和 set()，ArrayList 优于LinkedList，因为 LinkedList 要移动指针。 对于新增和删除操作 add() (特指插入) 和 remove()，LinkedList 比较占优势，因为 ArrayList 要移动数据。 ArrayList 和 Vector 的区别： Vector 和 ArrayList 几乎是完全相同的，唯一的区别在于 Vector 是同步类，属于强同步类。因此开销就比 ArrayList 要大，访问要慢。 正常情况下，大多数的 java 程序员使用 ArrayList 而不是 Vector，因为同步完全可以由程序员自己来控制。 Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。Vector 还有一个子类 Stack。 Set 接口 Set 集合存储无序的、不可重复的数据，如果试把两个相同的元素加入同一个 Set 集合中，则添加操作失败。 无序性：不等于随机性。以 HashSet 为例，存储的数据在底层数组中并非按照数组索引的顺序添加，而是根据数据的哈希值决定的。 不可重复性：保证添加的元素按照 equals() 判断时，不能返回 true。即：相同的元素只能添加一个。 Set 接口是 Collection 的子接口，Set 接口没有提供额外的方法，使用的都是Collection中声明过的方法。 Set 判断两个对象是否相同不是使用 == 运算符，而是根据 equals()。 对于存放在 Set (主要指：HashSet、LinkedHashSet) 容器中的对象，其对应的类一定要重写 equals() 和 hashCode()，以实现对象相等规则。 要求：重写的 hashCode() 和 equals() 尽可能保持一致性，即：相等的对象必须具有相等的散列码。 如果不重写所添加元素所在类的 hashCode()，则会调用 Object 类的 hashCode()，该方法是产生一个随机数，因此，即使添加两个一样的元素，其 hashCode 值也可能不同，也就都能添加成功。 重写两个方法的小技巧：对象中用作 equals() 方法比较的 Field，都应该用来计算 hashCode 值。 TreeSet 比较两个元素是否相同的方法，不是 equals() 和 hashCode()，而是元素对应类的排序方法。 重写 hashCode() 方法的基本原则： 在程序运行时，同一个对象多次调用 hashCode() 方法应该返回相同的值。 当两个对象的 equals() 方法比较返回 true 时，这两个对象的 hashCode() 方法的返回值也应相等。 对象中用作 equals() 方法比较的 Field，都应该用来计算 hashCode 值。 重写 equals() 方法的基本原则，以自定义的 Customer 类为例，何时需要重写 equals()： 如果一个类有自己特有的 “逻辑相等” 概念，当重写 equals() 的时候，总是需要重写 hashCode()。因为根据一个类改写后的 equals()，两个截然不同的实例有可能在逻辑上是相等的，但是，根据 Object 类的 hashCode()，它们仅仅是两个对象。这种情况，违反了 “相等的对象必须具有相等的散列码” 的原则。 结论：重写 equals() 的时候，一般都需要同时重写 hashCode() 方法。通常参与计算 hashCode 的对象的属性也应该参与到 equals() 中进行计算。 Eclipse/IDEA 工具里 hashCode() 的重写，为什么会有 31 这个数字： 123456@Overridepublic int hashCode() &#123; int result = name.hashCode(); result = 31 * result + age; return result;&#125; 选择系数的时候要选择尽量大的系数，因为如果计算出来的 hashCode 值越大，所谓的冲突就越少，查找起来效率也会提高。—&gt; 减少冲突 31 只占用 5 bits，相乘造成数据溢出的概率较小。 31 可以由 i * 31 == (i &lt;&lt; 5) - 1 来表示，现在很多虚拟机里面都有做相关优化。—&gt; 提高算法效率 31 是一个素数，素数作用就是如果用一个数字来乘以这个素数，那么最终出来的结果只能被素数本身和被乘数还有 1 来整除！—&gt; 减少冲突 HashSet HashSet 是 Set 接口的典型实现，大多数时候使用 Set 集合时都使用这个实现类。 HashSet 按 Hash 算法来存储集合中的元素，因此具有很好的存取、查找、删除性能。 HashSet 具有以下特点： 不保证元素的排列顺序。 不是线程安全的。 集合元素可以是 null。 向 HashSet 中添加元素的过程： 当向 HashSet 集合中存入一个元素 a 时，首先会调用元素 a 所在类的 hashCode()，计算元素 a 的 hashCode 值，然后根据 hashCode 值，通过某种散列函数，计算出元素 a 在 HashSet 底层数组中的存储位置 (即为：索引位置，这个索引位置不是像 List 那样有顺序的，而是无序的)。 说明：这个散列函数会根据元素的 hashCode 值和底层数组的长度相计算，得到该元素在数组中的下标 (存储位置)，并且这种散列函数计算还尽可能保证能均匀存储元素，越是散列分布，该散列函数设计的越好。 向 List 中添加元素时，会按照索引位置的顺序在数组中逐个添加，这是一种有序性。而向 HashSet 中添加元素时，可能第一个元素的索引位置在数组的中间，第二个元素的索引位置在数组的头，第三个元素的索引位置在数组的尾，是按照一种无序的状态添加的，是为无序性。 计算出元素 a 的存储位置后，首先判断数组此位置上是否已经有元素： 如果此位置上没有其他元素，则元素 a 添加成功。—&gt; 情况1 如果此位置上有其他元素 b (或以链表形式存在的多个元素)，则比较元素 a 与元素 b (或以链表形式存在的多个元素) 的 hashCode 值： 如果 hashCode 值不相同，则元素 a 添加成功。—&gt; 情况2 如果 hashCode 值相同，进而需要调用元素 a 所在类的 equals()： equals() 返回 true，则元素 a 添加失败。 equals() 返回 false，则元素 a 添加成功。—&gt; 情况3 对于添加成功的情况 2 和情况 3 而言：元素 a 与已经存在指定索引位置上的元素以链表的方式存储。 JDK 7.0：元素 a 存放到底层数组中，指向原来的元素。 JDK 8.0：原来的元素存放到数组中，指向元素 a。 总结：七上八下。 由以上向 HashSet 添加元素的过程，可以看出 HashSet 的底层：数组 + 链表的结构。(前提：JDK 7.0，JDK 8.0 见 HashMap。) HashSet 底层结构： HashSet 集合判断两个元素相等的标准：两个对象通过 hashCode() 比较相等，并且两个对象的 equals() 返回值也相等。 利用 HashSet 去除 List 中的重复元素： 1234567891011121314151617181920public class Test &#123; public static List duplicateList(List list) &#123; HashSet set = new HashSet(); set.addAll(list); return new ArrayList(set); &#125; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(new Integer(1)); list.add(new Integer(2)); list.add(new Integer(2)); list.add(new Integer(4)); list.add(new Integer(4)); List list2 = duplicateList(list); for (Object integer : list2) &#123; System.out.println(integer); &#125; &#125;&#125; 实例： 12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; Set set = new HashSet(); set.add(456); set.add(123); set.add(123); set.add(&quot;AA&quot;); set.add(&quot;CC&quot;); set.add(129); Iterator iterator = set.iterator(); while(iterator.hasNext())&#123; System.out.print(iterator.next() + &quot; &quot;);// AA CC 129 456 123，不是按照元素添加的顺序进行输出的 &#125; &#125;&#125; LinkedHashSet LinkedHashSet 是 HashSet 的子类，不允许集合元素重复 LinkedHashSet 根据元素的 hashCode 值来决定元素的存储位置，但它同时使用双向链表维护元素的次序，这使得元素看起来是以插入顺序保存的。 遍历 LinkedHashSet 内部数据时，可以按照添加的顺序遍历。 LinkedHashSet 插入性能略低于 HashSet，但在迭代访问 Set 里的全部元素时有很好的性能。 对于频繁的遍历操作，LinkedHashSet 效率高于 HashSet。 LinkedHashSet 底层结： 实例： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; // LinkedHashSet在添加数据的同时，每个数据还维护了两个引用，记录此数据前一个数据和后一个数据 Set set = new LinkedHashSet(); set.add(456); set.add(123); set.add(123); set.add(&quot;AA&quot;); set.add(&quot;CC&quot;); set.add(129); Iterator iterator = set.iterator(); while (iterator.hasNext()) &#123; System.out.print(iterator.next() + &quot; &quot;);// 456 123 AA CC 129，按照元素添加的顺序进行输出的 &#125; &#125;&#125; 面试题： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Test &#123; public static void main(String[] args) &#123; HashSet set = new HashSet(); User p1 = new User(1001, &quot;AA&quot;); User p2 = new User(1002, &quot;BB&quot;); set.add(p1);// 假设p1添加到HashSet底层数组的位置7(hashCode值以1001和AA计算出来) set.add(p2);// 假设p2添加到HashSet底层数组的位置3(hashCode值以1002和BB计算出来) System.out.println(set);// 位置3和7处对应的2个User p1.name = &quot;CC&quot;;// 更改p1指向的User对象的name为CC set.remove(p1);// 以新的p1在HashSet底层数组查找，没有相同的对象(hashCode值以1001和CC计算出来) System.out.println(set);// 位置3和7处对应的2个User，但位置7指向的User对象的name为C，不是AA，位置3指向的User对象的name为BB set.add(new User(1001, &quot;CC&quot;));// 新new出来的User，hashCode值以1001和CC计算出来，不同于最初的p1，其位置不会在7处，也不会在3处，假设在11处 System.out.println(set);// 位置3、7和11处对应的3个User，其中，位置7和11对应的User的id和name都是1001和CC，但不是堆中的同一个对象 set.add(new User(1001, &quot;AA&quot;));// 新new出来的User，hashCode值以1001和AA计算出来，等于最初的p1，位置在7处，但因为现在7处User对象的name为CC，所以equals()不相同，这个User对象链接到7位置 System.out.println(set);// 位置3、7和11处对应的4个User &#125;&#125;class User &#123; int id; String name; public User(int id, String name) &#123; this.id = id; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (id != user.id) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; int result = id; result = 31 * result + (name != null ? name.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125;输出结果：[User&#123;id=1002, name=&#x27;BB&#x27;&#125;, User&#123;id=1001, name=&#x27;AA&#x27;&#125;][User&#123;id=1002, name=&#x27;BB&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;][User&#123;id=1002, name=&#x27;BB&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;][User&#123;id=1002, name=&#x27;BB&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;, User&#123;id=1001, name=&#x27;CC&#x27;&#125;, User&#123;id=1001, name=&#x27;AA&#x27;&#125;] 解析： TreeSet TreeSet 是 SortedSet 接口的实现类，TreeSet 可以按照添加对象的指定属性，进行排序，确保集合元素处于排序状态。 TreeSet 特点：有序，查询速度比 List 快。 TreeSet 与 TreeMap 一样，底层使用红黑树结构存储数据。 红黑树参考：http://www.cnblogs.com/yangecnu/p/Introduce-Red-Black-Tree.html 新增方法： Comparator comparator() Object first() Object last() Object lower(Object e) Object higher(Object e) SortedSet subSet(fromElement, toElement) SortedSet headSet(toElement) SortedSet tailSet(fromElement) 向 TreeSet 中添加的数据，要求是相同类的对象。 TreeSet 两种排序方法：自然排序 (实现 Comparable 接口) 和定制排序 (Comparator)。默认情况下，TreeSet 采用自然排序。 在 TreeSet 中比较两个元素是否相同时，取决于使用的是自然排序还是定制排序，不再考虑 equals()，比如 add() 和 remove() 等方法，这点与 HashSet 不同。 自然排序： TreeSet 会调用集合元素的 compareTo(Object obj) 来比较元素之间的大小关系，然后将集合元素按升序 (默认情况) 排列。 如果试图把一个对象添加到 TreeSet 时，则该对象的类必须实现 Comparable 接口。 实现 Comparable 的类必须实现 compareTo(Object obj)，两个对象即通过 compareTo(Object obj) 的返回值来比较大小。 Comparable 的典型实现： BigDecimal、BigInteger 以及所有的数值型对应的包装类：按它们对应的数值大小进行比较。 Character：按字符的 unicode值来进行比较。 Boolean：true 对应的包装类实例大于 false 对应的包装类实例。 String：按字符串中字符的 unicode 值进行比较。 Date、Time：后边的时间、日期比前面的时间、日期大。 向 TreeSet 中添加元素时，只有第一个元素无须比较 compareTo()，后面添加的所有元素都会调用 compareTo() 进行比较。 因为只有相同类的两个实例才会比较大小，所以向 TreeSet 中添加的应该是同一个类的对象。 对于 TreeSet 集合而言，使用自然排序判断两个元素相等的标准是：两个元素通过 compareTo() 比较返回 0，不再是 equals()。 当需要把一个对象放入 TreeSet 中，在重写该对象对应的 equals() 时，应保证该方法与 compareTo() 有一致的结果：如果两个对象通过 equals() 比较返回 true，则通过 compareTo(Object obj) 比较应返回 0。否则，会让人难以理解。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class Test &#123; public static void main(String[] args) &#123; Set set = new TreeSet(); set.add(new User(&quot;Tom&quot;, 12)); set.add(new User(&quot;Jerry&quot;, 32)); set.add(new User(&quot;Jim&quot;, 2)); set.add(new User(&quot;Mike&quot;, 65)); set.add(new User(&quot;Jack&quot;, 33)); set.add(new User(&quot;Jack&quot;, 56)); Iterator iterator = set.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125;class User implements Comparable &#123; private String name; private int age; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; System.out.println(&quot;User equals()....&quot;); if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (age != user.age) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; //return name.hashCode() + age; int result = name != null ? name.hashCode() : 0; result = 31 * result + age; return result; &#125; // 按照姓名从大到小排列,年龄从小到大排列 @Override public int compareTo(Object o) &#123; if (o instanceof User) &#123; User user = (User) o; // return -this.name.compareTo(user.name); int compare = -this.name.compareTo(user.name); if (compare != 0) &#123; return compare; &#125; else &#123; return Integer.compare(this.age, user.age); &#125; &#125; else &#123; throw new RuntimeException(&quot;输入的类型不匹配&quot;); &#125; &#125;&#125; 定制排序： TreeSet 的自然排序要求元素所属的类实现 Comparable 接口，如果元素所属的类没有实现 Comparable 接口，或不希望按照升序 (默认情况 )的方式排列元素或希望按照其它属性大小进行排序，则考虑使用定制排序。定制排序，通过 Comparator 接口来实现。需要重写 compare() 方法。 利用 int compare(T o1,T o2) 方法，比较 o1 和 o2 的大小：如果方法返回正整数，则表示 o1 大于 o2；如果返回 0，表示相等；返回负整数，表示 o1 小于 o2。 要实现定制排序，需要将实现 Comparator 接口的实例作为形参传递给 TreeSet 的构造器。此时，仍然只能向 TreeSet 中添加类型相同的对象。否则会发生 ClassCastException 异常。 对于 TreeSet 集合而言，使用定制排序判断两个元素相等的标准是：两个元素通过 compare() 比较返回 0，不再是 equals()。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class Test &#123; public static void main(String[] args) &#123; // 定制排序 Comparator com = new Comparator() &#123; // 按照年龄从小到大排列 @Override public int compare(Object o1, Object o2) &#123; if (o1 instanceof User &amp;&amp; o2 instanceof User) &#123; User u1 = (User) o1; User u2 = (User) o2; return Integer.compare(u1.getAge(), u2.getAge()); &#125; else &#123; throw new RuntimeException(&quot;输入的数据类型不匹配&quot;); &#125; &#125; &#125;; TreeSet set = new TreeSet(com); set.add(new User(&quot;Tom&quot;, 12)); set.add(new User(&quot;Jerry&quot;, 32)); set.add(new User(&quot;Jim&quot;, 2)); set.add(new User(&quot;Mike&quot;, 65)); set.add(new User(&quot;Mary&quot;, 33)); set.add(new User(&quot;Jack&quot;, 33)); set.add(new User(&quot;Jack&quot;, 56)); Iterator iterator = set.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125;class User implements Comparable &#123; private String name; private int age; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; System.out.println(&quot;User equals()....&quot;); if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (age != user.age) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; //return name.hashCode() + age; int result = name != null ? name.hashCode() : 0; result = 31 * result + age; return result; &#125; // 按照姓名从大到小排列,年龄从小到大排列 @Override public int compareTo(Object o) &#123; if (o instanceof User) &#123; User user = (User) o; // return -this.name.compareTo(user.name); int compare = -this.name.compareTo(user.name); if (compare != 0) &#123; return compare; &#125; else &#123; return Integer.compare(this.age, user.age); &#125; &#125; else &#123; throw new RuntimeException(&quot;输入的类型不匹配&quot;); &#125; &#125;&#125; 实例： 1234567891011121314151617181920212223public class Test &#123; public static void main(String[] args) &#123; Set set = new TreeSet(); // 失败：不能添加不同类的对象 /*set.add(123); set.add(456); set.add(&quot;AA&quot;); set.add(new User(&quot;Tom&quot;,12));*/ // 举例：全部添加Integer对象 /*set.add(34); set.add(-34); set.add(43); set.add(11); set.add(8);*/ Iterator iterator = set.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125; Map 接口 Map 与 Collection 是并列存在，双列数据，用于保存具有映射关系的数据：key - value 对。 Map结构的理解： Map 中的 key：无序的、不可重复的，使用 Set 存储所有的 key。—&gt; key 所在的类要重写 hashCode() 和 equals() (以 HashMap 为例)。 Map 中的 value：无序的、可重复的，使用 Collection 存储所有的 value。—&gt; value 所在的类要重写 equals()。 一个键值对：key - value 构成了一个 entry 对象。 Map 中的映射关系的类型是 Map.Entry 类型，它是 Map 接口的内部接口。 Map 中的 entry：无序的、不可重复的，使用 Set 存储所有的 entry。 Map 中的 key 和 value 都可以是任何引用类型的数据。 常用 String 类作为 Map 的 key。 key 和 value 之间存在单向一对一关系，即通过指定的 key 总能找到唯一的、确定的 value。 Map 接口的常用实现类：HashMap、TreeMap、LinkedHashMap 和 Properties。其中，HashMap 是 Map 接口使用频率最高的实现类。 Map 常用方法： 添加、删除、修改操作： Object put(Object key, Object value)：将指定 key - value 对添加到 (或修改) 当前 map 对象中。 如果在 map 中已存在 key，则会用 value 替换 map 中该 key 对应的值。 void putAll(Map m)：将 m 中的所有 key - value 对存放到当前 map 中。 Object remove(Object key)：移除指定 key 的 key - value 对，并返回 value。若 key 不存在，返回 null。 void clear()：清空当前 map 中的所有数据，与 map = null; 操作不同。 实例： 12345678910111213141516171819202122232425262728293031public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); // put() map.put(&quot;AA&quot;, 123); map.put(45, 123); map.put(&quot;BB&quot;, 56); // 修改，key已存在，会替换其value map.put(&quot;AA&quot;, 87); System.out.println(map);// &#123;AA=87, BB=56, 45=123&#125; // putAll() Map map1 = new HashMap(); map1.put(&quot;CC&quot;, 123); map1.put(&quot;DD&quot;, 123); map.putAll(map1); System.out.println(map);// &#123;AA=87, BB=56, CC=123, DD=123, 45=123&#125; // remove(Object key) Object value = map.remove(&quot;CC&quot;); System.out.println(value);// 123 System.out.println(map);// &#123;AA=87, BB=56, DD=123, 45=123&#125; System.out.println(map.remove(&quot;EE&quot;));// key不存在，返回null // clear() map.clear();// 与map = null操作不同 System.out.println(map.size());// 0 System.out.println(map);// &#123;&#125; &#125;&#125; 元素查询的操作： Object get(Object key)：获取指定 key 对应的 value，如果 key 不存在，返回 null。 boolean containsKey(Object key)：是否包含指定的 key。 boolean containsValue(Object value)：是否包含指定的 value。 int size()：返回 map 中 key - value 对的个数。 boolean isEmpty()：判断当前 map 是否为空，以 size 是否为 0 判断。 boolean equals(Object obj)：判断当前 map 和参数对象 obj 是否相等。 实例： 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); map.put(&quot;AA&quot;, 123); map.put(45, 123); map.put(&quot;BB&quot;, 56); // Object get(Object key) System.out.println(map.get(45));// 123 System.out.println(map.get(43));// null // containsKey(Object key) boolean isExist = map.containsKey(&quot;BB&quot;); System.out.println(isExist);// true isExist = map.containsValue(123); System.out.println(isExist);// true map.clear(); System.out.println(map.isEmpty());// true &#125;&#125; 元视图操作的方法： Set keySet()：返回所有 key 构成的 Set 集合。 Collection values()：返回所有 value 构成的 Collection 集合。 Set entrySet()：返回所有 key - value 对构成的 Set 集合。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); map.put(&quot;AA&quot;, 123); map.put(45, 1234); map.put(&quot;BB&quot;, 56); // 遍历所有的key集：keySet() Set keys = map.keySet(); Iterator iterator = keys.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; System.out.println(); // 遍历所有的value集：values() Collection values = map.values(); for (Object obj : values) &#123; System.out.println(obj); &#125; System.out.println(); // 遍历所有的key-value // 方式一：entrySet() Set entrySet = map.entrySet(); Iterator iterator1 = entrySet.iterator(); while (iterator1.hasNext()) &#123; Object obj = iterator1.next(); // entrySet集合中的元素都是entry Map.Entry entry = (Map.Entry) obj; System.out.println(entry.getKey() + &quot;----&gt;&quot; + entry.getValue()); &#125; System.out.println(); // 方式二： Set keySet = map.keySet(); Iterator iterator2 = keySet.iterator(); while (iterator2.hasNext()) &#123; Object key = iterator2.next(); Object value = map.get(key); System.out.println(key + &quot;=====&quot; + value); &#125; &#125;&#125; 总结： 添加：put(Object key, Object value) 删除：remove(Object key) 修改：put(Object key, Object value) 查询：get(Object key) 长度：size() 遍历：keySet() / values() / entrySet() HashMap HashMap 是 Map 接口使用频率最高的实现类。 HashMap 允许使用 null 键和 null 值，与 HashSet 一样，不保证映射的顺序。 所有的 key 构成的集合是 Set：无序的、不可重复的。所以，key 所在的类要重写：hashCode() 和 equals()。 HashMap 判断两个 key 相等的标准是：两个 key 的 hashCode 值相等，同时通过 equals() 判断返回 true。 所有的 value 构成的集合是 Collection：无序的、可以重复的。所以，value 所在的类要重写：equals()。 HashMap 判断两个 value 相等的标准是：两个 value 通过 equals() 判断返回 true。 一个 key - value 对构成一个 entry，所有的 entry 构成的集合是 Set：无序的、不可重复的。 不要修改映射关系的 key： 映射关系存储到 HashMap 中时，会存储 key 的 hash 值，这样就不用在每次查找时重新计算每一个 Entry 或 Node (TreeNode) 的 hash 值了，因此如果已经 put 到 Map 中的映射关系，再修改 key 的属性，而这个属性又参与 hashcode 值的计算，那么会导致匹配不上。 HashMap 源码中的重要常量： DEFAULT_INITIAL_CAPACITY：HashMap 的默认容量，16。 1234/** * The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 MAXIMUM_CAPACITY：HashMap 的最大支持容量，2^30。 123456/** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; DEFAULT_LOAD_FACTOR：HashMap 的默认加载因子，0.75。不同于 ArrayList，HashMap 不是在底层数组全部填满时才进行扩容操作，因为数组上有一些位置可能会一直都没有添加元素，但其他位置上元素可能有很多，导致链表和二叉树结构变多。因此，会在元素添加到一定数量时，就执行扩容操作，即添加元素数量达到 threshold 值时扩容。默认加载因子如果过小，会导致数组还有很多空位置时扩容，数组利用率低；默认加载因子如果过大，会导致数组中存在很多元素时才扩容，链表和二叉树结构过多。因此，默认加载因子在 0.7 ~ 0.75 左右比较合适。 1234/** * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f; TREEIFY_THRESHOLD：Bucket 中链表存储的 Node 长度大于该默认值，判断是否转换为红黑树，默认为 8。Since JDK 8.0。 123456789/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8; UNTREEIFY_THRESHOLD：Bucket 中红黑树存储的 Node 长度小于该默认值，转换为链表，默认为 6，Since JDK 8.0。 123456/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6; MIN_TREEIFY_CAPACITY：桶中的 Node 被树化时最小的 hash 表容量，默认为 64。当桶中 Node 的数量大到需要变红黑树 (8) 时，若 hash 表容量小于 MIN_TREEIFY_CAPACITY，此时应执行 resize() 进行扩容操作。MIN_TREEIFY_CAPACITY 的值至少是 TREEIFY_THRESHOLD 的 4 倍。Since JDK 8.0。 1234567/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64; table ：存储元素的数组，长度总是 2 的 n 次幂。JDK 7.0 中是 transient Entry&lt;K, V&gt;[] table;，JDK 8.0 中是 transient Node&lt;K,V&gt;[] table;。 entrySet：存储具体元素的集。 size：HashMap 中已存储的键值对的数量。 modCount：HashMap 扩容和结构改变的次数。 threshold：扩容的临界值，其值一般等于容量 * 加载因子，(int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1);。扩容的操作不是当底层数组全部被填满后再扩容，而是达到临界值后的下一次添加操作进行扩容。 loadFactor：加载因子。 源码分析： JDK 7.0： 初始化操作，以无参构造器为例：HashMap hashMap = new HashMap();，在实例化以后，底层创建了长度是 16 的一维数组 Entry[] table： 1234567/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);// 默认初始化长度：16，加载因子：0.75。 &#125; 12345678910111213141516171819202122232425262728293031/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY)// map最大长度：1073741824 initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1;// map初始化时的长度，总是2的n次幂 this.loadFactor = loadFactor; threshold = (int)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1);// 扩容的临界值16*0.75=12 table = new Entry[capacity];// 底层创建了长度是16的一维数组Entry[] table useAltHashing = sun.misc.VM.isBooted() &amp;&amp; (capacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD); init(); &#125; 向数组中添加数据操作，hashMap.put(key1, value1);： 123456789101112131415161718192021222324252627282930313233/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) &#123; if (key == null) return putForNullKey(value);// HashMap可以添加key为null的键值对 int hash = hash(key);// 计算key的hash值，中间调用了key的hashCode()方法 int i = indexFor(hash, table.length);// 获取当前数据在数组中的索引位置 // 取出数组的i位置上的元素，i位置上的元素可能不止一个，需要一个一个对比 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 如果i位置上有元素，对比该元素与当前key的hash值和equals()是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value;// i位置上元素与当前key相同，则将当前value替换i位置上原值 e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i);// 如果数组的i位置上没有元素，则直接添加当前key-value对在i位置上 return null;&#125; 计算 key 的 hash值： 123456789101112131415161718192021222324/** * Retrieve object hash code and applies a supplemental hash function to the * result hash, which defends against poor quality hash functions. This is * critical because HashMap uses power-of-two length hash tables, that * otherwise encounter collisions for hashCodes that do not differ * in lower bits. Note: Null keys always map to hash 0, thus index 0. */final int hash(Object k) &#123; int h = 0; if (useAltHashing) &#123; if (k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h = hashSeed; &#125; h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 获取位置： 123456/** * Returns index for hash code h. */static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 添加数据： 123456789101112131415161718/** * Adds a new entry with the specified key, value and hash code to * the specified bucket. It is the responsibility of this * method to resize the table if appropriate. * * Subclass overrides this to alter the behavior of put method. */void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 如果已添加的元素数量≥扩容的临界值，且即将添加元素的数组bucketIndex位置上已存在元素 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length);// 扩容为原来数组长度的的2倍，并将原有的数据复制到新数组中 hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length);// 重新计算当前key在新数组中的位置 &#125; // 不需要扩容，或扩容完成，将当前元素存放到数组的bucketIndex位置上 createEntry(hash, key, value, bucketIndex);&#125; 123456789101112131415/** * Like addEntry except that this version is used when creating entries * as part of Map construction or &quot;pseudo-construction&quot; (cloning, * deserialization). This version needn&#x27;t worry about resizing the table. * * Subclass overrides this to alter the behavior of HashMap(Map), * clone, and readObject. */void createEntry(int hash, K key, V value, int bucketIndex) &#123; // 取出bucketIndex位置上原有的元素 Entry&lt;K,V&gt; e = table[bucketIndex]; // 将当前的元素存放在bucketIndex位置上，并指向原有的元素 table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 总结，JDK 7.0 中 HashMap 的底层实现原理，以 HashMap map = new HashMap(); 为例说明： 在实例化以后，底层创建了长度是 16 的一维数组 Entry[] table。 执行 map.put(key1, value1) 操作，可能已经执行过多次 put： 首先，计算 key1 所在类的 hashCode() 以及其他操作计算 key1 的哈希值，此哈希值经过某种算法计算以后，得到在 Entry 数组中的存放位置。 如果此位置上的数据为空，此时的 key1 - value1 添加成功。—&gt; 情况 1 如果此位置上的数据不为空，(意味着此位置上存在一个或多个数据(以链表形式存在))，比较 key1 和已经存在的一个或多个数据的哈希值： 如果 key1 的哈希值与已经存在的数据的哈希值都不相同，此时 key1 - value1 添加成功。—&gt; 情况 2 如果 key1 的哈希值和已经存在的某一个数据 (key2 - value2) 的哈希值相同，则调用 key1 所在类的 equals(key2)，继续比较： 如果 equals() 返回 false：此时 key1 - value1 添加成功。—&gt; 情况 3 如果 equals() 返回 true：使用 value1 替换 value2。 补充：关于情况 2 和情况 3，此时 key1 - value1 和原来的数据以链表的方式存储。 存储结构：HashMap是数组+链表结构 HashMap 的内部存储结构其实是数组和链表的结合 (即为链地址法)。当实例化一个 HashMap 时，系统会创建一个长度为 Capacity 的 Entry 数组，这个长度在哈希表中被称为容量 (Capacity)，在这个数组中可以存放元素的位置我们称之为 “桶” (bucket)，每个 bucket 都有自己的索引，系统可以根据索引快速的查找 bucket 中的元素。 每个 bucket 中存储一个元素，即一个 Entry 对象，但每一个 Entry 对象可以带一个引用变量，用于指向下一个元素，因此，在一个桶中，就有可能生成一个 Entry 链，而且新添加的元素是整个链表的 head。 结构图示意： 扩容过程： 当 HashMap 中的元素越来越多的时候，hash 冲突的几率也就越来越高，因为底层数组的长度是固定的。所以为了提高查询的效率，就要对 HashMap 的底层数组进行扩容，而在 HashMap 数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是 resize()。 当 HashMap 中的元素个数超过数组大小 (数组总大小 length，不是数组中存储的元素个数 size) * loadFactor 时 ， 就 会 进 行 数 组 扩 容 。loadFactor 的 默 认 值为 0.75，这是一个折中的取值，默认情况下，数组大小为 16，那么当 HashMap 中元素个数 ≥ 16 * 0.75 = 12 (这个值就是代码中的 threshold 值，也叫做临界值) 且要存放的位置非空的时候，就把数组的大小扩展为 2 * 16 = 32，即扩大一倍，然后重新计算每个元素在数组中的位置，把原有的数据复制到新数组中。 扩容是一个非常消耗性能的操作，如果已经预知 HashMap 中元素的个数，那么预设元素的个数能够有效的提高 HashMap 的性能。 JDK 8.0： 初始化操作，以无参构造器为例：HashMap hashMap = new HashMap();，在实例化时，底层没有创建一个长度为 16 的数组，只是给加载因子赋值 0.75： 123public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125; 底层的数组是 Node[]，而非 Entry[]，但 Node 实现了 Entry 接口： 1transient Node&lt;K,V&gt;[] table; 1static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123;&#125; 首次调用 put() 方法时： 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) // 第一次进入put()，table还未初始化，为null，进入resize()，如果不是第一次put()，不会进入此逻辑 n = (tab = resize()).length; // 查看当前元素在新创建的数组中的位置i所在的位置的元素p，是否为null if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null);// 如果p为null，当前位置i没有元素，添加成功 ---&gt; 情况1 else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p;// 位置i上的元素，与当前待添加元素的key相同 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123;// 位置i上的元素，与当前待添加元素的key不同 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123;// 位置i上只有一个元素 // 位置i上的原元素指向当前待添加的元素，&quot;八下&quot;，添加成功 ---&gt; 情况2和3 p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 链表的长度超过8时，判断是否转为红黑树结构 treeifyBin(tab, hash); break; &#125; // 位置i上不止一个元素，依次获得该链表中的每一个元素，与待添加元素的key，对比hash值和equals() if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 替换操作 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 从 put() 第一次进入 resize()，底层创建了长度为 16 的 Node 数组： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table;// 从put()第一次进入resize()，table为null int oldCap = (oldTab == null) ? 0 : oldTab.length;// 0 int oldThr = threshold;// 此时扩容的临界值为0 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;// 默认数组长度16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);// 默认扩容的临界值：12 &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr;// 赋值扩容的临界值 @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];// 创建一个长度为16的Node数组 table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 计算 key 的 hash 值： 1234 static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 链表转红黑树： 1234567891011121314151617181920 final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize();// 如果底层数组的长度小玉64，只扩容，不转红黑树 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; 总结，JDK 8.0 相较于 JDK 7.0 在底层实现方面的不同： new HashMap() 时，底层没有创建一个长度为 16 的数组。 JDK 8.0 底层的数组是 Node[]，而非 Entry[]。 首次调用 put() 时，底层创建长度为 16 的数组。 JDK 7.0 底层结构只有：数组 + 链表。JDK 8.0 中底层结构是：数组 + 链表 + 红黑树。 形成链表时，”七上八下”： JDK 7.0 中新的元素指向旧的元素，JDK 8.0 中旧的元素指向新的元素。 当数组的某一个索引位置上的元素以链表形式存在的数据个数 &gt; 8 且当前数组的长度 &gt; 64时，此时此索引位置上的所数据改为使用红黑树存储。 存储结构： HashMap 的内部存储结构其实是数组+ 链表 + 树的结合。当实例化一个 HashMap 时，会初始化 initialCapacity 和 loadFactor，在 put 第一对映射关系时，系统会创建一个长度为 initialCapacity 的 Node 数组，这个长度在哈希表中被称为容量 (Capacity)，在这个数组中可以存放元素的位置我们称之为 “桶” (bucket)，每个 bucket 都有自己的索引，系统可以根据索引快速的查找 bucket 中的元素。 每个 bucket 中存储一个元素，即一个 Node 对象，但每一个 Node 对象可以带一个引用变量 next，用于指向下一个元素，因此，在一个桶中，就有可能生成一个 Node 链。也可能是一个一个 TreeNode 对象，每一个 TreeNode 对象可以有两个叶子结点 left 和 right，因此，在一个桶中，就有可能生成一个 TreeNode 树。而新添加的元素作为链表的 last，或树的叶子结点。 结构图示意： 扩容过程： 扩容过程与 JDK 7.0 相同。 树形化：当 HashMap 中的其中一个链的对象个数如果达到了 8 个，此时如果 capacity 没有达到 64，那么 HashMap 会先扩容解决，如果已经达到了 64，那么这个链会变成树，结点类型由 Node 变成 TreeNode 类型。当然，如果当映射关系被移除后，下次 resize() 时判断树的结点个数低于 6 个，也会把树再转为链表。 面试题 谈谈你对 HashMap 中 put() 和 get() 的认识？如果了解再谈谈 HashMap 的扩容机制？默认大小是多少？什么是负载因子 (或填充比)？什么是吞吐临界值 (或阈值、threshold)？ 负载因子值的大小，对 HashMap 有什么影响？ 负载因子的大小决定了 HashMap 的数据密度。 负载因子越大，数据密度越大，发生碰撞的几率越高，数组中的链表越容易长，造成查询或插入时的比较次数增多，性能会下降。 负载因子越小，就越容易触发扩容，数据密度也越小，意味着发生碰撞的几率越小，数组中的链表也就越短，查询和插入时比较的次数也越小，性能会更高。但是会浪费一定的内容空间，而且经常扩容也会影响性能，建议初始化预设大一点的空间。 按照其他语言的参考及研究经验，会考虑将负载因子设置为 0.7 ~ 0.75，此时平均检索长度接近于常数。 LinkedHashMap LinkedHashMap 是 HashMap 的子类。 在 HashMap 存储结构的基础上，使用了一对双向链表来记录添加元素的顺序。 HashMap 中的内部类 Node： 123456static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next;&#125; LinkedHashMap 中的内部类 Entry，用以替换 Node： 123456static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; LinkedHashMap 在原有的 HashMap 底层结构基础上，添加了一对指针 befor 和 after，指向当前元素的前一个和后一个元素。 与 LinkedHashSet 类似，LinkedHashMap 可以维护 Map 的迭代顺序：迭代顺序与 key - value 对的插入顺序一致。 LinkedHashMap 在遍历元素时，可以按照添加的顺序实现遍历。 实例： 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); map.put(123, &quot;AA&quot;); map.put(345, &quot;BB&quot;); map.put(12, &quot;CC&quot;); System.out.println(map);// &#123;345=BB, 123=AA, 12=CC&#125; map = new LinkedHashMap(); map.put(123, &quot;AA&quot;); map.put(345, &quot;BB&quot;); map.put(12, &quot;CC&quot;); System.out.println(map);// &#123;123=AA, 345=BB, 12=CC&#125; &#125;&#125; 对于频繁的遍历操作，此类执行效率高于 HashMap。 TreeMap TreeMap 存储 key - value 对时，需要根据 key - value 对进行排序。TreeMap 可以保证所有的 key - value 对处于有序状态。 TreeMap 底层使用红黑树结构存储数据。 TreeMap 的 key 的排序： 自然排序：TreeMap 的所有的 key 应该是同一个类的对象，否则将会抛出 ClasssCastException 异常，同时，key 所在的类需要实现 Comparable 接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106public class Test &#123; public static void main(String[] args) &#123; TreeMap map = new TreeMap(); User u1 = new User(&quot;Tom&quot;, 23); User u2 = new User(&quot;Jerry&quot;, 32); User u3 = new User(&quot;Jack&quot;, 20); User u4 = new User(&quot;Rose&quot;, 18); map.put(u1, 98); map.put(u2, 89); map.put(u3, 76); map.put(u4, 100); Set entrySet = map.entrySet(); Iterator iterator1 = entrySet.iterator(); while (iterator1.hasNext()) &#123; Object obj = iterator1.next(); Map.Entry entry = (Map.Entry) obj; System.out.println(entry.getKey() + &quot;----&gt;&quot; + entry.getValue()); &#125; &#125;&#125;class User implements Comparable &#123; private String name; private int age; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; System.out.println(&quot;User equals()....&quot;); if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (age != user.age) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; //return name.hashCode() + age; int result = name != null ? name.hashCode() : 0; result = 31 * result + age; return result; &#125; // 按照姓名从大到小排列，年龄从小到大排列 @Override public int compareTo(Object o) &#123; if (o instanceof User) &#123; User user = (User) o; // return -this.name.compareTo(user.name); int compare = -this.name.compareTo(user.name); if (compare != 0) &#123; return compare; &#125; else &#123; return Integer.compare(this.age, user.age); &#125; &#125; else &#123; throw new RuntimeException(&quot;输入的类型不匹配&quot;); &#125; &#125;&#125;输出结果：User&#123;name=&#x27;Tom&#x27;, age=23&#125;----&gt;98User&#123;name=&#x27;Rose&#x27;, age=18&#125;----&gt;100User&#123;name=&#x27;Jerry&#x27;, age=32&#125;----&gt;89User&#123;name=&#x27;Jack&#x27;, age=20&#125;----&gt;76 定制排序：创建 TreeMap 时，传入一个 Comparator 对象，该对象负责对 TreeMap 中的所有 key 进行排序。此时不需要 Map 的 key 实现 Comparable 接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public class Test &#123; public static void main(String[] args) &#123; // 按照年龄从小到大排序 TreeMap map = new TreeMap(new Comparator() &#123; @Override public int compare(Object o1, Object o2) &#123; if (o1 instanceof User &amp;&amp; o2 instanceof User) &#123; User u1 = (User) o1; User u2 = (User) o2; return Integer.compare(u1.getAge(), u2.getAge()); &#125; throw new RuntimeException(&quot;输入的类型不匹配！&quot;); &#125; &#125;); User u1 = new User(&quot;Tom&quot;, 23); User u2 = new User(&quot;Jerry&quot;, 32); User u3 = new User(&quot;Jack&quot;, 20); User u4 = new User(&quot;Rose&quot;, 18); map.put(u1, 98); map.put(u2, 89); map.put(u3, 76); map.put(u4, 100); Set entrySet = map.entrySet(); Iterator iterator1 = entrySet.iterator(); while (iterator1.hasNext()) &#123; Object obj = iterator1.next(); Map.Entry entry = (Map.Entry) obj; System.out.println(entry.getKey() + &quot;----&gt;&quot; + entry.getValue()); &#125; &#125;&#125;class User implements Comparable &#123; private String name; private int age; public User() &#123; &#125; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125; @Override public boolean equals(Object o) &#123; System.out.println(&quot;User equals()....&quot;); if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; User user = (User) o; if (age != user.age) &#123; return false; &#125; return name != null ? name.equals(user.name) : user.name == null; &#125; @Override public int hashCode() &#123; //return name.hashCode() + age; int result = name != null ? name.hashCode() : 0; result = 31 * result + age; return result; &#125; // 按照姓名从大到小排列，年龄从小到大排列 @Override public int compareTo(Object o) &#123; if (o instanceof User) &#123; User user = (User) o; // return -this.name.compareTo(user.name); int compare = -this.name.compareTo(user.name); if (compare != 0) &#123; return compare; &#125; else &#123; return Integer.compare(this.age, user.age); &#125; &#125; else &#123; throw new RuntimeException(&quot;输入的类型不匹配&quot;); &#125; &#125;&#125;输出结果：User&#123;name=&#x27;Rose&#x27;, age=18&#125;----&gt;100User&#123;name=&#x27;Jack&#x27;, age=20&#125;----&gt;76User&#123;name=&#x27;Tom&#x27;, age=23&#125;----&gt;98User&#123;name=&#x27;Jerry&#x27;, age=32&#125;----&gt;89 TreeMap 判断两个 key 相等的标准：两个 key 通过 compareTo() 或者 compare() 返回 0。 Hashtable Hashtable 是个古老的 Map 实现类，JDK 1.0 就提供了。不同于 HashMap，Hashtable 是线程安全的，但效率低。 Hashtable 实现原理和 HashMap 相同，功能相同。底层都使用哈希表结构，查询速度快，很多情况下可以互用。 与 HashMap 不同，Hashtable 不允许使用 null 作为 key 和 value。 实例： 1234567891011public class Test &#123; public static void main(String[] args) &#123; Map map = new HashMap(); map.put(null, 123); map.put(123, null); map.put(null, null); System.out.println(map);// &#123;null=null, 123=null&#125; map = new Hashtable(); map.put(null, 123);// java.lang.NullPointerException &#125;&#125; 与 HashMap 一样，Hashtable 也不能保证其中 key - value 对的顺序。 Hashtable 判断两个 key 相等、两个 value 相等的标准，与 HashMap 一致。 Properties Properties 类是 Hashtable 的子类，常用于处理配置文件。 由于属性文件里的 key、value 都是字符串类型，所以 Properties 里的 key 和 value 都是字符串类型。 存取数据时，建议使用 setProperty(String key, String value) 和 getProperty(String key)。 public Object setProperty(String key, String value) ： 保存一对属性。 public String getProperty(String key) ：使用此属性列表中指定的键搜索属性值。 public Set&lt;String&gt; stringPropertyNames() ：所有键的名称的集合。 12345678910111213141516171819202122232425public class ProDemo &#123; public static void main(String[] args) &#123; // 创建属性集对象 Properties properties = new Properties(); // 添加键值对元素 properties.setProperty(&quot;filename&quot;, &quot;a.txt&quot;); properties.setProperty(&quot;length&quot;, &quot;209385038&quot;); properties.setProperty(&quot;location&quot;, &quot;D:\\\\a.txt&quot;); // 打印属性集对象 System.out.println(properties); // 通过键，获取属性值 System.out.println(properties.getProperty(&quot;filename&quot;)); System.out.println(properties.getProperty(&quot;length&quot;)); System.out.println(properties.getProperty(&quot;location&quot;)); // 遍历属性集，获取所有键的集合 Set&lt;String&gt; strings = properties.stringPropertyNames(); // 打印键值对 for (String key : strings) &#123; System.out.println(key + &quot; -- &quot; + properties.getProperty(key)); &#125; &#125;&#125; 与流有关的方法： public void load(InputStream inStream)： 从字节输入流中读取键值对。 public void load(Reader reader)：从字符输入流中读取键值对。 public void store(Writer writer,String comments) public void store(OutputStream out,String comments) 实例： 123456789101112Properties pros = new Properties();try (FileInputStream fis = new FileInputStream(&quot;jdbc.properties&quot;)) &#123; // 加载流对应的文件 pros.load(fis); // 遍历集合并打印 Set&lt;String&gt; strings = properties.stringPropertyNames(); for (String key : strings) &#123; System.out.println(key + &quot; -- &quot; + properties.getProperty(key)); &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; jdbc.properties 格式，以 = 连接，不要有空格： 12user=Tompassword=123qwe Collections 工具类 Collections 是一个操作 Set、List 和 Map 等集合的工具类。 操作数组的工具类：Arrays。 Collections 中提供了一系列静态的方法对集合元素进行排序、查询和修改等操作，还提供了对集合对象设置不可变、对集合对象实现同步控制等方法。 常用方法： 排序操作： reverse(List list)：反转 list 中元素的顺序。 shuffle(List list)：对 list 集合元素进行随机排序。 sort(List list)：根据元素的自然顺序对指定 list 集合元素按升序排序 (自然排序)。 sort(List list, Comparator comparator)：根据指定的 comparator 产生的顺序对 list 集合元素进行排序 (定制排序)。 swap(List list, int i, int j)：将指定 list 集合中的 i 处元素和 j 处元素进行交换。 实例： 1234567891011121314151617181920public class Test &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(765); list.add(765); list.add(-97); list.add(0); System.out.println(list);// [123, 43, 765, 765, 765, -97, 0] // Collections.reverse(list);// [0, -97, 765, 765, 765, 43, 123] // Collections.shuffle(list);// [765, 43, 123, 0, -97, 765, 765] // Collections.sort(list);// [-97, 0, 43, 123, 765, 765, 765] // Collections.swap(list, 1, 2);// [123, 765, 43, 765, 765, -97, 0] System.out.println(list); &#125;&#125; 查找和替换操作： Object max(Collection coll)：根据元素的自然顺序，返回给定集合 coll 中的最大元素 (自然排序)。 Object max(Collection coll, Comparator )：根据 comparator 指定的顺序，返回给定集合 coll 中的最大元素 (定制排序)。 Object min(Collection)：根据元素的自然顺序，返回给定集合 coll 中的最小元素 (自然排序)。 Object min(Collection，Comparator)：根据 comparator 指定的顺序，返回给定集合 coll 中的最小元素 (定制排序)。 int frequency(Collection coll, Object obj)：返回指定集合 coll 中指定元素 obj 出现的次数。 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(765); list.add(765); list.add(-97); list.add(0); int frequency = Collections.frequency(list, 123); System.out.println(frequency);// 1 &#125;&#125; void copy(List dest, List src)：将 src 中的内容复制到 dest 中。注意：需要将dest数组中填充元素，数量不低于src的长度。 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; List src = new ArrayList(); src.add(123); src.add(43); src.add(765); src.add(-97); src.add(0); // 错误的写法：java.lang.IndexOutOfBoundsException: Source does not fit in dest /*List dest = new ArrayList(); List dest = new ArrayList(src.size()); Collections.copy(dest, src);*/ // 正确的写法：需要将dest数组中填充元素，数量不低于src的长度 List dest = Arrays.asList(new Object[src.size()]);// 1.先填充dest集合 System.out.println(dest.size());// = list.size(); Collections.copy(dest, src);// 2.再复制src集合 System.out.println(dest);// [123, 43, 765, -97, 0] &#125;&#125; boolean replaceAll(List list, Object oldVal, Object newVal)：使用新值 newVal 替换 list 对象的所有旧值 oldVal。 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(765); list.add(765); list.add(-97); list.add(0); System.out.println(list);// [123, 43, 765, 765, 765, -97, 0] Collections.replaceAll(list, 765, 888); System.out.println(list);// [123, 43, 888, 888, 888, -97, 0] &#125;&#125; 同步控制操作： Collections 类中提供了多个 synchronizedXxx()，该方法可使将指定集合包装成线程同步的集合，从而可以解决多线程并发访问集合时的线程安全问题。有了这些方法，涉及到线程安全的集合，可以不需要使用 Vector 或 Hashtable。 12345678910111213public class Test &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add(123); list.add(43); list.add(765); list.add(-97); list.add(0); // 返回的list1即为线程安全的List List list1 = Collections.synchronizedList(list); &#125;&#125; 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的枚举类和注解","slug":"java-enumandannonation","date":"2021-03-18T13:18:56.000Z","updated":"2021-04-09T08:00:30.844Z","comments":true,"path":"2021/03/18/java-enumandannonation/","link":"","permalink":"http://example.com/2021/03/18/java-enumandannonation/","excerpt":"","text":"枚举类 (Enum) 的使用枚举类的理解： 类的对象只有有限个，确定的，则此类称为枚举类。 当需要定义一组常量时，强烈建议使用枚举类。 如果枚举类中只有一个对象，则可以作为单例模式的实现方式。 jdk 5.0 之后，可以在 switch 表达式中使用 Enum 定义的枚举类的对象作为表达式，case 子句可以直接使用枚举值的名字，无需添加枚举类作为限定。 如何定义枚举类： 方式一：jdk 5.0 之前需要自定义枚举类。 私有化类的构造器，保证不能在类的外部创建其对象。 在类的内部创建枚举类的实例。声明为：public static final。 对象如果有实例变量，应该声明为 private final，并在构造器中初始化。 123456789101112131415161718192021222324252627282930313233343536373839404142public class Test &#123; public static void main(String[] args) &#123; System.out.println(Season.SPRING);// Season&#123;SEASONNAME=&#x27;春天&#x27;, SEASONDESC=&#x27;春暖花开&#x27;&#125; &#125;&#125;// 自定义枚举类class Season &#123; // 1.声明Season对象的属性：private final 修饰 --- 常量 private final String SEASONNAME;//季节的名称 private final String SEASONDESC;//季节的描述 // 2.私有化类的构造器，并给对象的属性赋值 private Season(String seasonName, String seasonDesc) &#123; this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; &#125; // 3.提供当前枚举类的多个对象 public static final Season SPRING = new Season(&quot;春天&quot;, &quot;春暖花开&quot;); public static final Season SUMMER = new Season(&quot;夏天&quot;, &quot;夏日炎炎&quot;); public static final Season AUTUMN = new Season(&quot;秋天&quot;, &quot;秋高气爽&quot;); public static final Season WINTER = new Season(&quot;冬天&quot;, &quot;白雪皑皑&quot;); // 4.其他诉求一：获取枚举类对象的属性 public String getSEASONNAME() &#123; return SEASONNAME; &#125; public String getSEASONDESC() &#123; return SEASONDESC; &#125; // 5.其他诉求二：提供toString() @Override public String toString() &#123; return &quot;Season&#123;&quot; + &quot;SEASONNAME=&#x27;&quot; + SEASONNAME + &#x27;\\&#x27;&#x27; + &quot;, SEASONDESC=&#x27;&quot; + SEASONDESC + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 方式二：jdk 5.0 之后，可以使用新增的 enum 关键字定义枚举类。 使用 enum 关键字定义的枚举类默认继承了 java.lang.Enum 类，因此不能再继承其他类。 枚举类的构造器只能使用 private 权限修饰符。 枚举类的所有实例必须在枚举类中显式列出，以 “,” 分隔，以 “;” 结尾。列出的实例系统会默认添加 public static final 修饰。 必须在枚举类的第一行声明枚举类对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Test &#123; public static void main(String[] args) &#123; System.out.println(Season.SPRING);// SPRING System.out.println(&quot;enum的父类是：&quot; + Season.class.getSuperclass());// class java.lang.Enum &#125;&#125;// 使用enum关键字定义枚举类enum Season &#123; // 1.首行提供当前枚举类的多个对象: 多个对象之间以&quot;,&quot;隔开，末尾的对象以&quot;;&quot;结束 SPRING(&quot;春天&quot;, &quot;春暖花开&quot;), SUMMER(&quot;夏天&quot;, &quot;夏日炎炎&quot;), AUTUMN(&quot;秋天&quot;, &quot;秋高气爽&quot;), WINTER(&quot;冬天&quot;, &quot;白雪皑皑&quot;); // 2.声明Season对象的属性：private final 修饰 --- 常量 private final String SEASONNAME;//季节的名称 private final String SEASONDESC;//季节的描述 // 3.私有化类的构造器，并给对象的属性赋值 private Season(String seasonName, String seasonDesc) &#123; this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; &#125; // 4.其他诉求一：获取枚举类对象的属性 public String getSEASONNAME() &#123; return SEASONNAME; &#125; public String getSEASONDESC() &#123; return SEASONDESC; &#125; // 5.其他诉求二：提供toString()，一般不重写 /*@Override public String toString() &#123; return &quot;Season&#123;&quot; + &quot;SEASONNAME=&#x27;&quot; + SEASONNAME + &#x27;\\&#x27;&#x27; + &quot;, SEASONDESC=&#x27;&quot; + SEASONDESC + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;*/&#125; Enum 类中的常用方法： values()：返回枚举类型的对象数组，该方法可以很方便地遍历枚举类的所有的枚举值。 12345678public class Test &#123; public static void main(String[] args) &#123; Thread.State[] values = Thread.State.values(); for (Thread.State value : values) &#123; System.out.print(value + &quot; &quot;);// NEW RUNNABLE BLOCKED WAITING TIMED_WAITING TERMINATED &#125; &#125;&#125; valueOf(String str)：可以把一个字符串转为对应的枚举类对象。要求字符串必须是枚举类对象的 “名字”，如不是，会抛出运行时异常：java.lang.IllegalArgumentException。 12345678public class Test &#123; public static void main(String[] args) &#123; Thread.State aNew = Thread.State.valueOf(&quot;NEW&quot;); System.out.println(aNew);// NEW Thread.State dead = Thread.State.valueOf(&quot;DEAD&quot;); System.out.println(dead);// java.lang.IllegalArgumentException: No enum constant java.lang.Thread.State.DEAD &#125;&#125; toString()：返回当前枚举类对象常量的名称。 使用 enum 关键字定义的枚举类实现接口的情况： 和普通 java 类一样，枚举类可以实现一个或多个接口。 若每个枚举值在调用实现的接口的方法时，如果呈现出相同的行为方式，则只要统一实现该方法即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Test &#123; public static void main(String[] args) &#123; Season spring = Season.SPRING; spring.show();// 这是一个季节 &#125;&#125;interface Info &#123; void show();&#125;// 使用enum关键字定义枚举类enum Season implements Info &#123; // 1.首行提供当前枚举类的多个对象: 多个对象之间以&quot;,&quot;隔开，末尾的对象以&quot;;&quot;结束 SPRING(&quot;春天&quot;, &quot;春暖花开&quot;), SUMMER(&quot;夏天&quot;, &quot;夏日炎炎&quot;), AUTUMN(&quot;秋天&quot;, &quot;秋高气爽&quot;), WINTER(&quot;冬天&quot;, &quot;白雪皑皑&quot;); // 2.声明Season对象的属性：private final 修饰 --- 常量 private final String SEASONNAME;//季节的名称 private final String SEASONDESC;//季节的描述 // 3.私有化类的构造器，并给对象的属性赋值 private Season(String seasonName, String seasonDesc) &#123; this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; &#125; // 4.其他诉求一：获取枚举类对象的属性 public String getSEASONNAME() &#123; return SEASONNAME; &#125; public String getSEASONDESC() &#123; return SEASONDESC; &#125; // 每个枚举值在调用实现的接口的方法时，呈现出相同的行为方式 @Override public void show() &#123; System.out.println(&quot;这是一个季节&quot;); &#125; // 5.其他诉求二：提供toString()，一般不重写 /*@Override public String toString() &#123; return &quot;Season&#123;&quot; + &quot;SEASONNAME=&#x27;&quot; + SEASONNAME + &#x27;\\&#x27;&#x27; + &quot;, SEASONDESC=&#x27;&quot; + SEASONDESC + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;*/&#125; 若需要每个枚举值在调用实现的接口的方法时，呈现出不同的行为方式，则可以让每个枚举值分别来实现该方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class Test &#123; public static void main(String[] args) &#123; Season spring = Season.SPRING; spring.show();// 现在是春天 Season winter = Season.WINTER; winter.show();// 现在是冬天 &#125;&#125;interface Info &#123; void show();&#125;// 使用enum关键字定义枚举类enum Season implements Info &#123; // 1.首行提供当前枚举类的多个对象: 多个对象之间以&quot;,&quot;隔开，末尾的对象以&quot;;&quot;结束 SPRING(&quot;春天&quot;, &quot;春暖花开&quot;) &#123; @Override public void show() &#123; System.out.println(&quot;现在是春天&quot;); &#125; &#125;, SUMMER(&quot;夏天&quot;, &quot;夏日炎炎&quot;) &#123; @Override public void show() &#123; System.out.println(&quot;现在是夏天&quot;); &#125; &#125;, AUTUMN(&quot;秋天&quot;, &quot;秋高气爽&quot;) &#123; @Override public void show() &#123; System.out.println(&quot;现在是秋天&quot;); &#125; &#125;, WINTER(&quot;冬天&quot;, &quot;白雪皑皑&quot;) &#123; @Override public void show() &#123; System.out.println(&quot;现在是冬天&quot;); &#125; &#125;; // 2.声明Season对象的属性：private final 修饰 --- 常量 private final String SEASONNAME;//季节的名称 private final String SEASONDESC;//季节的描述 // 3.私有化类的构造器，并给对象的属性赋值 private Season(String seasonName, String seasonDesc) &#123; this.SEASONNAME = seasonName; this.SEASONDESC = seasonDesc; &#125; // 4.其他诉求一：获取枚举类对象的属性 public String getSEASONNAME() &#123; return SEASONNAME; &#125; public String getSEASONDESC() &#123; return SEASONDESC; &#125; // 5.其他诉求二：提供toString()，一般不重写 /*@Override public String toString() &#123; return &quot;Season&#123;&quot; + &quot;SEASONNAME=&#x27;&quot; + SEASONNAME + &#x27;\\&#x27;&#x27; + &quot;, SEASONDESC=&#x27;&quot; + SEASONDESC + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;*/&#125; 注解 (Annotation) 的使用注解的概述： 从 jdk 5.0 开始，java 增加了对元数据 (MetaData) 的支持，也就是 Annotation (注解)。 Annotation 其实就是代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取，并执行相应的处理。通过使用 Annotation，程序员可以在不改变原有逻辑的情况下，在源文件中嵌入一些补充信息。代码分析工具、开发工具和部署工具可以通过这些补充信息进行验证或者进行部署。 Annotation 可以像修饰符一样被使用，可用于修饰包、类、构造器、方法、 成员变量、参数、局部变量的声明，这些信息被保存在 Annotation 的 “name = value” 对中。 在 JavaSE 中，注解的使用目的比较简单，例如标记过时的功能，忽略警告等。在 JavaEE/Android 中注解占据了更重要的角色，例如用来配置应用程序的任何切面，代替 JavaEE 旧版中所遗留的繁冗代码和 XML 配置等。 未来的开发模式都是基于注解的，JPA 是基于注解的，Spring 2.5 以上都是基于注解的，Hibernate 3.x 以后也是基于注解的，现在的 Struts2 有一部分也是基于注解的。注解是一种趋势，一定程度上可以说：框架 = 注解 + 反射 + 设计模式。 常见的 Annotation 示例： 使用 Annotation 时要在其前面增加 @ 符号，并把该 Annotation 当成一个修饰符使用，用于修饰它支持的程序元素。 示例一：生成文档相关的注解。 @author：标明开发该类模块的作者，多个作者之间使用 “,”分割。 @version：标明该类模块的版本。 @see：参考转向，也就是相关主题。 @since：从哪个版本开始增加的。 @param：对方法中某参数的说明，如果没有参数就不能写。 @return：对方法返回值的说明，如果方法的返回值类型是 void 就不能写。 @exception：对方法可能抛出的异常进行说明 ，如果方法没有用 throws 显式抛出的异常就不能写其中。 @param、@return 和 @exception 这三个标记都是只用于方法的。 @param 的格式要求：@param 形参名 形参类型 形参说明。 @return 的格式要求：@return 返回值类型 返回值说明。 @exception 的格式要求：@exception 异常类型 异常说明。 @param 和 @exception 可以并列多个。 实例： 示例二： 在编译时进行格式查 (jdk 内置的三个基本注解)。 @Override：限定重写父类方法，该注解只能用于方法。 @Deprecated：用于表示所修饰的元素 (类，方法等) 已过时，通常是因为所修饰的结构危险或存在更好的选择。 @SuppressWarnings：抑制编译器警告。 实例： 示例三： 跟踪代码依赖性，实现替代配置文件功能。 Servlet 3.0 提供了注解 (annotation)，使得不再需要在 web.xml 文件中进行 Servlet 的部署： spring 框架中关于事务的管理： 自定义注解：参照 @SuppressWarnings定义。 注解声明为：@interface。 自定义注解自动继承了 java.lang.annotation.Annotation 接口。 Annotation 的成员变量在 Annotation 定义中以无参数方法的形式来声明。其方法名和返回值定义了该成员的名字和类型，我们称为配置参数。类型只能是八种基本数据类型、String 类型 、Class 类型 、Enum 类型 、Annotation 类型，以上所有类型的数组。 可以在定义 Annotation 的成员变量时为其指定初始值，指定成员变量的初始值可使用 default 关键字。 没有成员定义的 Annotation 称为标记 (是一个标识作用)；包含成员变量的 Annotation 称为元数据 Annotation。 如果注解只有一个成员，建议使用参数名为 value。 如果注解有成员，那么使用时必须指定参数值，除非它有默认值。格式是 “参数名 = 参数值”，如果只有一个参数成员，且名称为 value，可以省略 “value=”，直接写参数值。 自定义注解必须配上注解的信息处理流程才有意义 (使用反射，能够得到注解的内容，然后根据内容和注解的对象建立关系，比如 Servlet 的注解)。 实例： 123public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125; jdk 中的元注解： jdk 的元注解是用于修饰其他注解而定义的，即对现有注解进行解释说明的注解。 元数据：对数据进行修饰的数据。比如：String name = &quot;Tom&quot;;，Tom 是数据，而 String 和 name 就是修饰 Tom 的元数据。 jdk 5.0 提供了 4 个标准的 meta-annotation 类型，分别是： @Retention：只能用于修饰一个 Annotation 定义，用于指定该 Annotation 的生命周期。@Rentention 包含一个 RetentionPolicy 类型的成员变量，使用 @Rentention 时必须为该 value 成员变量指定值： 12345678910@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Retention &#123; /** * Returns the retention policy. * @return the retention policy */ RetentionPolicy value();&#125; 123456789101112131415161718192021public enum RetentionPolicy &#123; /** * Annotations are to be discarded by the compiler. */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior. */ CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. * * @see java.lang.reflect.AnnotatedElement */ RUNTIME&#125; RetentionPolicy.SOURCE：在源文件中有效，即源文件保留，编译器直接丢弃这种策略的注释。 RetentionPolicy.CLASS：在 class 文件 中有效，即 class 保留，当运行 java 程序时，JVM 不会保留注解。这是默认值。 RetentionPolicy.RUNTIME：在运行时有效，即运行时保留，当运行 java 程序时，JVM 会保留注释。程序可以通过反射获取该注释。 只有声明为 RUNTIME 生命周期的注解，才能通过反射获取。 实例： 1234@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125; @Target：用于修饰 Annotation 定义，用于指定被修饰的 Annotation 能用于修饰哪些程序元素。 @Target 也包含一个名为 value 的成员变量。 123456789101112@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Target &#123; /** * Returns an array of the kinds of elements an annotation type * can be applied to. * @return an array of the kinds of elements an annotation type * can be applied to */ ElementType[] value();&#125; 123456789101112131415161718192021222324252627282930313233343536373839public enum ElementType &#123; /** Class, interface (including annotation type), or enum declaration */ TYPE, /** Field declaration (includes enum constants) */ FIELD, /** Method declaration */ METHOD, /** Formal parameter declaration */ PARAMETER, /** Constructor declaration */ CONSTRUCTOR, /** Local variable declaration */ LOCAL_VARIABLE, /** Annotation type declaration */ ANNOTATION_TYPE, /** Package declaration */ PACKAGE, /** * Type parameter declaration * * @since 1.8 */ TYPE_PARAMETER, /** * Use of a type * * @since 1.8 */ TYPE_USE&#125; 各取值含义如下： 实例： 12345678910@Target(ElementType.TYPE)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125;@MyAnnotation(&quot;修改了默认值&quot;)class Person &#123; public Person() &#123; &#125;&#125; @Documented：用于指定被该元 Annotation 修饰的 Annotation 类将被 javadoc 工具提取成文档。默认情况下，javadoc 是不包括注解的。如果希望一个注解在被 javadoc 解析生成文档时能保存下来，需要添加此注解。 12345@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Documented &#123;&#125; 定义为 @Documented 的注解，必须设置 @Retention 值为 RUNTIME。 @Inherited：被它修饰的 Annotation 将具有继承性。如果某个类使用了被 @Inherited 修饰的 Annotation，则其子类将自动具有该注解。 比如：如果把标有 @Inherited 注解的自定义的注解标注在类级别上，子类则可以继承父类类级别的注解。 实际应用中，使用较少。 自定义注解时，通常都会指明 @Retention 和 @Target 这两个注解。 利用反射获取注解信息： jdk 5.0 在 java.lang.reflect 包下新增了 AnnotatedElement 接口，该接口代表程序中可以接受注解的程序元素。 当一个 Annotation 类型被定义为运行时 Annotation 后，该注解才是运行时可见，当 class 文件被载入时保存在 class 文件中的 Annotation 才会被虚拟机读取。 程序可以调用 AnnotatedElement 对象的如下方法来访问 Annotation 信息： 实例： 1234567891011121314151617181920212223242526@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125;@MyAnnotation(&quot;修改了默认值&quot;)class Person &#123; public Person() &#123; &#125; public static void main(String[] args) &#123; Class&lt;Person&gt; clazz = Person.class; Annotation[] annotations = clazz.getAnnotations(); for (Annotation annotation : annotations) &#123; System.out.println(annotation);// @cn.xisun.java.base.MyAnnotation(value=[修改了默认值]) &#125; Annotation annotation = clazz.getAnnotation(MyAnnotation.class); MyAnnotation myAnnotation = (MyAnnotation) annotation; String[] info = myAnnotation.value(); System.out.println(Arrays.toString(info));// [修改了默认值] &#125;&#125; jdk 8.0 中注解的新特性： java 8.0 对注解处理提供了两点改进：可重复的注解及可用于类型的注解。此外，反射也得到了加强，在 java 8.0 中能够得到方法参数的名称。这会简化标注在方法参数上的注解。 可重复注解： jdk 8.0 之前的写法： 123456789101112131415161718192021222324252627282930313233343536@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125;// 定义新数组，值为需要重复注解对象的数组 @Retention(RetentionPolicy.RUNTIME)@interface MyAnnotations &#123; MyAnnotation[] value();&#125;// jdk 8.0之前的写法：@MyAnnotations(&#123;@MyAnnotation(&quot;注解1&quot;), @MyAnnotation(&quot;注解2&quot;)&#125;)class Person &#123; public Person() &#123; &#125; public static void main(String[] args) &#123; Class&lt;Person&gt; clazz = Person.class; Annotation[] annotations = clazz.getAnnotations(); for (Annotation annotation : annotations) &#123; System.out.println(annotation); &#125; Annotation annotation = clazz.getAnnotation(MyAnnotations.class); MyAnnotations myAnnotation = (MyAnnotations) annotation; MyAnnotation[] info = myAnnotation.value(); System.out.println(Arrays.toString(info)); &#125;&#125;输出结果：@cn.xisun.java.base.MyAnnotations(value=[@cn.xisun.java.base.MyAnnotation(value=[注解1]), @cn.xisun.java.base.MyAnnotation(value=[=注解2])])[@cn.xisun.java.base.MyAnnotation(value=[注解1]), @cn.xisun.java.base.MyAnnotation(value=[注解2])] jdk 8.0 之后的写法：利用 @Repeatable。 12345678910111213141516171819202122232425262728293031323334@Repeatable(MyAnnotations.class)@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotation &#123; String[] value() default &quot;自定义的注解&quot;;&#125;@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@interface MyAnnotations &#123; MyAnnotation[] value();&#125;@MyAnnotation(&quot;注解1&quot;)@MyAnnotation(&quot;注解2&quot;)class Person &#123; public Person() &#123; &#125; public static void main(String[] args) &#123; Class&lt;Person&gt; clazz = Person.class; Annotation[] annotations = clazz.getAnnotations(); for (Annotation annotation : annotations) &#123; System.out.println(annotation);// @cn.xisun.java.base.MyAnnotation(value=[修改了默认值]) &#125; Annotation annotation = clazz.getAnnotation(MyAnnotations.class); MyAnnotations myAnnotation = (MyAnnotations) annotation; MyAnnotation[] info = myAnnotation.value(); System.out.println(Arrays.toString(info));// [修改了默认值] &#125;&#125; 类型注解： jdk 8.0 之后，关于元注解 @Target 的参数类型 ElementType 枚举值多了两个：TYPE_PARAMETER 和 TYPE_USE。、 ElementType.TYPE_PARAMETER：表示该注解能写在类型变量的声明语句中，如：泛型声明。 123456789public class TestTypeDefine&lt;@TypeDefine() U&gt; &#123; private U u; public &lt;@TypeDefine() T&gt; void test(T t)&#123; &#125;&#125;@Target(&#123;ElementType.TYPE_PARAMETER&#125;)@interface TypeDefine&#123;&#125; ElementType.TYPE_USE：表示该注解能写在使用类型的任何语句中。 1234567891011121314151617181920@Target(ElementType.TYPE_USE)@interface MyAnnotation &#123;&#125;@MyAnnotationpublic class AnnotationTest&lt;U&gt; &#123; @MyAnnotation private String name;// 对属性添加注解 public static &lt;@MyAnnotation T&gt; void method(T t) &#123;&#125;// 对泛型添加注解 public static void test(@MyAnnotation String arg) throws @MyAnnotation Exception &#123;&#125;// 对异常添加注解 public static void main(String[] args) &#123; AnnotationTest&lt;@MyAnnotation String&gt; t = null; int a = (@MyAnnotation int) 2L; @MyAnnotation int b = 10; &#125;&#125; 在 java 8.0 之前，注解只能是在声明的地方所使用，从 java 8.0 开始，注解可以应用在任何地方。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中与数字计算相关的类","slug":"java-math","date":"2021-03-18T03:47:03.000Z","updated":"2021-04-09T08:01:11.261Z","comments":true,"path":"2021/03/18/java-math/","link":"","permalink":"http://example.com/2021/03/18/java-math/","excerpt":"","text":"java.lang.Math 类java.lang.Math 类提供了一系列静态方法用于科学计算。其方法的参数和返回值类型一般为 double 型。常用的方法有： abs()：绝对值。 acos()，asin()，atan()，cos()，sin()，tan()： 三角函数。 sqrt()：平方根。 pow(double a,doble b)：a 的 b 次幂。 log()：自然对数。 exp()：以 e 为底的指数。 max(double a,double b)：较大值。 min(double a,double b)：较小值。 random()：返回 0.0 到 1.0 的随机数。 long round(double a)：double 型数据 a 转换为 long 型 (四舍五入)。 toDegrees(double angrad)：弧度转换角度。 toRadians(double angdeg)：角度转换弧度。 java.math.BigInteger 类与 java.math.BigDecimal 类BigInteger Integer 类作为 int 的包装类，能存储的最大整型值为 2^31 - 1，Long 类也是有限的，最大为 2^63 - 1。如果要表示再大的整数，不管是基本数据类型还是他们的包装类都无能为力，更不用说进行运算了。 java.math 包的 BigInteger 类，可以表示不可变的任意精度的整数。BigInteger 提供所有 java 的基本整数操作符的对应物，并提供 java.lang.Math 的所有相关方法。另外，BigInteger 还提供以下运算：模算术、GCD 计算、质数测试、素数生成、位操作以及一些其他操作。 构造器： BigInteger(String val)：常用字符串构建 BigInteger 对象。 常用方法： public BigInteger abs()：返回此 BigInteger 的绝对值的 BigInteger。 BigInteger add(BigInteger val)：返回其值为 (this + val) 的 BigInteger。 BigInteger subtract(BigInteger val)：返回其值为 (this - val) 的 BigInteger。 BigInteger multiply(BigInteger val)：返回其值为 (this * val) 的 BigInteger。 BigInteger divide(BigInteger val)：返回其值为 (this / val) 的 BigInteger。整数相除只保留整数部分。 BigInteger remainder(BigInteger val)：返回其值为 (this % val) 的 BigInteger。 BigInteger[] divideAndRemainder(BigInteger val)：返回包含 (this / val) 后跟 (this % val) 的两个 BigInteger 的数组。 BigInteger pow(int exponent)：返回其值为 (this^exponent ) 的 BigInteger。 实例： 123456public class Test &#123; public static void main(String[] args) &#123; BigInteger bi = new BigInteger(&quot;12433241123223262154841264166142223&quot;); System.out.println(bi); &#125;&#125; BigDecimal 一般的 Float 类和 Double 类可以用来做科学计算或工程计算，但在商业计算中，要求数字精度比较高，故用到 java.math.BigDecimal 类。 BigDecimal 类支持不可变的、任意精度的有符号十进制定点数。 构造器： public BigDecimal(double val) public BigDecimal(String val) 常用方法： public BigDecimal add(BigDecimal augend)：加。 public BigDecimal subtract(BigDecimal subtrahend)：减。 public BigDecimal multiply(BigDecimal multiplicand)：乘。 public BigDecimal divide(BigDecimal divisor, int scale, int roundingMode)：除。 实例： 123456789public class Test &#123; public static void main(String[] args) &#123; BigDecimal bd = new BigDecimal(&quot;12435.351&quot;); BigDecimal bd2 = new BigDecimal(&quot;11&quot;); // System.out.println(bd.divide(bd2));// 未指定精度，如果除不尽，会报错 System.out.println(bd.divide(bd2, BigDecimal.ROUND_HALF_UP));// 四舍五入 System.out.println(bd.divide(bd2, 15, BigDecimal.ROUND_HALF_UP));// 保留15位小数 &#125;&#125; 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的比较器","slug":"java-compare","date":"2021-03-17T07:54:07.000Z","updated":"2021-04-09T08:00:12.063Z","comments":true,"path":"2021/03/17/java-compare/","link":"","permalink":"http://example.com/2021/03/17/java-compare/","excerpt":"","text":"在 java 中经常会涉及到对象数组等的排序问题，那么就涉及到对象之间的比较问题。 java 实现对象排序的方式有两种： 自然排序：java.lang.Comparable 定制排序：java.util.Comparator java.lang.Comparable — 自然排序 Comparable 接口强行对实现它的每个类的对象进行整体排序，这种排序被称为类的自然排序。 实现 Comparable 接口的类必须实现 compareTo(Object obj)，两个对象通过 compareTo(Object obj) 的返回值来比较大小。 重写 compareTo(Object obj) 的规则：如果当前对象 this 大于形参对象 obj，则返回正整数，如果当前对象 this 小于形参对象 obj，则返回负整数，如果当前对象 this 等于形参对象 obj，则返回零。 实现 Comparable 接口的对象列表或数组，可以通过 Collections.sort() (针对集合)或 Arrays.sort() (针对数组)进行自动排序。实现此接口的对象可以用作有序映射中的键或有序集合中的元素，无需指定比较器。 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; // 集合排序 List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;AA&quot;); list.add(&quot;VV&quot;); list.add(&quot;BB&quot;); list.add(&quot;AC&quot;); list.add(&quot;CC&quot;); list.add(&quot;EE&quot;); list.add(&quot;DE&quot;); for (String str : list) &#123; System.out.print(str + &quot; &quot;);// AA VV BB AC CC EE DE &#125; System.out.println(); Collections.sort(list); for (String str : list) &#123; System.out.print(str + &quot; &quot;);// AA AC BB CC DE EE VV &#125; System.out.println(); // 数组排序 String[] strings = &#123;&quot;AA&quot;, &quot;VV&quot;, &quot;BB&quot;, &quot;AC&quot;, &quot;CC&quot;, &quot;EE&quot;, &quot;DE&quot;&#125;; System.out.println(Arrays.toString(strings));// [AA, VV, BB, AC, CC, EE, DE] Arrays.sort(strings); System.out.println(Arrays.toString(strings));// [AA, AC, BB, CC, DE, EE, VV] &#125;&#125; 对于类 C 的每一个 e1 和 e2 来说，当且仅当 e1.compareTo(e2) == 0 与 e1.equals(e2) 具有相同的 boolean 值时，类 C 的自然排序才叫做与 equals 一致。建议 (虽然不是必需的) 最好使自然排序与 equals 一致。 Comparable 的典型实现：(默认都是从小到大排列的) String 类：按照字符串中字符的 Unicode 值进行比较。 Character 类：按照字符的 Unicode 值来进行比较。 数值类型对应的包装类以及 BigInteger 类、BigDecimal 类：按照它们对应的数值大小进行比较。 Boolean 类：true 对应的包装类实例大于 false 对应的包装类实例。 Date 类、Time 类等：后面的日期时间比前面的日期时间大。 对于自定义类来说，如果需要排序，我们可以让自定义类实现 Comparable 接口，并重写 compareTo(Object obj)，在 compareTo(Object obj) 中，指明如何排序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Test &#123; public static void main(String[] args) &#123; Goods[] arr = new Goods[5]; arr[0] = new Goods(&quot;lenovo&quot;, 34); arr[1] = new Goods(&quot;dell&quot;, 43); arr[2] = new Goods(&quot;xiaomi&quot;, 12); arr[3] = new Goods(&quot;huawei&quot;, 65); arr[4] = new Goods(&quot;microsoft&quot;, 43); System.out.println(&quot;排序前：&quot; + Arrays.toString(arr)); Arrays.sort(arr); System.out.println(&quot;排序后：&quot; + Arrays.toString(arr)); &#125;&#125;class Goods implements Comparable &#123; private String name; private double price; public Goods() &#123; &#125; public Goods(String name, double price) &#123; this.name = name; this.price = price; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; @Override public String toString() &#123; return &quot;Goods&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, price=&quot; + price + &#x27;&#125;&#x27;; &#125; // 先按照价格从低到高进行排序，再按照名称从高到低进行排序 @Override public int compareTo(Object o) &#123; if (o instanceof Goods) &#123; Goods goods = (Goods) o; if (this.price &gt; goods.price) &#123; return 1; &#125; else if (this.price &lt; goods.price) &#123; return -1; &#125; else &#123; return -this.name.compareTo(goods.name); &#125; // return Double.compare(this.getPrice(), goods.getPrice()); &#125; throw new RuntimeException(&quot;传入的数据类型有误&quot;); &#125;&#125; java.util.Comparator — 定制排序 当元素的类型没有实现 java.lang.Comparable 接口而又不方便修改代码，或者实现了 java.lang.Comparable 接口的排序规则不适合当前的操作，那么可以考虑使用 Comparator 的对象来排序，强行对多个对象进行整体排序的比较。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test &#123; public static void main(String[] args) &#123; // 集合排序 List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;AA&quot;); list.add(&quot;VV&quot;); list.add(&quot;BB&quot;); list.add(&quot;AC&quot;); list.add(&quot;CC&quot;); list.add(&quot;EE&quot;); list.add(&quot;DE&quot;); for (String str : list) &#123; System.out.print(str + &quot; &quot;);// AA VV BB AC CC EE DE &#125; System.out.println(); // 不再以String本身默认的从小到大排序，而是从大到小排序 Collections.sort(list, new Comparator&lt;String&gt;() &#123; @Override public int compare(String o1, String o2) &#123; return o2.compareTo(o1); &#125; &#125;); for (String str : list) &#123; System.out.print(str + &quot; &quot;);// VV EE DE CC BB AC AA &#125; System.out.println(); // 数组排序 String[] strings = &#123;&quot;AA&quot;, &quot;VV&quot;, &quot;BB&quot;, &quot;AC&quot;, &quot;CC&quot;, &quot;EE&quot;, &quot;DE&quot;&#125;; System.out.println(Arrays.toString(strings));// [AA, VV, BB, AC, CC, EE, DE] // 不再以String本身默认的从小到大排序，而是从大到小排序 Arrays.sort(strings, new Comparator&lt;String&gt;() &#123; @Override public int compare(String o1, String o2) &#123; return -o1.compareTo(o2); &#125; &#125;); System.out.println(Arrays.toString(strings));// [VV, EE, DE, CC, BB, AC, AA] &#125;&#125; 重写 compare(Object o1,Object o2)，比较 o1 和 o2 的大小： 如果方法返回正整数，则表示 o1 大于 o2；如果返回 0，表示相等；返回负整数，表示 o1 小于 o2。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class Test &#123; public static void main(String[] args) &#123; Goods[] arr = new Goods[5]; arr[0] = new Goods(&quot;lenovo&quot;, 34); arr[1] = new Goods(&quot;dell&quot;, 43); arr[2] = new Goods(&quot;xiaomi&quot;, 12); arr[3] = new Goods(&quot;huawei&quot;, 65); arr[4] = new Goods(&quot;lenovo&quot;, 43); System.out.println(&quot;排序前：&quot; + Arrays.toString(arr)); // 不以Goods本身的自然排序方式排序，更改为：按产品名称从低到高进行排序，再按照价格从高到低进行排序 Arrays.sort(arr, new Comparator&lt;Goods&gt;() &#123; @Override public int compare(Goods o1, Goods o2) &#123; if (!o1.getName().equals(o2.getName())) &#123; return o1.getName().compareTo(o2.getName()); &#125; else &#123; if (o1.getPrice() &lt; o2.getPrice()) &#123; return 1; &#125; else if (o1.getPrice() &gt; o2.getPrice()) &#123; return -1; &#125; &#125; return 0; &#125; &#125;); System.out.println(&quot;排序后：&quot; + Arrays.toString(arr)); &#125;&#125;class Goods implements Comparable &#123; private String name; private double price; public Goods() &#123; &#125; public Goods(String name, double price) &#123; this.name = name; this.price = price; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; @Override public String toString() &#123; return &quot;Goods&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, price=&quot; + price + &#x27;&#125;&#x27;; &#125; // 先按照价格从低到高进行排序，再按照名称从高到低进行排序 @Override public int compareTo(Object o) &#123; if (o instanceof Goods) &#123; Goods goods = (Goods) o; if (this.price &gt; goods.price) &#123; return 1; &#125; else if (this.price &lt; goods.price) &#123; return -1; &#125; else &#123; return -this.name.compareTo(goods.name); &#125; // return Double.compare(this.getPrice(), goods.getPrice()); &#125; throw new RuntimeException(&quot;传入的数据类型有误&quot;); &#125;&#125; 可以将 Comparator 传递给 sort()，比如：Collections.sort() 或 Arrays.sort()，从而允许在排序顺序上实现精确控制。 还可以使用 Comparator 来控制某些数据结构 (如有序 set 或有序映射) 的顺序，或者为那些没有自然顺序的对象 collection 提供排序。 Comparable 和 Comparator 的对比 Comparable 接口的方式一旦指定，能够保证 Comparable 接口实现类的对象在任何位置都可以比较大小。 Comparator 接口属于临时性的比较，什么时候需要什么时候实现。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的日期和时间","slug":"java-date","date":"2021-03-15T09:00:58.000Z","updated":"2021-04-09T08:00:17.338Z","comments":true,"path":"2021/03/15/java-date/","link":"","permalink":"http://example.com/2021/03/15/java-date/","excerpt":"","text":"jdk 8 之前的日期和时间 APIjava.lang.System 类 long currentTimeMillis()：返回当前时间与 1970 年 1 月 1 日 0 时 0 分 0 秒之间以毫秒为单位的时间差，也被称为时间戳。 System 类的其他说明： System 类代表系统，系统级的很多属性和控制方法都放置在该类的内部。 由于该类的构造器是 private 的，所以无法创建该类的对象，也就是无法实例化该类。其内部的成员变量和成员方法都是 static 的，所以也可以很方便的进行调用。 成员变量： System 类内部包含 in、out 和 err 三个成员变量，分别代表标准输入流 (键盘输入)，标准输出流 (显示器) 和标准错误输出流 (显示器)。 成员方法： native long currentTimeMillis()：该方法的作用是返回当前的计算机时间，时间的表达格式为当前计算机间和 GMT 时间 (格林威治时间) 1970 年 1 月 1 日 0 时 0 分 0 秒之间的毫秒数。 void exit(int status)：该方法的作用是退出程序。其中 status 的值为 0 代表正常退出，非零代表异常退出。使用该方法可以在图形界面编程中实现程序的退出功能等。 void gc()：该方法的作用是请求系统进行垃圾回收。至于系统是否立刻回收，则取决于系统中垃圾回收算法的实现以及系统执行时的情况。 String getProperty(String key)：该方法的作用是获得系统中属性名为 key 的属性对应的值。系统中常见的属性名以及属性的作用如下表所示： 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; String javaVersion = System.getProperty(&quot;java.version&quot;); System.out.println(&quot;java的version: &quot; + javaVersion); String javaHome = System.getProperty(&quot;java.home&quot;); System.out.println(&quot;java的home: &quot; + javaHome); String osName = System.getProperty(&quot;os.name&quot;); System.out.println(&quot;os的name: &quot; + osName); String osVersion = System.getProperty(&quot;os.version&quot;); System.out.println(&quot;os的version: &quot; + osVersion); String userName = System.getProperty(&quot;user.name&quot;); System.out.println(&quot;user的name: &quot; + userName); String userHome = System.getProperty(&quot;user.home&quot;); System.out.println(&quot;user的home: &quot; + userHome); String userDir = System.getProperty(&quot;user.dir&quot;); System.out.println(&quot;user的dir: &quot; + userDir); &#125;&#125; java.util.Date 类 两个构造器的使用： Date date = new Date();：创建一个对应当前时间的 Date 对象。 Date date = new Date(1615816891380L);：创建指定毫秒数的 Date 对象。 两个方法的使用： toString()：把此 Date 对象转换为以下形式的 String：dow mon dd hh:mm:ss zzz yyyy，其中： dow 是一周中的某一天 (Sun，Mon，Tue，Wed，Thu，Fri，Sat)，zzz 是时间标准。 getTime()：返回自 1970 年 1 月 1 日 00:00:00 GMT 以来，此 Date 对象表示的毫秒数，即时间戳。 其它很多方法都过时了，不建议使用。 区别于 java.sql.Date 类： java.sql.Date 继承于 java.util.Date，是后者的子类，用于数据库中的日期。 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; // 创建java.sal.Date对象 java.sql.Date date = new java.sql.Date(System.currentTimeMillis()); System.out.println(date);// 2021-03-15 // java.util.Date对象转换为java.sql.Date对象 // 情况一：多态 java.util.Date date1 = new java.sql.Date(System.currentTimeMillis()); java.sql.Date date2 = (java.sql.Date) date1; System.out.println(date2); // 情况二： java.util.Date date3 = new java.util.Date(); Date date4 = new Date(date3.getTime()); System.out.println(date4);// 2021-03-15 &#125;&#125; java.text.SimpleDateFormat 类 java.util.Date 类的 API 不易于国际化，大部分被废弃了，java.text.SimpleDateFormat 类是一个与语言环境无关的方式来格式化和解析日期的具体类。 它允许对 Date 类的格式化和解析。 格式化：日期 —&gt; 字符串。 解析：字符串 —&gt; 日期。 SimpleDateFormat 类的实例化 使用默认构造器 public SimpleDateFormat()： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; SimpleDateFormat sdf = new SimpleDateFormat(); // 格式化：日期---&gt;字符串 Date date = new Date(); String format = sdf.format(date); System.out.println(format);// 21-3-16 下午8:46，默认格式化后的输出结果 // 解析：字符串---&gt;日期 String str = &quot;21-3-16 下午8:46&quot;;// 默认能解析的格式 try &#123; Date parse = sdf.parse(str); System.out.println(parse); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 使用带参构造器 public SimpleDateFormat(String pattern)： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); // 格式化：日期---&gt;字符串 Date date = new Date(); String format = sdf.format(date); System.out.println(format);// 2021-03-16 21:59:37，按指定格式格式化后的输出结果 // 解析：字符串---&gt;日期 String str = &quot;2021-3-16 9:02:13&quot;;// 按照指定格式书写的日期字符串 try &#123; Date parse = sdf.parse(str); System.out.println(parse); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 对于带参的构造器，在解析的时候，字符串必须是符合该参数指定的格式，否则，会解析发生异常。 实例： 1234567891011121314151617181920212223242526272829303132// 一个人从1990-1-1开始，三天打鱼两天晒网，求指定时间是在打渔还是晒网。public class Test &#123; public static void main(String[] args) &#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); Date startDate = null; try &#123; startDate = sdf.parse(&quot;1990-1-1&quot;); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; Date nowDate = null; try &#123; nowDate = sdf.parse(&quot;1990-1-1&quot;); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; if (startDate != null &amp;&amp; nowDate != null) &#123; long time = nowDate.getTime() - startDate.getTime(); long day = time / 1000 / 60 / 60 / 24 + 1; long l = day % 5; System.out.println(day + &quot;, &quot; + l); if (l == 1 || l == 2 | l == 3) &#123; System.out.println(&quot;在打渔&quot;); &#125; if (l == 0 || l == 4) &#123; System.out.println(&quot;在晒网&quot;); &#125; &#125; &#125;&#125; java.util.Calendar 类 (日历类) Calendar 是一个抽象类，主用用于完成日期字段之间相互操作的功能。 1public abstract class Calendar implements Serializable, Cloneable, Comparable&lt;Calendar&gt; &#123;&#125; 获取 Calendar实例的方法： 创建它的子类 GregorianCalendar 的对象。 调用静态方法 Calendar.getInstance()。 一个 Calendar 的实例是系统当前时间的抽象表示，常用方法如下： int get(int field)：获取想要的时间信息。比如：YEAR、MONTH、DAY_OF_WEEK、HOUR_OF_DAY、MINUTE、SECOND 等。 获取月份时：一月是 0，二月是 1，以此类推，十二月是 11。 获取星期时：周日是 1，周二是 2，以此类推，周六是 7。 void set(int field,int value)：设置时间。 void add(int field,int amount)：当前时间基础上做增减。 final Date getTime()：Calendar 对象转换为 Date对象。 final void setTime(Date date)：Date 对象转换为 Calendar 对象。 实例： 123456789101112131415161718192021222324252627282930313233public class Test &#123; public static void main(String[] args) &#123; // 1.实例化 // 方式一：创建其子类(GregorianCalendar)的对象 // 方式二：调用其静态方法getInstance() Calendar calendar = Calendar.getInstance();// 当前时间 System.out.println(calendar.getClass());// class java.util.GregorianCalendar // 2.常用方法 // get() System.out.println(&quot;年：&quot; + calendar.get(Calendar.YEAR));// 2021 System.out.println(&quot;月：&quot; + (calendar.get(Calendar.MONTH) + 1));// 月份，0代表1月，1代表2月，类推 System.out.println(&quot;日：&quot; + calendar.get(Calendar.DAY_OF_MONTH));// 一个月中的第几天 System.out.println(&quot;时：&quot; + calendar.get(Calendar.HOUR)); System.out.println(&quot;分：&quot; + calendar.get(Calendar.MINUTE)); System.out.println(&quot;秒：&quot; + calendar.get(Calendar.SECOND)); System.out.println(&quot;星期：&quot; + (calendar.get(Calendar.DAY_OF_WEEK) - 1));// 一周中的第几天，1代表周日，2代表周一，类推 System.out.println(&quot;一年中第：&quot; + calendar.get(Calendar.DAY_OF_YEAR));// 一年中的第几天 // set() calendar.set(Calendar.YEAR, 2020);// 更改calendar本身 System.out.println(&quot;重设之后的年：&quot; + calendar.get(Calendar.YEAR));// 2020 // add() calendar.add(Calendar.YEAR, 2); System.out.println(&quot;加2年之后的年：&quot; + calendar.get(Calendar.YEAR));// 2022 calendar.add(Calendar.YEAR, -1); System.out.println(&quot;减1年之后的年：&quot; + calendar.get(Calendar.YEAR));// 2021 // getTime()：Calendar---&gt;java.util.Date Date date = calendar.getTime(); // setTime()：java.util.Date---&gt;Calendar Date date1 = new Date(234234235235L); calendar.setTime(date1);// 设置calendar为指定时间 &#125;&#125; jdk 8 之后的日期和时间 API如果我们可以跟别人说：”我们在 1502643933071 见面，别晚了！”那么就再简单不过了。但是我们希望时间与昼夜和四季有关，于是事情就变复杂了。jdk 1.0 中包含了一个 java.util.Date 类，但是它的大多数方法已经在 jdk 1.1 引入 Calendar 类之后被弃用了，但 Calendar 并不比 Date 好多少。它们面临的问题是： 可变性：像日期和时间这样的类应该是不可变的。 偏移性：Date 中的年份是从 1900 开始的，而月份都从 0 开始。 格式化：格式化只对 Date 有用，Calendar 则不行。 此外，它们也不是线程安全的，也不能处理闰秒等。 第三次引入的 API 是成功的，并且 java 8 中引入的 java.time API 已经纠正了过去的缺陷，将来很长一段时间内它都会为我们服务。 java 8 吸收了 Joda-Time 的精华，以一个新的开始为 java 创建优秀的 API。新的 java.time 中包含了所有关于本地日期 (LocalDate)、本地时间 (LocalTime)、本地日期时间 (LocalDateTime)、时区 (ZonedDateTime) 和持续时间 (Duration) 的类。历史悠久的 Date 类新增了 toInstant() 方法，用于把 Date 转换成新的表示形式。这些新增的本地化时间日期 API 大大简化了日期时间和本地化的管理。 新时间日期 API： java.time – 包含值对象的基础包。 java.time.chrono – 提供对不同的日历系统的访问。 java.time.format – 格式化和解析时间和日期。 java.time.temporal – 包括底层框架和扩展特性。 java.time.zone – 包含时区支持的类。 说明：大多数开发者只会用到基础包和 format 包，也可能会用到 temporal 包。因此，尽管有 68 个新的公开类型，大多数开发者，大概将只会用到其中的三分之一。 java.time.LocalDate、java.time.LocalTime 和 java.time.LocalDateTime 类 LocalDate、LocalTime、LocalDateTime 类是其中较重要的几个类，它们的实例是不可变的对象，分别表示使用 ISO-8601日历系统的日期、时间、日期和时间。它们提供了简单的本地日期或时间，并不包含当前的时间信息，也不包含与时区相关的信息。 LocalDate：代表 IOS 格式 (yyyy-MM-dd) 的日期，可以存储生日、纪念日等日期。 LocalTime：表示一个时间，而不是日期。 LocalDateTime：是用来表示日期和时间的，这是一个最常用的类之一。 ISO-8601日历系统是国际标准化组织制定的现代公民的日期和时间的表示法，也就是公历。 常用方法： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142public class Test &#123; public static void main(String[] args) &#123; // now()：获取当前的日期、时间、日期+时间 LocalDate date = LocalDate.now(); LocalTime time = LocalTime.now(); LocalDateTime dateTime = LocalDateTime.now(); System.out.println(date);// 2021-03-17 System.out.println(time);// 11:37:43.400 System.out.println(dateTime);// 2021-03-17T11:37:43.400 // of()：自定义指定的年、月、日、时、分、秒对应的时间对象，没有偏移量 LocalDate date1 = LocalDate.of(2020, 3, 17); LocalTime time1 = LocalTime.of(11, 4, 25); LocalDateTime dateTime1 = LocalDateTime.of(2020, 3, 17, 11, 05, 45); System.out.println(date1);// 2020-03-17 System.out.println(time1);// 11:04:25 System.out.println(dateTime1);// 2020-03-17T11:05:45 // getXxx()：获取指定的时间信息 System.out.println(&quot;年：&quot; + dateTime.getYear());// 2021 System.out.println(&quot;月：&quot; + dateTime.getMonth());// MARCH System.out.println(&quot;月份数值：&quot; + dateTime.getMonthValue());// 3 System.out.println(&quot;日：&quot; + dateTime.getDayOfMonth());// 17 System.out.println(&quot;星期：&quot; + dateTime.getDayOfWeek());// WEDNESDAY System.out.println(&quot;时：&quot; + dateTime.getHour());// 11 System.out.println(&quot;分：&quot; + dateTime.getMinute());// 37 System.out.println(&quot;秒：&quot; + dateTime.getSecond());// 18 // withXX()：设置时间为指定的值并返回新的对象---不可变性 LocalDateTime dateTime2 = dateTime.withYear(2022); System.out.println(dateTime);// 2021-03-17T11:37:43.400 System.out.println(dateTime2);// 2022-03-17T11:37:43.400 // plusXxx()：在当前时间基础上做增减操作并返回新的对象---不可变性 LocalDateTime dateTime3 = dateTime.plusYears(2);// 加2年 System.out.println(dateTime);// 2021-03-17T11:37:43.400 System.out.println(dateTime3);// 2023-03-17T11:37:43.400 LocalDateTime dateTime4 = dateTime.minusYears(2);// 减2年 System.out.println(dateTime);// 2021-03-17T11:37:43.400 System.out.println(dateTime4);// 2019-03-17T11:37:43.400 &#125;&#125; java.time.Instant 类 — 瞬时 Instant：时间线上的一个瞬时点，这可能被用来记录应用程序中的事件时间戳。 在处理时间和日期的时候，我们通常会想到年，月，日，时，分，秒。然而，这只是时间的一个模型，是面向人类的。第二种通用模型是面向机器的，或者说是连续的。在此模型中，时间线中的一个点表示为一个很大的数，这有利于计算机处理。在 UNIX 中，这个数从 1970 年开始，以秒为的单位；同样的，在 java 中，也是从 1970 年开始，但以毫秒为单位。 java.time 包通过值类型 Instant 提供机器视图，不提供处理人类意义上的时间单位。Instant 表示时间线上的一点，而不需要任何上下文信息，例如，时区。概念上讲，它只是简单的表示自 1970 年 01 月 01 日 00 时 00 分 00 秒 (UTC) 开始的秒数。因为 java.time 包是基于纳秒计算的，所以 Instant 的精度可以达到纳秒级。 1秒 = 1000 毫秒 = 10^6 微秒 = 10^9 纳秒，即：1 ns = 10^-9 s。 常用方法： 时间戳是指格林威治时间 1970 年 01 月 01 日 00 时 00 分 00 秒 (北京时间 1970 年 01 月 01日 08 时 00 分 00 秒) 起至现在的总秒数。 实例： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; Instant instant = Instant.now();// 默认UTC时区，本初子午线对应的标准时间 System.out.println(instant);// 2021-03-17T03:50:15.672Z // 添加时间的偏移量 OffsetDateTime now = instant.atOffset(ZoneOffset.ofHours(8));// 东八区时间，要加上8小时 System.out.println(now); // 获取自1970-01-01 00:00:00(UTC)到当前时间的毫秒数 ---&gt; Date类的getTime()方法 long milli = instant.toEpochMilli(); System.out.println(milli); // 通过给定的毫秒数，获取Instant实例 ---&gt; new Date(long millis); Instant instant1 = Instant.ofEpochMilli(1615953468824L); &#125;&#125; java.time.format.DateTimeFormatter 类 格式化日期或时间，类似 SimpleDateFormat。 常用方法： 实例化方式一：预定义的标准格式。如：ISO_LOCAL_DATE_TIME、ISO_LOCAL_DATE、ISO_LOCAL_TIME。 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; DateTimeFormatter formatter = DateTimeFormatter.ISO_LOCAL_DATE_TIME; // 格式化：日期 ---&gt; 字符串 LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime);// 2021-03-17T13:18:37.907 String str = formatter.format(localDateTime); System.out.println(str);// 2021-03-17T13:18:37.907 // 解析：字符串 ---&gt; 日期 String str1 = &quot;2021-03-17T13:17:33.274&quot;;// 只能解析此种格式的字符串 TemporalAccessor parse = formatter.parse(str1); System.out.println(parse);// &#123;&#125;,ISO resolved to 2021-03-17T13:17:33.274 &#125;&#125; 实例化方式二，本地化相关的格式： ofLocalizedDateTime()，三种格式：FormatStyle.LONG / FormatStyle.MEDIUM / FormatStyle.SHORT 适用于 LocalDateTime。 12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime);// 2021-03-17T13:29:37.732 DateTimeFormatter formatter1 = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.LONG); String str1 = formatter1.format(localDateTime); System.out.println(str1);// 2021年3月17日 下午01时29分37秒 DateTimeFormatter formatter2 = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.MEDIUM); String str2 = formatter2.format(localDateTime); System.out.println(str2);// 2021-3-17 13:29:37 DateTimeFormatter formatter3 = DateTimeFormatter.ofLocalizedDateTime(FormatStyle.SHORT); String str3 = formatter3.format(localDateTime); System.out.println(str3);// 21-3-17 下午1:29 &#125;&#125; ofLocalizedDate()，四种格式：FormatStyle.FULL / FormatStyle.LONG / FormatStyle.MEDIUM / FormatStyle.SHORT 适用于LocalDate。 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; LocalDate localDate = LocalDate.now(); System.out.println(localDate);// 2021-03-17 DateTimeFormatter formatter1 = DateTimeFormatter.ofLocalizedDate(FormatStyle.FULL); String str1 = formatter1.format(localDate); System.out.println(str1);// 2021年3月17日 星期三 DateTimeFormatter formatter2 = DateTimeFormatter.ofLocalizedDate(FormatStyle.LONG); String str2 = formatter2.format(localDate); System.out.println(str2);// 2021年3月17日 DateTimeFormatter formatter3 = DateTimeFormatter.ofLocalizedDate(FormatStyle.MEDIUM); String str3 = formatter3.format(localDate); System.out.println(str3);// 2021-3-17 DateTimeFormatter formatter4 = DateTimeFormatter.ofLocalizedDate(FormatStyle.SHORT); String str4 = formatter4.format(localDate); System.out.println(str4);// 21-3-17 &#125;&#125; 实例化方式三：自定义的格式，最常用。如：ofPattern(&quot;yyyy-MM-dd hh:mm:ss&quot;)。 123456789public class Test &#123; public static void main(String[] args) &#123; DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;); String str = formatter.format(LocalDateTime.now()); System.out.println(str);// 2021-03-17 13:13:52 TemporalAccessor accessor = formatter.parse(&quot;2021-02-17 13:18:09&quot;);// 字符串需要严格匹配自定义的格式 System.out.println(accessor);// &#123;&#125;,ISO resolved to 2021-02-17T13:18:09 &#125;&#125; 其他 API java.time.ZoneId：该类中包含了所有的时区信息，一个时区的 ID，如 Europe/Paris。 1234567891011121314151617181920public class Test &#123; public static void main(String[] args) &#123; // ZoneId: 类中包含了所有的时区信息 // ZoneId的getAvailableZoneIds(): 获取所有的ZoneId Set&lt;String&gt; zoneIds = ZoneId.getAvailableZoneIds(); for (String s : zoneIds) &#123; System.out.println(s); &#125; // ZoneId的of(): 获取指定时区的时间 LocalDateTime localDateTime = LocalDateTime.now(ZoneId.of(&quot;Asia/Tokyo&quot;)); System.out.println(localDateTime); // ZonedDateTime: 带时区的日期时间 // ZonedDateTime的now(): 获取本时区的ZonedDateTime对象 ZonedDateTime zonedDateTime = ZonedDateTime.now(); System.out.println(zonedDateTime); // ZonedDateTime的now(ZoneId id): 获取指定时区的ZonedDateTime对象 ZonedDateTime zonedDateTime1 = ZonedDateTime.now(ZoneId.of(&quot;Asia/Tokyo&quot;)); System.out.println(zonedDateTime1); &#125;&#125; java.time.ZonedDateTime：一个在 ISO-8601日历系统时区的日期时间，如 2007-12-03T10:15:30+01:00 Europe/Paris。 其中每个时区都对应着 ID，地区 ID 都为 “{区域}/{城市}” 的格式，例如：Asia/Shanghai 等。 java.time.Clock：使用时区提供对当前即时、日期和时间的访问的时钟。 java.time.Duration：持续时间，用于计算两个 “时间” 间隔。 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; // Duration: 用于计算两个&quot;时间&quot;间隔，以秒和纳秒为基准 // between(): 静态方法，返回Duration对象，表示两个时间的间隔 LocalTime localTime = LocalTime.now(); LocalTime localTime1 = LocalTime.of(15, 23, 32); Duration duration = Duration.between(localTime1, localTime); System.out.println(duration); System.out.println(duration.getSeconds()); System.out.println(duration.getNano()); LocalDateTime localDateTime = LocalDateTime.of(2016, 6, 12, 15, 23, 32); LocalDateTime localDateTime1 = LocalDateTime.of(2017, 6, 12, 15, 23, 32); Duration duration1 = Duration.between(localDateTime1, localDateTime); System.out.println(duration1.toDays());// -365 &#125;&#125; java.time.Period：日期间隔，用于计算两个 “日期” 间隔。 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; // Period:用于计算两个&quot;日期&quot;间隔，以年、月、日衡量 LocalDate localDate = LocalDate.now();// 2021-3-17 LocalDate localDate1 = LocalDate.of(2028, 3, 18); Period period = Period.between(localDate, localDate1); System.out.println(period);// P7Y1D System.out.println(period.getYears());// 7 System.out.println(period.getMonths());// 0 System.out.println(period.getDays());// 1 Period period1 = period.withYears(2); System.out.println(period1); &#125;&#125; java.time.temporal.TemporalAdjuster：时间校正器。有时我们可能需要获取诸如将日期调整到 “下一个工作日” 等操作。 java.time.temporal.TemporalAdjusters：该类通过静态方法 firstDayOfXxx()/lastDayOfXxx()/nextXxx()，提供了大量的常用 TemporalAdjuster 的实现。 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; // TemporalAdjuster: 时间校正器 // 获取当前日期的下一个周日是哪天？当前日期：2021-3-17 TemporalAdjuster temporalAdjuster = TemporalAdjusters.next(DayOfWeek.SUNDAY); LocalDate localDateTime = LocalDate.now().with(temporalAdjuster);// LocalDateTime.now().with(temporalAdjuster) System.out.println(&quot;下一个周日是：&quot; + localDateTime);// 下一个周日是：2021-03-21 // 获取下一个工作日是哪天？ LocalDate localDate = LocalDate.now().with(new TemporalAdjuster() &#123; @Override public Temporal adjustInto(Temporal temporal) &#123; LocalDate date = (LocalDate) temporal; if (date.getDayOfWeek().equals(DayOfWeek.FRIDAY)) &#123; return date.plusDays(3); &#125; else if (date.getDayOfWeek().equals(DayOfWeek.SATURDAY)) &#123; return date.plusDays(2); &#125; else &#123; return date.plusDays(1); &#125; &#125; &#125;); System.out.println(&quot;下一个工作日是：&quot; + localDate);// 下一个工作日是：2021-03-18 &#125;&#125; 与传统日期处理的转换 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 线程","slug":"java-thread","date":"2021-03-04T12:17:34.000Z","updated":"2021-04-09T08:01:35.602Z","comments":true,"path":"2021/03/04/java-thread/","link":"","permalink":"http://example.com/2021/03/04/java-thread/","excerpt":"","text":"程序、进程和线程程序 (program)：是为完成特定任务、用某种语言编写的一组指令的集合。即指一段静态的代码，静态对象。 进程 (process)：是程序的一次执行过程，或是正在运行的一个程序。是一个动态的过程：有它自身的产生、存在和消亡的过程 —— 生命周期。如：运行中的QQ，运行中的MP3播放器。 程序是静态的，进程是动态的。 进程作为资源分配的单位，系统在运行时会为每个进程分配不同的内存区域。 线程 (thread)：进程可进一步细化为线程，是一个程序内部的一条执行路径。 若一个进程同一时间并行执行多个线程，就是支持多线程的。 线程作为调度和执行的单位，**每个线程拥有独立的运行栈和程序计数器 (pc)**，线程切换的开销小。 一个进程中的多个线程共享相同的内存单元 / 内存地址空间 (方法区、堆)：它们从同一堆中分配对象，可以访问相同的变量和对象。这就使得线程间通信更简便、高效。但多个线程操作共享的系统资源可能就会带来安全的隐患。 单核 CPU 和多核 CPU 的理解： 单核 CPU，其实是一种假的多线程，因为在一个时间单元内，也只能执行一个线程的任务。只是因为 CPU 时间单元特别短，因此感觉不出来。例如：虽然有多车道，但是收费站只有一个工作人员在收费，只有收了费才能通过，那么 CPU 就好比收费人员。如果有某个人没准备好交钱，那么收费人员可以把他 “挂起”，晾着他，等他准备好了钱，再去收费。 如果是多核的话，才能更好的发挥多线程的效率，现在的服务器基本都是多核的。 一个 java 应用程序 java.exe，其实至少有三个线程：main() 主线程，gc() 垃圾回收线程，异常处理线程。当然如果发生异常，会影响主线程。 并行与并发： 并行：多个 CPU 同时执行多个任务。 比如：多个人同时做不同的事。 并发：一个 CPU (采用时间片) 同时执行多个任务。比如：秒杀、多个人做同一件事。 多线程程序的优点： 以单核 CPU 为例，只使用单个线程先后完成多个任务 (调用多个方法)，肯定比用多个线程来完成用的时间更短 (因为单核 CPU，在多线程之间进行切换时，也需要花费时间 )，为何仍需多线程呢？ 提高应用程序的响应。对图形化界面更有意义，可增强用户体验。 提高计算机系统 CPU 的利用率。 改善程序结构。将既长又复杂的进程分为多个线程，独立运行，利于理解和修改。 何时需要多线程： 程序需要同时执行两个或多个任务。 程序需要实现一些需要等待的任务时，如用户输入、文件读写操作、网络操作、搜索等。 需要一些后台运行的程序时。 Thread 类java 语言的 JVM 允许程序运行多个线程，它通过 java.lang.Thread 类来体现。 Thread 类的特性： 每个线程都是通过某个特定 Thread 对象的 run() 方法来完成操作的，经常把 run() 方法的主体称为线程体。 通过该 Thread 对象的 start() 方法来启动这个线程，而非直接调用 run()。 如果手动调用 run()，那么就只是普通的方法，并没有启动多线程。 调用 start() 之后，run() 由 JVM 调用，什么时候调用以及执行的过程控制都由操作系统的 CPU 调度决定。 构造器： Thread() Thread(String threadname) Thread(Runnable target) Thread(Runnable target, String name) 方法： **void start()**：启动当前线程，并执行当前线程对象的 run() 方法。 **run()**：通常需要重写 Thread 类中的此方法，将创建的线程在被调度时需要执行的操作声明在此方法中。 **static Thread currentThread()**：静态方法，返回执行当前代码的线程。在 Thread 子类中就是 this，通常用于主线程和 Runnable 实现类。 12345678910111213141516public class Test &#123; public static void main(String[] args) &#123; System.out.println(Thread.currentThread().getName());// main new MyThread().start(); &#125;&#125;class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName());// Thread-0 System.out.println(currentThread().getName());// Thread-0 System.out.println(this.currentThread().getName());// Thread-0，实际代码中，不应该通过类实例访问静态成员 &#125;&#125; **String getName()**：返回当前线程的名称。 **void setName(String name)**：设置当前线程的名称。 12345678910111213141516171819202122232425262728293031public class Test &#123; public static void main(String[] args) &#123; // 设置main线程的名字 Thread.currentThread().setName(&quot;主线程&quot;); System.out.println(Thread.currentThread().getName()); // 设置自定义线程的名字 MyThread myThread = new MyThread(); myThread.setName(&quot;自定义线程一&quot;); myThread.start(); // 构造器设置自定义线程的名字 new MyThread(&quot;自定义线程二&quot;).start(); &#125;&#125;class MyThread extends Thread &#123; public MyThread() &#123; &#125; public MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125;&#125; **static void yield()**：释放当前线程 CPU 的执行权，但有可能 CPU 再次分配资源时，仍然优先分配到当前线程。 **join()**：在某个线程 a 中调用线程 b 的 join() 方法时，调用线程 a 将进入阻塞状态，直到线程 b 执行完之后，线程 a 才结束阻塞状态，等待 CPU 重新分配资源执行剩下的任务。注意：调用 join() 方法之后，低优先级的线程也可以获得执行。 **static void sleep(long millis)**：让当前线程 “睡眠” 指定的 millis 毫秒时间，在指定的 millis 毫秒时间内，当前线程是阻塞状态。时间到达时，重新排队等待 CPU 分配资源。 **stop()**：强制结束当前线程，已过时。 **boolean isAlive()**：返回boolean，判断线程是否存活。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Test &#123; public static void main(String[] args) &#123; MyThread myThread = new MyThread(); myThread.start(); System.out.println(myThread.isAlive()); for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; if (i == 20) &#123; try &#123; myThread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; ; &#125; System.out.println(myThread.isAlive()); &#125;&#125;class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; if (i % 20 == 0) &#123; yield(); &#125; if (i % 30 == 0) &#123; try &#123; sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 线程线程的调度策略调度策略： 时间片策略： 抢占式策略：高优先级的线程抢占 CPU。 java 的调度方法： 同优先级线程，组成先进先出队列 (先到先服务)，使用时间片策略。 高优先级线程，使用优先调度的抢占式策略。 线程的优先级线程的优先级等级： MAX_PRIORITY：10，最大优先级。 MIN _PRIORITY：1，最小优先级。 NORM_PRIORITY：5，默认优先级。 涉及的方法： getPriority()：获取线程的优先值。 setPriority(int newPriority)：设置线程的优先级。 12System.out.println(Thread.currentThread().getPriority());Thread.currentThread().setPriority(8); 说明： 线程创建时继承父线程的优先级。 低优先级只是获得调度的概率低，但并非一定是在高优先级线程之后才被调用。 线程的分类java 中的线程分为两类：一种是用户线程，一种是守护线程。 用户线程和守护线程，几乎在每个方面都是相同的，唯一的区别是判断 JVM 何时离开。 守护线程是用来服务用户线程的，通过在 start() 方法前调用 thread.setDaemon(true) 可以把一个用户线程变成一个守护线程。 java 垃圾回收就是一个典型的守护线程。 若 JVM 中都是守护线程，当前 JVM 将退出。 线程的生命周期要想实现多线程，必须在主线程中创建新的线程对象。java 语言使用 Thread 类及其子类的对象来表示线程，并用 Thread.State 类定义了线程的几种状态，在它的一个完整的生命周期中通常要经历如下的五种状态： 新建：当一个 Thread 类或其子类的对象被声明并创建时，新生的线程对象处于新建状态。 就绪：处于新建状态的线程被 start() 后，将进入线程队列等待 CPU 时间片，此时它已具备了运行的条件，只是没分配到 CPU 资源。 运行：当就绪的线程被调度并获得 CPU 资源时,便进入运行状态，run() 定义了线程的操作和功能。 阻塞：在某种特殊情况下，被人为挂起或执行输入输出操作时，让出 CPU 并临时中止自己的执行，进入阻塞状态。 死亡：线程完成了它的全部工作或线程被提前强制性地中止或出现异常导致结束。 线程的创建线程创建的一般过程： 方式一：继承 Thread 类 创建一个继承 Thread 类的子类。 重写 Thread 类的 run() — 将此线程执行的操作声明在 run() 方法体中。 创建 Thread 类的子类的对象。 通过该对象调用 start()。 启动当前线程。 调用当前线程的 run()。 不能通过直接调用对象的 run() 的形式启动线程。 不能再次调用当前对象的 start() 去开启一个新的线程，否则报 java.lang.IllegalThreadStateException 异常 。 如果要启动一个新的线程，需要重新创建一个 Thread 类的子类的对象，并调用其 start()。 实例一： 1234567891011121314151617181920212223242526272829public class Test &#123; public static void main(String[] args) &#123; // 启动一个子线程 MyThread myThread = new MyThread(); myThread.start(); // 启动一个新的子线程，并执行run方法 MyThread myThread2 = new MyThread(); myThread2.start(); // main线程 for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125;&#125;class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125;&#125; 实例二： 1234567891011121314151617181920212223242526public class Test &#123; public static void main(String[] args) &#123; // 匿名内部类 new Thread()&#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125; &#125;.start(); new Thread()&#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125; &#125;.start(); &#125;&#125; 方式二：实现 Runnable 接口 创建一个实现了 Runnable 接口的类。 实现类去实现 Runnable 接口中的抽象方法：run()。 创建实现类的对象。 将此对象作为参数传递到 Thread 类的构造器中，然后创建 Thread 类的对象。 通过 Thread 类的对象，调用 start()，最终执行的是上面重写的 run()。 实例： 123456789101112131415161718192021222324252627282930313233public class Test &#123; public static void main(String[] args) &#123; // 启动多个子线程时，只需要创建一个Runnable接口实现类的对象 MyRunnable myRunnable = new MyRunnable(); // 启动一个子线程 Thread thread = new Thread(myRunnable); thread.start(); // 启动一个新的子线程，并执行run方法 Thread thread2 = new Thread(myRunnable); thread2.start(); // main线程 for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125;&#125;class MyRunnable implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;：&quot; + i); &#125; &#125; &#125;&#125; 方式一和方式二的对比 开发中，优先选择实现 Runnable 接口的方式。 实现 Runnable 接口的方式，没有类的单继承性的局限性。 实现 Runnable 接口的方式，更适合处理多个线程有共享数据的情况。 Thread 类也实现了 Runnable 接口，无论是方式一，还是方式二，都需要重写 Runnable 接口的 run() 方法，并将创建的线程需要执行的逻辑声明在 run() 方法中。 方式三：实现 Callable 接口 从 JDK 5.0 开始。 创建一个实现 Callable 接口的实现类。 实现 call()，将此线程需要执行的操作声明在 call() 的方法体中。 创建 Callable 接口实现类的对象。 将此 Callable 接口实现类的对象作为参数传递到 FutureTask 的构造器中，创建 FutureTask 的对象。 Future 接口可以对具体 Runnable 或 Callable 任务的执行结果进行取消、查询是否完成、获取结果等操作。 FutrueTask 是 Futrue 接口的唯一的实现类。 FutureTask 同时实现了 Runnable 和 Future 接口。它既可以作为 Runnable 被线程执行，又可以作为 Future 得到 Callable 的返回值。 Runnable 接口的 run() 没有返回值。 Callable 接口的 call() 有返回值。 将 FutureTask 的对象作为参数传递到 Thread 类的构造器中，创建 Thread 类的对象，并调用 start()，启动线程。 根据实际需求，选择是否获得 Callable 中 call() 的返回值。 实例： 123456789101112131415161718192021222324252627282930313233public class Test &#123; public static void main(String[] args) &#123; // 3.创建Callable接口实现类的对象 MyCallable myCallable = new MyCallable(); // 4.将此Callable接口实现类的对象作为参数传递到FutureTask的构造器中，创建FutureTask的对象 FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(myCallable); // 5.将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread类的对象，并调用start()，启动线程 new Thread(futureTask).start(); // 6.获得Callable中call()的返回值 try &#123; // get()返回值即为FutureTask构造器参数Callable实现类重写的call()的返回值 Integer sum = futureTask.get(); System.out.println(&quot;100以内偶数的总和为：&quot; + sum); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125;// 1.创建一个实现Callable接口的实现类class MyCallable implements Callable&lt;Integer&gt; &#123; // 2.实现call()方法，将此线程需要执行的操作声明在call()的方法体中 @Override public Integer call() throws Exception &#123; int sum = 0; for (int i = 1; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; sum += i; &#125; &#125; return sum; &#125;&#125; 与使用 Runnable 接口相比，Callable 接口功能更强大些： 相比 run() 方法，call() 可以有返回值。 call() 可以抛出异常，能够被外面的操作捕获，获取异常的信息。 Callable 支持泛型的返回值。 Callable 需要借助 FutureTask 类，比如获取 call() 的返回结果。 方式四：线程池 背景： 经常创建和销毁、使用量特别大的资源，比如并发情况下的线程，会对性能影响很大。 思路：提前创建好多个线程，放入线程池中，使用时直接获取，使用完放回池中，这样可以避免频繁创建销毁，实现重复利用。类似生活中的公共交通工具。 好处： 提高响应速度，减少了创建新线程的时间。 降低资源消耗，重复利用线程池中线程，不需要每次都创建。 便于线程管理。 corePoolSize：核心池的大小。 maximumPoolSize：最大线程数。 keepAliveTime：线程没有任务时最多保持多长时间后会终止。 JDK 5.0 起，提供了线程池相关 API：ExecutorService 和 Executors。 ExecutorService：真正的线程池接口，常用子类 ThreadPoolExecutor。 void execute(Runnable command)：执行任务/命令，没有返回值，一般用来执行 Runnable。 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task)：执行任务，有返回值，一般用来执行 Callable。 void shutdown()：关闭连接池。 Executors：工具类、线程池的工厂类，用于创建并返回不同类型的线程池。 Executors.newCachedThreadPool()：创建一个可根据需要创建新线程的线程池。 Executors.newFixedThreadPool(n)：创建一个可重用固定线程数的线程池。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class ThreadPool &#123; public static void main(String[] args) &#123; // 1.提供指定线程数量的线程池 ExecutorService executorService = Executors.newFixedThreadPool(10); // 2.执行指定的线程的操作，需要提供实现Runnable接口或Callable接口的实现类的对象 // 2-1.execute()适合使用于Runnable executorService.execute(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + i); &#125; &#125; &#125; &#125;); executorService.execute(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + i); &#125; &#125; &#125; &#125;); // 2-2.submit()适合适用于Callable Future&lt;Integer&gt; evenSum = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; int evenSum = 0; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 == 0) &#123; evenSum += i; &#125; &#125; return evenSum; &#125; &#125;); try &#123; System.out.println(&quot;100以内的偶数和: &quot; + evenSum.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; Future&lt;Integer&gt; oddSum = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; int oddSum = 0; for (int i = 0; i &lt;= 100; i++) &#123; if (i % 2 != 0) &#123; oddSum += i; &#125; &#125; return oddSum; &#125; &#125;); try &#123; System.out.println(&quot;100以内的奇数和: &quot; + oddSum.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; // 3.使用完线程池后，需要关闭线程池 executorService.shutdown(); &#125;&#125; Executors.newSingleThreadExecutor()：创建一个只有一个线程的线程池。 Executors.newScheduledThreadPool(n)：创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。 线程的同步线程的安全问题多线程安全问题实例，模拟火车站售票程序，开启三个窗口售票。 方式一：继承 Thread 类。 12345678910111213141516171819202122232425262728293031323334353637383940public class TestThread &#123; public static void main(String[] args) &#123; // 启动第一个售票窗口 TicketThread thread1 = new TicketThread(); thread1.setName(&quot;售票窗口一&quot;); thread1.start(); // 启动第二个售票窗口 TicketThread thread2 = new TicketThread(); thread2.setName(&quot;售票窗口二&quot;); thread2.start(); // 启动第三个售票窗口 TicketThread thread3 = new TicketThread(); thread3.setName(&quot;售票窗口三&quot;); thread3.start(); &#125;&#125;class TicketThread extends Thread &#123; // 总票数，必须定义为static，随类只加载一次，因为每新建一个线程，都需要new一次TicketThread private static int ticketNum = 100; @Override public void run() &#123; while (true) &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; &#125;&#125; 方式二：实现 Runnable 接口。 1234567891011121314151617181920212223242526272829303132333435363738public class Test &#123; public static void main(String[] args) &#123; TicketRunnable ticket = new TicketRunnable(); // 启动第一个售票窗口 Thread thread1 = new Thread(ticket, &quot;售票窗口1&quot;); thread1.start(); // 启动第二个售票窗口 Thread thread2 = new Thread(ticket, &quot;售票窗口2&quot;); thread2.start(); // 启动第三个售票窗口 Thread thread3 = new Thread(ticket, &quot;售票窗口3&quot;); thread3.start(); &#125;&#125;class TicketRunnable implements Runnable &#123; // 总票数，不必定义为static，因为只需要new一次TicketRunnable private int ticketNum = 100; @Override public void run() &#123; while (true) &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; &#125;&#125; 说明： 如上程序，在买票的过程中，出现了重票、错票，说明多线程的执行过程中，出现了安全问题。 问题的原因：当多条语句在操作同一个线程的共享数据时，当一个线程对多条语句只执行了一部分，还没有执行完时，另一个线程参与进来执行，从而导致了共享数据的错误。 解决办法：对多条操作共享数据的语句，让一个线程全部执行完，在执行的过程中，其他线程不可以参与执行。 线程的同步机制对于多线程的安全问题，java 提供了专业的解决方式：同步机制。实现同步机制的方式，有同步代码块、同步方法、Lock 锁等多种形式。 同步的范围 如何找问题，即代码是否存在线程安全？— 非常重要（1）明确哪些代码是多线程运行的代码。（2）明确多个线程是否有共享数据。（3）明确多线程运行代码中是否有多条语句操作共享数据。 如何解决呢？— 非常重要 对多条操作共享数据的语句，只能让一个线程都执行完，在执行过程中，其他线程不可以参与执行，即所有操作共享数据的这些语句都要放在同步范围中。 切记 ： 范围太小：没锁住所有有安全问题的代码。 范围太大：没发挥多线程的功能。 同步机制的特点 优点：同步的方式，能够解决线程的安全问题。 局限性：操作同步代码时，只能有一个线程参与，其他线程等待，相当于是一个单线程的过程，效率低。 需要被同步的代码：操作共享数据的代码。 共享数据：多个线程共同操作的变量。 同步监视器，俗称：锁。任何一个类的对象，都可以充当锁。 要求：多个线程必须要公用同一把锁！！！针对不同实现同步机制的方式，都要保证同步监视器是同一个！！！ 同步机制中的锁同步锁机制：在《Thinking in Java》中，是这么说的：对于并发工作，你需要某种方式来防止两个任务访问相同的资源 (其实就是共享资源竞争)。防止这种冲突的方法就是当资源被一个任务使用时，在其上加锁。第一个访问某项资源的任务必须锁定这项资源，使其他任务在其被解锁之前，就无法访问它了，而在其被解锁之时，另一个任务就可以锁定并使用它了。 synchronized 的锁是什么： 任意对象都可以作为同步锁，所有对象都自动含有单一的锁 (监视器)。 同步代码块的锁：自己指定，很多时候也是指定为 this 或 类名.class。 同步方法的锁：静态方法 — 类名.class、非静态方法 — this。 注意： 必须确保使用同一个资源的多个线程共用的是同一把锁，这个非常重要，否则就无法保证共享资源的安全。 一个线程类中的所有静态方法共用同一把锁 — 类名.class，所有非静态方法共用同一把锁 — this，同步代码块在指定锁的时候需谨慎。 能够释放锁的操作： 当前线程的同步方法、同步代码块执行结束。 当前线程在同步代码块、同步方法中遇到 break、return 终止了该代码块、该方法的继续执行。 当前线程在同步代码块、同步方法中出现了未处理的Error或Exception，导致异常结束。 当前线程在同步代码块、同步方法中执行了线程对象的wait()方法，当前线程暂停，并释放锁。 不会释放锁的操作： 线程执行同步代码块或同步方法时，程序调用 Thread.sleep()、Thread.yield() 暂停当前线程的执行。 线程执行同步代码块时，其他线程调用了该线程的 suspend() 将该线程挂起，该线程不会释放锁 (同步监视器)。 应尽量避免使用 suspend() 和 resume() 来控制线程。 同步机制一：同步代码块格式： 123synchronized (同步监视器)&#123; // 需要被同步的代码&#125; 继承 Thread 类方式的修正： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class TestThread &#123; public static void main(String[] args) &#123; // 启动第一个售票窗口 TicketThread thread1 = new TicketThread(); thread1.setName(&quot;售票窗口一&quot;); thread1.start(); // 启动第二个售票窗口 TicketThread thread2 = new TicketThread(); thread2.setName(&quot;售票窗口二&quot;); thread2.start(); // 启动第三个售票窗口 TicketThread thread3 = new TicketThread(); thread3.setName(&quot;售票窗口三&quot;); thread3.start(); &#125;&#125;class TicketThread extends Thread &#123; // 总票数，必须定义为static，随类只加载一次，因为每新建一个线程，都需要new一次TicketThread private static int ticketNum = 100; // 锁，必须定义为static private static Object obj = new Object(); @Override public void run() &#123; while (true) &#123; synchronized (obj) &#123;// 可以使用：synchronized (TicketThread.class)，不能建议使用：synchronized (this) if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; &#125; &#125;&#125; obj 可以使用 TicketThread.class (当前类) 替代，TicketThread 类只会加载一次，类也是对象。 实现 Runnable 接口方式的修正： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class TestRunnable &#123; public static void main(String[] args) &#123; TicketRunnable ticket = new TicketRunnable(); // 启动第一个售票窗口 Thread thread1 = new Thread(ticket, &quot;售票窗口1&quot;); thread1.start(); // 启动第二个售票窗口 Thread thread2 = new Thread(ticket, &quot;售票窗口2&quot;); thread2.start(); // 启动第三个售票窗口 Thread thread3 = new Thread(ticket, &quot;售票窗口3&quot;); thread3.start(); &#125;&#125;class TicketRunnable implements Runnable &#123; // 总票数，不必定义为static，因为只需要new一次TicketRunnable private int ticketNum = 100; // 锁，不必定义为static Object obj = new Object(); @Override public void run() &#123; while (true) &#123; synchronized (obj) &#123;// 可以使用：synchronized (this) if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; &#125; &#125;&#125; obj 对象可以使用 this 代替，指代唯一的 TicketRunnable 对象。 同步机制二：同步方法格式： 1修饰符 synchronized 返回值类型 方法名 (形参列表) &#123;&#125; 如果操作共享数据的代码，完整的声明在一个方法中，则可以将此方法声明为同步方法。 同步方法仍然涉及到同步监视器，只是不需要显示的声明： 非静态的同步方法，同步监视器是：this。 静态的同步方法，同步监视器是：当前类本身。 继承 Thread 类方式的修正： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class TestMethod1 &#123; public static void main(String[] args) &#123; // 启动第一个售票窗口 TicketMethod1 thread1 = new TicketMethod1(); thread1.setName(&quot;售票窗口一&quot;); thread1.start(); // 启动第二个售票窗口 TicketMethod1 thread2 = new TicketMethod1(); thread2.setName(&quot;售票窗口二&quot;); thread2.start(); // 启动第三个售票窗口 TicketMethod1 thread3 = new TicketMethod1(); thread3.setName(&quot;售票窗口三&quot;); thread3.start(); &#125;&#125;class TicketMethod1 extends Thread &#123; // 总票数，必须定义为static，随类只加载一次，因为每新建一个线程，都需要new一次TicketThread private static int ticketNum = 100; @Override public void run() &#123; while (true) &#123; handleTicket(); &#125; &#125; // 必须设置成static的，此时的同步监视器是TicketMethod1.class private static synchronized void handleTicket() &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125;&#125; 此时，同步方法要设置成 static 的，此时的同步监视器是 TicketMethod1.class (当前类)。 实现 Runnable 接口方式的修正： 123456789101112131415161718192021222324252627282930313233343536373839404142public class TestMethod2 &#123; public static void main(String[] args) &#123; TicketMethod2 ticket = new TicketMethod2(); // 启动第一个售票窗口 Thread thread1 = new Thread(ticket, &quot;售票窗口1&quot;); thread1.start(); // 启动第二个售票窗口 Thread thread2 = new Thread(ticket, &quot;售票窗口2&quot;); thread2.start(); // 启动第三个售票窗口 Thread thread3 = new Thread(ticket, &quot;售票窗口3&quot;); thread3.start(); &#125;&#125;class TicketMethod2 implements Runnable &#123; private int ticketNum = 100; @Override public void run() &#123;// 有时可以直接设置run方法为synchronized，但本例不行 while (true) &#123; handleTicket(); &#125; &#125; // 非静态同步方法中，同步监视器：this private synchronized void handleTicket() &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125;&#125; 此时，同步方法中的同步监视器是：this，即当前 TicketMethod2 类的对象。 同步机制三：Lock 锁 从 JDK 5.0 开始，java 提供了更强大的线程同步机制——通过显式定义同步锁对象来实现同步。同步锁使用 Lock 对象充当。 java.util.concurrent.locks.Lock 接口是控制多个线程对共享资源进行访问的工具。锁提供了对共享资源的独占访问，每次只能有一个线程对 Lock 对象加锁，线程开始访问共享资源之前应先获得 Lock 对象。 在实现线程安全的控制中，比较常用的是 ReentrantLock，ReentrantLock 类实现了 Lock 接口，它拥有与 synchronized 相同的并发性和内存语义，可以显式加锁、释放锁。 声明格式： 继承 Thread 类方式的修正： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class LockTest &#123; public static void main(String[] args) &#123; // 启动第一个售票窗口 Ticket thread1 = new Ticket(); thread1.setName(&quot;售票窗口一&quot;); thread1.start(); // 启动第二个售票窗口 Ticket thread2 = new Ticket(); thread2.setName(&quot;售票窗口二&quot;); thread2.start(); // 启动第三个售票窗口 Ticket thread3 = new Ticket(); thread3.setName(&quot;售票窗口三&quot;); thread3.start(); &#125;&#125;class Ticket extends Thread &#123; private static int ticketNum = 100; // 1.实例化静态ReentrantLock private static Lock lock = new ReentrantLock(); @Override public void run() &#123; while (true) &#123; // 2.调用锁定方法: lock() lock.lock(); try &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; finally &#123; lock.unlock();// 3.调用解锁方法: unlock() &#125; &#125; &#125;&#125; ReentrantLock 实例对象需要设置为 static。 实现 Runnable 接口方式的修正： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class LockTest &#123; public static void main(String[] args) &#123; Ticket ticket = new Ticket(); // 启动第一个售票窗口 Thread thread1 = new Thread(ticket, &quot;售票窗口1&quot;); thread1.start(); // 启动第二个售票窗口 Thread thread2 = new Thread(ticket, &quot;售票窗口2&quot;); thread2.start(); // 启动第三个售票窗口 Thread thread3 = new Thread(ticket, &quot;售票窗口3&quot;); thread3.start(); &#125;&#125;class Ticket implements Runnable &#123; private int ticketNum = 100; // 1.实例化ReentrantLock private Lock lock = new ReentrantLock(); @Override public void run() &#123; while (true) &#123; // 2.调用锁定方法: lock() lock.lock(); try &#123; if (ticketNum &gt; 0) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;售出车票，tick号为：&quot; + ticketNum--); &#125; else &#123; break; &#125; &#125; finally &#123; lock.unlock();// 3.调用解锁方法: unlock() &#125; &#125; &#125;&#125; synchronized 和 Lock 的对比 synchronized 是隐式锁，出了作用域自动释放同步监视器，而 Lock 是显式锁，需要手动开启和关闭锁。 synchronized 有代码块锁和方法锁，而 Lock 只有代码块锁。 使用 Lock 锁，JVM 将花费较少的时间来调度线程，性能更好，并且具有更好的扩展性 (Lock 接口能提供更多的实现类)。 优先使用顺序：Lock → 同步代码块 (已经进入了方法体，分配了相应资源) → 同步方法 (在方法体之外) 经典实例 银行有一个账户，有两个储户分别向这个账户存钱，每次存 1000，存 10 次，要求每次存完打印账户余额。 实现方式一： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class AccountTest &#123; public static void main(String[] args) &#123; // 一个账户 Account account = new Account(0.0); // 两个储户 Customer c1 = new Customer(account); Customer c2 = new Customer(account); c1.setName(&quot;甲&quot;); c2.setName(&quot;乙&quot;); c1.start(); c2.start(); &#125;&#125;class Account &#123; private double balance; public Account(double balance) &#123; this.balance = balance; &#125; public double getBalance() &#123; return balance; &#125; // 此时的锁是Accout的对象，本例的写法中，Account只有一个，所以两个线程公用的是一个同步锁 public synchronized void deposit(double amt) &#123; if (amt &gt; 0) &#123; balance += amt; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;存钱成功，余额为：&quot; + balance); &#125; &#125;&#125;class Customer extends Thread &#123; private Account account; public Customer(Account account) &#123; this.account = account; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; account.deposit(1000.0); &#125; &#125;&#125; 实现方式二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class AccountTest &#123; public static void main(String[] args) &#123; // 一个账户 Account account = new Account(0.0); // 两个储户 Customer c1 = new Customer(account); Customer c2 = new Customer(account); c1.setName(&quot;甲&quot;); c2.setName(&quot;乙&quot;); c1.start(); c2.start(); &#125;&#125;class Account &#123; private double balance; public Account(double balance) &#123; this.balance = balance; &#125; public double getBalance() &#123; return balance; &#125; public void deposit(double amt) &#123; if (amt &gt; 0) &#123; balance += amt; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;存钱成功，余额为：&quot; + balance); &#125; &#125;&#125;class Customer extends Thread &#123; private Account account; // static的Lock private static Lock lock = new ReentrantLock(); public Customer(Account account) &#123; this.account = account; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; lock.lock(); try &#123; account.deposit(1000.0); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 线程的通信 wait() 与 notify() 和 notifyAll() wait()：一旦执行此方法，当前线程就进入阻塞状态，并释放同步监视器。 当前线程排队等候其他线程调用 notify() 或 notifyAll() 方法唤醒，唤醒后等待重新获得对监视器的所有权后才能继续执行。 被唤醒的线程从断点处继续代码的执行。 notify()：一旦执行此方法，就会唤醒被 wait() 的一个线程。如果有多个线程被 wait()，则唤醒优先级高的。 notifyAll()：一旦执行此方法，就会唤醒所有被 wait() 的线程。 wait() 与 notify() 和 notifyAll() 这三个方法必须使用在同步代码块或同步方法中。 wait() 与 notify() 和 notifyAll() 这三个方法的调用者必须是同步代码块或同步方法中的同步监视器。 否则会出现 java.lang.IllegalMonitorStateException 异常。 wait() 与 notify() 和 notifyAll() 这三个方法是定义在 java.lang.Object 类中的。 因为这三个方法必须由同步监视器调用，而任意对象都可以作为同步监视器，因此这三个方法只能在 Object 类中声明。 实例一：使用两个线程打印 1 - 100，要求线程 1 和线程 2 交替打印。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class CommunicationTest &#123; public static void main(String[] args) &#123; Number number = new Number(); Thread t1 = new Thread(number); Thread t2 = new Thread(number); t1.setName(&quot;线程1&quot;); t2.setName(&quot;线程2&quot;); t1.start(); t2.start(); &#125;&#125;class Number implements Runnable &#123; private int number = 1; @Override public void run() &#123; while (true) &#123; synchronized (this) &#123; // 唤醒被wait()的一个线程 notify();// 等同于：this.notify(); if (number &lt;= 100) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + number); number++; try &#123; // 使调用wait()方法的线程进入阻塞状态 wait();// 等同于：this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; break; &#125; &#125; &#125; &#125;&#125; 实例二：生产者 / 消费者问题。 生产者 (Producer)将产品交给店员 (Clerk)，而消费者 (Customer) 从店员处取走产品，店员一次只能持有固定数量的产品 (比如 20)，如果生产者试图生产更多的产品，店员会叫生产者停一下，如果店中有空位放产品了再通知生产者继续生产；如果店中没有产品了，店员会告诉消费者等一下，如果店中有产品了再通知消费者来取走产品。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class ProductTest &#123; public static void main(String[] args) &#123; Clerk clerk = new Clerk(); Producer producer1 = new Producer(clerk); producer1.setName(&quot;生产者1&quot;); Consumer consumer1 = new Consumer(clerk); consumer1.setName(&quot;消费者1&quot;); Consumer consumer2 = new Consumer(clerk); consumer2.setName(&quot;消费者2&quot;); producer1.start(); consumer1.start(); consumer2.start(); &#125;&#125;class Clerk &#123; private int productCount = 0; public synchronized void produceProduct() &#123; if (productCount &lt; 20) &#123; productCount++; System.out.println(Thread.currentThread().getName() + &quot;开始生产第&quot; + productCount + &quot;个产品&quot;); notify(); &#125; else &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public synchronized void consumerProduct() &#123; if (productCount &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + &quot;开始消费第&quot; + productCount + &quot;个产品&quot;); productCount--; notify(); &#125; else &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;// 生产者class Producer extends Thread &#123; private Clerk clerk; public Producer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + &quot;开始生产产品...&quot;); while (true) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; clerk.produceProduct(); &#125; &#125;&#125;// 消费者class Consumer extends Thread &#123; private Clerk clerk; public Consumer(Clerk clerk) &#123; this.clerk = clerk; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + &quot;开始消费产品...&quot;); while (true) &#123; try &#123; Thread.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; clerk.consumerProduct(); &#125; &#125;&#125; 面试题：sleep() 和 wait() 的异同。 相同点：一旦执行方法，都可以使得当前的线程进入阻塞状态。 不同点： 两个方法声明的位置不同：`sleep()` 声明在 Thread 类中，`wait()` 声明在 Object 类中。 调用的要求不同：`sleep()` 可以在任何需要的场景下调用，`wait()` 必须使用在同步代码块或同步方法中。 关于是否释放同步监视器：如果两个方法都是用在同步代码块或同步方法中，`sleep()` 不会释放锁，`wait()` 会释放锁。 线程的死锁问题死锁： 不同的线程分别占用对方需要的同步资源不放弃，都在等待对方放弃自己需要的同步资源，就形成了线程的死锁。 出现死锁后，不会出现异常，不会出现提示，只是所有的线程都处于阻塞状态，无法继续。 实例一： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class DeadLock &#123; public static void main(String[] args) &#123; StringBuilder s1 = new StringBuilder(); StringBuilder s2 = new StringBuilder(); // 继承Thread类 new Thread() &#123; @Override public void run() &#123; synchronized (s1) &#123; s1.append(&quot;a&quot;); s2.append(1); // 添加sleep()，增加死锁触发的概率 try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (s2) &#123; s1.append(&quot;b&quot;); s2.append(2); System.out.println(s1); System.out.println(s2); &#125; &#125; &#125; &#125;.start(); // 实现Runnable接口 new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (s2) &#123; s1.append(&quot;c&quot;); s2.append(3); // 添加sleep()，增加死锁触发的概率 try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (s1) &#123; s1.append(&quot;d&quot;); s2.append(4); System.out.println(s1); System.out.println(s2); &#125; &#125; &#125; &#125;).start(); &#125;&#125; 实例二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class A &#123; public synchronized void foo(B b) &#123;// 同步监视器：A的对象 System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot;, 进入了A实例的foo方法&quot;); // ① try &#123; Thread.sleep(200); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot;, 企图调用B实例的last方法&quot;); // ③ b.last(); &#125; public synchronized void last() &#123; System.out.println(&quot;进入了A类的last方法内部&quot;); &#125;&#125;class B &#123; public synchronized void bar(A a) &#123;// 同步监视器：B的对象 System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot;, 进入了B实例的bar方法&quot;); // ② try &#123; Thread.sleep(200); &#125; catch (InterruptedException ex) &#123; ex.printStackTrace(); &#125; System.out.println(&quot;当前线程名: &quot; + Thread.currentThread().getName() + &quot;, 企图调用A实例的last方法&quot;); // ④ a.last(); &#125; public synchronized void last() &#123; System.out.println(&quot;进入了B类的last方法内部&quot;); &#125;&#125;public class DeadLock implements Runnable &#123; A a = new A(); B b = new B(); public void init() &#123; Thread.currentThread().setName(&quot;主线程&quot;); // 调用a对象的foo方法 a.foo(b); System.out.println(&quot;进入了主线程之后&quot;); &#125; @Override public void run() &#123; Thread.currentThread().setName(&quot;副线程&quot;); // 调用b对象的bar方法 b.bar(a); System.out.println(&quot;进入了副线程之后&quot;); &#125; public static void main(String[] args) &#123; DeadLock deadLock = new DeadLock(); new Thread(deadLock).start(); deadLock.init(); &#125;&#125; 解决死锁的方法： 专门的算法、原则。 尽量减少同步资源的定义。 尽量避免嵌套同步。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的异常处理","slug":"java-exception","date":"2021-03-03T09:30:12.000Z","updated":"2021-04-09T08:00:36.326Z","comments":true,"path":"2021/03/03/java-exception/","link":"","permalink":"http://example.com/2021/03/03/java-exception/","excerpt":"","text":"在使用计算机语言进行项目开发的过程中，即使程序员把代码写得 尽善尽美，在系统的运行过程中仍然会遇到一些问题，因为很多问题不是靠代码能够避免的，比如：客户输入数据的格式，读取文件是否存在，网络是否始终保持通畅等等。 在 java 语言中，将程序执行中发生的不正常情况称为异常。注意：开发过程中的语法错误和逻辑错误不是异常。 对于这些错误，一般有两种解决方法：一是遇到错误就终止程序的运行；另一种方法是由程序员在编写程序时，就考虑到错误的检测、错误消息的提示，以及错误的处理。捕获错误最理想的是在编译期间，但有的错误只有在运行时才会发生。比如：除数为0，数组下标越界等。 异常体系结构 父类：java.lang.Throwable。常见的异常分类如下： java.lang.Error：java 虚拟机无法解决的严重问题。如：JVM 系统内部错误、资源耗尽等严重情况。比如：StackOverflowError 和 OutOfMemoryError (OOM)。一般不编写针对性的代码进行处理 (需要更改代码逻辑等去解决问题)。 123456789public class ErrorTest &#123; public static void main(String[] args) &#123; // 1.栈溢出：java.lang.StackOverflowError main(args); // 2.堆溢出：java.lang.OutOfMemoryError: Java heap space Integer[] arr = new Integer[1024 * 1024 * 1024]; &#125;&#125; java.lang.Exception：其它因编程错误或偶然的外在因素导致的一般性问题，可以使用针对性的代码进行处理。例如：空指针访问、试图读取不存在的文件、网络连接中断和数组角标越界等。 编译时异常：是指编译器要求必须处置的异常。即程序在运行时由于外界因素造成的一般性异常。编译器要求 java 程序必须捕获或声明所有编译时异常。对于这类异常，如果程序不处理，可能会带来意想不到的结果。 java.io.IOException 和 java.io.FileNotFoundException 1234567891011public class IOEx &#123; public static void main(String[] args) &#123; File file = new File(&quot;hello.txt&quot;); FileInputStream fis = new FileInputStream(file);// !java.io.FileNotFoundException int data; while ((data = fis.read()) != -1) &#123;// !java.io.IOException System.out.println((char) data); &#125; fis.close();// !java.io.IOException &#125;&#125; 如上代码，在编译期 (javac.exe) 就会出错，编译不通过，无法生成字节码文件。 运行时异常：是指编译器不要求强制处置的异常。一般是指编程时的逻辑错误，是程序员应该积极避免其出现的异常。java.lang.RuntimeException 类及它的子类都是运行时异常。对于这类异常，可以不作处理，因为这类异常很普遍，若全处理可能会对程序的可读性和运行效率产生影响。 java.lang.NullPointerException 123456789public class NullRef &#123; int i = 1; public static void main(String[] args) &#123; NullRef t = new NullRef(); t = null; System.out.println(t.i); &#125;&#125; java.lang.ArrayIndexOutOfBoundsException 123456789public class IndexOutExp &#123; public static void main(String[] args) &#123; String[] friends = &#123;&quot;lisa&quot;, &quot;bily&quot;, &quot;kessy&quot;&#125;; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(friends[i]); // friends[4]? &#125; System.out.println(&quot;\\nthis is the end&quot;); &#125;&#125; java.lang.ClassCastException 12345678public class Order &#123; public static void main(String[] args) &#123; Object obj = new Date(); Order order; order = (Order) obj; System.out.println(order); &#125;&#125; java.lang.NumberFormatException 1234567public class NumFormat &#123; public static void main(String[] args) &#123; String str = &quot;abc&quot;; int num = Integer.parseInt(str); System.out.println(&quot;num = &quot; + num); &#125;&#125; java.util.InputMismatchExcjavaeption 12345678public class NumFormat &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); int num = scanner.nextInt();// 输入的非整数 System.out.println(&quot;num = &quot; + num); scanner.close(); &#125;&#125; java.lang.ArithmeticException 12345678910public class DivideZero &#123; int x; public static void main(String[] args) &#123; DivideZero c = new DivideZero(); int y = 3 / c.x; System.out.println(&quot;y = &quot; + y); System.out.println(&quot;program ends ok!&quot;); &#125;&#125; 异常处理机制在编写程序时，经常要在可能出现错误的地方加上检测的代码，如进行 x / y 运算时，要检测分母为 0、数据为空、输入的不是数据而是字符等。而过多的 if - else 分支会导致程序的代码加长、臃肿，可读性差。因此采用异常处理机制。 java 采用的异常处理机制，是将异常处理的程序代码集中在一起，与正常的程序代码分开，使得程序简洁、优雅，并易于维护。 异常的处理：抓抛模型 过程一 — “抛”：程序在正常执行的过程中，一旦出现异常，就会在异常代码处生成一个对应异常类的对象，并将此对象抛出。一旦抛出对象以后，其后的代码就不再继续执行。 关于异常对象的产生： 系统自动生成的异常对象。 手动的生成一个异常对象，并抛出 (throw)。 过程二 — “抓”：可以理解为异常的处理方式，分为两种。 try -catch -finally throws try - catch - finally格式： try：捕获异常的第一步是用 try 语句块选定捕获异常的范围，将可能出现异常的代码放在 try 语句块中。 catch (ExceptionType e)：在 catch 语句块中是对异常对象进行处理的代码。 每个 try 语句块可以伴随一个或多个 catch 语句，用于处理可能产生的不同类型的异常对象，当异常对象匹配到某一个 catch 时，就进入该 catch 中进行异常的处理，一旦处理完成，就跳出当前的 try - catch，结构，继续执行 finally 结构和其后的代码。 如果明确知道产生的是何种异常，可以用该异常类作为 catch 的参数；也可以用其父类作为 catch 的参数。比如：可以用 ArithmeticException 类作为参数的地方，就可以用 RuntimeException 类作为参数，或者用所有异常的父类 Exception 类作为参数。但不能是与 ArithmeticException 类无关的异常，如 NullPointerException，此时 catch 中的语句将不会执行。 与其它对象一样，可以访问 catch 到的异常对象的成员变量或调用它的方法。 getMessage() 获取异常的说明信息，返回字符串。 printStackTrace() 获取异常类名和异常的说明信息，以及异常出现在程序中的位置，返回值 void。 在 try 结构中声明的变量，出了 try 结构以后，就不能再使用了。 finally：捕获异常的最后一步是通过 finally 语句为异常处理提供一个统一的出口，使得在控制流转到程序的其它部分以前，能够对程序的状态作统一的管理。 不论在 try 代码块中是否发生了异常事件，catch 语句是否执行，catch 语句是否有异常，try 或 catch 语句中是否有 return，finally 块中的语句都会被执行。 123456789101112131415161718192021222324public class ExceptionTest &#123; public int method() &#123; try &#123; int[] arr = new int[10]; System.out.println(arr[10]); return 1; &#125; catch (ArrayIndexOutOfBoundsException e) &#123; e.printStackTrace(); return 2; &#125; finally &#123; System.out.println(&quot;finally一定会被执行&quot;); return 3;// finally中不建议使用return &#125; &#125; public static void main(String[] args) &#123; ExceptionTest exceptionTest = new ExceptionTest(); int method = exceptionTest.method(); System.out.println(method); &#125;&#125;输出结果：finally一定会被执行3 由上面代码也可以看出，finally 语句的内容，会在 try 或 catch 的 return 语句之前执行。 像数据库连接、输入输出流、网络编程 Socket 等资源，JVM 是不能自动回收的，需要手动的进行资源的释放。此时的资源释放，就需要声明在 finally 中。 1234567891011121314151617181920212223public class ExceptionTest &#123; public static void main(String[] args) &#123; File file = new File(&quot;hello.txt&quot;); FileInputStream fis = null; try &#123; fis = new FileInputStream(file); int data; while ((data = fis.read()) != -1) &#123; System.out.println((char) data); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fis != null) &#123; fis.close(); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125;&#125; try - catch - finally 结构可以嵌套。 try - catch - finally 结构中，try 语句必须存在，catch 和 finally 语句，必须至少存在一个，其中，catch 语句可以出现多个。 使用 try - catch - finally 处理编译时异常，使得程序在编译时就不再报错，但在运行时仍可能报错。相当于使用 try - catch - finally 将一个编译时可能出现的异常，延迟到运行时出现。 开发中，由于运行时异常比较常见，通常不针对运行时异常编写 try - catch - finally，针对编译时异常，一定要考虑异常的处理。 try - catch - finally：真正的将异常进行了处理。 throws如果一个方法中的语句执行时可能生成某种异常，但是并不能确定如何处理这种异常，则此方法应显示地声明抛出异常，表明该方法将不对这些异常进行处理，而由该方法的调用者负责处理。在方法声明中用 throws 语句可以声明抛出异常的列表，throws 后面的异常类型可以是方法中产生的异常类型，也可以是它的父类。注意：不同于 try - catch - finally，throws 并没有真正的将异常进行了处理，而是抛给了方法的调用者去处理。 “throws + 异常类型”，写在方法的声明处。指明此方法执行时，可能会抛出的异常类型。一旦当方法体执行时出现异常，仍会在异常代码处生成一个异常类的对象，此对象满足 throws 后的异常类型时，就会被抛出。异常代码后续的代码，就不再被执行。 重写方法声明抛出异常的原则：子类重写的方法不能抛出比父类被重写的方法范围更大的异常类型。 1234567891011121314public class A &#123; public void methodA() throws IOException &#123; &#125;&#125;public class B1 extends A &#123; public void methodA() throws FileNotFoundException &#123; &#125;&#125;public class B2 extends A &#123; public void methodA() throws Exception &#123;// 报错 &#125;&#125; 开发中如何选择使用 try - catch -finally 和 throws 如果父类中被重写的方法没有 throws 方式处理异常，则子类重写的方法也不能使用 throws，意味着如果子类重写的方法中由异常，必须使用 try - catch -finally 方式处理。 执行的方法中，先后又调用了另外的几个方法，这几个方法是递进关系执行的，则建议这几个方法使用 throws 的方式进行处理。而执行的方法中，可以考虑使用 try - catch -finally 方式进行处理。 手动生成并抛出异常java 异常类对象除在程序执行过程中出现异常时由系统自动生成并抛出，也可根据需要使用人工创建并抛出 。 首先要生成异常类对象，然后通过 throw 语句实现抛出操作 (提交给 java 运行环境)。如：IOException e = new IOException(); throw e;。 可以抛出的异常必须是 Throwable 或其子类的实例。下面的语句在编译时将会产生语法错误：throw new String(&quot;want to throw&quot;);。 实例： 1234567891011121314151617181920212223242526272829303132public class Student &#123; public static void main(String[] args) &#123; Student student = new Student(); student.regist(-100); try &#123; student.regist2(-200); &#125; catch (Exception exception) &#123; System.out.println(exception.getMessage()); &#125; &#125; private int id; public void regist(int id) &#123; if (id &gt; 0) &#123; this.id = id; &#125; else &#123; // 手动抛出异常 throw new RuntimeException(&quot;输入的数据非法：&quot; + id); &#125; &#125; public void regist2(int id) throws Exception &#123; if (id &gt; 0) &#123; this.id = id; &#125; else &#123; // 手动抛出异常，需要在方法中声明 throw new Exception(&quot;输入的数据非法：&quot; + id); &#125; &#125;&#125; 执行 throw 后，后面代码还会执行吗？ 12345678910111213141516171819public class ExceptionTest &#123; public static void main(String[] args) &#123; int i = 1; if (i &gt; 5) &#123; System.out.println(i); &#125; else &#123; try &#123; throw new Exception(&quot;数据非法！&quot;);// 异常被try-catch &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;123&quot;); &#125; &#125;&#125;输出结果：java.lang.Exception: 数据非法！ at cn.xisun.java.base.ExceptionTest.main(ExceptionTest.java:17)123 1234567891011public class ExceptionTest &#123; public static void main(String[] args) throws Exception &#123; int i = 1; if (i &gt; 5) &#123; System.out.println(i); &#125; else &#123; throw new Exception(&quot;数据非法！&quot;);// 异常被throws System.out.println(&quot;123&quot;);// 编译不通过 &#125; &#125;&#125; 1234567891011121314public class ExceptionTest &#123; public static void main(String[] args) throws Exception &#123; int i = 1; if (i &gt; 5) &#123; System.out.println(i); &#125; else &#123; throw new Exception(&quot;数据非法！&quot;); &#125; System.out.println(&quot;123&quot;); &#125;&#125;输出结果：Exception in thread &quot;main&quot; java.lang.Exception: 数据非法！ at cn.xisun.java.base.ExceptionTest.main(ExceptionTest.java:16) 1234567891011121314151617181920212223public class ExceptionTest &#123; private static void get(int i) &#123; try &#123; int a = i / 0; System.out.println(a); &#125; catch (Exception e) &#123; throw new RuntimeException(&quot;发生异常&quot;); &#125; System.out.println(&quot;123&quot;); &#125; public static void main(String[] args) &#123; for (int i = 1; i &lt; 4; i++) &#123; System.out.println(i); get(i); &#125; &#125;&#125;输出结果：1Exception in thread &quot;main&quot; java.lang.RuntimeException: 发生异常 at cn.xisun.java.base.ExceptionTest.get(ExceptionTest.java:16) at cn.xisun.java.base.ExceptionTest.main(ExceptionTest.java:24) 用户自定义异常类如何自定义异常类： 继承于现有的异常结构：RuntimeException、Exception 等。 提供全局变量：serialVersionUID，唯一的标识当前类。 提供重载的构造器。 异常类的名字应做到见名知义，当异常出现时，可以根据名字判断异常类型。 实例： 1234567891011121314151617181920212223242526272829303132333435363738public class MyException extends Exception &#123; static final long serialVersionUID = 13465653435L; public MyException() &#123; &#125; public MyException(String message) &#123; super(message); &#125;&#125;class MyExpTest &#123; public void regist(int num) throws MyException &#123; if (num &lt; 0) &#123; throw new MyException(&quot;人数为负值，不合理&quot;); &#125; else &#123; System.out.println(&quot;登记人数&quot; + num); &#125; &#125; public void manager() &#123; try &#123; regist(-100); &#125; catch (MyException e) &#123; System.out.print(&quot;登记失败，出错信息：&quot; + e.getMessage()); &#125; System.out.print(&quot;本次登记操作结束&quot;); &#125; public static void main(String args[]) &#123; MyExpTest t = new MyExpTest(); t.manager(); &#125;&#125;输出结果：登记失败，出错信息：人数为负值，不合理本次登记操作结束 12345678910111213141516171819202122232425262728293031323334public class ReturnExceptionDemo &#123; static void methodA() &#123; try &#123; System.out.println(&quot;进入方法A&quot;); throw new RuntimeException(&quot;制造异常&quot;); &#125; finally &#123; System.out.println(&quot;调用A方法的finally&quot;); &#125; &#125; static void methodB() &#123; try &#123; System.out.println(&quot;进入方法B&quot;); return; &#125; finally &#123; System.out.println(&quot;调用B方法的finally&quot;); &#125; &#125; public static void main(String[] args) &#123; try &#123; methodA(); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; methodB(); &#125;&#125;输出结果：进入方法A调用A方法的finally制造异常进入方法B调用B方法的finally 异常处理的 5 个关键字 面试题： final、finally 和 finalize 的区别？ finalize 是一个方法。 throw 和 throws 的区别？ throw 表示抛出一个异常类的对象，生成异常对象的过程，声明在方法体内。 throws 属于异常处理的一种方式，声明在方法的声明处。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 的设计模式","slug":"java-design-mode","date":"2021-03-01T02:23:19.000Z","updated":"2021-04-09T08:00:21.316Z","comments":true,"path":"2021/03/01/java-design-mode/","link":"","permalink":"http://example.com/2021/03/01/java-design-mode/","excerpt":"","text":"设计模式是在大量的实践中总结和理论化之后优选的代码结构、编程风格、以及解决问题的思考方式。设计模式就像是经典的棋谱，不同的棋局，我们用不同的棋谱，免去自己再思考和摸索。 单例 (Singleton) 设计模式所谓类的单例设计模式，就是采取一定的方法保证在整个的软件系统中，对某个类只能存在一个对象实例，并且该类只提供一个取得其对象实例的方法。如果我们要让类在一个虚拟机中只能产生一个对象，我们首先必须将类的构造器的访问权限设置为 private，这样，就不能用 new 操作符在类的外部产生类的对象了，但在类内部仍可以产生该类的对象。因为在类的外部开始还无法得到类的对象，只能调用该类的某个静态方法以返回类内部创建的对象，静态方法只能访问类中的静态成员变量，所以，指向类内部产生的该类对象的变量也必须定义成静态的。 单例设计模式的优点：由于单例模式只生成一个实例，减少了系统性能开销，当一个对象的产生需要比较多的资源时，如读取配置、产生其他依赖对象时，则可以通过在应用启动时直接产生一个单例对象，然后永久驻留内存的方式来解决。比如，java.lang.Runtime： 单例设计模式的应用场景： 网站的计数器，一般也是单例模式实现，否则难以同步。 应用程序的日志应用，一般都使用单例模式实现，这一般是由于共享的日志文件一直处于打开状态，因此只能有一个实例去操作，否则内容不好追加。 数据库连接池的设计一般也是采用单例模式，因为数据库连接是一种数据库资源。 项目中，读取配置文件的类，一般也只有一个对象。没有必要每次使用配置文件数据，都生成一个对象去读取。 Application 也是单例的典型应用。 Windows 的 Task Manager (任务管理器) 就是很典型的单例模式。 Windows 的 Recycle Bin (回收站) 也是典型的单例应用。在整个系统运行过程中，回收站一直维护着仅有的一个实例。 单例设计模式的实现方法： 饿汉式： 1234567891011121314151617181920212223public class SingletonTest &#123; public static void main(String[] args) &#123; Bank bank1 = Bank.getInstance(); Bank bank2 = Bank.getInstance(); System.out.println(bank1 == bank2);// true，二者指向同一个对象 &#125;&#125;class Bank &#123; // 1.私有化类的构造器 private Bank() &#123; &#125; // 2.内部创建类的对象 // 4.要求此对象也必须声明为静态的 private static Bank instance = new Bank(); // 3.提供公共的静态方法，返回类的对象 public static Bank getInstance() &#123; return instance; &#125;&#125; 123456789101112131415public class SingletonTest &#123; public static void main(String[] args) &#123; Bank bank1 = Bank.getInstance(); Bank bank2 = Bank.getInstance(); System.out.println(bank1 == bank2);// true，二者指向同一个对象 &#125;&#125;class Bank &#123; private Bank() &#123; &#125; public static final Bank instance = new Bank();// 添加final是防止instance属性被外部修改&#125; 懒汉式： 123456789101112131415161718192021222324252627282930313233343536373839public class SingletonTest &#123; public static void main(String[] args) &#123; Bank bank1 = Bank.getInstance(); Bank bank2 = Bank.getInstance(); System.out.println(bank1 == bank2);// true，二者指向同一个对象 &#125;&#125;class Bank &#123; // 1.私有化类的构造器 private Bank() &#123; &#125; // 2.内部声明类的对象，没有初始化 // 4.要求此对象也必须声明为静态的 private static Bank instance = null; // 3.提供公共的静态方法，返回类的对象 public static Bank getInstance() &#123; // 同步方式一：效率稍差，等同于在方法上直接添加synchronized /*synchronized (Bank.class) &#123; if (instance == null) &#123; instance = new Bank(); &#125; &#125; return instance;*/ // 同步方式二：效率稍好 if (instance == null) &#123; synchronized (Bank.class) &#123; if (instance == null) &#123; instance = new Bank(); &#125; &#125; &#125; return instance; &#125;&#125; 区分饿汉式和懒汉式： 饿汉式 好处：天然就是线程安全的。 坏处：类加载时就创建了对象，导致对象加载时间过长。 懒汉式 好处：延迟对象的创建。 坏处：不是线程安全的，多线程情况下需要考虑线程安全问题。 模板式方法设计模式 (TemplateMethod)抽象类体现的就是一种模板模式的设计，抽象类作为多个子类的通用模板，子类在抽象类的基础上进行扩展、改造，但子类总体上会保留抽象类的行为方式。 解决的问题： 当功能内部一部分实现是确定的，一部分实现是不确定的。这时可以把不确定的部分暴露出去，让子类去实现。 换句话说，在软件开发中实现一个算法时，整体步骤很固定、通用，这些步骤已经在父类中写好了。但是某些部分易变，易变部分可以抽象出来，供不同子类实现。这就是一种模板模式。 模板方法设计模式是编程中经常用得到的模式。各个框架、类库中都有他的影子，比如常见的有： 数据库访问的封装 Junit 单元测试 JavaWeb 的 Servlet 中关于 doGet/doPost 方法调用 Hibernate 中模板程序 Spring 中 JDBCTemlate、HibernateTemplate 等 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; SubTemplate subTemplate = new SubTemplate(); subTemplate.getTime(); &#125;&#125;abstract class Template &#123; // 计算一段代码的执行时间 public final void getTime() &#123; long start = System.currentTimeMillis(); code(); long end = System.currentTimeMillis(); System.out.println(&quot;执行时间是：&quot; + (end - start)); &#125; // 代码不确定，由子类自己实现 --- 不确定的、异变的部分 public abstract void code();&#125;class SubTemplate extends Template &#123; @Override public void code() &#123; for (int i = 0; i &lt; 10000; i++) &#123; System.out.println(i); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 抽象类的应用：模板方法的设计模式public class TemplateMethodTest &#123; public static void main(String[] args) &#123; BankTemplateMethod btm = new DrawMoney(); btm.process(); BankTemplateMethod btm2 = new ManageMoney(); btm2.process(); &#125;&#125;abstract class BankTemplateMethod &#123; // 具体方法 public void takeNumber() &#123; System.out.println(&quot;取号排队&quot;); &#125; public abstract void transact(); // 办理具体的业务 --- 钩子方法 public void evaluate() &#123; System.out.println(&quot;反馈评分&quot;); &#125; // 模板方法，把基本操作组合到一起，子类一般不能重写 public final void process() &#123; this.takeNumber(); this.transact();// 像个钩子，具体执行时，挂哪个子类，就执行哪个子类的实现代码 this.evaluate(); &#125;&#125;class DrawMoney extends BankTemplateMethod &#123; @Override public void transact() &#123; System.out.println(&quot;我要取款！！！&quot;); &#125;&#125;class ManageMoney extends BankTemplateMethod &#123; @Override public void transact() &#123; System.out.println(&quot;我要理财！我这里有2000万美元!!&quot;); &#125;&#125; 代理模式 (Proxy)应用场景： 安全代理：屏蔽对真实角色的直接访问。 远程代理：通过代理类处理远程方法调用 (RMI)。 延迟加载：先加载轻量级的代理对象，真正需要再加载真实对象。比如，要开发一个大文档查看软件，大文档中有大的图片，有可能一个图片有 100 MB，在打开文件时，不可能将所有的图片都显示出来，这样就可以使用代理模式，当需要查看图片时，用 proxy 来进行大图片的打开。 分类： 静态代理 (静态定义代理类) 动态代理 (动态生成代理类) JDK 自带的动态代理，需要反射等知识。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class NetWorkTest &#123; public static void main(String[] args) &#123; Server1 server1 = new Server1(); Server1 server2 = new Server1(); ProxyServer proxyServer1 = new ProxyServer(server1); ProxyServer proxyServer2 = new ProxyServer(server2); // 表面上是代理类执行了browse()方法，实际上是被代理类执行的browse()方法 proxyServer1.browse(); proxyServer2.browse(); &#125;&#125;interface Network &#123; public void browse();&#125;// 被代理类1class Server1 implements Network &#123; @Override public void browse() &#123; System.out.println(&quot;真实的服务器1访问网络&quot;); &#125;&#125;// 被代理类class Server2 implements Network &#123; @Override public void browse() &#123; System.out.println(&quot;真实的服务器2访问网络&quot;); &#125;&#125;// 代理类class ProxyServer implements Network &#123; private Network work; public ProxyServer(Network work) &#123; this.work = work; &#125; public void check() &#123; System.out.println(&quot;联网之前的检查工作&quot;); &#125; @Override public void browse() &#123; // 代理类除了执行核心功能外，还执行了其他的一些工作 // 被代理类不需要关系这些其他的工作，只需要完成核心功能即可 check(); work.browse(); &#125;&#125;输出结果：联网之前的检查工作真实的服务器1访问网络联网之前的检查工作真实的服务器1访问网络 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class StaticProxyTest &#123; public static void main(String[] args) &#123; Star s = new Proxy(new RealStar()); s.confer(); s.signContract(); s.bookTicket(); s.sing(); s.collectMoney(); &#125;&#125;interface Star &#123; void confer();// 面谈 void signContract();// 签合同 void bookTicket();// 订票 void sing();// 唱歌 void collectMoney();// 收钱&#125;class RealStar implements Star &#123; @Override public void confer() &#123; &#125; @Override public void signContract() &#123; &#125; @Override public void bookTicket() &#123; &#125; @Override public void sing() &#123; System.out.println(&quot;明星：歌唱~~~&quot;); &#125; @Override public void collectMoney() &#123; &#125;&#125;class Proxy implements Star &#123; private Star real; public Proxy(Star real) &#123; this.real = real; &#125; @Override public void confer() &#123; System.out.println(&quot;经纪人面谈&quot;); &#125; @Override public void signContract() &#123; System.out.println(&quot;经纪人签合同&quot;); &#125; @Override public void bookTicket() &#123; System.out.println(&quot;经纪人订票&quot;); &#125; @Override public void sing() &#123; real.sing(); &#125; @Override public void collectMoney() &#123; System.out.println(&quot;经纪人收钱&quot;); &#125;&#125;输出结果：经纪人面谈经纪人签合同经纪人订票明星：歌唱~~~经纪人收钱 工厂模式","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 中的排序算法","slug":"java-algorithm-sort","date":"2021-02-19T08:30:35.000Z","updated":"2021-04-09T07:59:44.704Z","comments":true,"path":"2021/02/19/java-algorithm-sort/","link":"","permalink":"http://example.com/2021/02/19/java-algorithm-sort/","excerpt":"","text":"算法的五大特征： 说明：满足确定性的算法也称为确定性算法。现在人们也关注更广泛的概念，例如考虑各种非确定性的算法，如并行算法、概率算法等。另外，人们也关注并不要求终止的计算描述，这种描述有时被称为过程 (procedure)。 排序：假设含有 n 个记录的序列为 {R1, R2, …, Rn}，其相应的关键字序列为 {K1, K2, …, Kn}。将这些记录重新排序为 {Ri1, Ri2, …, Rin}，使得相应的关键字值满足条 Ki1&lt;= Ki2 &lt;= … &lt;= Kin，这样的一种操作称为排序。通常来说，排序的目的是快速查找。 衡量排序算法的优劣： 时间复杂度：分析关键字的比较次数和记录的移动次数。 空间复杂度：分析排序算法中需要多少辅助内存。 稳定性：若两个记录 A 和 B 的关键字值相等，但排序后 A、B 的先后次序保持不变，则称这种排序算法是稳定的。 排序算法分类：内部排序和外部排序。 内部排序：整个排序过程不需要借助于外部存储器 (如磁盘等)，所有排序操作都在内存中完成。 外部排序：参与排序的数据非常多，数据量非常大，计算机无法把整个排序过程放在内存中完成，必须借助于外部存储器（如磁盘）。外部排序最常见的是多路归并排序。可以认为外部排序是由多次内部排序组成。 十大内部排序算法：","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 的基础知识","slug":"java-base","date":"2021-02-10T06:47:42.000Z","updated":"2021-04-09T07:59:53.474Z","comments":true,"path":"2021/02/10/java-base/","link":"","permalink":"http://example.com/2021/02/10/java-base/","excerpt":"","text":"bit 和 byte计算机本质是一系列的电路开关。每个开关存在两种状态：开 (on) 和关 (off)。如果电路是开的，它的值是 1，如果电路是关的，它的值是 0。 一个 0 或者一个 1 存储为一个比特 (bit)，是计算机中最小的存储单位。 计算机中最基本的存储单元是字节 (byte) 。每个字节由 8 个比特构成。 计算机的存储能力是以字节来衡量的。如下： 千字节 (kilobyte，KB) = 1024 B 兆字节 (megabyte，MB) = 1024 KB 千兆字节 (gigabyte，GB) = 1024 MB 万亿字节 (terabyte，TB) = 1024 GB JDK、JRE 和 JVM JDK = JRE + 开发工具集（例如 Javac 编译工具等） JRE = JVM + Java SE 标准类库 阶段 编写：编写的 java 代码保存在以 .java 为结尾的源文件中。 在一个 java 源文件中，可以声明多个 class 类，但是，只能最多有一个类声明为 public。而且，声明为 public 的类名，必须与源文件名相同。 编译：使用 javac.exe 命令编译 java 源文件。格式：javac 源文件名.java 编译之后，会生成一个或多个以 .class 结尾的字节码文件，字节码文件的文件名与 java 源文件中的类名相同，二者是一一对应的。 运行：使用 java.exe 命令解释运行字节码文件。格式：java 类名 运行的字节码文件，需要有入口函数 main() 方法，且书写格式是固定的。 编译完源文件后，生成一个或多个字节码文件。然后运行时，使用 JVM 中的类的加载器和解释器，对生成的字节码文件进行解释运行。即：此时，需要将字节码文件对应的类加载到内存中，这个过程涉及到内存解析。 注释单行注释：// 注释文字 多行注释： /* 注释文字 */ 文档注释：/** 注释文字 */ 对于单行注释和多行注释，被注释的文字，不会被 JVM 解释执行； 多行注释里面不允许有多行注释嵌套； 文档注释内容可以被 JDK 提供的工具 javadoc 所解析，生成一套以网页文件形式体现该程序的说明文档。 关键字和保留字关键字 (key word)定义：被 java 语言赋予了特殊含义，用做专门用途的字符串 (单词)。 特点：关键字中所有字母都为小写。 官方地址：https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html 保留字 (reserved word)现有 java 版本尚未使用，但以后版本可能会作为关键字使用。自己命名标识符时要避免使用这些保留字：goto 、const。 标识符java 对各种变量、方法和类等要素命名时使用的字符序列称为标识符。 技巧：凡是自己可以起名字的地方都叫标识符。 定义合法标识符规则： 由 26 个英文字母大小写，0 - 9，_ 或 $ 组成； 数字不可以开头； 不可以使用关键字和保留字，但能包含关键字和保留字； java 中严格区分大小写，长度无限制； 标识符不能包含空格。 如果不遵守以上规则，编译不通过。 名称命名规范： 包名：多单词组成时所有字母都小写：xxxyyyzzz； 类名、接口名：多单词组成时，所有单词的首字母大写：XxxYyyZzz； 变量名、方法名：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词首字母大写：xxxYyyZzz； 常量名：所有字母都大写。多单词时每个单词用下划线连接：XXX_YYY_ZZZ。 在命名时，为了提高阅读性，要尽量有意义，做到见名知意。 java 采用 unicode 字符集，因此标识符也可以使用汉字声明，但是不建议使用。 变量定义变量是内存中的一个存储区域，该区域的数据可以在同一类型范围内不断变化。 变量是程序中最基本的存储单元。包含变量类型、变量名和存储的值。 java 中每个变量必须先声明，后使用，使用变量名来访问这块区域的数据。 变量的作用域：其定义所在的一对 { } 内，变量只有在其作用域内才有效，在同一个作用域内，不能定义重名的变量。 1234567891011121314151617181920212223public class VariableTest&#123; public static void main(String[] args)&#123; // 变量的定义 int myAge = 12; // 变量的使用 System.out.println(myAge); // 编译错误：使用myNumber之前未定义myNumber // System.out.println(myNumber); // 变量的定义 int myNumber; // 编译错误：使用myNumber之前未赋值myNumber // System.out.println(myNumber); // 变量的赋值 myNumber = 1001; // 变量的使用 System.out.println(myNumber); &#125;&#125; 按数据类型分类java 是强类型语言，对于每一种数据都定义了明确的具体数据类型，并在内存中分配了不同大小的内存空间。 基本数据类型整数类型 java 各整数类型有固定的表数范围和字段长度，不受具体 OS 的影响，以保证 java 程序的可移植性。 java 的整型常量默认为 int 型，声明 long 型常量须后加 ‘l’ 或 ‘L’。java 程序中变量通常声明为 int 型，除非不足以表示较大的数，才使用 long。 浮点类型 与整数类型类似，java 浮点类型也有固定的表数范围和字段长度，不受具体操作系统的影响。 float：单精度，尾数可以精确到 7 位有效数字。很多情况下，精度很难满足需求。 double：双精度，精度是float的两倍。通常采用此类型。 java 的浮点型常量默认为 double 型，声明 float 型常量，须后加 ‘f’ 或 ‘F’。 字符类型char 型数据用来表示通常意义上的 “字符”，占用 2 个字节。char 类型是可以进行运算的。因为它都对应有 Unicode 码。 java 中的所有字符都使用Unicode编码，故一个字符可以存储一个字母，一个汉字，或其他书面语的一个字符。 字符型变量的三种表现形式： 字符常量是用单引号括起来的单个字符。例如：char c1 = &#39;a&#39;; char c2= &#39;中&#39;; char c3 = &#39;9&#39;;。 java 中还允许使用转义字符 \\ 来将其后的字符转变为特殊字符型常量。例如：char c3 = &#39;\\n&#39;; // &#39;\\n&#39;表示换行符。常用的转义字符如下： 直接使用 Unicode 值来表示字符型常量：’\\uXXXX’。其中，XXXX 代表一个十六进制整数。如：\\u000a 表示 \\n。 布尔类型boolean 类型用来判断逻辑条件，一般用于程序流程控制。 boolean 类型数据只允许取值 true 和 false，无 null。 java 虚拟机中没有任何供 boolean 值专用的字节码指令，java 语言表达所操作的 boolean 值，在编译之后都使用 java 虚拟机中的 int 数据类型来代替：true 用 1 表示，false 用 0 表示。———《java 虚拟机规范 8 版》 基本数据类型之间的转换自动类型转换：不同数据类型的变量做运算时，容量小的数据类型自动转换为容量大的数据类型。数据类型按容量大小排序为： 此处的容量大小，指的是该数据类型表示数的范围的大和小。 有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算。 byte，short 和 char 之间不会相互转换，他们三者在计算时首先转换为 int 类型。 boolean 类型不能与其它数据类型运算。 当把任何基本数据类型的值和字符串 (String) 进行连接运算时 (+)，基本数据类型的值将自动转化为字符串 (String) 类型。 强制类型转换：自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符：()，但可能造成精度降低或溢出。 通常，字符串不能直接转换为基本类型，但通过基本类型对应的包装类，可以实现把字符串转换成基本类型。如：String a = “43”; int i = Integer.parseInt(a);。 boolean 类型不可以转换为其它的数据类型。 引用数据类型StringString 不是基本数据类型，属于引用数据类型 (class)。使用方式与基本数据类型一致，例如：String str = “abcd”;。 一个字符串可以串接另一个字符串，也可以直接串接其他类型的数据。例如：str = str + “xyz” ; int n = 100; str = str + n;。 String 与 8 种基本数据类型做运算时，但只能是连接运算。 按声明位置分类成员变量：在方法体外，类体内声明的变量。 局部变量：在方法体内部声明的变量。 成员变量和局部变量在初始化值方面的异同：同：都有生命周期；异：局部变量除形参外，需显式初始化。 进制所有数字在计算机底层都以二进制形式存在。 对于整数，有四种表示方式： 二进制 (binary) ：0 - 1，满 2 进 1，以 0b 或 0B 开头表示。 十进制 (decimal) ：0 - 9，满 10 进 1。 八进制 (octal) ：0 - 7，满 8 进 1，以数字 0 开头表示。 十六进制 (hex) ：0 - 9 及 A - F，满 16 进 1，以 0x 或 0X 开头表示。此处的 A - F 不区分大小写。如：0x21AF +1= 0X21B0。 二进制java 整数常量默认是 int 类型，当用二进制定义整数时，其第 32 位是符号位；当是 long 类型时，二进制默认占 64 位，第 64 位是符号位。 二进制的整数有如下三种形式： 原码：直接将一个数值换成二进制数，最高位是符号位。 负数的反码：是对原码按位取反，但最高位 (符号位) 不变，确定为1。 负数的补码：其反码加 1。 正数的原码、反码、补码都相同。 计算机以二进制补码的形式保存所有的整数。 原码到补码的转换： 不同进制间转换十进制转二进制：除 2 取余的逆。 二进制和八进制、十六进制转换： 运算符运算符是一种特殊的符号，用以表示数据的运算、赋值和比较等。 算术运算符 如果对负数取模，可以把模数负号忽略不记，如：5 % -2 = 1。 如果被模数是负数，则不可忽略，如： -5 % 2 = -1。此外，取模运算的结果不一定总是整数。 对于除号 “/“，它的整数除和小数除是有区别的：整数之间做除法时，只保留整数部分而舍弃小数部分。 例如：int x = 3510; x = x / 1000 * 1000;，x 的结果是 3000。 “+” 除字符串相加功能外，还能把非字符串转换成字符串。例如：System.out.println(&quot;5 + 5 = &quot; + 5 + 5); ，打印结果是：5 + 5 = 55 。 赋值运算符符号：=。当 “=” 两侧数据类型不一致时，可以使用自动类型转换或使用强制类型转换原则进行处理。支持连续赋值。 扩展赋值运算符： +=，-=，*=，/=，%=。这几个赋值运算符不会改变变量本身的数据类型。 12345int i = 1;i *= 0.1;System.out.println(i);// 0i++;System.out.println(i);// 1 12345int m = 2;int n = 3;n *= m++;// n = n * m++;System.out.println(&quot;m = &quot; + m);// 3System.out.println(&quot;n = &quot; + n);// 6 123int n = 10;n += (n++) + (++n);// n = n + (n++) + (++n); → n = 10 + 10 + 12;System.out.println(n);// 32 比较运算符 (关系运算符) 比较运算符的结果都是 boolean 型。 逻辑运算符&amp;：逻辑与，|：逻辑或，!：逻辑非。&amp;&amp;：短路与，||：短路或，^：逻辑异或。 逻辑运算符用于连接布尔型表达式，在 java 中不可以写成 3 &lt; x &lt; 6，应该写成 x &gt; 3 &amp; x &lt; 6。 “&amp;” 和 “&amp;&amp;” 的区别：&amp; 表示，左边无论真假，右边都进行运算；&amp;&amp; 表示，如果左边为真，右边参与运算，如果左边为假，右边不参与运算。 “|” 和 “||” 的区别同理：| 表示，左边无论真假，右边都进行运算；||表示，如果左边为假，右边参与运算，如果左边为真，右边不参与运算。 异或 (^) 与或 (|) 的不同之处是：当左右都为 true 时，结果为 false。即：异或，追求的是异! 123456789101112131415161718192021222324252627int x = 1;int y = 1;if (x++ == 2 &amp; ++y == 2) &#123; x = 7;&#125;System.out.println(&quot;x = &quot; + x + &quot;, y = &quot; + y);// x = 2, y = 2x = 1;y = 1;if (x++ == 2 &amp;&amp; ++y == 2) &#123; x = 7;&#125;System.out.println(&quot;x = &quot; + x + &quot;, y = &quot; + y);// x = 2, y = 1x = 1;y = 1;if (x++ == 1 | ++y == 1) &#123; x = 7;&#125;System.out.println(&quot;x = &quot; + x + &quot;, y = &quot; + y);// x = 7, y = 2x = 1;y = 1;if (x++ == 1 || ++y == 1) &#123; x = 7;&#125;System.out.println(&quot;x = &quot; + x + &quot;, y = &quot; + y);// x = 7, y = 1 12345678910boolean x = true;boolean y = false;short z = 42;if ((z++ == 42) &amp;&amp; (y = true)) &#123; z++;&#125;if ((x = false) || (++z == 45)) &#123; z++;&#125;System.out.println(&quot;z = &quot; + z);// z = 46 位运算符 无 &lt;&lt;&lt;。 位运算是直接对整数的二进制进行的运算。 &lt;&lt; ：在一定范围内，每向左移一位，相当于乘以 2。 &gt;&gt;：在一定范围内，每向右移一位，相当于除以2。 面试题：最高效的计算 2 * 8。利用：2 &lt;&lt; 3，或者 8 &lt;&lt; 1。 交换两个数： 123456789101112131415161718192021222324252627282930313233int num1 = 10;int num2 = 20;System.out.println(num1 + &quot;, &quot; + num2);// 方式一int temp;temp = num1;num1 = num2;num2 = temp;System.out.println(num1 + &quot;, &quot; + num2);// 方式二num1 = 10;num2 = 20;num1 = num1 + num2;num2 = num1 - num2;num1 = num1 - num2;System.out.println(num1 + &quot;, &quot; + num2);// 方式三num1 = 10;num2 = 20;num1 = num1 ^ num2;num2 = num1 ^ num2;num1 = num1 ^ num2;System.out.println(num1 + &quot;, &quot; + num2);// 方式四num1 = 10;num2 = 20;num1 = num1 &lt;&lt; 1;num2 = num2 &gt;&gt; 1;System.out.println(num1 + &quot;, &quot; + num2); 三元运算符格式： 表达式 1 和表达式 2 要求类型是一致的，因为要与接受的参数类型相同。 凡是可以使用三元运算符的地方，都可以改写为 if - else 结构，反之，不成立。如果既可以使用三元运算符，又可以使用 if - else 结构，优先使用三元运算符，因为更简洁、效率更高。 三元运算符与 if - else 的联系与区别： 三元运算符可简化 if - else 语句。 三元运算符要求必须返回一个结果。 if 后的代码块可有多个语句。 运算符的优先级 运算符有不同的优先级，所谓优先级就是表达式运算中的运算顺序。如上表，上一行运算符总优先于下一行。 只有单目运算符、三元运算符、赋值运算符是从右向左运算的。 程序流程控制流程控制语句是用来控制程序中各语句执行顺序的语句，可以把语句组合成能完成一定功能的小逻辑模块。 流程控制方式采用结构化程序设计中规定的三种基本流程结构，即： 顺序结构：程序从上到下逐行地执行，中间没有任何判断和跳转。 分支结构：根据条件，选择性地执行某段代码。有 if - else 和 switch - case 两种分支语句。 循环结构：根据循环条件，重复性的执行某段代码。有while、do - while、for三种循环语句。 注：JDK 1.5 提供了 foreach 循环，方便遍历集合、数组元素。 if - else 结构 switch - case 结构 switch (表达式) 中表达式的值，必须是下述几种类型之一：byte ，short，char，int，枚举类 (jdk 5.0)，String 类 (jdk 7.0)。 case 子句中的值必须是常量，不能是变量名或不确定的表达式值。 同一个 switch 语句，所有 case 子句中的常量值互不相同。 break 语句用来在执行完一个 case 分支后使程序跳出 switch 语句块；如果没有 break，程序会顺序执行到 switch 结尾。 default 子句是可任选的。同时，位置也是灵活的。当没有匹配的 case 时，执行 default。 如果多个 case 的执行语句相同，则可以将其合并。 同等情况下，switch - case 结构比 if - else 结构的效率稍高。 123456789101112Scanner scanner = new Scanner(System.in);int num = scanner.nextInt();switch (num) &#123; case 0: System.out.println(0); case 1: System.out.println(1); case 2: System.out.println(2); default: System.out.println(&quot;other&quot;);&#125; 添加 break 和不添加 break 的结果是不同的。 12345678910111213141516Scanner scanner = new Scanner(System.in);int num = scanner.nextInt();switch (num) &#123; case 0: System.out.println(0); break; case 1: System.out.println(1); break; case 2: System.out.println(2); break; default: System.out.println(&quot;other&quot;); break;// default位于最后，此break可以不添加。&#125; 键盘输入一个月份和天数，判断其是一年中的第几天： 12345678910111213141516171819202122232425262728293031323334353637public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;请输入month：&quot;); int month = scanner.nextInt(); System.out.println(&quot;请输入day：&quot;); int day = scanner.nextInt(); int sumDays = 0; switch (month) &#123; case 12: sumDays += 30; case 11: sumDays += 31; case 10: sumDays += 30; case 9: sumDays += 31; case 8: sumDays += 31; case 7: sumDays += 30; case 6: sumDays += 31; case 5: sumDays += 30; case 4: sumDays += 31; case 3: sumDays += 28; case 2: sumDays += 31; case 1: sumDays += day; &#125; System.out.println(month + &quot;月&quot; + day + &quot;日，是当年的第&quot; + sumDays + &quot;天。&quot;);&#125; 键盘输入一个年份、月份和天数，判断其是该年中的第几天： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;请输入year：&quot;); int year = scanner.nextInt(); System.out.println(&quot;请输入month：&quot;); int month = scanner.nextInt(); System.out.println(&quot;请输入day：&quot;); int day = scanner.nextInt(); int sumDays = 0; switch (month) &#123; case 12: sumDays += 30; case 11: sumDays += 31; case 10: sumDays += 30; case 9: sumDays += 31; case 8: sumDays += 31; case 7: sumDays += 30; case 6: sumDays += 31; case 5: sumDays += 30; case 4: sumDays += 31; case 3: if ((year % 4 == 0 &amp;&amp; year % 100 != 0) || year % 400 == 0) &#123; sumDays += 29;// 闰年2月29天 &#125; else &#123; sumDays += 28;// 平年2月28天 &#125; case 2: sumDays += 31; case 1: sumDays += day; &#125; System.out.println(year + &quot;年&quot; + month + &quot;月&quot; + day + &quot;日，是当年的第&quot; + sumDays + &quot;天。&quot;);&#125; 判断一年是否是闰年的标准： 1）可以被 4 整除，但不可被 100 整除。 或 2）可以被 400 整除。 for 循环语法格式： 执行过程：① - ② - ③ - ④ - ② - ③ - ④ - ② - ③ - ④ - ….. - ② 说明： ② 循环条件部分为 boolean 类型表达式，当值为 false 时，退出循环。 ① 初始化部分可以声明多个变量，但必须是同一个类型，用逗号分隔。 ④ 迭代部分可以有多个变量更新，用逗号分隔。 键盘输入两个正整数，求他们的最大公约数和最小公倍数： 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;请输入第一个正整数：&quot;); int firstNum = scanner.nextInt(); System.out.println(&quot;请输入第二个正整数：&quot;); int secondNum = scanner.nextInt(); int min = Math.min(firstNum, secondNum); int max = Math.max(firstNum, secondNum); // 最大公约数 for (int i = min; i &gt;= 1; i--) &#123; if (firstNum % i == 0 &amp;&amp; secondNum % i == 0) &#123; System.out.println(firstNum + &quot;和&quot; + secondNum + &quot;的最大公约数为：&quot; + i); break; &#125; &#125; // 最小公倍数 for (int i = max; i &lt;= firstNum * secondNum; i++) &#123; if (i % firstNum == 0 &amp;&amp; i % secondNum == 0) &#123; System.out.println(firstNum + &quot;和&quot; + secondNum + &quot;的最小公倍数为：&quot; + i); break; &#125; &#125;&#125;请输入第一个正整数：12请输入第二个正整数：2012和20的最大公约数为：412和20的最小公倍数为：60 while 循环语法格式： 执行过程：① - ② - ③ - ④ - ② - ③ - ④ - ② - ③ - ④ - ….. - ② 说明： 注意不要忘记声明 ④ 迭代部分。否则，循环将不能结束，变成死循环。 for 循环和 while 循环可以相互转换。 do - while 循环语法格式： 执行过程：① - ③ - ④ - ② - ③ - ④ - ② - ③ - ④ - ② - ③ - ④ - ….. - ② 说明： do - while 循环至少执行一次循环体 。 嵌套循环 将一个循环放在另一个循环体内，就形成了嵌套循环。其中，for，while，do - while 均可以作为外层循环或内层循环。 实质上，嵌套循环就是把内层循环当成外层循环的循环体。当只有内层循环的循环条件为 false 时，才会完全跳出内层循环，才可结束外层的当次循环，开始下一次的循环。 设外层循环次数为 m 次，内层为 n 次，则内层循环体实际上需要执行 m * n 次。 九九乘法表： 123456789101112131415161718public static void main(String[] args) &#123; for (int i = 1; i &lt;= 9; i++) &#123; for (int j = 1; j &lt;= i; j++) &#123; System.out.print(j + &quot; * &quot; + i + &quot; = &quot; + (j * i) + &quot;\\t&quot;); &#125; System.out.println(); &#125;&#125;1 * 1 = 1 1 * 2 = 2 2 * 2 = 4 1 * 3 = 3 2 * 3 = 6 3 * 3 = 9 1 * 4 = 4 2 * 4 = 8 3 * 4 = 12 4 * 4 = 16 1 * 5 = 5 2 * 5 = 10 3 * 5 = 15 4 * 5 = 20 5 * 5 = 25 1 * 6 = 6 2 * 6 = 12 3 * 6 = 18 4 * 6 = 24 5 * 6 = 30 6 * 6 = 36 1 * 7 = 7 2 * 7 = 14 3 * 7 = 21 4 * 7 = 28 5 * 7 = 35 6 * 7 = 42 7 * 7 = 49 1 * 8 = 8 2 * 8 = 16 3 * 8 = 24 4 * 8 = 32 5 * 8 = 40 6 * 8 = 48 7 * 8 = 56 8 * 8 = 64 1 * 9 = 9 2 * 9 = 18 3 * 9 = 27 4 * 9 = 36 5 * 9 = 45 6 * 9 = 54 7 * 9 = 63 8 * 9 = 72 9 * 9 = 81 10000 以内所有的质数： 1234567891011121314151617181920212223242526// 方式一public static void main(String[] args) &#123; // 质数：素数，只能被1和它本身整除的自然数，2是最小的质数。 int count = 0; boolean ifFlag = true; for (int i = 2; i &lt;= 100000; i++) &#123; // 优化一：使用Math.sqrt(i)代替i，减少循环的次数 // i除以一个从2开始的小数，会得到一个从i-1开始的大数，因此，除以2开始的小数与除以从i-1开始的大数， // 可以省略一个，以减少次数，这样计算的中点是i开方的值。 for (int j = 2; j &lt;= Math.sqrt(i); j++) &#123; if (i % j == 0) &#123; ifFlag = false; // 优化二：使用break，跳出不必要的循环 break; &#125; &#125; if (ifFlag) &#123; // 优化三：不打印，i越大，打印的耗时越长 // System.out.println(&quot;质数：&quot; + i); count++; &#125; // 重置 ifFlag = true; &#125; System.out.println(&quot;质数的个数有：&quot; + count);// 质数的个数有：9592&#125; 123456789101112131415161718// 方式二public static void main(String[] args) &#123; // 质数：素数，只能被1和它本身整除的自然数，2是最小的质数。 int count = 0; label: for (int i = 2; i &lt;= 100000; i++) &#123; // 优化一：使用Math.sqrt(i)代替i，减少循环的次数 // i除以一个从2开始的小数，会得到一个从i-1开始的大数，因此，除以2开始的小数与除以从i-1开始的大数， // 可以省略一个，以减少次数，这样计算的中点是i开方的值。 for (int j = 2; j &lt;= Math.sqrt(i); j++) &#123; if (i % j == 0) &#123; continue label; &#125; &#125; count++; &#125; System.out.println(&quot;质数的个数有：&quot; + count);// 质数的个数有：9592&#125; break 和 continue break 使用在 switch - case 结构或者循环结构中。 continue 只能使用在循环结构中。 break 语句用于终止某个语句块的执行，跳出当前循环，continue 语句用于跳过其所在循环语句块的当次执行，继续下一次循环。 123456789public static void main(String[] args) &#123; for (int i = 1; i &lt;= 10; i++) &#123; if (i % 4 == 0) &#123; break;// 输出结果：1 2 3 continue;// 输出结果：1 2 3 5 6 7 9 10 &#125; System.out.print(i + &quot;\\t&quot;); &#125;&#125; break 语句出现在多层嵌套的语句块中时，可以通过标签指明要终止的是哪一层语句块 (默认跳出包裹 break 最近的一层循环)： continue 语句出现在多层嵌套的循环语句体中时，可以通过标签指明要跳过的是哪一层循环 (默认跳出包裹 continue 最近的一层循环)。 1234567891011121314151617public static void main(String[] args) &#123; label: for (int i = 1; i &lt;= 4; i++) &#123; for (int j = 1; j &lt;= 10; j++) &#123; if (j % 4 == 0) &#123; break label;// 结束指定标识label层的当前循环 continue label;// 结束指定标识label层的当次循环 &#125; System.out.print(j); &#125; System.out.println(); &#125;&#125;break label输出结果：1 2 3continue label输出结果：1 2 3 2 3 1 2 3 1 2 3 break 和 continue 关键字后面不能直接声明执行语句。 随机数获取 [a, b] 之间的随机数： 1int v = (int) (Math.random() * (b - a + 1) + a); 如获取 [10, 99] 之间的随机数： 1int v = (int) (Math.random() * 90 + 10); 数组数组 (Array)，是多个相同类型数据按一定顺序排列的集合，使用一个名字命名，并通过编号的方式对这些数据进行统一管理。 数组的相关概念： 数组名 元素 下标 (或索引) 数组的长度 数组的特点： 数组是有序排列的。 创建数组对象会在内存中开辟一整块连续的空间，而数组名中引用的是这块连续空间的首地址。 数组本身是引用数据类型的变量，而数组中的元素可以是任何数据类型，既可以是基本数据类型，也可以是引用数据类型。 可以直接通过下标 (或索引) 的方式调用指定位置的元素，速度很快。 数组的长度一旦确定，就不能修改。 数组的分类： 按照维度：一维数组、二维数组、三维数组、… 按照元素的数据类型分：基本数据类型元素的数组、引用数据类型元素的数组 (即对象数组)。 一维数组声明方式：type var[] 或 type[] var;。例如：int a[]; int[] a1; double b[]; String[] c;// 引用类型变量数组。 不同写法：int[] x;，int x[];。 java 语言中声明数组时，不能指定其长度 (数组中元素的数)， 例如：int a[5];// 非法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public static void main(String[] args) &#123; // 1-1.静态初始化，方式一 int[] ids = new int[]&#123;1001, 1002, 1003, 1004, 1005&#125;; // 1-2.静态初始化，方式二，类型推断 int[] ids2 = &#123;1001, 1002, 1003, 1004, 1005&#125;; // 2.动态初始化 String[] names = new String[5]; names[0] = &quot;Student A&quot;; names[1] = &quot;Student B&quot;; names[2] = &quot;Student C&quot;; names[3] = &quot;Student D&quot;; names[4] = &quot;Student E&quot;; // 3.数组的长度 System.out.println(&quot;ids 的长度：&quot; + ids.length);// 5 System.out.println(&quot;names 的长度：&quot; + names.length);// 5 // 4.遍历数组 for (int i = 0; i &lt; ids.length; i++) &#123; System.out.println(ids[i]); &#125; for (int i = 0; i &lt; names.length; i++) &#123; System.out.println(names[i]); &#125; // 5.简写方式遍历数组 for (int id : ids) &#123; System.out.println(id); &#125; for (String name : names) &#123; System.out.println(name); &#125; // 6.数组元素的默认初始化值 int[] arrs = new int[5]; for (int arr : arrs) &#123; System.out.println(arr);// 0 &#125; String[] arrs2 = new String[5]; for (String arr2 : arrs2) &#123; System.out.println(arr2);// null &#125;&#125; 静态初始化：数组的初始化，和数组元素的赋值操作同时进行。 动态初始化 ：数组的初始化，和数组元素的赋值操作分开进行。 定义数组并用运算符 new 为之分配空间后，才可以引用数组中的每个元素。 数组元素的引用方式：数组名[数组元素下标]。 数组元素下标从 0 开始，长度为 n 的数组的合法下标取值范围：0 — n-1。如：int a[] = new int[3];，则可引用的数组元素为 a[0]、a[1] 和 a[2]。 数组元素下标可以是整型常量或整型表达式。如 a[3]，b[i]，c[6*i]。 数组一旦初始化完成，其长度也随即确定，且长度不可变。每个数组都有一个属性 length 指明它的长度，例如：a.length 指明数组 a 的长度 (元素个数)。 数组是引用类型，它的元素相当于类的成员变量，因此数组一经分配空间，其中的每个元素也被按照成员变量同样的方式被隐式初始化。然后，再根据实际代码设置，将数组相应位置的元素进行赋值，即显示赋值。 对于基本数据类型而言，默认的初始化值各有不同；对于引用数据类型而言，默认的初始化值为 null。 char 类型的默认值是 0，不是 ‘0’，表现的是类似空格的一种效果。 一维数组内存解析： 一个计算联系方式的数组： 123456789public static void main(String[] args) &#123; int[] arr = new int[]&#123;8, 2, 1, 0, 3&#125;; int[] index = new int[]&#123;2, 0, 3, 2, 4, 0, 1, 3, 2, 3, 3&#125;; String tel = &quot;&quot;; for (int i = 0; i &lt; index.length; i++) &#123; tel += arr[index[i]]; &#125; System.out.println(&quot;联系方式：&quot; + tel);// 联系方式：18013820100&#125; 二维数组java 语言里提供了支持多维数组的语法。如果把一维数组当成几何中的线性图形，那么二维数组就相当于是一个表格。 对于二维数组的理解，可以看成是一维数组 array1，作为另一个一维数组 array2 的元素而存在。其实，从数组底层的运行机制来看，没有多维数组。 不同写法：int[][] x;，int[] x[];，int x[][];。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public static void main(String[] args) &#123; // 1-1.静态初始化，方式一 int[][] arr = new int[][]&#123;&#123;3, 8, 2&#125;, &#123;2, 7&#125;, &#123;9, 0, 1, 6&#125;&#125;; // 1-2.静态初始化，方式二，类型推断 int[][] arr2 = &#123;&#123;3, 8, 2&#125;, &#123;2, 7&#125;, &#123;9, 0, 1, 6&#125;&#125;; System.out.println(arr2[2][3]); // 2-1.动态初始化，方式一 /* 定义了名称为arr3的二维数组，二维数组中有3个一维数组，内层每一个一维数组中有2个元素 内层一维数组的名称分别为arr3[0]，arr3[1]，arr3[2]，返回的是地址值 给内层第一个一维数组1脚标位赋值为78写法是：arr3[0][1] = 78; */ int[][] arr3 = new int[3][2]; arr3[0][1] = 78; System.out.println(arr3[0]);// [I@78308db1 System.out.println(arr3[0][1]);// 78 // 2-2.动态初始化，方式二 /* 二维数组arr4中有3个一维数组，内层每个一维数组都是默认初始化值null(注意：区别于格式2-1) 可以对内层三个一维数组分别进行初始化 */ int[][] arr4 = new int[3][]; // 初始化第一个 arr4[0] = new int[3]; // 初始化第二个 arr4[1] = new int[1]; // 初始化第三个 arr4[2] = new int[2]; // 3.特殊写法 int[] x, y[];// x是一维数组，y是二维数组 x = new int[3]; y = new int[3][2]; // 4.获取数组长度 System.out.println(&quot;arr的长度：&quot; + arr.length);// 3 System.out.println(&quot;arr第一个元素的长度：&quot; + arr[0].length);// 3 // 5.遍历二维数组 for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = 0; j &lt; arr[i].length; j++) &#123; System.out.print(arr[i][j] + &quot;\\t&quot;);// 3 8 2 2 7 9 0 1 6 &#125; &#125; System.out.println(); // 6.简写遍历二维数组 for (int[] valueArr : arr) &#123; for (int value : valueArr) &#123; System.out.print(value + &quot;\\t&quot;);// 3 8 2 2 7 9 0 1 6 &#125; &#125; System.out.println(); // 7.二维数组元素的默认初始化值 int[][] arr5 = new int[3][2]; System.out.println(arr5);// [[I@27c170f0 System.out.println(arr5[1]);// [I@5451c3a8 System.out.println(arr5[1][1]);// 0 String[][] arr6 = new String[3][2]; System.out.println(arr6);// [[Ljava.lang.String;@2626b418 System.out.println(arr6[1]);// [Ljava.lang.String;@5a07e868 System.out.println(arr6[1][1]);// null String[][] arr7 = new String[3][]; System.out.println(arr7[1]);// null，因为内层数组未初始化 System.out.println(arr7[1][1]);// NullPointerException&#125; 动态初始化方式一，初始化时直接规定了内层一维数组的长度，动态初始化方式二，可以在使用过程中根据需要另行初始化内层一维数组的长度。利用动态初始化方式二时，必须要先初始化内层一维数组才能对其使用，否则报空指针异常。 int[][] arr = new int[][3]; 的方式是非法的。 注意特殊写法情况：int[] x,y[];// x是一维数组，y是二维数组。 java 中多维数组不必都是规则矩阵形式。 数组元素的默认初始化值：针对形如 int[][] arr = new int[4][3]; 的初始化方式，外层元素的初始化值为地址值，内层元素的初始化值与一维数组初始化情况相同；针对形如 int[][] arr = new int[4][]; 的初始化方式，外层元素的初始化值为 null，内层元素没有初始化，不能调用。 二维数组内存解析： 杨辉三角： 使用二维数组打印一个 10 行杨辉三角。 提示：1. 第一行有 1 个元素，第 n 行有 n 个元素；2. 每一行的第一个元素和最后一个元素都是 1；3. 从第三行开始，对于非第一个元素和最后一个元素的元素，有：yanghui[i][j] = yanghui[i-1][j-1] + yanghui[i-1][j];。 1234567891011121314151617181920212223public static void main(String[] args) &#123; // 1.声明二维数组并初始化 int[][] arrs = new int[10][]; for (int i = 0; i &lt; arrs.length; i++) &#123; System.out.print(&quot;[&quot; + i + &quot;]\\t&quot;); // 2.初始化内层数组，并给内层数组的首末元素赋值 arrs[i] = new int[i + 1]; arrs[i][0] = 1; arrs[i][arrs[i].length - 1] = 1; for (int j = 0; j &lt; arrs[i].length; j++) &#123; // 3.给从第三行开始内层数组的非首末元素赋值 if (i &gt;= 2 &amp;&amp; j &gt; 0 &amp;&amp; j &lt; arrs[i].length - 1) &#123; arrs[i][j] = arrs[i - 1][j - 1] + arrs[i - 1][j]; &#125; System.out.print(arrs[i][j] + &quot;\\t&quot;); &#125; System.out.println(); &#125; System.out.print(&quot;\\t&quot;); for (int i = 0; i &lt; arrs.length; i++) &#123; System.out.print(&quot;[&quot; + i + &quot;]\\t&quot;); &#125;&#125; 数组中涉及到的常见算法数组元素的赋值 (杨辉三角、回形数等) 创建一个长度为 6 的 int 型数组，要求数组元素的值都在 1 - 30 之间，且是随机赋值。同时，要求元素的值各不相同。 12345678910111213141516171819public static void main(String[] args) &#123; int[] arr = new int[6]; for (int i = 0; i &lt; arr.length; i++) &#123; // 获取一个1-30之间的随机数 arr[i] = (int) (Math.random() * 30 + 1); // 判断是否有相同的值 for (int item : arr) &#123; if (item == arr[i]) &#123; i--; break; &#125; &#125; &#125; // 遍历数组 for (int value : arr) &#123; System.out.println(value); &#125;&#125; 回形数 从键盘输入一个 1 - 20 的整数，然后以该数字为矩阵的大小，把 1，2，3 … n*n 的数字按照顺时针螺旋的形式填入其中。例如： 输入数字 2，则程序输出： 输入数字 3，则程序输出： 输入数字 4， 则程序输出： 方式一： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;输入一个数字&quot;); int len = scanner.nextInt(); int[][] arr = new int[len][len]; int s = len * len; /* * k = 1：向右，k = 2：向下，k = 3：向左，k = 4：向上 */ int k = 1; int i = 0, j = 0; for (int m = 1; m &lt;= s; m++) &#123; if (k == 1) &#123; if (j &lt; len &amp;&amp; arr[i][j] == 0) &#123; arr[i][j++] = m; &#125; else &#123; k = 2; i++; j--; m--; &#125; &#125; else if (k == 2) &#123; if (i &lt; len &amp;&amp; arr[i][j] == 0) &#123; arr[i++][j] = m; &#125; else &#123; k = 3; i--; j--; m--; &#125; &#125; else if (k == 3) &#123; if (j &gt;= 0 &amp;&amp; arr[i][j] == 0) &#123; arr[i][j--] = m; &#125; else &#123; k = 4; i--; j++; m--; &#125; &#125; else if (k == 4) &#123; if (i &gt;= 0 &amp;&amp; arr[i][j] == 0) &#123; arr[i--][j] = m; &#125; else &#123; k = 1; i++; j++; m--; &#125; &#125; &#125; // 遍历数组 for (int m = 0; m &lt; arr.length; m++) &#123; for (int n = 0; n &lt; arr[m].length; n++) &#123; System.out.print(arr[m][n] + &quot;\\t&quot;); &#125; System.out.println(); &#125;&#125; 方式二： 12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.println(&quot;输入一个数字&quot;); int n = scanner.nextInt(); int[][] arr = new int[n][n]; int count = 0;// 要显示的数据 int maxX = n - 1;// x轴的最大下标 int maxY = n - 1;// Y轴的最大下标 int minX = 0;// x轴的最小下标 int minY = 0;// Y轴的最小下标 while (minX &lt;= maxX) &#123; // 向右 for (int x = minX; x &lt;= maxX; x++) &#123; arr[minY][x] = ++count; &#125; minY++; // 向下 for (int y = minY; y &lt;= maxY; y++) &#123; arr[y][maxX] = ++count; &#125; maxX--; // 向左 for (int x = maxX; x &gt;= minX; x--) &#123; arr[maxY][x] = ++count; &#125; maxY--; // 向上 for (int y = maxY; y &gt;= minY; y--) &#123; arr[y][minX] = ++count; &#125; minX++; &#125; // 遍历数组 for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = 0; j &lt; arr.length; j++) &#123; String space = (arr[i][j] + &quot;&quot;).length() == 1 ? &quot;0&quot; : &quot;&quot;; System.out.print(space + arr[i][j] + &quot; &quot;); &#125; System.out.println(); &#125;&#125; 求数值型数组中元素的最大值、最小值、平均数、总和等 定义一个 int 型的一维数组，包含 10 个元素，分别赋一些随机整数，然后求出所有元素的最大值，最小值，和值，平均值，并输出出来 。要求：所有随机数都是两位数。 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; // 初始化及赋值 int[] arr = new int[10]; int length = arr.length; for (int i = 0; i &lt; length; i++) &#123; int value = (int) (Math.random() * 90 + 10); arr[i] = value; &#125; // 遍历 for (int value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 计算 int max = arr[0]; int min = arr[0]; int sum = 0; double average = 0; for (int value : arr) &#123; max = max &lt; value ? value : max; min = min &gt; value ? value : min; sum += value; &#125; average = sum / (length * 1.0); System.out.println(&quot;最大值：&quot; + max); System.out.println(&quot;最小值：&quot; + min); System.out.println(&quot;和值：&quot; + sum); System.out.println(&quot;平均值：&quot; + average);&#125; 数组的复制、反转、查找 (线性查找、二分法查找) 复制 虚假的复制： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) &#123; // 声明arr1和arr2 int[] arr1, arr2; arr1 = new int[]&#123;2, 3, 5, 7, 11, 13, 17, 19&#125;; // 遍历arr1 for (int value : arr1) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 赋值arr2变量等于arr1 // 不能称作数组的复制，实际上是把arr1指向的地址(以及其他一些信息)赋给了arr2，堆空间中只有一个数组对象 arr2 = arr1; // 遍历arr2 for (int value : arr2) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 更改arr2 for (int i = 0; i &lt; arr1.length; i++) &#123; if (i % 2 == 0) &#123; arr2[i] = i; continue; &#125; arr2[i] = arr1[i]; &#125; // 遍历arr2 for (int value : arr2) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 遍历arr1 for (int value : arr1) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println();&#125;输出结果：2 3 5 7 11 13 17 19 2 3 5 7 11 13 17 19 0 3 2 7 4 13 6 19 0 3 2 7 4 13 6 19 arr1 和 arr2 地址值相同，都指向了堆空间中唯一的一个数组实体： 真实的复制： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public static void main(String[] args) &#123; // 声明arr1和arr2 int[] arr1, arr2; arr1 = new int[]&#123;2, 3, 5, 7, 11, 13, 17, 19&#125;; // 遍历arr1 for (int value : arr1) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 数组的复制 arr2 = new int[arr1.length]; for (int i = 0; i &lt; arr1.length; i++) &#123; arr2[i] = arr1[i]; &#125; // 遍历arr2 for (int value : arr2) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 更改arr2 for (int i = 0; i &lt; arr1.length; i++) &#123; if (i % 2 == 0) &#123; arr2[i] = i; continue; &#125; arr2[i] = arr1[i]; &#125; // 遍历arr2 for (int value : arr2) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 遍历arr1 for (int value : arr1) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println();&#125;输出结果：2 3 5 7 11 13 17 19 2 3 5 7 11 13 17 19 0 3 2 7 4 13 6 19 2 3 5 7 11 13 17 19 arr1 和 arr2 地址值不同，指向了堆空间中两个不同的数组实体： 反转 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; String[] arr = &#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;&#125;; // 遍历arr for (String value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 反转，方式一 for (int i = 0; i &lt; arr.length / 2; i++) &#123; String temp = arr[i]; arr[i] = arr[arr.length - 1 - i]; arr[arr.length - 1 - i] = temp; &#125; for (String value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 反转，方式二 for (int i = 0, j = arr.length - 1; i &lt; j; i++, j--) &#123; String temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; for (String value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println();&#125; 线性查找 12345678910111213141516171819202122232425public static void main(String[] args) &#123; String[] arr = &#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;&#125;; // 遍历arr for (String value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); String dest = &quot;D&quot;; boolean isFlag = true; for (int i = 0; i &lt; arr.length; i++) &#123; if (dest.equals(arr[i])) &#123; System.out.println(&quot;找到了指定的元素：&quot; + dest + &quot;，位置为：&quot; + i); isFlag = false; break; &#125; &#125; if (isFlag) &#123; System.out.println(&quot;没找到指定的元素：&quot; + dest); &#125;&#125;输出结果：A B C D E F G 找到了指定的元素：D，位置为：3 二分法查找，前提：所要查找的数组必须有序。 12345678910111213141516171819202122232425262728293031323334public static void main(String[] args) &#123; int[] arr = &#123;2, 5, 7, 8, 10, 15, 18, 20, 22, 25, 28&#125;; // 遍历arr for (int value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); int dest = 10; // 初始的首索引 int head = 0; // 初始的末索引 int end = arr.length - 1; boolean isFlag = true; while (head &lt;= end) &#123; int middle = (head + end) / 2; if (dest == arr[middle]) &#123; System.out.println(&quot;找到了指定的元素：&quot; + dest + &quot;，位置为：&quot; + middle); isFlag = false; break; &#125; else if (dest &lt; arr[middle]) &#123; end = middle - 1; &#125; else &#123;// dest2 &gt; arr2[middle] head = middle + 1; &#125; &#125; if (isFlag) &#123; System.out.println(&quot;没找到指定的元素：&quot; + dest); &#125;&#125;输出结果：2 5 7 8 10 15 18 20 22 25 28 找到了指定的元素：10，位置为：4 数组元素的排序算法排序：假设含有 n 个记录的序列为 {R1, R2, …, Rn}，其相应的关键字序列为 {K1, K2, …, Kn}。将这些记录重新排序为 {Ri1, Ri2, …, Rin}，使得相应的关键字值满足条 Ki1&lt;= Ki2 &lt;= … &lt;= Kin，这样的一种操作称为排序。通常来说，排序的目的是快速查找。 衡量排序算法的优劣： 时间复杂度：分析关键字的比较次数和记录的移动次数。 空间复杂度：分析排序算法中需要多少辅助内存。 稳定性：若两个记录 A 和 B 的关键字值相等，但排序后 A、B 的先后次序保持不变，则称这种排序算法是稳定的。 排序算法分类：内部排序和外部排序。 内部排序：整个排序过程不需要借助于外部存储器 (如磁盘等)，所有排序操作都在内存中完成。 外部排序：参与排序的数据非常多，数据量非常大，计算机无法把整个排序过程放在内存中完成，必须借助于外部存储器 (如磁盘等)。外部排序最常见的是多路归并排序。可以认为外部排序是由多次内部排序组成。 十大内部排序算法： 排序算法性能对比： 从平均时间而言：快速排序最佳。但在最坏情况下时间性能不如堆排序和归并排序。 从算法简单性看：由于直接选择排序、直接插入排序和冒泡排序的算法比较简单，将其认为是简单算法。对于 Shell 排序、堆排序、快速排序和归并排序算法，其算法比较复杂，认为是复杂排序。 从稳定性看：直接插入排序、冒泡排序和归并排序时稳定的；而直接选择排序、快速排序、 Shell 排序和堆排序是不稳定排序。 从待排序的记录数 n 的大小看，n 较小时，宜采用简单排序；而 n 较大时宜采用改进排序。 排序算法的选择： 若 n 较小 (如 n ≤50)，可采用直接插入或直接选择排序。当记录规模较小时，直接插入排序较好；否则因为直接选择移动的记录数少于直接插入，应选直接选择排序为宜。 若文件初始状态基本有序 (指正序)，则应选用直接插入、 冒泡或随机的快速排序为宜。 若 n 较大，则应采用时间复杂度为 O(nlgn) 的排序方法： 快速排序、 堆排序或归并排序。 冒泡排序冒泡排序的原理非常简单，它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。 排序思想： 比较相邻的元素。如果第一个比第二个大 (升序)，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较为止。 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; int[] arr = new int[]&#123;43, 32, 76, -98, 0, 64, 33, -21, 32, 99&#125;; // 遍历arr for (int value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println(); // 冒泡排序 for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125; // 遍历arr for (int value : arr) &#123; System.out.print(value + &quot;\\t&quot;); &#125; System.out.println();&#125;输出结果：43 32 76 -98 0 64 33 -21 32 99-98 -21 0 32 32 33 43 64 76 99 快速排序快速排序通常明显比同为 O(nlogn) 的其他算法更快，因此常被采用，而且快速排序采用了分治法的思想，所以在很多笔试面试中能经常看到快速排序的影子，可见掌握快速排序的重要性。 快速排序 (Quick Sort) 由图灵奖获得者 Tony Hoare 发明，被列为 20 世纪十大算法之一，是迄今为止所有内排序算法中速度最快的一种。 快速排序属于冒泡排序的升级版，交换排序的一种。快速排序的时间复杂度为 O(nlog(n))。 排序思想： 从数列中挑出一个元素，称为”基准” (pivot)。 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面 (相同的数可以到任一边)。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区 (partition) 操作。 递归地 (recursive) 把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代 (iteration) 中，它至少会把一个元素摆到它最后的位置去。 Arrays 工具类的使用java.util.Arrays 类为操作数组的工具类，包含了用来操作数组 (比如排序和搜索) 的各种方法。常用的方法有： 123456789101112131415161718192021222324252627282930313233343536public static void main(String[] args) &#123; // 1.boolean equals(int[] a,int[] b)：判断两个数组是否相等 int[] arr1 = new int[]&#123;1, 2, 3, 4&#125;; int[] arr2 = new int[]&#123;1, 3, 2, 4&#125;; boolean isEquals = Arrays.equals(arr1, arr2); System.out.println(&quot;arr1和arr2是否相等：&quot; + isEquals); // 2.String toString(int[] a)：遍历数组信息 System.out.println(&quot;arr1：&quot; + Arrays.toString(arr1)); // 3.void fill(int[] a,int val)：将指定值填充到数 组之中 Arrays.fill(arr1, 10); System.out.println(&quot;arr1填充后：&quot; + Arrays.toString(arr1)); // 4.void sort(int[] a)：对数组进行排序，底层使用的是快速排序 System.out.println(&quot;arr2排序前：&quot; + Arrays.toString(arr2)); Arrays.sort(arr2); System.out.println(&quot;arr2排序后：&quot; + Arrays.toString(arr2)); // 5.int binarySearch(int[] a,int key)：对排序后的数组进行二分法检索指定的值 int[] arr3 = new int[]&#123;-98, -34, 2, 34, 54, 66, 79, 105, 210, 333&#125;; int dest = 211; int index = Arrays.binarySearch(arr3, dest); if (index &gt;= 0) &#123; System.out.println(dest + &quot;在数组中的位置为：&quot; + index); &#125; else &#123; System.out.println(dest + &quot;在数组中未找到：&quot; + index); &#125;&#125;输出结果：arr1和arr2是否相等：falsearr1：[1, 2, 3, 4]arr1填充后：[10, 10, 10, 10]arr2排序前：[1, 3, 2, 4]arr2排序后：[1, 2, 3, 4]211在数组中未找到：-10 数组中的常见异常12345678910111213141516171819public static void main(String[] args) &#123; // ArrayIndexOutOfBoundsException int[] arr = new int[]&#123;7, 10&#125;; System.out.println(arr[2]);// 数组脚标越界 System.out.println(arr[-1]);// 访问了数组中不存在的脚标 // NullPointerException：空指针异常，arr引用没有指向实体，却被操作实体中的元素 // 情形一 int[] arr2 = null; System.out.println(arr2[0]); // 情形二 int[][] arr3 = new int[4][]; System.out.println(arr3[0]);// null System.out.println(arr3[0][0]);// NullPointerException // 情形三 String[] arr4 = new String[]&#123;&quot;AA&quot;, &quot;BB&quot;, &quot;CC&quot;&#125;; arr4[0] = null; System.out.println(arr4[0].toString());// 对null调用了方法&#125; ArrayIndexOutOfBoundsException 和 NullPointerException，在编译时，不报错！！ 面向对象三条主线： java 类及类的成员：属性、方法、构造器、代码块、内部类。 面向对象的三大特征：封装性、继承性、多态性、(抽象性)。 其他关键字：this、super、static、final、abstract、interface、package、import等。 面向过程 (POP) 与面向对象 (OOP)： 二者都是一种思想，面向对象是相对于面向过程而言的。 面向过程，强调的是功能行为，以函数为最小单位，考虑怎么做。面向对象，将功能封装进对象，强调具备了功能的对象，以类/对象为最小单位，考虑谁来做。 面向对象更加强调运用人类在日常的思维逻辑中采用的思想方法与原则，如抽象、分类、继承、聚合、多态等。 例如，人把大象装进冰箱： 面向对象的三大特征： 封装 (Encapsulation) 继承 (Inheritance) 多态 (Polymorphism) 面向对象的思想概述： 程序员从面向过程的执行者转化成了面向对象的指挥者。 面向对象分析方法分析问题的思路和步骤： 根据问题需要，选择问题所针对的现实世界中的实体。 从实体中寻找解决问题相关的属性和功能，这些属性和功能就形成了概念世界中的类。 把抽象的实体用计算机语言进行描述，形成计算机世界中类的定义。即借助某种程序语言，把类构造成计算机能够识别和处理的数据结构。 将类实例化成计算机世界中的对象。对象是计算机世界中解决问题的最终工具。 java基本元素：类和对象类 (Class) 和对象 (Object) 是面向对象的核心概念。 类是对一类事物的描述，是抽象的、概念上的定义。 对象是实际存在的该类事物的每个个体，因而也称为实例 (instance)。 常见的类的成员有： 属性：对应类中的成员变量。 方法：对应类中的成员方法。 类的成员构成 version 1.0： 类的成员构成 version 2.0： 类的语法格式： 创建 java 自定义类步骤： 定义类：考虑修饰符、类名。 编写类的属性：考虑修饰符、属性类型、属性名、初始化值。 编写类的方法：考虑修饰符、返回值类型、方法名、形参等。 类的访问机制： 在一个类中的访问机制：类中的方法可以直接访问类中的成员变量。例外：static 方法访问非 static 属性，编译不通过。 在不同类中的访问机制： 先创建要访问类的对象， 再用对象访问类中定义的成员。 对象的创建和使用： 创建对象语法： 使用 对象名.对象成员 的方式访问对象成员，包括属性和方法。 如果创建了一个类的多个对象，则每个对象都独立的拥有一套类的属性 (非 static 的)，即：修改一个对象的属性 a，不影响另外一个对象属性 a 的值。 对象的产生： 对象的使用： 对象的生命周期： 对象的内存解析： 例如，下面一段代码的内存图如下： 匿名对象： 不定义对象的句柄，而直接调用这个对象的方法，这样的对象叫做匿名对象。如：new Person().shout();。 使用情况：如果对一个对象只需要进行一次方法调用，那么就可以使用匿名对象。我们经常将匿名对象作为实参传递给一个方法调用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 二、创建类的对象 = 类的实例化 */public class PersonTest &#123; public static void main(String[] args) &#123; // 1.创建Person类的对象 Person person = new Person(); // 2.调用对象的结构：属性和方法 // 2-1.调用属性：&quot;对象.属性&quot; person.name = &quot;Tom&quot;; person.isMale = true; System.out.println(&quot;年龄：&quot; + person.age);// 1 // 2-2.调用方法：&quot;对象.方法&quot; person.eat();// 人可以吃饭 person.sleep();// 人可以睡觉 person.talk(&quot;Chinese&quot;);// 人可以说话，语言是：Chinese // 3.创建一个新的Person类的对象 Person person2 = new Person(); System.out.println(person2.name);// null，非Tom // 4.将person变量保存的地址值赋值给person3，此时，二者指向堆空间中的同一个对象实体 // 修改person和person3，效果相同 Person person3 = person; System.out.println(person3.name);// Tom person3.age = 10; System.out.println(person.age);// 10 &#125;&#125;/** * 一、类的设计，其实就是类的成员的设计： * 属性 = 成员变量 = Field = 域、字段 * 方法 = 成员方法 = 函数 = Method */class Person &#123; // 属性 String name; int age = 1; boolean isMale; // 方法 public void eat() &#123; System.out.println(&quot;人可以吃饭&quot;); &#125; public void sleep() &#123; System.out.println(&quot;人可以睡觉&quot;); &#125; public void talk(String language) &#123; System.out.println(&quot;人可以说话，语言是：&quot; + language); &#125;&#125; 类的成员之一：属性 (field)语法格式： 常用的权限修饰符有：private、缺省、protected、public。其他修饰符：static、final。 数据类型：任何基本数据类型 (如 int、boolean 等) 或任何引用数据类型。 属性名：属于标识符，符合命名规则和规范即可。 属性 (成员变量) 与局部变量的区别： 成员变量的默认初始化值：当一个对象被创建时，会对其中各种类型的成员变量自动进行初始化赋值。除了基本数据类型之外的变量类型都是引用类型。 局部变量的默认初始化值：局部变量声明后，没有默认初始化值，必须显式赋值，方可使用。特别的，形参在调用时，赋值即可。 成员变量 vs 局部变量的内存位置： 属性赋值的方式和先后顺序： 赋值的方式： ① 默认初始化 ② 显示初始化 ③ 构造器中初始化 ④ 通过 “对象.属性” 或 “对象.方法” 的方式赋值 ⑤ 在代码块中初始化 赋值的先后顺序：① - ② / ⑤ - ③ - ④ ② 和 ⑤，谁定义在前，谁先赋值： 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; Order order = new Order(); System.out.println(order.orderId);// 4 &#125;&#125;class Order &#123; int orderId = 3; &#123; orderId = 4; &#125;&#125; 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; Order order = new Order(); System.out.println(order.orderId);// 3 &#125;&#125;class Order &#123; &#123; orderId = 4; &#125; int orderId = 3;&#125; 总结：程序中成员变量赋值的执行顺序 类的成员之二：方法 (method)什么是方法 (method 、函数)： 方法是类或对象行为特征的抽象，用来完成某个功能操作。在某些语言中也称为函数或过程。 将功能封装为方法的目的是，可以实现代码重用，简化代码。 java 里的方法不能独立存在，所有的方法必须定义在类里。 声明格式： 权限修饰符：public，缺省，private，protected 等。 返回值类型： 没有返回值：使用void。 有返回值：在方法声明时，必须指定返回值的类型，同时，方法体中需要使用 return 关键字返回指定类型的变量或常量。 方法名 ：属于标识符，命名时遵循标识符命名规则和规范，能够见名知意。 形参列表：可以包含零个，一个或多个参数。多个参数时，中间用 “,” 隔开。 方法体程序代码：方法功能的具体实现。 返回值：方法在执行完毕后返还给调用它的程序的数据。 方法的分类：按照是否有形参及返回值。 方法的调用： 方法通过方法名被调用，且只有被调用才会执行。 方法调用的过程： 方法被调用一次，就会执行一次。 没有具体返回值的情况，返回值类型用关键字 void 表示，此时方法体中可以不必使用 return 语句。如果使用，表示用来结束方法。 定义方法时，方法的结果应该返回给调用者，交由调用者处理。 方法中可以调用当前类的属性或方法，不可以在方法内部定义方法。 方法的重载 (overload) 概念：在同一个类中，允许存在一个以上的同名方法，只要它们的参数个数或者参数类型不同即可。 特点：与方法的权限修饰符、返回值类型、形参变量名、方法体都无关，只看参数列表，且参数列表 (参数个数或参数类型) 必须不同。调用时，根据方法参数列表的不同来区别。 如果方法一不存在，main 方法依然正常执行，此时涉及到的是自动类型转换： 12345678910111213// 方法一public static int getSum(int m, int n) &#123; return m + n;&#125;// 方法二public static double getSum(double m, double n) &#123; return m + n;&#125;public static void main(String[] args) &#123; System.out.println(getSum(1, 2));&#125; 可变个数的形参： JavaSE 5.0 中提供了 Varargs (variable number of arguments) 机制，允许直接定义能和多个实参相匹配的形参。从而，可以用一种更简单的方式，来传递个数可变的实参。 声明格式：方法名(参数的类型名 … 参数名) 可变参数：方法参数部分指定类型的参数个数是可变多个 — 0个，1个或多个。 可变个数形参的方法与同名的方法之间，彼此构成重载。 可变参数方法的使用与方法参数部分使用数组是一致的，二者不共存。如下所示，方法二与方法三是相同的，不共存： 123456789// 方法二public static void show(int... m) &#123; System.out.println(Arrays.toString(m));// m参数等同于数组，与数组的使用方法相同。&#125;// 方法三public static void show(int[] m) &#123; System.out.println(m);&#125; 方法的参数部分有可变形参，需要放在形参声明的最后。 123456789// 合法public static void show(String str, int... m) &#123; System.out.println(Arrays.toString(m));&#125;// 不合法public static void show(int... m, String str) &#123; System.out.println(Arrays.toString(m));&#125; 在一个方法的形参位置，最多只能声明一个可变个数形参。 方法参数的值传递机制 方法，必须由其所在类或对象调用才有意义。若方法含有参数： 形参：方法声明时的参数。 实参：方法调用时实际传给形参的数据。 java 的实参值如何传入方法呢？ java 里方法的参数传递方式只有一种：值传递。 即将实际参数值的副本 (复制品) 传入方法内，而参数本身不受影响。 形参是基本数据类型：将实参基本数据类型变量的 “数据值” 传递给形参。 形参是引用数据类型：将实参引用数据类型变量的 “地址值” 传递给形参。 形参时基本数据类型与引用数据类型之间的区别: 1234567891011121314151617181920212223242526272829public class ValueTransferTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;***************基本数据类型***************&quot;); int m = 10; int n = m; System.out.println(&quot;m = &quot; + m + &quot;, n = &quot; + n); n = 20; System.out.println(&quot;m = &quot; + m + &quot;, n = &quot; + n); System.out.println(&quot;***************引用数据类型***************&quot;); Order o1 = new Order(); o1.orderId = 1001; Order o2 = o1;// 赋值后, o1和o2的地址值相同, 都指向了堆空间中的同一个实体 System.out.println(&quot;o1.orderId = &quot; + o1.orderId + &quot;, o2.orderId = &quot; + o2.orderId); o2.orderId = 1002; System.out.println(&quot;o1.orderId = &quot; + o1.orderId + &quot;, o2.orderId = &quot; + o2.orderId); &#125;&#125;class Order &#123; int orderId;&#125;输出结果：***************基本数据类型***************m = 10, n = 10m = 10, n = 20***************引用数据类型***************o1.orderId = 1001, o2.orderId = 1001o1.orderId = 1002, o2.orderId = 1002 对于基本数据类型，两个不同方法内的局部变量，互不影响，不因变量名相同而改变，因为是将实参基本数据类型变量的 “数据值” 传递给形参： 12345678910111213141516171819202122232425262728293031323334public class ValueTransferTest &#123; public void swap(int m, int n) &#123; System.out.println(&quot;swap方法中, 交换之前: m = &quot; + m + &quot;, n = &quot; + n); int temp = m; m = n; n = temp; System.out.println(&quot;swap方法中, 交换之后: m = &quot; + m + &quot;, n = &quot; + n); &#125; public static void main(String[] args) &#123; int m = 10; int n = 20; System.out.println(&quot;main方法中, 交换之前: m = &quot; + m + &quot;, n = &quot; + n); // 能够交换m和n的值 int temp = m; m = n; n = temp; System.out.println(&quot;main方法中, 交换之后: m = &quot; + m + &quot;, n = &quot; + n); // 不能够交换m和n的值 ValueTransferTest valueTransferTest = new ValueTransferTest(); System.out.println(&quot;main方法中, 调用swap方法之前: m = &quot; + m + &quot;, n = &quot; + n); valueTransferTest.swap(m, n);// // swap方法调用完成后, 该方法内的局部变量temp, 形参m和n从栈内存中弹出回收 System.out.println(&quot;main方法中, 调用swap方法之后: m = &quot; + m + &quot;, n = &quot; + n); &#125;&#125;输出结果：main方法中, 交换之前: m = 10, n = 20main方法中, 交换之后: m = 20, n = 10main方法中, 调用swap方法之前: m = 20, n = 10swap方法中, 交换之前: m = 20, n = 10swap方法中, 交换之后: m = 10, n = 20main方法中, 调用swap方法之后: m = 20, n = 10 内存解析图参考： 对于引用数据类型，两个不同方法的局部变量，会互相影响，因为是将实参引用数据类型变量的 “地址值” 传递给形参，二者指向的是堆内存中的同一个对象： 12345678910111213141516171819202122232425262728293031323334353637383940public class ValueTransferTest &#123; public void swap(Data data) &#123; System.out.println(&quot;swap方法中, 交换之前: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); int temp = data.m; data.m = data.n; data.n = temp; System.out.println(&quot;swap方法中, 交换之后: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); &#125; public static void main(String[] args) &#123; Data data = new Data(); data.m = 10; data.n = 20; System.out.println(&quot;main方法中, 交换之前: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); // 能够交换m和n的值 int temp = data.m; data.m = data.n; data.n = temp; System.out.println(&quot;main方法中, 交换之后: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); // 能够交换m和n的值 ValueTransferTest valueTransferTest = new ValueTransferTest(); System.out.println(&quot;main方法中, 调用swap方法之前: data.m = &quot; + data.m + &quot;, data. = &quot; + data.n); valueTransferTest.swap(data);// swap方法调用完成后, 该方法内的局部变量temp和形参data从栈内存中弹出回收 System.out.println(&quot;main方法中, 调用swap方法之后: data.m = &quot; + data.m + &quot;, data.n = &quot; + data.n); &#125;&#125;class Data &#123; int m; int n;java&#125;输出结果：main方法中, 交换之前: data.m = 10, data.n = 20main方法中, 交换之后: data.m = 20, data.n = 10main方法中, 调用swap方法之前: data.m = 20, data. = 10swap方法中, 交换之前: data.m = 20, data.n = 10swap方法中, 交换之后: data.m = 10, data.n = 20main方法中, 调用swap方法之后: data.m = 10, data.n = 20 内存解析图参考： 实例一： 1234567891011121314151617181920212223242526public class ValueTransferTest &#123; public void first() &#123; int i = 5; Value v = new Value(); v.i = 25; second(v, i); System.out.println(v.i);// 20 &#125; public void second(Value v, int i) &#123; i = 0; v.i = 20; Value val = new Value(); v = val; System.out.println(v.i + &quot; &quot; + i);// 15 0 &#125; public static void main(String[] args) &#123; ValueTransferTest test = new ValueTransferTest(); test.first(); &#125;&#125;class Value &#123; int i = 15;&#125; 实例二： 方法一： 123456789101112131415public class Test &#123; public static void method(int a, int b) &#123; System.out.println(&quot;a = &quot; + a * 10); System.out.println(&quot;b = &quot; + b * 20); System.exit(0); &#125; public static void main(String[] args) &#123; int a = 10; int b = 10; method(a, b); System.out.println(&quot;a = &quot; + a); System.out.println(&quot;b = &quot; + b); &#125;&#125; 方法二：重写 PrintStream 的 println 方法。 123456789101112131415161718192021222324public class Test &#123; public static void method(int a, int b) &#123; PrintStream printStream = new PrintStream(System.out) &#123; @Override public void println(String x) &#123; if (&quot;a = 10&quot;.equals(x)) &#123; x = &quot;a = 100&quot;; &#125; else if (&quot;b = 10&quot;.equals(x)) &#123; x = &quot;b = 200&quot;; &#125; super.println(x); &#125; &#125;; System.setOut(printStream); &#125; public static void main(String[] args) &#123; int a = 10; int b = 10; method(a, b); System.out.println(&quot;a = &quot; + a); System.out.println(&quot;b = &quot; + b); &#125;&#125; 实例三： 定义一个 int 型的数组：int[] arr = new int[]&#123;12,3,3,34,56,77,432&#125;;，让数组的每个位置上的值去除以首位置的元素，得到的结果，作为该位置上的新值，然后遍历新的数组。 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;12, 3, 3, 34, 56, 77, 432&#125;; System.out.println(&quot;计算前: &quot; + Arrays.toString(arr)); // 正确写法一 int temp = arr[0]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = arr[i] / temp; &#125; // 正确写法二 for (int i = arr.length - 1; i &gt;= 0; i--) &#123; arr[i] = arr[i] / arr[0]; &#125; // 错误写法 /*for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = arr[i] / arr[0]; &#125;*/ System.out.println(&quot;计算后: &quot; + Arrays.toString(arr)); &#125;&#125; 实例四： 123456789101112public class Test &#123; public static void main(String[] args) &#123; int[] arr = new int[]&#123;1, 2, 3&#125;; System.out.println(arr);// 地址值 char[] arr1 = new char[]&#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;; System.out.println(arr1);// 传入char数组时，println方法体内是遍历这个数组 &#125;&#125;输出结果：[I@78308db1abc 递归方法 (recursion)： 一个方法体内调用它自身。方法递归包含了一种隐式的循环，它会重复执行某段代码，但这种重复执行无须循环控制。 递归一定要向已知方向递归，否则这种递归就变成了无穷递归，类似于死循环 实例一： 1234567891011121314151617181920212223242526272829303132public class PassObject &#123; // 1-n之间所有自然数的和 public static int getSum(int n) &#123; if (n == 1) &#123; return 1; &#125; else &#123; return n + getSum(n - 1); &#125; &#125; // 1-n之间所有自然数的乘积 public static long getProduct(int n) &#123; if (n == 1) &#123; return 1; &#125; else &#123; return n * getProduct(n - 1); &#125; &#125; public static void main(String[] args) &#123; // 方式一：循环 int sum = 0; for (int i = 1; i &lt;= 100; i++) &#123; sum += i; &#125; System.out.println(&quot;1-100之间自然数的和: &quot; + sum); // 方式二：递归 System.out.println(&quot;1-100之间自然数的和: &quot; + getSum(100)); System.out.println(&quot;1-100之间自然数的积: &quot; + getProduct(5)); &#125;&#125; 实例二： 123456789101112131415161718192021/** * 已知有一个数列：f(0) = 1, f(1) = 4, f(n+2) = 2 * f(n+1) + f(n), 其中n是大于0的整数, 求f(10)的值。 */public class PassObject &#123; public static int f(int n) &#123; if (n == 0) &#123; return 1; &#125; else if (n == 1) &#123; return 4; &#125; else &#123; return 2 * f(n - 1) + f(n - 2); &#125; &#125; public static void main(String[] args) &#123; int f = f(10); System.out.println(f); &#125;&#125;输出结果：10497 实例三： 123456789101112131415161718192021/** * 已知一个数列: f(20) = 1, f(21) = 4, f(n+2) = 2 * f(n+1) + f(n), 其中n是大于0的整数, 求f(10)的值。 */public class PassObject &#123; public static int f(int n) &#123; if (n == 20) &#123; return 1; &#125; else if (n == 21) &#123; return 4; &#125; else &#123; return f(n + 2) - 2 * f(n + 1); &#125; &#125; public static void main(String[] args) &#123; int f = f(10); System.out.println(f); &#125;&#125;输出结果：-3771 实例四： 12345678910111213141516171819202122232425262728293031323334/** * 斐波那契数列: 1 1 2 3 5 8 13 21 34 55 ... * 规律: 一个数等于前两个数之和 * 要求：计算斐波那契数列(Fibonacci)的第n个值，并将整个数列打印出来 */public class PassObject &#123; public static int f(int n) &#123; if (n &lt;= 0 || n &gt;= 30) &#123; return 0; &#125; if (n == 1) &#123; return 1; &#125; else if (n == 2) &#123; return 1; &#125; else &#123; return f(n - 1) + f(n - 2); &#125; &#125; public static void main(String[] args) &#123; int[] arr = new int[5]; int sum = 0; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = f(i + 1); sum += arr[i]; &#125; System.out.println(Arrays.toString(arr)); System.out.println(&quot;和: &quot; + sum); &#125;&#125;输出结果：[1, 1, 2, 3, 5]和: 12 实例五： 1234567891011121314151617181920212223242526272829303132public class PassObject &#123; private static int count = 0; public static int recursion(int k) &#123; count++; System.out.println(&quot;count1: &quot; + count + &quot;, k: &quot; + k); if (k &lt;= 0) &#123; return 0; &#125; return recursion(k - 1) + recursion(k - 2); &#125; public static void main(String[] args) &#123; recursion(4); &#125;&#125;输出结果：count1: 1, k: 4count1: 2, k: 3count1: 3, k: 2count1: 4, k: 1count1: 5, k: 0count1: 6, k: -1count1: 7, k: 0count1: 8, k: 1count1: 9, k: 0count1: 10, k: -1count1: 11, k: 2count1: 12, k: 1count1: 13, k: 0count1: 14, k: -1count1: 15, k: 0 递归过程： 遍历过程相当于二叉树的前序遍历。 OOP 特征一：封装和隐藏封装性的设计思想：隐藏对象内部的复杂性，只对外公开简单的接口。便于外界调用，从而提高系统的可扩展性、可维护性。通俗的说，把该隐藏的隐藏起来，该暴露的暴露出来。 程序设计追求 “高内聚，低耦合”： 高内聚：类的内部数据操作细节自己完成，不允许外部干涉。 低耦合：仅对外暴露少量的方法用于使用。 信息的封装和隐藏： java 中通过将对象的属性声明为私有的 (private)，再提供公共的 (public) 方法 — getXxx() 和 setXxx()，来实现对属性的操作，并以此达到下述目的： 隐藏一个类中不需要对外提供的实现细节。 使用者只能通过事先定制好的方法来访问数据，可以方便地加入控制逻辑，限制对属性的不合理操作。 便于修改，增强代码的可维护性。 封装性的体现：属性私有、方法私有、构造器私有 (单例模式) 等。 封装性的体现，需要权限修饰符的配合。 四种权限修饰符 从小到大排列：private、缺省 (什么都不写)、protected、public。 权限修饰符置于类的成员定义前，用来限定对象对该类成员的访问权限： 权限修饰符可以用来修饰类及类的内部结构：属性、方法、构造器、内部类。 对于 class 的权限修饰只可以用 public 和 default (缺省)。 public 类可以在任意地方被访问。 default 类只可以被同一个包内部的类访问。 对于 class 的内部结构，四种权限修饰符都可以使用。 封装性总结：java 提供了 4 种权限修饰符来修饰类及类的内部结构，体现类及类的内部结构在被调用时的可见性的大小。 本类中任意调用： 123456789101112131415161718192021222324252627282930313233343536package cn.xisun.database;public class Order &#123; private int orderPrivate; int orderDefault; protected int orderProtected; public int orderPublic; private void methodPrivate() &#123; orderPrivate = 1; orderDefault = 2; orderProtected = 3; orderPublic = 4; &#125; void methodDefault() &#123; orderPrivate = 1; orderDefault = 2; orderProtected = 3; orderPublic = 4; &#125; protected void methodProtected() &#123; orderPrivate = 1; orderDefault = 2; orderProtected = 3; orderPublic = 4; &#125; public void methodPublic() &#123; orderPrivate = 1; orderDefault = 2; orderProtected = 3; orderPublic = 4; &#125;&#125; 同包中的其他类： 1234567891011121314151617181920package cn.xisun.database;public class OrderTest &#123; public static void main(String[] args) &#123; Order order = new Order(); order.orderDefault = 1; order.orderProtected = 2; order.orderPublic = 3; order.methodDefault(); order.methodProtected(); order.methodPublic(); // 同一个包中的其他类，不可以调用Order类中private的属性和方法 /*order.orderPrivate = 4; order.methodPrivate();*/ &#125;&#125; 不同包中的子类： 1234567891011121314151617181920package cn.xisun.database.postgresql;import cn.xisun.database.Order;public class SubOrder extends Order &#123; public void method() &#123; orderProtected = 1; orderPublic = 2; methodProtected(); methodPublic(); // 不同包的子类中，不可以调用Order类中private和缺省的属性和方法 /*orderPrivate = 3; orderDefault = 4; methodPrivate(); methodDefault();*/ &#125;&#125; 不同包中的其他类 (非子类)： 12345678910111213141516171819202122package cn.xisun.database.postgresql;import cn.xisun.database.Order;public class OtherOrderTest &#123; public static void main(String[] args) &#123; Order order = new Order(); order.orderPublic = 1; order.methodPublic(); // 不同包下的普通类(非子类)，不可以调用Order类中private、缺省和protected的属性和方法 /*order.orderPrivate = 2; order.orderDefault = 3; order.orderProtected = 4; order.methodPrivate(); order.methodDefault(); order.methodProtected();*/ &#125;&#125; 类的成员之三：构造器 (构造方法，constructor)构造器的作用： 创建对象；给对象进行初始化。如：Order o = new Order(); Person p = new Person(“Peter”, 15); 语法格式： 根据参数不同，构造器可以分为如下两类： 隐式无参构造器 (系统默认提供)。 显定义一个或多个构造器 (无参、有参)。 构造器的特征： 构造器具有与类相同的名称，不声明返回值类型，与声明为 void 不同。 java 语言中，每个类都至少有一个构造器。 如果没有显示的定义类的构造器，则系统默认提供一个无参构造器。一旦显式定义了构造器， 则系统不再提供默认构造器。 一般情况下，为了防止一些框架出异常，无论要不要自定义其他构造器，都应该把类的无参构造器显示的定义出来。 构造器的修饰符默认与所属类的修饰符一致，即：public 或 default (缺省)。 构造器不能被 static、final、synchronized、abstract、native 修饰，不能有 return 语句返回值。 一个类中定义的多个构造器，彼此构成重载。 父类的构造器不可被子类继承。 JavaBean JavaBean 是一种 java 语言写成的可重用组件。 所谓 JavaBean，是指符合如下标准的 java 类： 类是公共的。 有一个无参的公共的构造器。 有属性，且有对应的 get、set 方法。 用户可以使用 JavaBean 将功能、处理、值、数据库访问和其他任何可以用 java 代码创造的对象进行打包，并且其他的开发者可以通过内部的 JSP 页面、Servlet、其他 JavaBean、applet 程序或者应用来使用这些对象。用户可以认为 JavaBean 提供了一种随时随地的复制和粘贴的功能，而不用关心任何改变。 UML 类图 关键字：thisthis 关键字的使用： this 可以用来修饰或调用：属性、方法、构造器。 this 修饰属性和方法： this理解为：当前对象或当前正在创建的对象。 在类的方法中，可以使用 “this.属性” 或 “this.方法” 的方式，调用当前属性或方法。 通常情况下，可以省略 “this.”。 特殊情况下，如果方法的形参和类的属性同名，则必须显示的使用 “this.变量” 的方式，表明此变量是属性，而非形参。 在类的构造器中，可以使用 “this.属性” 或 “this.方法” 的方式，调用当前正在创建的对象的属性或方法。 通常情况下，可以省略 “this.”。 特殊情况下，如果构造器的形参和类的属性同名，则必须显示的使用 “this.变量” 的方式，表明此变量是属性，而非形参。 使用 this 访问属性和方法时，如果在本类中未找到，会从父类中查找。 this 调用构造器： 在类的构造器中，可以显示的使用 “this(形参列表)” 的方式，调用本类中的其他构造器。 存在构造器的多重调用时，创建的对象仍然是只有一个，而不是调用一个构造器就创造了一个新的对象，只有最开始被调用的构造器才创造了对象。 构造器中，不能使用 “this(形参列表)” 的方式调用自己。 如果一个类中有 n 个构造器，则最多有 n - 1 个构造器中使用了 “this(形参列表)”。 构造器在彼此调用时，不能形参一个封闭环，如：构造器 A 中调用了构造器 B，则在构造器 B 中不能再调用构造器 A，多构造器调用类推。 规定：”this(形参列表)” 必须声明在当前构造器的首行。 一个构造器内部，最多只能声明一个 “this(形参列表)”，即只能调用一个其他的构造器。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Person &#123; private String name; private int age; // 无参构造器 public Person() &#123; this.eat(); &#125; // 带name的构造器 public Person(String name) &#123; this();// 调用无参构造器 this.name = name; &#125; // 带name和age的构造器 public Person(String name, int age) &#123; this(name);// 调用带name的构造器 this.age = age; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return this.name;// 此处this可以省略 &#125; public void setAge(int age) &#123; this.age = age; &#125; public int getAge() &#123; return this.age;// 此处this可以省略 &#125; public void eat() &#123; System.out.println(&quot;人吃饭&quot;); this.study();// this调用方法，此处this可以省略 &#125; public void study() &#123; this.eat();// this调用方法，此处this可以省略 System.out.println(&quot;人学习&quot;); &#125; public static void main(String[] args) &#123; &#125;&#125; 实例二： 1234567891011121314151617181920212223242526272829303132333435363738public class Boy &#123; private String name; private int age; public Boy() &#123; &#125; public Boy(String name, int age) &#123; this.name = name; this.age = age; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setAge(int age) &#123; this.age = age; &#125; public int getAge() &#123; return age; &#125; public void marray(Girl girl) &#123; System.out.println(&quot;我想娶&quot; + girl.getName()); &#125; public void shout() &#123; System.out.println(&quot;我想找对象&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839public class Girl &#123; private String name; private int age; public Girl() &#123; &#125; public Girl(String name, int age) &#123; this.name = name; this.age = age; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setAge(int age) &#123; this.age = age; &#125; public int getAge() &#123; return age; &#125; public void marry(Boy boy) &#123; System.out.println(&quot;我想嫁给&quot; + boy.getName()); boy.marray(this);// 传入当前Girl对象 &#125; public int compare(Girl girl) &#123; return this.age - girl.age; &#125;&#125; 1234567891011public class Person &#123; public static void main(String[] args) &#123; Boy boy = new Boy(&quot;罗密欧&quot;, 20); boy.shout(); Girl girl = new Girl(&quot;朱丽叶&quot;, 18); girl.marry(boy); Girl girl2 = new Girl(&quot;祝英台&quot;, 19); System.out.println(&quot;年龄差：&quot; + girl.compare(girl2)); &#125;&#125; 关键字：packagepackage 语句作为 java 源文件的第一条语句，指明该文件中定义的类所在的包。若缺省该语句，则指定为无名包。 语法格式： 包对应于文件系统的目录，package 语句中，用 . 来指明包 (目录) 的层次。 包属于标识符，遵循标识符的命名规范，通常用小写单词标识。通常使用所在公司域名的倒置，如：com.atguigu.xxx。 同一个包下，不能命名同名的接口、类。不同的包下，可以命名同名的接口、类。 JDK 中主要的包介绍： java.lang —- 包含一些 java 语言的核心类，如 String、Math、Integer、 System 和 Thread，提供常用功能。 java.net —- 包含执行与网络相关的操作的类和接口。 java.io —- 包含能提供多种输入/输出功能的类。 java.util —- 包含一些实用工具类，如定义系统特性、接口的集合框架类、使用与日期日历相关的函数。 java.text —- 包含了一些 java 格式化相关的类。 java.sql —- 包含了 java 进行 JDBC 数据库编程的相关类/接口。 java.awt —- 包含了构成抽象窗口工具集 (abstract window toolkits) 的多个类，这些类被用来构建和管理应用程序的图形用户界面 (GUI)。(B/S 和 C/S) 关键字：import为使用定义在不同包中的 java 类，需用 import 语句来引入指定包层次下所需要的类或全部类 (.*)。import 语句告诉编译器到哪里去寻找类。 语法格式： 在源文件中使用 import 语句，可以显式的导入指定包下的类或接口。 声明在包的声明和类的声明之间。 如果需要导入多个类或接口，那么就并列显式声明多个 import 语句即可。 举例：可以使用 import java.util.*; 的方式，一次性导入 java.util 包下所有的类或接口。 如果导入的类或接口是 java.lang 包下的，或者是当前包下的，则可以省略此 import 语句。 如果在代码中使用不同包下的同名的类，那么就需要使用类的全类名的方式指明调用的是哪个类。 如果已经导入 java.a 包下的类，那么如果需要使用 a 包的子包下的类的话，仍然需要导入。 import static 组合的使用：导入指定类或接口下的静态的属性或方法。 1234567import static java.lang.System.*;public class Person &#123; public static void main(String[] args) &#123; out.println(&quot;打印方法&quot;);// 可以省略System &#125;&#125; OOP 特征二：继承性如果多个类中存在相同的属性和行为时，将这些内容抽取到单独一个类中，那么这多个类无需再定义这些属性和行为，只要继承那个抽出来的类即可。 此处的多个类称为子类 (派生类、subclass)**，单独的这个类称为父类 (基类、超类、superclass)**。可以理解为：”子类 is a 父类”。 类继承语法规则： 继承性的作用： 继承的出现减少了代码冗余，提高了代码的复用性。 继承的出现，更有利于功能的扩展。 继承的出现，让类与类之间产生了关系，提供了多态的前提。 继承性的特点： 子类继承了父类，就继承了父类中声明的所有属性和方法。特别的，父类中声明为 private 的属性和方法，子类继承父类以后，仍然认为子类获取了父类中私有的结构，只是因为封装性的影响，使得子类的实例不能直接调用父类的这些私有的结构而已 (事实上，父类的实例，也不能直接调用这些私有的结构)。 在子类中，可以使用父类中定义的方法和属性，也可以声明创建子类特有的属性和方法，以实现功能的扩展。 在 java 中，继承的关键字用的是 extends，即子类不是父类的子集，而是对父类的扩展。 继承性的规则： 子类不能直接访问父类中私有的 (private) 的成员变量和方法。 java 只支持单继承和多层继承，不允许多重继承。 一个子类只能有一个父类。 一个父类可以派生出多个子类。 此处强调的是 java 类的单继承性，java 中，接口是可以多继承的。 子类和父类是一个相对概念。子类直接继承的父类，称为直接父类，间接继承的父类，称为间接父类。 子类继承父类后，就获取了直接父类及所有间接父类中声明的属性和方法。 所有的 java 类 (除 java.lang.Object 类之外)，都直接或间接继承 java.lang.Object。 方法的重写 (override/overwrite)在子类中可以根据需要，对从父类中继承来的方法进行改造，也称为方法的重置、覆盖。在程序执行时，子类的方法将覆盖父类的方法。 重写的要求： 子类重写的方法必须和父类被重写的方法具有相同的方法名称、参数列表。 子类重写的方法使用的访问权限不能小于父类被重写的方法的访问权限 (权限修饰符)。 子类不能重写父类中声明为 private 权限的方法。 子类中可以声明与父类 private 方法相同名称和参数列表的方法，但不属于重写。 子类重写的方法的返回值类型不能大于父类被重写的方法的返回值类型。 父类被重写的方法的返回值类型是 void，则子类重写的方法的返回值类型只能是 void。 父类被重写的方法的返回值类型是 A 类型，则子类重写的方法的返回值类型可以是 A 类或 A 类的子类。 父类被重写的方法的返回值类型是基本数据类型 (比如：double)，则子类重写的方法的返回值类型必须是相同的基本数据类型 (即，只能是 double)。 子类重写的方法抛出的异常类型不能大于父类被重写的方法抛出的异常类型。 子类与父类中同名同参数的方法必须同时声明为非 static 的(此时属于重写)，或者同时声明为 static 的 (此时不属于重写)。因为 static 方法是属于类的，子类无法覆盖父类的方法。 实例一： 实例二： 方法重载与重写的区别： 二者的定义细节：略。 从编译和运行的角度看：重载，是指允许存在多个同名方法，而这些方法的参数不同。编译器根据方法不同的参数表，对同名方法的名称做修饰。对于编译器而言，这些同名方法就成了不同的方法。它们的调用地址在编译期就绑定了。java 的重载是可以包括父类和子类的，即子类可以重载父类的同名不同参数的方法。所以：对于重载而言，在方法调用之前，编译器就已经确定了所要调用的方法，这称为 “早绑定” 或 “静态绑定”**；而对于多态，只有等到方法调用的那一刻，解释运行器才会确定所要调用的具体方法，这称为 **”晚绑定” 或 **”动态绑定”**。引用一句 Bruce Eckel 的话：“不要犯傻，如果它不是晚绑定，它就不是多态。” 重载不表现为多态性，重写表现为多态性。 关键字：super super 理解为：父类的。 super 可以用来调用父类的：属性、方法、构造器。 在子类的方法或构造器中，可以通过使用 “super.属性” 或 “super.方法” 的形式，显示的调用父类中声明的属性或方法。 通常情况下，可以省略 “super.”。 特殊情况：当子类和父类中定义了同名的属性时，要想在子类中调用父类中声明的该属性，则必须显示的使用 “super.属性” 的方式，表明调用的是父类中声明的属性。 特殊情况：当子类重写了父类中的方法以后，要想在子类中调用父类中被重写的方法时，则必须显示的使用 “super.方法” 的方式，表明调用的是父类中被重写的方法。 在子类的构造器中，可以通过使用 “super(形参列表)” 的形式，显示的调用父类中声明的指定的构造器。 “super(形参列表)” 的使用，必须声明在子类构造器的首行。 在类的构造器中，针对于 “this(形参列表)” 或 “super(形参列表)”，只能二选一，不能同时出现。 在构造器的首行，如果没有现实的声明 “this(形参列表)” 或 “super(形参列表)”，则默认调用的是父类中空参的构造器，即：super();。 子类中所有的构造器默认都会访问父类中空参的构造器。 当父类中没有空参的构造器时，子类的构造器必须通过 “this(形参列表)” 或 “super(形参列表)” 语句，指定调用本类或者父类中相应的构造器。同时，只能二选一，且必须放在构造器的首行。 如果子类构造器中既未显式调用父类或本类的构造器，且父类中又没有无参的构造器，则编译出错。 在类的多个构造器中，至少有一个类的构造器中使用了 “super(形参列表)”，调用父类中的构造器。 实例： 父类： 1234567891011121314151617181920212223public class Person &#123; String name; int age; int id = 1000; public Person() &#123; System.out.println(&quot;父类的空参构造器&quot;); &#125; public Person(String name, int age, int id) &#123; this.name = name; this.age = age; this.id = id; &#125; public void eat() &#123; System.out.println(&quot;吃饭&quot;); &#125; public void sleep() &#123; System.out.println(&quot;睡觉&quot;); &#125;&#125; 子类： 1234567891011121314151617181920212223242526272829303132333435363738394041public class Student extends Person &#123;// String name;// 父类中已有的属性，可以省略// int age;// 父类中已有的属性，可以省略 String major; int id = 1001; public Student() &#123; &#125; public Student(String name, int age, String major) &#123; this.name = name; this.age = age; this.major = major; &#125; // 父类中已有的方法，可以省略，如有需要，可以重写，// public void eat() &#123;// System.out.println(&quot;吃饭&quot;);// &#125; // 重写父类的方法 @Override public void sleep() &#123; System.out.println(&quot;学生睡觉&quot;); &#125; public void study() &#123; System.out.println(&quot;学习&quot;); &#125; public void show() &#123; System.out.println(&quot;子类中的id: &quot; + this.id);// this可以省略，就近原则 System.out.println(&quot;父类中的id: &quot; + super.id);// 子类与父类有同名的属性id，此时super不可以省略 &#125; public static void main(String[] args) &#123; Student student = new Student(); student.show(); &#125;&#125; this 和 super 的区别： 思考： 为什么 “super(形参列表)” 和 “this(形参列表)” 调用语句不能同时在一个构造器中出现？ 因为 “super(形参列表)” 和 “this(形参列表)” 调用语句都必须出现在构造器中的首行。 为什么 “super(形参列表)” 和 “this(形参列表)” 只能作为构造器中的第一句出现？ 因为无论通过哪个构造器创建子类对象，都需要保证先初始化父类。这样做的目的是：当子类继承父类后，可以获得父类中所有的属性和方法，这样子类就有必要在一开始就知道父类是如何为对象进行初始化。 子类对象实例化过程 从结果上看： 子类继承父类之后，就获取了父类中声明的属性和方法。(继承性) 创建子类的对象，在堆空间中，就会加载所有父类中声明的属性。 从过程上看： 当通过子类的构造器创建子类对象时，一定会直接或间接的调用其父类的构造器，进而调用父类的父类的构造器，直到调用了 java.lang.Object 类中空参的构造器为止。正因为加载过所有的父类的结构，所以才可以看到内存中有父类中的结构，子类对象才能够进行调用。 明确：虽然创建子类对象时，调用了父类的构造器，但是自始至终只创建了一个对象，即为 new 出来的子类对象。 实例： 从输出结果可以看出，在创建 Man 的实例时，先进入了父类的空参构造器，然后执行子类的空参构造器。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Person &#123; String name; int age; public Person() &#123; System.out.println(&quot;父类空参构造器&quot;); &#125; public void eat() &#123; System.out.println(&quot;人吃饭&quot;); &#125; public void walk() &#123; System.out.println(&quot;人走路&quot;); &#125; public static void main(String[] args) &#123; Person person = new Man(); person.eat(); person.walk(); &#125;&#125;class Man extends Person &#123; boolean isSmoking; public void earnMoney() &#123; System.out.println(&quot;男人负责挣钱养家&quot;); &#125; @Override public void eat() &#123; System.out.println(&quot;男人多吃肉，长肌肉&quot;); &#125; @Override public void walk() &#123; System.out.println(&quot;男人霸气的走路&quot;); &#125;&#125;输出结果：父类空参构造器子类空参构造器男人多吃肉，长肌肉男人霸气的走路 OOP 特征三：多态性多态性，也叫对象的多态性：父类的引用指向子类的对象 (或子类的对象赋给父类的引用)。 一个变量只能有一种确定的数据类型。 一个引用类型变量可能指向 (引用) 多种不同类型的对象。 子类可看做是特殊的父类，所以父类类型的引用可以指向子类的对象：向上转型 (upcasting)。 一个引用类型变量如果声明为父类的类型，但实际引用的是子类对象，那么该变量就不能再访问子类中添加的属性和方法： 多态的使用： 虚拟方法调用。 有了对象的多态性以后，在编译期，只能调用父类中声明的方法，但在运行期，实际执行的是子类中重写的父类的方法。 编译，看左边；运行，看右边。 java 引用变量有两个类型：编译时类型和运行时类型。编译时类型由声明该变量时使用的类型决定，运行时类型由实际赋给该变量的对象决定。 若编译时类型和运行时类型不一致，就出现了对象的多态性。 多态情况下，看左边：看的是父类的引用 (父类中不具备子类特有的方法)，看右边：看的是子类的对象 (实际运行的是子类重写父类的方法)。 对象的多态性，只适用于方法，不适用于属性。对于属性，编译期和运行期，看的都是左边，即都是父类中声明的那个属性。 成员方法：编译时，要查看引用变量所声明的类中是否有所调用的方法。运行时，调用实际 new 的对象所属的类中的重写方法。 成员变量：不具备多态性，只看引用变量所声明的类。 子类继承父类： 若子类重写了父类方法，就意味着子类里定义的方法彻底覆盖了父类里的同名方法，系统将不可能把父类里的方法转移到子类中。 编译，看左边；运行，看右边。 对于实例变量则不存在这样的现象，即使子类里定义了与父类完全相同的实例变量，这个实例变量依然不可能覆盖父类中定义的实例变量。 编译，运行，都看左边。 实例： 父类： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Person &#123; String name; int age; public void eat() &#123; System.out.println(&quot;人吃饭&quot;); &#125; public void walk() &#123; System.out.println(&quot;人走路&quot;); &#125; public static void main(String[] args) &#123; // 对象的多态性：父类的引用指向子类的对象 Person person = new Man(); // 多态的使用：当调用子父类同名同参数的方法时，实际执行的是子类重写的父类的方法---虚拟方法调用 // 编译期，只能调用父类Person类中的方法；运行期，执行的是子类Man类中的方法。 person.eat(); person.walk(); // 不能调用子类特有的属性或方法，因为编译时，person是Person类型，而Person类中没有子类的这个特有属性或方法。 // 有了对象的多态性以后，内存中实际上是加载了子类特有的属性或方法的，但是由于变量声明为父类类型，导致编译时，只能调用父类中 // 声明的属性和方法，子类中特有的属性和方法不能调用。 // person.isSmoking = true; // person.earnMoney(); System.out.println(&quot;*********************************&quot;) // 如何才能使用子类特有的属性和方法？ // 向下转型：使用强制类型转换符 Man man = (Man) person; man.isSmoking = true; man.earnMoney(); // 使用强转时，可能出现java.lang.ClassCastException异常 Woman woman = (Woman) person; woman.goShopping(); &#125;&#125;输出结果：父类空参构造器子类空参构造器男人多吃肉，长肌肉男人霸气的走路*********************************男人负责挣钱养家 子类： 123456789101112131415161718public class Man extends Person &#123; boolean isSmoking; public void earnMoney() &#123; System.out.println(&quot;男人负责挣钱养家&quot;); &#125; @Override public void eat() &#123; System.out.println(&quot;男人多吃肉，长肌肉&quot;); &#125; @Override public void walk() &#123; System.out.println(&quot;男人霸气的走路&quot;); &#125;&#125; 实例二： 1234567891011121314151617181920212223242526272829public class FieldMethodTest &#123; public static void main(String[] args) &#123; Sub s = new Sub(); System.out.println(s.count);// 20 s.display();// 20 Base b = s; // 对于引用数据，==比较的是两个引用数据类型变量的地址值 System.out.println(b == s);// true System.out.println(b.count);// 10 b.display();// 20 &#125;&#125;class Base &#123; int count = 10; public void display() &#123; System.out.println(this.count); &#125;&#125;class Sub extends Base &#123; int count = 20; @Override public void display() &#123; System.out.println(this.count); &#125;&#125; 实例三： 12345678910111213141516171819202122232425262728public class InterviewTest1 &#123; public static void main(String[] args) &#123; Base base = new Sub(); base.add(1, 2, 3);// sub_1 Sub s = (Sub) base; s.add(1, 2, 3);// sub_2 &#125;&#125;class Base &#123; public void add(int a, int... arr) &#123; System.out.println(&quot;base&quot;); &#125;&#125;class Sub extends Base &#123; @Override public void add(int a, int[] arr) &#123; System.out.println(&quot;sub_1&quot;); &#125; // 这个方法没有重写，在Base类中不存在这样声明的方法， // 也就没有多态，所以base.add(1, 2, 3)方法输出sub_1 public void add(int a, int b, int c) &#123; System.out.println(&quot;sub_2&quot;); &#125;&#125; 多态性的使用前提： 有类的继承关系。 有方法的重写。 如果没有以上两个前提，就不存在多态。 多态性的优点： 提高了代码的通用性，常称作接口重用。 方法声明的形参类型为父类类型，可以使用子类的对象作为实参调用该方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class AnimalTest &#123; // 多态的使用：传入的是Animal对象，但实际传入的可以是Animal的子类 public void func(Animal animal) &#123; animal.eat(); animal.shout(); &#125; public static void main(String[] args) &#123; AnimalTest animalTest = new AnimalTest(); animalTest.func(new Dog()); animalTest.func(new Cat()); &#125;&#125;class Animal &#123; public void eat() &#123; System.out.println(&quot;动物：进食&quot;); &#125; public void shout() &#123; System.out.println(&quot;动物：叫&quot;); &#125;&#125;class Dog extends Animal &#123; @Override public void eat() &#123; System.out.println(&quot;狗吃骨头&quot;); &#125; @Override public void shout() &#123; System.out.println(&quot;汪！汪！汪！&quot;); &#125;&#125;class Cat extends Animal &#123; @Override public void eat() &#123; System.out.println(&quot;猫吃鱼&quot;); &#125; @Override public void shout() &#123; System.out.println(&quot;喵！喵！喵！&quot;); &#125;&#125;// 举例二class Order &#123; // 此方法可以传入任意对象，而不需要每个特定对象都创建一次method()方法 public void method(Object object) &#123; // 方法体 &#125;&#125; 抽象类、接口的使用。(抽象类和接口不能实例化，它们的使用也体现了多态) 虚拟方法调用： 正常的方法调用： 1234Person e = new Person();e.getInfo();Student e = new Student();e.getInfo(); 虚拟方法调用 (多态情况下) 子类中定义了与父类同名同参数的方法，在多态情况下，将此时父类的方法称为虚拟方法，父类根据赋给它的不同子类对象，动态调用属于子类的该方法。这样的方法调用在编译期是无法确定的。 12Person e = new Student();e.getInfo();// 调用Student类的getInfo()方法 编译时类型和运行时类型 上面代码中，编译时 e 为 Person 类型，而方法的调用是在运行时确定的，所以调用的是 Student 类的 getInfo() 方法 —— 动态绑定。 重写是多态，重载不是。 实例： 多态是编译时行为还是运行时行为? 多态是运行时行为，证明方法如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class InterviewTest &#123; public static Animal getInstance(int key) &#123; switch (key) &#123; case 0: return new Cat(); case 1: return new Dog(); default: return new Sheep(); &#125; &#125; public static void main(String[] args) &#123; // 因为key需要在运行时才能得到值，编译期时无法判断getInstance()方法输出什么 int key = new Random().nextInt(3); System.out.println(key); Animal animal = getInstance(key); animal.eat(); &#125;&#125;class Animal &#123; protected void eat() &#123; System.out.println(&quot;animal eat food&quot;); &#125;&#125;class Cat extends Animal &#123; @Override protected void eat() &#123; System.out.println(&quot;cat eat fish&quot;); &#125;&#125;class Dog extends Animal &#123; @Override public void eat() &#123; System.out.println(&quot;Dog eat bone&quot;); &#125;&#125;class Sheep extends Animal &#123; @Override public void eat() &#123; System.out.println(&quot;Sheep eat grass&quot;); &#125;&#125; 关键字：instanceofa instanceof A：检验对象 a 是否为类 A 的对象实例，如果是，返回 true，如果不是，返回 false。 使用情景：为了避免向下转型时出现 java.lang.ClassCastException，在向下转型之前，先进行 instanceof 判断，在返回 true 时，才进行向下转型。 要求 a 所属的类与类 A 必须是子类和父类的关系，否则编译错误。 如果 a 属于类 A 的子类 B，a instanceof A 的返回值也为 true。 对象类型转换 (casting) 基本数据类型的 Casting 自动类型转换：小的数据类型可以自动转换成大的数据类型。如 long g = 20; double d = 12.0f;。 强制类型转换：可以把大的数据类型强制转换 (casting) 成小的数据类型。如 float f = (float)12.0; int a = (int)1200L;。 对 java 对象的强制类型转换称为造型 从子类到父类的类型转换可以自动进行。 从父类到子类的类型转换必须通过造型 (强制类型转换) 实现。 无继承关系的引用类型间的转换是非法的。 在造型前可以使用 instanceof。 实例： 123456789101112131415public class ConversionTest &#123; public static void main(String[] args) &#123; double d = 13.4; long l = (long) d; System.out.println(l); int in = 5; // boolean b = (boolean)in; Object obj = &quot;Hello&quot;; String objStr = (String) obj; System.out.println(objStr); Object objPri = new Integer(5); // 下面代码运行时引发ClassCastException异常 String str = (String) objPri; &#125;&#125; 123456789101112131415public class Test &#123; public void method(Person e) &#123;// 设Person类中没有getschool()方法 // System.out.pritnln(e.getschool());// 非法，编译时错误 if (e instanceof Student) &#123; Student me = (Student) e;// 将e强制转换为Student类型 System.out.pritnln(me.getschool()); &#125; &#125; public static void main(String[] args) &#123; Test t = new Test(); Student m = new Student(); t.method(m); &#125;&#125; Object 类的使用 Object 类是所有 java 类的根父类。 如果在类的声明中未使用 extends 关键字指明其父类，则默认父类为 java.lang.Object 类。 验证方法： 123456789public class ObjectTest &#123; public static void main(String[] args) &#123; Base base = new Base(); System.out.println(&quot;父类：&quot; + base.getClass().getSuperclass());// 父类：class java.lang.Object &#125;&#125;class Base &#123;&#125; Object 类中的主要结构 == 操作符与 equals() 方法== 运算符： 如果比较的是基本数据类型变量：比较两个变量保存的数据是否相等，不一定类型要相同。 12345678910111213141516public static void main(String[] args) &#123; int i = 10; int j = 10; System.out.println(i == j);// true double k = 10.0; System.out.println(i == k);// true char c = 10; System.out.println(i == c);// true char c1 = &#x27;A&#x27;; char c2 = 65; System.out.println(c1 == c2);// true&#125; 如果比较的是引用数据类型变量：比较两个变量的地址值是否相同，即两个引用是否指向同一个对象实体。 12345678910// String类比较特殊，要注意。public static void main(String[] args) &#123; String s1 = &quot;javacdfa&quot;;// 这样写的javacdfa，位于常量池中 String s2 = &quot;javacdfa&quot;; System.out.println(s1 == s2);// true String s3 = new String(&quot;iam&quot;);// 这样new的，在堆内存中 String s4 = new String(&quot;iam&quot;); System.out.println(s3 == s4);// false&#125; 用 == 进行比较时，符号两边的数据类型必须兼容 (可自动转换的基本数据类型除外)，否则编译出错。 equals() 方法： 是一个方法，而非运算符，只能适用于引用数据类型。 使用格式：obj1.equals(obj2)。 所有类都继承了 Object，也就获得了 equals() 方法，也可以对其重写 。 Object 类中 equals() 方法的定义： 123public boolean equals(Object obj) &#123; return (this == obj);&#125; 说明：其作用与 == 相同, 比较是否指向同一个对象 。 像 File、String、Date 及包装类等，都重写了 Object 类中的 equals() 方法，重写以后，比较的不是两个引用对象的地址是否相同，而是比较两个引用对象的 “实体内容” 是否相同。 通常情况下，自定义的类使用 equals() 方法时，也是比较两个引用对象的 “实体内容” 是否相同。那么，就应该重写 equals() 方法。比如 String 类的 equals() 方法： 1234567891011121314151617181920212223public boolean equals(Object anObject) &#123; // 先判断地址 if (this == anObject) &#123; return true; &#125; // 再判断内容 if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false;&#125; 重写 equals() 方法的原则： 对称性：如果 x.equals(y) 返回是 true，那么 y.equals(x) 也应该返回是 true。 自反性：x.equals(x) 必须返回是 true。 传递性：如果 x.equals(y) 返回是 true，而且 y.equals(z) 返回是 true，那么 z.equals(x) 也应该返回是 true。 一致性：如果 x.equals(y) 返回是 true，只要 x 和 y 内容一直不变，不管重复 x.equals(y) 多少次，返回都是 true。 任何情况下，x.equals(null) 永远返回是 false；x.equals(和x不同类型的对象) 永远返回是 false。 面试题：== 和 equals() 的区别？ == 既可以比较基本类型也可以比较引用类型。对于基本类型是比较值，对于引用类型是比较内存地址。 equals() 方法属于 java.lang.Object 类里面的方法，如果该方法没有被重写过，默认也是 ==。 具体到特定自定义的类，要看该类里有没有重写 Object 的 equals() 方法以及重写的逻辑。 通常情况下，重写 equals() 方法，是比较类中的相应属性是否都相等。 toString() 方法 当输出一个对象的引用时，实际上就是调用当前对象的 toString() 方法。 1234567891011public class ObjectTest &#123; public static void main(String[] args) &#123; Order order = new Order(); System.out.println(order);// cn.xisun.database.Order@78308db1 System.out.println(order.toString());// cn.xisun.database.Order@78308db1 &#125;&#125;class Order &#123;&#125; Object 类中 toString() 方法的定义： 123public String toString() &#123; return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode());&#125; 像 File、String、Date 及包装类等，都重写了 Object 类中的 toString() 方法，使得在调用对象的 toString() 方法时，返回相应的 “实体内容”。 自定义类也可以重写 toString() 方法，当调用此方法时，返回 相应的 “实体内容”。比如 String 类的 equals() 方法： 123public String toString() &#123; return this;&#125; 基本类型数据转换为 String 类型时，调用了对应包装类的 toString() 方法。 面试题： 12345678public static void main(String[] args) &#123; char[] arr = new char[] &#123; &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27; &#125;; System.out.println(arr);// abc int[] arr1 = new int[] &#123; 1, 2, 3 &#125;; System.out.println(arr1);// [I@78308db1 double[] arr2 = new double[] &#123; 1.1, 2.2, 3.3 &#125;; System.out.println(arr2);// [D@27c170f0&#125; 包装类 (Wrapper) 的使用包装类：也叫封装类，是针对八种基本数据类型定义的相应的引用数据类型，以使得基本数据类型的变量具有类的特征。 JDK 1.5 之后，支持自动装箱，自动拆箱，但类型必须匹配。 基本类型、包装类与 String 类之间的转换： 基本数据类型转换成包装类 装箱：基本数据类型包装成包装类的实例，通过包装类的构造器实现。例如：int i = 500; Integer t = new Integer(i);。 自动装箱，例如：int i =500; Integer t = i;。 包装类转换成基本数据类型 拆箱：获得包装类对象中包装的基本类型变量，通过调用包装类的 .xxxValue() 方法。例如：boolean b = bObj.booleanValue();。 自动拆箱，例如：Integer t = 500; int i = t;。 基本数据类型/包装类转换成字符串 调用字符串重载的 valueOf() 方法，例如：String fstr = String.valueOf(2.34f);。 更直接的方式，连接运算，例如：String intStr = 5 + &quot;&quot;;。 字符串转换成基本数据类型/包装类 通过包装类的构造器实现，例如：int i = new Integer(&quot;12&quot;);。 通过包装类的 parseXxx(String s) 静态方法，例如：Float f = Float.parseFloat(“12.1”);。 面试题： 1234567891011121314public static void main(String[] args) &#123; // 三目运算符比较基本数据类型，在编译阶段自动拆箱为int和double类型，由于三目运算符要求表达式2和表达式3类型一致， // 所以在编译阶段自动类型提升(即int自动类型转换为double类型)，再自动装箱为Object，输出时使用多态调用重写 // 的toString()，即Double包装类的toString()方法 Object o1 = true ? new Integer(1) : new Double(2.0); System.out.println(o1);// 1.0 Object o2; if (true) o2 = new Integer(1); else o2 = new Double(2.0); System.out.println(o2);// 1&#125; 1234567891011public static void main(String[] args) &#123; Integer i = new Integer(1); Integer j = new Integer(1); System.out.println(i == j);// new了两个对象，false Integer m = 1; Integer n = 1; System.out.println(m == n);// 自动装箱，且在-128~127范围内，true Integer x = 128;// 相当于new Integer(128); Integer y = 128;// 相当于new Integer(128); System.out.println(x == y);// false&#125; Integer 类内部定义了 IntegerCache 结构，IntegerCache 中定义了一个 Integer[] 数组，保存了从 -128127 范围的整数。如果使用了自动装箱的方式，给 Integer 赋值在 -128127 范围内时，可以直接使用数组中的元素，不用 new。目的：提高效率。如果赋值超过了此范围，会 new 一个新对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 利用Vector代替数组处理：从键盘读入学生成绩(以负数代表输入结束)，找出最高分，并输出学生成绩等级。 * 提示：数组一旦创建，长度就固定不变，所以在创建数组前就需要知道它的长度。而向量类java.util.Vector可以根据需要动态伸缩。 *  创建Vector对象：Vector v=new Vector(); *  给向量添加元素：v.addElement(Object obj);// obj必须是对象 *  取出向量中的元素：Object obj=v.elementAt(0); *  注意第一个元素的下标是0，返回值是Object类型的。 *  计算向量的长度：v.size(); *  若与最高分相差10分内：A等；20分内：B等；30分内：C等；其它：D等。 */public class ScoreTest &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); Vector&lt;Object&gt; objects = new Vector&lt;&gt;(); int maxScore = -1; while (true) &#123; int nextScore = scanner.nextInt(); if (nextScore &lt; 0) &#123; break; &#125; if (nextScore &gt; 100) &#123; continue; &#125; objects.add(nextScore);// 自动装箱 if (maxScore &lt; nextScore) &#123; maxScore = nextScore; &#125; &#125; char level; for (int i = 0; i &lt; objects.size(); i++) &#123; Object object = objects.elementAt(i); int score = (Integer) object;// 自动拆箱 if (maxScore - score &lt; 10) &#123; level = &#x27;A&#x27;; &#125; else if (maxScore - score &lt; 20) &#123; level = &#x27;B&#x27;; &#125; else if (maxScore - score &lt; 30) &#123; level = &#x27;C&#x27;; &#125; else &#123; level = &#x27;D&#x27;; &#125; System.out.println(&quot;Student-&quot; + i + &quot; score is &quot; + score + &quot;, level is &quot; + level); &#125; &#125;&#125; 关键字：static当编写一个类时，其实就是在描述其对象的属性和行为，而并没有产生实质上的对象，只有通过 new 关键字才会产生出对象，这时系统才会分配内存空间给对象，其方法才可以供外部调用。有时候，希望无论是否产生了对象或无论产生了多少对象的情况下，某些特定的数据在内存空间里只有一份。例如：所有的中国人都有个国家名称，每一个中国人都共享这个国家名称，不必在每一个中国人的实例对象中都单独分配一个用于代表国家名称的变量。 实例变量： 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; Circle c1 = new Circle(2.0); // c1.radius=2.0 Circle c2 = new Circle(3.0); // c2.radius=3.0 &#125;&#125;class Circle &#123; private double radius; public Circle(double radius) &#123; this.radius = radius; &#125; public double findArea() &#123; return Math.PI * radius * radius; &#125;&#125; 上述代码中，c1 的 radius 独立于 c2 的 radius，存储在不同的空间。c1 中的 radius 变化不会影响 c2 的 radius，反之亦然。 像 Circle 类中的变量 radius 这样的，叫**实例变量 (instance variable)**，它属于类的每一个对象，不能被同一个类的不同对象所共享。 如果想让一个类的所有实例共享数据，就用类变量。类变量的定义，就需要用到 static 关键字。 static 关键字的使用： static：静态的。 static 可以用来修饰：属性、方法、代码块、内部类。 static 修饰后的成员具备以下特点： 随着类的加载而加载。 优先于对象存在。 修饰的成员，被所有对象所共享。 访问权限允许时，可不创建对象，直接被类调用。 使用 static 修饰属性：静态变量 (类变量/class variable) 属性，按是否使用 static 修饰，分为：静态属性和非静态属性 (实例变量)。 实例变量：当创建了类的多个对象，每个对象都独立的拥有一套类中的非静态属性。当修改其中一个对象的非静态属性时，不会导致其他对象中同样的属性值被修改。 静态变量：当创建了类的多个对象，每个对象都共用同一个静态变量。当通过某一个对象修改静态变量时，会导致其他对象调用此静态变量时，是修改之后的值。 注意：实际操作时，虽然编译能过通过，但不应该通过类的实例对象来访问静态成员。 静态变量随着类的加载而加载。可以通过 “类.静态变量”的方式进行调用。 静态变量的加载要早于对象的创建。(实例变量在创建对象的过程中，或创建对象之后，才创建。) 由于类只会加载一次，则静态变量在内存中也只会存在一份：保存在方法区的静态域中。 类可以访问静态变量，但不能访问实例变量 (实例变量在对象产生时才生成)，对象可以访问实例变量，也能访问静态变量 (不推荐)。 静态变量举例：System.out，Math.PI。 使用 static 修饰方法：静态方法 (类方法/class method) 随着类的加载而加载，可以通过 “类.静态方法” 的方式进行调用。 类可以访问静态方法，但不能访问非静态方法 (非静态方法在对象产生时才生成)，对象可以访问非静态方法，也能访问静态方法 (不推荐)。 静态方法中，只能调用静态属性或静态方法，它们的生命周期是一致的。非静态方法中，既可以调用非静态属性或非静态方法，也能调用静态属性或静态方法。 static 使用的注意点： 在静态方法内，不能使用 this 关键字、super 关键字。(this 和 super 指向当前类对象和父类对象，需要创建实例对象后才有这些概念。) 12345public static void show() &#123; // 省略的是Chiese.，而不是this. walk();// 等同于Chinese.walk(); System.out.println(&quot;nation: &quot; + nation);// 等同于System.out.println(Chinese.nation);&#125; static 修饰的方法不能被重写。 关于静态属性和静态方法的使用，从生命周期的角度去理解。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Test &#123; public static void main(String[] args) &#123; Chinese c1 = new Chinese(); c1.name = &quot;姚明&quot;; c1.age = 40; Chinese c2 = new Chinese(); c2.name = &quot;马龙&quot;; c2.age = 30; // 通过c1对象修改nation的值，c2对象也能获得 // 实际操作时，虽然编译能过通过，但不应该通过类的实例对象来访问静态成员 c1.nation = &quot;CHN&quot;; System.out.println(c2.nation); // 对象实例调用非静态方法 c1.eat(); // 类调用静态方法 Chinese.show(); // 通过c1对象也能调用非静态方法 // 实际操作时，虽然编译能过通过，但不应该通过类的实例对象来访问静态成员 c1.show(); &#125;&#125;class Chinese &#123; String name; int age; static String nation; public void eat() &#123; System.out.println(&quot;吃饭&quot;); // 调用非静态结构 this.info(); System.out.println(&quot;name: &quot; + this.name); // 调用静态结构 walk(); System.out.println(&quot;nation: &quot; + nation); &#125; public void info() &#123; System.out.println(&quot;name: &quot; + name + &quot;, age: &quot; + age + &quot;, nation: &quot; + nation); &#125; public static void show() &#123; System.out.println(&quot;我是中国人&quot;); // 不能调用非静态的结构 // eat(); // name = &quot;Tom&quot;; // 调用静态的结构 walk(); System.out.println(&quot;nation: &quot; + nation);// 省略的是Chiese.，而不是this.，等同于System.out.println(Chinese.nation); &#125; public static void walk() &#123; System.out.println(&quot;走路&quot;); &#125;&#125; 类变量和实例变量内存解析： 类属性、类方法的设计思想： 类属性作为该类各个对象之间共享的变量，在设计类时，分析哪些属性不因对象的不同而改变，将这些属性设置为类属性，相应的方法设置为类方法。 如果方法与调用者无关，则这样的方法通常被声明为类方法，由于不需要创建对象就可以调用类方法，从而简化了方法的调用。 类中的常量，通常也声明为 static 的。 操作静态属性的方法，通常设置为 static 的。 工具类中的方法，习惯上声明为 static 的。 main() 方法的语法由于 java 虚拟机需要调用类的 main() 方法，所以该方法的访问权限必须是 public，又因为 java 虚拟机在执行 main() 方法时不必创建对象，所以该方法必须是 static 的，该方法接收一个 String 类型的数组参数，该数组中保存执行 java 命令时传递给所运行的类的参数。 又因为 main() 方法是静态的，我们不能直接访问该类中的非静态成员，必须创建该类的一个实例对象后，才能通过这个对象去访问类中的非静态成员，这种情况，我们在之前的例子中多次碰到。 main() 方法的使用说明： main() 方法是程序的入口。 main() 方法也是一个普通的静态方法，在执行某个类的 main() 方法之前，需要先加载这个类，这个过程是早于 main() 方法中首行的执行语句的。 main() 方法可以作为程序与控制台交互的方式之一，其他的还可以使用 Scanner 类。 命令行参数用法举例： 类的成员之四：代码块 (或初始化块) 代码块的作用：对 java 类或对象进行初始化。 代码块的分类：一个类中代码块若有修饰符，则只能被 static 修饰，称为静态代码块 (static block)，没有使用 static 修饰的，为非静态代码块。 静态代码块： 内部可以有输出语句。 随着类的加载而执行，而且只执行一次。(不同于静态方法，静态方法必须在被类显示的调用后，才会执行方法内的语句。) 作用：初始化类的信息。 如果一个类定义了多个静态代码块，则按照声明的先后顺序来执行。一般情况下，不建议定义多个。 静态代码块的执行要优先于非静态代码块的执行，与声明的先后顺序无关。 静态代码块中，只能调用静态的属性、静态的方法，不能调用非静态的属性、非静态的方法。 非静态代码块： 内部可以有输出语句。 随着对象的创建而执行。(不同于非静态方法，非静态方法必须在被类的对象显示的调用后，才会执行方法内的语句。) 每创建一个对象，就执行一次非静态代码块。且先于构造器执行。 作用：可以再创建对象时，对对象的属性等进行初始化。 如果一个类定义了多个非静态代码块，则按照声明的先后顺序来执行。一般情况下，不建议定义多个。 非静态代码块中，可以调用静态的属性、静态的方法，也可以调用非静态的属性、非静态的方法。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class BlockTest &#123; public static void main(String[] args) &#123; System.out.println(&quot;*********类加载*********&quot;); String desc = Person.desc; System.out.println(desc); System.out.println(&quot;*********对象加载*********&quot;); Person person = new Person(); &#125;&#125;class Person &#123; // 属性 String name; int age; static String desc = &quot;我是一个人&quot;; // 静态代码块 static &#123; System.out.println(&quot;我是一个静态代码块-1&quot;); // 调用静态结构 desc = &quot;我是一个中国人&quot;;// 对类的静态属性重新赋值 info(); // 不能调用非静态结构 // name = &quot;Tom&quot;; // eat(); &#125; static &#123; System.out.println(&quot;我是一个静态代码块-2&quot;); &#125; // 非静态代码块 &#123; System.out.println(&quot;我是一个非静态代码块-1&quot;); // 调用静态结构 desc = &quot;我是一个中国人&quot;; info(); // 调用非静态结构 name = &quot;Tom&quot;; eat(); &#125; &#123; System.out.println(&quot;我是一个非静态代码块-2&quot;); &#125; // 构造器 public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; // 静态方法 public static void info() &#123; System.out.println(&quot;我是一个静态方法&quot;); &#125; // 非静态方法 public void eat() &#123; System.out.println(&quot;我是一个非静态方法&quot;); &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125; 代码块及构造器的执行顺序： 由父及子，静态先行。 注意：调用 main() 方法时，需要先加载类，这个过程是早于 main() 方法中的首行执行语句的。 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576class Root &#123; static &#123; System.out.println(&quot;Root的静态初始化块&quot;); &#125; &#123; System.out.println(&quot;Root的普通初始化块&quot;); &#125; public Root() &#123; System.out.println(&quot;Root的无参数的构造器&quot;); &#125;&#125;class Mid extends Root &#123; static &#123; System.out.println(&quot;Mid的静态初始化块&quot;); &#125; &#123; System.out.println(&quot;Mid的普通初始化块&quot;); &#125; public Mid() &#123; System.out.println(&quot;Mid的无参数的构造器&quot;); &#125; public Mid(String msg) &#123; // 通过this调用同一类中重载的构造器 this(); System.out.println(&quot;Mid的带参数构造器，其参数值：&quot; + msg); &#125;&#125;class Leaf extends Mid &#123; static &#123; System.out.println(&quot;Leaf的静态初始化块&quot;); &#125; &#123; System.out.println(&quot;Leaf的普通初始化块&quot;); &#125; public Leaf() &#123; // 通过super调用父类中有一个字符串参数的构造器 super(&quot;尚硅谷&quot;); System.out.println(&quot;Leaf的构造器&quot;); &#125;&#125;public class LeafTest &#123; public static void main(String[] args) &#123; new Leaf(); System.out.println(); new Leaf(); &#125;&#125;输出结果：Root的静态初始化块Mid的静态初始化块Leaf的静态初始化块Root的普通初始化块Root的无参数的构造器Mid的普通初始化块Mid的无参数的构造器Mid的带参数构造器，其参数值：尚硅谷Leaf的普通初始化块Leaf的构造器Root的普通初始化块Root的无参数的构造器Mid的普通初始化块Mid的无参数的构造器Mid的带参数构造器，其参数值：尚硅谷Leaf的普通初始化块Leaf的构造器 实例二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Father &#123; static &#123; System.out.println(&quot;11111111111&quot;); &#125; &#123; System.out.println(&quot;22222222222&quot;); &#125; public Father() &#123; System.out.println(&quot;33333333333&quot;); &#125; // main方法是一个静态方法，执行某个类的main方法之前，要先加载这个类，此处是Father类 public static void main(String[] args) &#123; System.out.println(&quot;77777777777&quot;); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Father(); &#125;&#125;class Son extends Father &#123; static &#123; System.out.println(&quot;44444444444&quot;); &#125; &#123; System.out.println(&quot;55555555555&quot;); &#125; public Son() &#123; System.out.println(&quot;66666666666&quot;); &#125; // main方法是一个静态方法，执行某个类的main方法之前，要先加载这个类，此处是先加载Son类 public static void main(String[] args) &#123; // 由父及子 静态先行 System.out.println(&quot;77777777777&quot;); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Father(); &#125;&#125;public class Test &#123; // main方法是一个静态方法，执行某个类的main方法之前，要先加载这个类，此处是先加载Test类 public static void main(String[] args) &#123; System.out.println(&quot;77777777777&quot;); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Son(); System.out.println(&quot;************************&quot;); new Father(); &#125;&#125; 调用 Father 类的 main() 方法，要先加载 Father 类。输出结果： 12345678910111213141516F: 11111111111F/m: 77777777777F/m: ************************S: 44444444444F: 22222222222F: 33333333333S: 55555555555S: 66666666666F/m: ************************F: 22222222222F: 33333333333S: 55555555555S: 66666666666F/m: ************************F: 22222222222F: 33333333333 调用 Son 类的 main() 方法，要先加载 Son 类。输出结果： 12345678910111213141516F: 11111111111S: 44444444444S/m: 77777777777S/m: ************************F: 22222222222F: 33333333333S: 55555555555S: 66666666666S/m: ************************F: 22222222222F: 33333333333S: 55555555555S: 66666666666S/m: ************************F: 22222222222F: 33333333333 调用 Test 类的 main() 方法，要先加载 Test 类。输出结果： 12345678910111213141516T/m: 77777777777T/m: ************************F: 11111111111S: 44444444444F: 22222222222F: 33333333333S: 55555555555S: 66666666666T/m: ************************F: 22222222222F: 33333333333S: 55555555555S: 66666666666T/m: ************************F: 22222222222F: 33333333333 关键字：finalfinal：最终的。 final 可以用来修饰的结构：类、方法、变量 (属性是成员变量，是变量的其中一种。)。 final 用来修饰类：此类不能被其他类所继承。例如：String 类、System 类、StringBuffer 类。 final 用来修饰方法：此方法不能被子类重写。例如：Object 类中的 getClass()。 final 用来修饰变量：此时的 “变量” 称为常量，名称大写，且只能被赋值一次。 final 修饰成员变量：必须在声明时或代码块中或在每个构造器中显式赋值，否则编译不通过。 1234567891011121314151617181920212223242526public class FinalTest &#123; // 1.显式初始化：所有对象的这个常量值都是相同的，可以考虑直接显式初始化 final int WIDTH = 0; // 2.代码块中初始化：如果涉及到调用方法，或赋值操作较多，可以考虑代码块中初始化 final int HEIGHT; &#123; HEIGHT = show(); &#125; // 3.构造器中初始化：如果涉及到调用方法，或赋值操作较多，可以考虑代码块中初始化 final int LEFT; public FinalTest() &#123; LEFT = show(); &#125; public int show() &#123; return 0; &#125; public static void main(String[] args) &#123; FinalTest finalTest = new FinalTest(); &#125;&#125; final 修饰局部变量：修饰方法内局部变量时，表明该变量是一个常量，不能被修改；修饰形参时，表明此形参是一个常量，当调用此方法时，给常量形参赋一个实参，一旦赋值以后，就只能在方法体内使用此形参，但不能被修改。 123456789101112131415161718public class FinalTest &#123; public int show() &#123; // 1.修饰方法内局部变量：常量，不能被再次更改。 final int NUM = 10; return NUM; &#125; // 2.修饰形参：当方法被调用时，传入的实参，不能被再次更改。 public void show(final int num) &#123; System.out.println(num); &#125; public static void main(String[] args) &#123; FinalTest finalTest = new FinalTest(); finalTest.show(20); &#125;&#125; static final 用来修饰属性：全局常量。 12345678910111213141516171819202122public class FinalTest &#123; static final int WIDTH = 0; static final int HEIGHT; static &#123; HEIGHT = show(); &#125; public FinalTest() &#123; &#125; public static int show() &#123; return 0; &#125; public static void main(String[] args) &#123; FinalTest finalTest = new FinalTest(); &#125;&#125; 面试题： 123456public class Something &#123; public int addOne(final int x) &#123; // return ++x;// 编译不通过 return x + 1;// 正常 &#125;&#125; 123456789101112131415public class Something &#123; public void addOne(final Other o) &#123; // o = new Other();// 编译不通过 o.i++;// 正常 &#125; public static void main(String[] args) &#123; Other o = new Other(); new Something().addOne(o); &#125;&#125;class Other &#123; public int i;&#125; 抽象类和抽象方法随着继承层次中一个个新子类的定义，类变得越来越具体，而父类则更一般，更通用。类的设计应该保证父类和子类能够共享特征。有时将一个父类设计得非常抽象，以至于它没有具体的实例，这样的类叫做抽象类。 抽象类应用：抽象类是用来模型化那些父类无法确定全部实现，而是由其子类提供具体实现的对象的类。 abstract 关键字的使用： abstract：抽象的。 abstract 可以用来修饰的结构：类、方法。 abstract 修饰类：抽象类。 抽象类不能实例化。 抽象类中一定有构造器 (抽象类本身不能使用构造器)，便于子类实例化时调用。 开发中，会提供抽象类的子类，让子类对象实例化，完成相关操作。 abstract 修饰方法：抽象方法。 抽象方法只有方法声明，没有方法体，以分号结束。比如：public abstract void talk(); 包含抽象方法的类，一定是一个抽象类。反之，抽象类中可以没有抽象方法。 若子类重写了父类 (不仅包括直接父类，也包括间接父类) 中的所有的抽象方法后，此子类方可实例化；若子类没有重写父类中的所有的抽象方法，则次子类也是一个抽象类，需要使用 abstract 修饰。 abstract 不能修饰变量、代码块、构造器。 abstract 不能修饰私有方法、静态方法、final 的方法、final 的类。 抽象类的匿名子类对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Test &#123; public static void method(Student student) &#123; &#125; public static void method1(Person person) &#123; person.eat(); &#125; public static void main(String[] args) &#123; // 匿名对象 method(new Student()); // 1.创建非匿名类的非匿名的对象 Worker worker = new Worker(); method1(worker); // 2.创建非匿名类的匿名的对象 method1(new Worker()); // 3.创建匿名子类的非匿名的对象：p Person p = new Person() &#123; @Override public void eat() &#123; // 重写方法体 System.out.println(&quot;...&quot;); &#125; &#125;; method1(p); // 4.创建匿名子类的匿名对象 method1(new Person() &#123; @Override public void eat() &#123; // 重写方法体 System.out.println(&quot;,,,&quot;); &#125; &#125;); &#125;&#125;abstract class Person &#123; public abstract void eat();&#125;class Student &#123;&#125;class Worker extends Person &#123; @Override public void eat() &#123; // 重写方法体 System.out.println(&quot;、、、&quot;); &#125;&#125; 接口 (interface)一方面，有时必须从几个类中派生出一个子类，继承它们所有的属性和方法。但是，java 不支持多重继承。有了接口，就可以得到多重继承的效果。另一方面，有时必须从几个类中抽取出一些共同的行为特征，而它们之间又没有 is - a 的关系，仅仅是具有相同的行为特征而已。例如：鼠标、键盘、打印机、扫描仪、摄像头、充电器、MP3 机、手机、数码相机、移动硬盘等都支持 USB 连接。 接口就是规范，定义的是一组规则，体现了现实世界中 “如果你是/要…则必须能…” 的思想。继承是一个 “是不是” 的关系，而接口实现则是 “能不能” 的关系。 接口的本质是契约，标准，规范，就像我们的法律一样，制定好后大家都要遵守。 接口的使用： 接口使用 interface 定义。 java 中，接口和类是并列的两个结构，或者可以理解为一种特殊的类。从本质上讲，接口是一种特殊的抽象类。 如何定义接口：定义接口中的成员 JDK 7 及以前：只能定义全局常量和抽象方法。 全局常量：接口中的所有成员变量都默认是由 public static final 修饰的。书写时，可以省略，但含义不变，常量不能被更改。 抽象方法：接口中的所有抽象方法都默认是由 public abstract 修饰的。 JDK 8：除了定义全局常量和抽象方法之外，还可以定义静态方法、默认方法。 静态方法：使用 static 关键字修饰，默认为 public 的。 只能通过接口直接调用，并执行其方法体。 默认方法：使用 default 关键字修饰，默认为 public 的。 可以通过实现类的对象来调用，如果实现类重写了接口中的默认方法，调用时，执行的是重写后的方法。 如果子类 (或实现类) 继承的父类和实现的接口中，声明了同名同参数的mo人方法，那么子类在没有重写此方法的情况下， 默认调用的是父类中的同名同参数的方法 — 类优先原则。如果重写了，调用子类重写的方法。 如果实现类实现了多个接口，而多个接口中定义了同名同参数的默认方法，那么在实现类没有重写此方法的情况下，编译不通过 — 接口冲突。如果要避免接口冲突，则在实现类中，必须重写此方法。 在子类 (或实现类) 的方法中，使用 “super.方法名” 调用父类的方法，使用 “接口名.super.方法名” 调用接口中的方法。 实例： 1234567891011121314151617181920212223 public interface InterfaceA &#123; // 静态方法 static void method1() &#123; System.out.println(&quot;接口A：静态方法1&quot;); &#125; // 默认方法 default void method2() &#123; System.out.println(&quot;接口A：默认方法2&quot;); &#125; default void method3() &#123; System.out.println(&quot;接口A：默认方法3&quot;); &#125; default void method4() &#123; System.out.println(&quot;接口A：默认方法4&quot;); &#125; default void method5() &#123; System.out.println(&quot;接口A：默认方法5&quot;); &#125;&#125; 12345 public interface InterfaceB &#123; default void method5() &#123; System.out.println(&quot;接口B：默认方法5&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 public class SubClassTest &#123; public static void main(String[] args) &#123; // 1.静态方法 InterfaceA.method1(); SubClass subClass = new SubClass(); // 2.默认方法 subClass.method2(); // 3.重写的默认方法 subClass.method3(); // 4.调用的是父类中的method4() subClass.method4(); &#125; &#125; class SuperClass &#123; public void method4() &#123; System.out.println(&quot;父类：方法4&quot;); &#125; &#125; class SubClass extends SuperClass implements InterfaceA, InterfaceB &#123; // 重写接口InterfaceA中的method3() @Override public void method3() &#123; System.out.println(&quot;实现类：方法3&quot;); &#125; // 重写了父类SuperClass的method4() @Override public void method4() &#123; System.out.println(&quot;实现类：方法4&quot;); &#125; // InterfaceA和InterfaceB声明了同名同参的method5()，SubClass中必须重写此方法，否则接口冲突，编译不通过 // 如果继承的父类SuperClass中也声明了同名同参的method5()，则不会出现接口冲突 @Override public void method5() &#123; System.out.println(&quot;实现类：方法5&quot;); &#125; public void myMethod() &#123; method2();// InterfaceA的method2() method3();// 重写的InterfaceA的method3() InterfaceA.super.method3();// InterfaceA的method3() method4();// 重写的SuperClass的method4() super.method4();// 父类SuperClass的method4() InterfaceA.super.method5();// InterfaceA的method5() InterfaceB.super.method5();// InterfaceB的method5() &#125;&#125; 接口中不能定义构造器，意味着接口不可以实例化。 java 开发中，接口都通过让类去实现的方式 (implements) 来使用 (面向接口编程)。 如果实现类覆盖了接口中 (包括直接接口和间接接口) 的所有抽象方法，则此实现类可以实例化。如果实现类没有覆盖接口 (包括直接接口和间接接口) 中所有的抽象方法，则此实现类仍为一个抽象类。 1234567891011121314151617181920interface MyInterface&#123; String s = &quot;MyInterface&quot;; public void absM1();&#125;interface SubInterface extends MyInterface&#123; public void absM2();&#125;// 实现类SubAdapter必须给出接口SubInterface以及父接口MyInterface中所有方法的实现。// 否则，SubAdapter仍需声明为abstract的。public class SubAdapter implements SubInterface&#123; public void absM1()&#123; System.out.println(&quot;absM1&quot;); &#125; public void absM2()&#123; System.out.println(&quot;absM2&quot;); &#125;&#125; java 类可以实现多个接口，弥补了 java 单继承性的局限性。 格式：class SubClass extends SuperClass implements InterfaceA, InterfaceB, InterfaceC &#123;&#125; 接口与接口之间可以继承，而且可以多继承。 与继承关系类似，接口与实现类之间体现了多态性。 接口，实际上可以看作是一种规范。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class InterfaceTest &#123; public static void main(String[] args) &#123; System.out.println(Flyable.MAX_SPEED); System.out.println(Flyable.MIN_SPEED); Plane plane = new Plane(); plane.fly(); &#125;&#125;interface Flyable &#123; // 全局常量，可以省略 public static final int MAX_SPEED = 7900;// 第一宇宙速度 int MIN_SPEED = 1; // 抽象方法，可以省略 public abstract public abstract void fly(); void stop();&#125;interface Attackable &#123; void attack();&#125;// 全部实现接口中的方法，可以实例化class Plane implements Flyable &#123; @Override public void fly() &#123; System.out.println(&quot;飞机起飞&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;飞机降落&quot;); &#125;&#125;// 未全部实现接口中的方法，仍是一个抽象类abstract class Kite implements Flyable &#123; @Override public void fly() &#123; System.out.println(&quot;风筝在飞&quot;); &#125;&#125;// 实现多个接口class Bullet implements Flyable, Attackable &#123; @Override public void fly() &#123; System.out.println(&quot;子弹起飞&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;子弹停止&quot;); &#125; @Override public void attack() &#123; System.out.println(&quot;子弹具有攻击性&quot;); &#125;&#125; 面试题： 抽象类与接口有哪些异同？ 接口能继承接口； 抽象类能继承接口 (如不完全实现接口方法的类，还是抽象类)； 抽象类能继承非抽象类 (如抽象类的父类 Object)。 排错： 因为接口 A 和父类 B 是并列的，所以需要明确变量 x 的所属，如果 A 是 B 的父类，那么在 C 中就近原则，x 会认为是 B 的属性： 1234567891011121314151617181920212223interface A &#123; int x = 0; int x1 = 2;&#125;class B &#123; int x = 1; int x2 = 3;&#125;class C extends B implements A &#123; public void pX() &#123; System.out.println(x);// error: Reference to &#x27;x&#x27; is ambiguous, both &#x27;B.x&#x27; and &#x27;A.x&#x27; match // System.out.println(A.x);// 0 // System.out.println(super.x);// 1 System.out.println(x1);// 2 System.out.println(x2);// 3 &#125; public static void main(String[] args) &#123; new C().pX(); &#125;&#125; 接口中的所有成员变量都默认是 public static final 的，不能在实现类中被重写： 123456789101112131415161718192021222324252627282930interface Playable &#123; void play();&#125;interface Bounceable &#123; void play();&#125;interface Rollable extends Playable, Bounceable &#123; Ball BALL = new Ball(&quot;PingPang&quot;);&#125;class Ball implements Rollable &#123; private String name; public String getName() &#123; return name; &#125; public Ball(String name) &#123; this.name = name; &#125; // play()方法被认为是即重写了接口Playable，又重写了接口Bounceable @Override public void play() &#123; BALL = new Ball(&quot;Football&quot;);// error: Cannot assign a value to final variable &#x27;BALL&#x27; System.out.println(BALL.getName()); &#125;&#125; 接口匿名实现类的对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class InterfaceTest &#123; public static void main(String[] args) &#123; Computer computer = new Computer(); // 1.创建接口的非匿名实现类的非匿名对象 Flash flash = new Flash(); computer.transferData(flash); // 2.创建接口的非匿名实现类的匿名对象 computer.transferData(new Printer()); // 3.创建接口的匿名实现类的非匿名对象 USB phone = new USB() &#123; @Override public void start() &#123; System.out.println(&quot;手机开始工作&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;手机停止工作&quot;); &#125; &#125;; computer.transferData(phone); // 4.创建接口的匿名实现类的匿名对象 computer.transferData(new USB() &#123; @Override public void start() &#123; System.out.println(&quot;mp3开始工作&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;mp3停止工作&quot;); &#125; &#125;); &#125;&#125;class Computer &#123; public void transferData(USB usb) &#123; usb.start(); transferDetails(); usb.stop(); &#125; private void transferDetails() &#123; System.out.println(&quot;具体传输数据的细节&quot;); &#125;&#125;interface USB &#123; void start(); void stop();&#125;class Flash implements USB &#123; @Override public void start() &#123; System.out.println(&quot;U盘开启工作&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;U盘停止工作&quot;); &#125;&#125;class Printer implements USB &#123; @Override public void start() &#123; System.out.println(&quot;打印机开启工作&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;打印机停止工作&quot;); &#125;&#125; 类的成员之五：内部类当一个事物的内部，还有一个部分需要一个完整的结构进行描述，而这个内部的完整的结构又只为外部事物提供服务，那么整个内部的完整结构最好使用内部类。 在 java 中，允许一个类 A 声明在另一个类 B 的内部，则类 A 称为内部类，类 B 称为外部类。 Inner class一般用在定义它的类或语句块之内，在外部引用它时必须给出完整的名称。Inner class 的名字不能与包含它的外部类类名相同。 内部类的分类：成员内部类 (静态的、非静态的) vs 局部内部类 (代码块内、构造器内、方法内) 成员内部类： 一方面，作为外部类的成员： 调用外部类的结构，注意生命周期，如静态成员内部类不能调用外部类非静态的方法。 可以被 static 修饰，但此时就不能再使用外层类的非 static 的成员变量。注意，外部类不能被 static 修饰。 可以被 private、protected、缺省和 public 四种权限修饰符修饰。注意，外部类不能被 private 和 protected 修饰。 另一方面，作为一个类： 类内可以定义属性、方法、构造器、代码块、内部类等。 可以被 final 修饰，表示此类不能被继承，如果不使用 final，就可以被继承。 可以被 abstract 修饰，表示此类不能被实例化，可以被其它的内部类继承。 编译以后生成 OuterClass$InnerClass.class 字节码文件 (也适用于局部内部类)。 非 static 的成员内部类中的成员不能声明为 static 的，只有在外部类或 static 的成员内部类中才可声明 static 成员。 外部类访问成员内部类的成员，需要 “内部类.成员” 或 “内部类对象.成员” 的方式。 成员内部类可以直接使用外部类的所有成员，包括私有的数据。 当想要在外部类的静态成员部分使用内部类时，可以考虑内部类声明为静态的。 局部内部类： 局部内部类仍然是一个独立的类，在编译之后内部类会被编译成独立的 .class 文件，但是前面冠以外部类的类名和 $ 符号，以及数字编号。 只能在声明它的方法或代码块中使用，而且是先声明后使用，除此之外的任何地方都不能使用该类。 局部内部类的对象可以通过外部方法的返回值返回使用，返回值类型只能是局部内部类的父类或父接口类型。 局部内部类可以使用外部类的成员，包括私有的。 局部内部类可以使用外部方法的局部变量，但是必须是 final 的，final 可以省略 (jdk 8 及之后)，但这个局部变量赋值后不能有再次修改操作，否则编译不通过。这是因为局部内部类和局部变量的声明周期不同所致。 局部内部类和局部变量地位类似，不能使用 public，缺省，protected 和 private 修饰。 局部内部类不能使用static修饰，因此也不能包含静态成员。 关注如下的 3 个问题： 如何实例化成员内部类的对象？ 静态成员内部类：外部类.静态内部类 变量名 = new 外部类.静态内部类();。 非静态成员内部类：外部类.非静态内部类 变量名 = new 外部类().new 非静态内部类();。 如何在成员内部类中区分调用外部类的结构？ 静态成员内部类，参考： 12345public void show(int age) &#123; System.out.println(&quot;形参：&quot; + age); System.out.println(&quot;静态成员内部类的静态属性：&quot; + Brain.age); System.out.println(&quot;外部类的静态属性：&quot; + Person.age);&#125; 非静态成员内部类，参考： 12345public void show(String name) &#123; System.out.println(&quot;形参：&quot; + name); System.out.println(&quot;非静态成员内部类的非静态属性：&quot; + this.name);// 非静态成员内部类，不能定义static的变量 System.out.println(&quot;外部类的非静态属性：&quot; + Person.this.name);&#125; 开发者局部内部类的使用？ 12345678910111213141516171819202122232425262728293031public class InnerClassTest1 &#123; // 这种局部内部类，开发中很少见 public void method() &#123; class AA &#123; &#125; &#125; // 返回一个实现类Comparable接口的类的对象 public Comparable getComparable() &#123; // 创建一个实现了Comparable接口的类：局部内部类 // 方式一：创建Comparable接口的非匿名实现类的匿名对象 /*class MyComparable implements Comparable &#123; @Override public int compareTo(Object o) &#123; return 0; &#125; &#125; return new MyComparable();*/ // 方式二：创建Comparable接口的匿名实现类的匿名对象 return new Comparable() &#123; @Override public int compareTo(Object o) &#123; return 0; &#125; &#125;; &#125;&#125; 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class InnerClassTest &#123; public static void main(String[] args) &#123; // 1.创建Brain实例---静态的成员内部类 Person.Brain brain = new Person.Brain(); brain.think(); brain.show(8); // 2.创建Hand实例---非静态的成员内部类 Person.Hand hand = new Person().new Hand(); hand.grasp(); hand.show(&quot;外来手&quot;); &#125;&#125;class Person &#123; String name = &quot;小明&quot;; static int age = 8; // 静态成员内部类 static class Brain &#123; static int age = 8; public Brain() &#123; &#125; public void think() &#123; System.out.println(&quot;大脑想东西&quot;); &#125; public void show(int age) &#123; System.out.println(&quot;形参：&quot; + age); System.out.println(&quot;静态成员内部类的静态属性：&quot; + Brain.age); System.out.println(&quot;外部类的静态属性：&quot; + Person.age); &#125; &#125; // 非静态成员内部类 class Hand &#123; String name = &quot;内部手&quot;; public Hand() &#123; &#125; public void grasp() &#123; System.out.println(&quot;手抓东西&quot;); // 调用Person外部类的方法 Person.this.eat();// 等价于eat()，注意方法的生命周期 &#125; public void show(String name) &#123; System.out.println(&quot;形参：&quot; + name); System.out.println(&quot;非静态成员内部类的非静态属性：&quot; + this.name); System.out.println(&quot;外部类的非静态属性：&quot; + Person.this.name); &#125; &#125; static &#123; // 静态代码块内局部内部类 class AA &#123; &#125; &#125; &#123; // 非静态代码块内局部内部类 class BB &#123; &#125; &#125; public Person() &#123; // 构造器内局部内部类 class CC &#123; &#125; &#125; public static void method1() &#123; // 静态方法内局部内部类 class DD &#123; &#125; &#125; public void method() &#123; // 非静态方法内局部内部类 class EE &#123; &#125; &#125; public void eat() &#123; &#125;&#125; 匿名内部类： 匿名内部类不能定义任何静态成员、方法和类，只能创建匿名内部类的一个实例。一个匿名内部类一定是在 new 的后面，用其隐含实现一个接口或实现一个类。 格式： 特点： 匿名内部类必须继承父类或实现接口。 匿名内部类只能有一个对象。 匿名内部类对象只能使用多态形式引用 实例： 12345678910111213141516171819202122232425262728interface Product &#123; public double getPrice(); public String getName();&#125;public class AnonymousTest &#123; public void test(Product p) &#123; System.out.println(&quot;购买了一个&quot; + p.getName() + &quot;，花掉了&quot; + p.getPrice()); &#125; public static void main(String[] args) &#123; AnonymousTest ta = new AnonymousTest(); // 调用test方法时，需要传入一个Product参数， // 此处传入其匿名实现类的实例 ta.test(new Product() &#123; @Override public double getPrice() &#123; return 567.8; &#125; @Override public String getName() &#123; return &quot;AGP显卡&quot;; &#125; &#125;); &#125;&#125; 面试题： 1234567891011121314151617181920212223public class Test &#123; public Test() &#123; Inner s1 = new Inner(); s1.a = 10; Inner s2 = new Inner(); s2.a = 20; Test.Inner s3 = new Test.Inner(); System.out.println(s3.a); &#125; class Inner &#123; public int a = 5; &#125; public static void main(String[] args) &#123; Test t = new Test(); Inner r = t.new Inner(); System.out.println(r.a); &#125;&#125;输出结果：55 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"maven 的配置文件","slug":"maven-configfiles","date":"2021-01-23T02:30:26.000Z","updated":"2021-01-27T02:31:46.290Z","comments":true,"path":"2021/01/23/maven-configfiles/","link":"","permalink":"http://example.com/2021/01/23/maven-configfiles/","excerpt":"","text":"settings.xmlsettings.xml是Maven的全局配置文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Licensed to the Apache Software Foundation (ASF) under oneor more contributor license agreements. See the NOTICE filedistributed with this work for additional informationregarding copyright ownership. The ASF licenses this fileto you under the Apache License, Version 2.0 (the&quot;License&quot;); you may not use this file except in compliancewith the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing,software distributed under the License is distributed on an&quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANYKIND, either express or implied. See the License for thespecific language governing permissions and limitationsunder the License.--&gt;&lt;!-- | 官方文档：https://maven.apache.org/settings.html | | maven提供以下两种level的配置: | | 1. User Level. 当前用户独享的配置，通常在$&#123;user.home&#125;/.m2/settings.xml目录下。 | 可在CLI命令行中通过以下参数设置：-s /path/to/user/settings.xml | | 2. Global Level. 同一台计算机上的所有maven用户共享的全局配置。通常在$&#123;maven.home&#125;/conf/settings.xml目录下。 | 可在CLI命令行中通过以下参数设置：-gs /path/to/global/settings.xml | | 备注： | 优先级：User Level &gt; Global Level | 默认情况，$&#123;user.home&#125;/.m2目录下没有settings.xml文件，需手动复制$&#123;maven.home&#125;/conf/settings.xml。 |--&gt;&lt;!-- | This is the configuration file for Maven. It can be specified at two levels: | | 1. User Level. This settings.xml file provides configuration for a single user, | and is normally provided in $&#123;user.home&#125;/.m2/settings.xml. | | NOTE: This location can be overridden with the CLI option: | | -s /path/to/user/settings.xml | | 2. Global Level. This settings.xml file provides configuration for all Maven | users on a machine (assuming they&#x27;re all using the same Maven | installation). It&#x27;s normally provided in | $&#123;maven.conf&#125;/settings.xml. | | NOTE: This location can be overridden with the CLI option: | | -gs /path/to/global/settings.xml | | The sections in this sample file are intended to give you a running start at | getting the most out of your Maven installation. Where appropriate, the default | values (values used when the setting is not specified) are provided. | |--&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;!-- 本地仓库路径，默认值：$&#123;user.home&#125;/.m2/repository --&gt; &lt;!-- localRepositor y | The path to the local repository maven will use to store artifacts. | | Default: $&#123;user.home&#125;/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;localRepository&gt;D:\\Program Files\\Maven\\apache-maven-3.6.3-maven-repository&lt;/localRepository&gt; &lt;!-- 当maven需要输入值的时候，是否交由用户输入，默认为true；false情况下maven将根据使用配置信息进行填充。 --&gt; &lt;!-- interactiveMode | This will determine whether maven prompts you when it needs input. If set to false, | maven will use a sensible default value, perhaps based on some other setting, for | the parameter in question. | | Default: true &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; --&gt; &lt;!-- 是否支持联网进行artifact下载、部署等操作，默认false。 --&gt; &lt;!-- offline | Determines whether maven should attempt to connect to the network when executing a build. | This will have an effect on artifact downloads, artifact deployment, and others. | | Default: false &lt;offline&gt;false&lt;/offline&gt; --&gt; &lt;!-- | 搜索插件时，如果groupId没有显式提供时，则以此处配置的groupId为默认值， | 可以简单理解为默认导入这些groupId下的所有artifact(需要时才下载)。 | 默认情况下该列表包含了：org.apache.maven.plugins和org.codehaus.mojo。 | | 查看插件信息： | mvn help:describe -Dplugin=org.apache.maven.plugins:maven-compiler-plugin:3.5.1 -Ddetail |--&gt; &lt;!-- pluginGroups | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e. | when invoking a command line like &quot;mvn prefix:goal&quot;. Maven will automatically add the group identifiers | &quot;org.apache.maven.plugins&quot; and &quot;org.codehaus.mojo&quot; if these are not already contained in the list. |--&gt; &lt;pluginGroups&gt; &lt;!-- pluginGroup | Specifies a further group identifier to use for plugin lookup. | plugin 的 groupId &lt;pluginGroup&gt;com.your.plugins&lt;/pluginGroup&gt; --&gt; &lt;/pluginGroups&gt; &lt;!-- 用来配置不同的代理，多代理profiles可以应对笔记本或移动设备的工作环境：通过简单的设置profile id就可以很容易的更换整个代理配置。 --&gt; &lt;!-- proxies | This is a list of proxies which can be used on this machine to connect to the network. | Unless otherwise specified (by system property or command-line switch), the first proxy | specification in this list marked as active will be used. |--&gt; &lt;proxies&gt; &lt;!-- proxy | Specification for one proxy, to be used in connecting to the network. | | 代理元素包含配置代理时需要的信息 &lt;proxy&gt; | 代理的唯一定义符，用来区分不同的代理元素 &lt;id&gt;optional&lt;/id&gt; | 该代理是否是激活的那个。true则激活代理。当我们声明了一组代理，而某个时候只需要激活一个代理的时候，该元素就可以派上用处。 &lt;active&gt;true&lt;/active&gt; | 代理的协议 &lt;protocol&gt;http&lt;/protocol&gt; | 代理服务器认证的登录名 &lt;username&gt;proxyuser&lt;/username&gt; | 代理服务器认证登录密码 &lt;password&gt;proxypass&lt;/password&gt; | 代理的主机名 &lt;host&gt;proxy.host.net&lt;/host&gt; | 代理的端口 &lt;port&gt;80&lt;/port&gt; | 不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，使用逗号分隔也很常见。 &lt;nonProxyHosts&gt;local.net|some.host.com&lt;/nonProxyHosts&gt; &lt;/proxy&gt; --&gt; &lt;/proxies&gt; &lt;!-- 进行远程服务器访问时所需的授权配置信息。通过系统唯一的server-id进行唯一关联。 --&gt; &lt;!-- servers | This is a list of authentication profiles, keyed by the server-id used within the system. | Authentication profiles can be used whenever maven must make a connection to a remote server. |--&gt; &lt;servers&gt; &lt;!-- server | Specifies the authentication information to use when connecting to a particular server, identified by | a unique name within the system (referred to by the &#x27;id&#x27; attribute below). | | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are | used together. | | 方式一：使用用户名和密码 &lt;server&gt; | 当前server的id，该id与distributionManagement中repository元素的id相匹配。 &lt;id&gt;deploymentRepo&lt;/id&gt; | 鉴权用户名 &lt;username&gt;repouser&lt;/username&gt; | 鉴权密码 &lt;password&gt;repopwd&lt;/password&gt; &lt;/server&gt; --&gt; &lt;!-- Another sample, using keys to authenticate. | 方式二：使用私钥 &lt;server&gt; &lt;id&gt;siteServer&lt;/id&gt; | 鉴权时使用的私钥位置，默认是/home/hudson/.ssh/id_dsa。 &lt;privateKey&gt;/path/to/private/key&lt;/privateKey&gt; | 鉴权时使用的私钥密码，非必要，非必要时留空。 &lt;passphrase&gt;optional; leave empty if not used.&lt;/passphrase&gt; &lt;/server&gt; --&gt; &lt;!-- 实例：对应pom.xml文件中配置的id为ChemAxon Public Repository的仓库。 --&gt; &lt;server&gt; &lt;id&gt;ChemAxon Public Repository&lt;/id&gt; &lt;username&gt;huxiongfeng95@gmail.com&lt;/username&gt; &lt;password&gt;AKCp5dL3HsJftZjXR4wLS7UMnJvQL7oarx8sad8Wh21UV7xQUMmNcZ7TMEHaBVoSrM8jAv48Q&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;!-- | 从远程仓库下载artifacts时，用于替代指定远程仓库的镜像服务器配置； | 例如当无法连接上国外的仓库时，可以指定连接到国内的镜像服务器； | 私服的配置推荐用profile配置而不是mirror。 |--&gt; &lt;!-- mirrors | This is a list of mirrors to be used in downloading artifacts from remote repositories. | | It works like this: a POM may declare a repository to use in resolving certain artifacts. | However, this repository may have problems with heavy traffic at times, so people have mirrored | it to several places. | | That repository definition will have a unique id, so we can create a mirror reference for that | repository, to be used as an alternate download site. The mirror site will be the preferred | server for that repository. |--&gt; &lt;mirrors&gt; &lt;!-- | mirrors匹配顺序： | 多个mirror优先级：按照id字母顺序进行排列，即与编写的顺序无关。 | 在第一个mirror找不到artifact，不会继续查找下一个镜像。 | 只有当前一个mirror无法链接的时候，才会尝试链接下一个镜像，类似容灾备份。 |--&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;!-- maven中央仓库的aliyun镜像，maven中央仓库的id为central。 --&gt; &lt;mirror&gt; &lt;!-- 当前镜像的唯一标识符，id用来区分不同的mirror元素，同时会套用使用server中id相同授权配置链接到镜像。 --&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;!-- 镜像名称，无特殊作用，可视为简述。 --&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;!-- 镜像地址 --&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;!-- 被镜像的服务器的id，必须与repository节点设置的id一致。但是&quot;This must not match the mirror id&quot;。 | mirrorOf 的配置语法: | * = 匹配所有远程仓库。这样所有pom中定义的仓库都不生效。 | external:* = 匹配除localhost、使用file://协议外的所有远程仓库。 | repo1,repo2 = 匹配仓库repo1和repo2。 | *,!repo1 = 匹配所有远程仓库，repo1除外。 |--&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;!-- profiles | This is a list of profiles which can be activated in a variety of ways, and which can modify | the build process. Profiles provided in the settings.xml are intended to provide local machine- | specific paths and repository locations which allow the build to work in the local environment. | | For example, if you have an integration testing plugin - like cactus - that needs to know where | your Tomcat instance is installed, you can provide a variable here such that the variable is | dereferenced during the build process to configure the cactus plugin. | | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles | section of this document (settings.xml) - will be discussed later. Another way essentially | relies on the detection of a system property, either matching a particular value for the property, | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a | value of &#x27;1.4&#x27; might activate a profile when the build is executed on a JDK version of &#x27;1.4.2_07&#x27;. | Finally, the list of active profiles can be specified directly from the command line. | | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact | repositories, plugin repositories, and free-form properties to be used as configuration | variables for plugins in the POM. | |--&gt; &lt;profiles&gt; &lt;!-- profile | Specifies a set of introductions to the build process, to be activated using one or more of the | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles/&gt; | or the command line, profiles have to have an ID that is unique. | | An encouraged best practice for profile identification is to use a consistent naming convention | for profiles, such as &#x27;env-dev&#x27;, &#x27;env-test&#x27;, &#x27;env-production&#x27;, &#x27;user-jdcasey&#x27;, &#x27;user-brett&#x27;, etc. | This will make it more intuitive to understand what the set of introduced profiles is attempting | to accomplish, particularly when you only have a list of profile id&#x27;s for debug. | | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo. &lt;profile&gt; &lt;id&gt;jdk-1.4&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.4&lt;/jdk&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jdk14&lt;/id&gt; &lt;name&gt;Repository for JDK 1.4 builds&lt;/name&gt; &lt;url&gt;http://www.myhost.com/maven/jdk14&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshotPolicy&gt;always&lt;/snapshotPolicy&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; --&gt; &lt;!-- | Here is another profile, activated by the system property &#x27;target-env&#x27; with a value of &#x27;dev&#x27;, | which provides a specific path to the Tomcat instance. To use this, your plugin configuration | might hypothetically look like: | | ... | &lt;plugin&gt; | &lt;groupId&gt;org.myco.myplugins&lt;/groupId&gt; | &lt;artifactId&gt;myplugin&lt;/artifactId&gt; | | &lt;configuration&gt; | &lt;tomcatLocation&gt;$&#123;tomcatPath&#125;&lt;/tomcatLocation&gt; | &lt;/configuration&gt; | &lt;/plugin&gt; | ... | | NOTE: If you just wanted to inject this configuration whenever someone set &#x27;target-env&#x27; to | anything, you could just leave off the &lt;value/&gt; inside the activation-property. | &lt;profile&gt; &lt;id&gt;env-dev&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;target-env&lt;/name&gt; &lt;value&gt;dev&lt;/value&gt; &lt;/property&gt; &lt;/activation&gt; &lt;properties&gt; &lt;tomcatPath&gt;/path/to/tomcat/instance&lt;/tomcatPath&gt; &lt;/properties&gt; &lt;/profile&gt; --&gt; &lt;/profiles&gt; &lt;!-- | 手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 | 任何activeProfile，不论环境设置如何，其对应的profile都会被激活，maven会忽略无效(找不到)的profile。 |--&gt; &lt;!-- activeProfiles | List of profiles that are active for all builds. | &lt;activeProfiles&gt; &lt;activeProfile&gt;alwaysActiveProfile&lt;/activeProfile&gt; &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;/activeProfile&gt; &lt;/activeProfiles&gt; --&gt;&lt;/settings&gt; 关于 profiles 节点的详解： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184&lt;!-- | 构建方法的配置清单，maven将根据不同环境参数来使用这些构建配置。 | settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。 | settings.xml负责的是整体的构建过程，pom.xml负责单独的项目对象构建过程。 | settings.xml只包含了id，activation，repositories，pluginRepositories和properties元素。 | | 如果settings.xml中的profile被激活，它的值会覆盖任何其它定义在pom.xml中或profile.xml中的相同id的profile。 | | 查看当前激活的profile: | mvn help:active-profiles |--&gt;&lt;profiles&gt; &lt;profile&gt; &lt;!-- 该配置的唯一标识符 --&gt; &lt;id&gt;profile_id&lt;/id&gt; &lt;!-- | profile的激活条件配置。 | 除此之外的其他激活方式： | 1. 通过settings.xml文件中的activeProfile元素进行指定激活。 | 2. 在命令行，使用-P标记和逗号分隔的列表来显式的激活，如：mvn clean package -P myProfile |--&gt; &lt;activation&gt; &lt;!-- 是否默认激活 --&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!-- 内建的java版本检测，匹配规则：https://maven.apache.org/enforcer/enforcer-rules/versionRanges.html --&gt; &lt;jdk&gt;9.9&lt;/jdk&gt; &lt;!-- 内建操作系统属性检测， 配置规则：https://maven.apache.org/enforcer/enforcer-rules/requireOS.html --&gt; &lt;os&gt; &lt;!-- 操作系统 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!-- 操作系统家族 --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!-- 操作系统 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!-- 操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!-- | 如果maven检测到某一个属性(其值可以在POM中通过$&#123;名称&#125;引用)，并且其拥有对应的名称和值，Profile就会被激活。 | 如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段。 |--&gt; &lt;property&gt; &lt;!-- 属性名 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!-- 属性值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!-- 根据文件存在/不存在激活profile --&gt; &lt;file&gt; &lt;!-- 如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/path/to/active_on_exists&lt;/exists&gt; &lt;!-- 如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/path/to/active_on_missing&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!-- 扩展属性设置。扩展属性可以在POM中的任何地方通过$&#123;扩展属性名&#125;进行引用。 | | 属性引用方式(包括扩展属性，共5种属性可以引用)： | | env.x：引用shell环境变量，例如，&quot;env.PATH&quot;指代了$path环境变量(在Linux/Windows上是%PATH%)。 | project.x：引用pom.xml(根元素是project)中xml元素内容。例如$&#123;project.artifactId&#125;可以获取pom.xml中设置的&lt;artifactId /&gt;元素的内容。 | settings.x：引用setting.xml(根元素是setting)中xml元素内容，例如$&#123;settings.offline&#125;。 | Java System Properties：所有可通过java.lang.System.getProperties()访问的属性都能在通过$&#123;property_name&#125;访问，例如$&#123;java.home&#125;。 | x：在&lt;properties/&gt;或者外部文件中设置的属性，都可以$&#123;someVar&#125;的形式使用。 | |--&gt; &lt;properties&gt; &lt;!-- 在当前profile被激活时，$&#123;profile.property&#125;就可以被访问到了。 --&gt; &lt;profile.property&gt;this.property.is.accessible.when.current.profile.actived&lt;/profile.property&gt; &lt;/properties&gt; &lt;!-- 远程仓库列表，settings.xml中的repositories不被直接支持，需要在profiles中配置。 --&gt; &lt;repositories&gt; &lt;!-- | releases vs snapshots | maven针对releases和snapshots有不同的处理策略，POM可以在每个单独的仓库中，为每种类型的artifact采取不同的策略。 | 例如： | 开发环境使用snapshots模式实时获取最新的快照版本进行构建 | 生成环境使用releases模式获取稳定版本进行构建 | 参见repositories/repository/releases元素。 |--&gt; &lt;!-- | 依赖包不更新问题： | 1. maven在下载依赖失败后会生成一个.lastUpdated为后缀的文件。如果这个文件存在，那么即使换一个有资源的仓库后， | maven依然不会去下载新资源。可以通过-U参数进行强制更新、手动删除.lastUpdated 文件： | find . -type f -name &quot;*.lastUpdated&quot; -exec echo &#123;&#125;&quot; found and deleted&quot; \\; -exec rm -f &#123;&#125; \\; | | 2. updatePolicy设置更新频率不对，导致没有触发maven检查本地artifact与远程artifact是否一致。 |--&gt; &lt;repository&gt; &lt;!-- 远程仓库唯一标识 --&gt; &lt;id&gt;maven_repository_id&lt;/id&gt; &lt;!-- 远程仓库名称 --&gt; &lt;name&gt;maven_repository_name&lt;/name&gt; &lt;!-- 远程仓库URL，按protocol://hostname/path形式。 --&gt; &lt;url&gt;http://host/maven&lt;/url&gt; &lt;!-- | 用于定位和排序artifact的仓库布局类型-可以是default(默认)或者legacy(遗留)。 | Maven2为其仓库提供了一个默认的布局；然而，Maven1.x有一种不同的布局。 | 我们可以使用该元素指定布局是default(默认)还是legacy(遗留)。 | --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;!-- 如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!-- 是否允许该仓库为artifact提供releases下载功能 --&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;!-- | 每次执行构建命令时，Maven会比较本地POM和远程POM的时间戳，该元素指定比较的频率。 | 有效选项是： | always ：每次构建都检查 | daily ：默认，距上次构建检查时间超过一天 | interval: x ：距上次构建检查超过x分钟 | never从不 ：从不 | | 重要： | 设置为daily时，如果artifact一天更新了几次，在一天之内进行构建，也不会从仓库中重新获取最新版本。 |--&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!-- 当maven验证artifact校验文件失败时该怎么做：ignore(忽略)，fail(失败)，或者warn(警告)。 --&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载 --&gt; &lt;snapshots&gt; &lt;!-- 是否允许该仓库为artifact提供snapshots下载功能 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;!-- | 国内可用的maven仓库地址(updated @ 2019-02-08)： | http://maven.aliyun.com/nexus/content/groups/public | http://maven.wso2.org/nexus/content/groups/public/ | http://jcenter.bintray.com/ | http://maven.springframework.org/release/ | http://repository.jboss.com/maven2/ | http://uk.maven.org/maven2/ | http://repo1.maven.org/maven2/ | http://maven.springframework.org/milestone | http://maven.jeecg.org/nexus/content/repositories/ | http://repo.maven.apache.org/maven2 | http://repo.spring.io/release/ | http://repo.spring.io/snapshot/ | http://mavensync.zkoss.org/maven2/ | https://repository.apache.org/content/groups/public/ | https://repository.jboss.org/nexus/content/repositories/releases/ |--&gt; &lt;/repositories&gt; &lt;!-- | maven插件的远程仓库配置。maven插件实际上是一种特殊类型的artifact。 | 插件仓库独立于artifact仓库。pluginRepositories元素的结构和repositories元素的结构类似。 |--&gt; &lt;!-- &lt;pluginRepositories&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; --&gt; &lt;/profile&gt;&lt;/profiles&gt; pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;!-- 父项目的坐标。如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值。 坐标包括groupID，artifactID和version。 --&gt; &lt;parent&gt; &lt;!-- 被继承的父项目的构件标识符 --&gt; &lt;artifactId /&gt; &lt;!-- 被继承的父项目的全球唯一标识符 --&gt; &lt;groupId /&gt; &lt;!-- 被继承的父项目的版本 --&gt; &lt;version /&gt; &lt;!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是：../pom.xml。 Maven首先在构建当前项目的地方寻找父项目的pom，其次在文件系统的这个位置(relativePath位置)， 然后在本地仓库，最后在远程仓库寻找父项目的pom。 --&gt; &lt;relativePath /&gt; &lt;/parent&gt; &lt;!-- 声明项目描述符遵循哪一个POM模型版本。模型本身的版本很少改变，虽然如此，但它仍然是必不可少的， 这是为了当Maven引入了新的特性或者其他模型变更的时候，确保稳定性。 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 项目的全球唯一标识符，通常使用全限定的包名区分该项目和其他项目。 并且构建时生成的路径也是由此生成，如com.mycompany.app生成的相对路径为：/com/mycompany/app --&gt; &lt;groupId&gt;asia.banseon&lt;/groupId&gt; &lt;!-- 构件的标识符，它和groupID一起唯一标识一个构件。换句话说，你不能有两个不同的项目拥有同样的artifactID和groupID； 在某个特定的groupID下，artifactID也必须是唯一的。 构件是项目产生的或使用的一个东西，Maven为项目产生的构件包括：JARs，源码，二进制发布和WARs等。 --&gt; &lt;artifactId&gt;banseon-maven2&lt;/artifactId&gt; &lt;!-- 项目产生的构件类型，例如jar、war、ear、pom。插件可以创建他们自己的构件类型，所以前面列的不是全部构件类型 --&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;!-- 项目当前版本，格式为：主版本.次版本.增量版本-限定版本号 --&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!-- 项目的名称，Maven产生的文档用 --&gt; &lt;name&gt;banseon-maven&lt;/name&gt; &lt;!-- 项目主页的URL，Maven产生的文档用 --&gt; &lt;url&gt;http://www.baidu.com/banseon&lt;/url&gt; &lt;!-- 项目的详细描述，Maven产生的文档用。当这个元素能够用HTML格式描述时(例如，CDATA中的文本会被解析器忽略， 就可以包含HTML标签)，不鼓励使用纯文本描述。如果你需要修改产生的web站点的索引页面， 你应该修改你自己的索引页文件，而不是调整这里的文档。 --&gt; &lt;description&gt;A maven project to study maven.&lt;/description&gt; &lt;!-- 描述了这个项目构建环境中的前提条件。 --&gt; &lt;prerequisites&gt; &lt;!-- 构建该项目或使用该插件所需要的Maven的最低版本 --&gt; &lt;maven /&gt; &lt;/prerequisites&gt; &lt;!-- 项目的问题管理系统(Bugzilla，Jira，Scarab，或任何你喜欢的问题管理系统)的名称和URL，本例为jira --&gt; &lt;issueManagement&gt; &lt;!-- 问题管理系统(例如jira)的名字 --&gt; &lt;system&gt;jira&lt;/system&gt; &lt;!-- 该项目使用的问题管理系统的URL --&gt; &lt;url&gt;http://jira.xxxx.com/xxxx&lt;/url&gt; &lt;/issueManagement&gt; &lt;!-- 项目持续集成信息 --&gt; &lt;ciManagement&gt; &lt;!-- 持续集成系统的名字，例如continuum --&gt; &lt;system /&gt; &lt;!-- 该项目使用的持续集成系统的URL(如果持续集成系统有web接口的话) --&gt; &lt;url /&gt; &lt;!-- 构建完成时，需要通知的开发者/用户的配置项。包括被通知者信息和通知条件(错误，失败，成功，警告) --&gt; &lt;notifiers&gt; &lt;!-- 配置一种方式，当构建中断时，以该方式通知用户/开发者 --&gt; &lt;notifier&gt; &lt;!-- 传送通知的途径 --&gt; &lt;type /&gt; &lt;!-- 发生错误时是否通知 --&gt; &lt;sendOnError /&gt; &lt;!-- 构建失败时是否通知 --&gt; &lt;sendOnFailure /&gt; &lt;!-- 构建成功时是否通知 --&gt; &lt;sendOnSuccess /&gt; &lt;!-- 发生警告时是否通知 --&gt; &lt;sendOnWarning /&gt; &lt;!-- 不赞成使用。通知发送到哪里 --&gt; &lt;address /&gt; &lt;!-- 扩展配置项 --&gt; &lt;configuration /&gt; &lt;/notifier&gt; &lt;/notifiers&gt; &lt;/ciManagement&gt; &lt;!-- 项目创建年份，4位数字。当产生版权信息时需要使用这个值。 --&gt; &lt;inceptionYear /&gt; &lt;!-- 项目相关邮件列表信息 --&gt; &lt;mailingLists&gt; &lt;!-- 该元素描述了项目相关的所有邮件列表。自动产生的网站引用这些信息。 --&gt; &lt;mailingList&gt; &lt;!-- 邮件的名称 --&gt; &lt;name&gt;Demo&lt;/name&gt; &lt;!-- 发送邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;post&gt;Demo@126.com&lt;/post&gt; &lt;!-- 订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;subscribe&gt;Demo@126.com&lt;/subscribe&gt; &lt;!-- 取消订阅邮件的地址或链接，如果是邮件地址，创建文档时，mailto：链接会被自动创建 --&gt; &lt;unsubscribe&gt;Demo@126.com&lt;/unsubscribe&gt; &lt;!-- 你可以浏览邮件信息的URL --&gt; &lt;archive&gt;http://localhost:8080/demo/dev/&lt;/archive&gt; &lt;/mailingList&gt; &lt;/mailingLists&gt; &lt;!-- 项目开发者列表 --&gt; &lt;developers&gt; &lt;!-- 某个项目开发者的信息 --&gt; &lt;developer&gt; &lt;!-- SCM里项目开发者的唯一标识符 --&gt; &lt;id&gt;HELLO WORLD&lt;/id&gt; &lt;!-- 项目开发者的全名 --&gt; &lt;name&gt;youname&lt;/name&gt; &lt;!-- 项目开发者的email --&gt; &lt;email&gt;youname@qq.com&lt;/email&gt; &lt;!-- 项目开发者的主页的URL --&gt; &lt;url /&gt; &lt;!-- 项目开发者在项目中扮演的角色，角色元素描述了各种角色 --&gt; &lt;roles&gt; &lt;role&gt;Project Manager&lt;/role&gt; &lt;role&gt;Architect&lt;/role&gt; &lt;/roles&gt; &lt;!-- 项目开发者所属组织 --&gt; &lt;organization&gt;demo&lt;/organization&gt; &lt;!-- 项目开发者所属组织的URL --&gt; &lt;organizationUrl&gt;http://www.xxx.com/&lt;/organizationUrl&gt; &lt;!-- 项目开发者属性，如即时消息如何处理等 --&gt; &lt;properties&gt; &lt;dept&gt;No&lt;/dept&gt; &lt;/properties&gt; &lt;!-- 项目开发者所在时区， -11到12范围内的整数。 --&gt; &lt;timezone&gt;+8&lt;/timezone&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;!-- 项目的其他贡献者列表 --&gt; &lt;contributors&gt; &lt;!-- 项目的其他贡献者。参见developers/developer元素 --&gt; &lt;contributor&gt; &lt;name /&gt; &lt;email /&gt; &lt;url /&gt; &lt;organization /&gt; &lt;organizationUrl /&gt; &lt;roles /&gt; &lt;timezone /&gt; &lt;properties /&gt; &lt;/contributor&gt; &lt;/contributors&gt; &lt;!-- 该元素描述了项目所有license列表。应该只列出该项目的license列表，不要列出依赖项目的license列表。 如果列出多个license，用户可以选择它们中的一个而不是接受所有license。 --&gt; &lt;licenses&gt; &lt;!-- 描述了项目的license，用于生成项目的web站点的license页面，其他一些报表和validation也会用到该元素。 --&gt; &lt;license&gt; &lt;!-- license用于法律上的名称 --&gt; &lt;name&gt;Apache 2&lt;/name&gt; &lt;!-- 官方的license正文页面的URL --&gt; &lt;url&gt;http://www.xxxx.com/LICENSE-2.0.txt&lt;/url&gt; &lt;!-- 项目分发的主要方式：repo，可以从Maven库下载manual，用户必须手动下载和安装依赖 --&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;!-- 关于license的补充信息 --&gt; &lt;comments&gt;A business-friendly OSS license&lt;/comments&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;!-- SCM(Source Control Management)标签允许你配置你的代码库，供Maven web站点和其它插件使用。 --&gt; &lt;scm&gt; &lt;!-- SCM的URL，该URL描述了版本库和如何连接到版本库。欲知详情，请看SCMs提供的URL格式和列表。该连接只读。 --&gt; &lt;connection&gt; scm:svn:http://svn.xxxx.com/maven/xxxxx-maven2-trunk(dao-trunk) &lt;/connection&gt; &lt;!-- 给开发者使用的，类似connection元素。即该连接不仅仅只读。 --&gt; &lt;developerConnection&gt; scm:svn:http://svn.xxxx.com/maven/dao-trunk &lt;/developerConnection&gt; &lt;!-- 当前代码的标签，在开发阶段默认为HEAD --&gt; &lt;tag /&gt; &lt;!-- 指向项目的可浏览SCM库(例如ViewVC或者Fisheye)的URL。 --&gt; &lt;url&gt;http://svn.xxxxx.com/&lt;/url&gt; &lt;/scm&gt; &lt;!-- 描述项目所属组织的各种属性。Maven产生的文档用。 --&gt; &lt;organization&gt; &lt;!-- 组织的全名 --&gt; &lt;name&gt;demo&lt;/name&gt; &lt;!-- 组织主页的URL --&gt; &lt;url&gt;http://www.xxxxxx.com/&lt;/url&gt; &lt;/organization&gt; &lt;!-- 构建项目需要的信息 --&gt; &lt;build&gt; &lt;!-- 该元素设置了项目源码目录 当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;sourceDirectory /&gt; &lt;!-- 该元素设置了项目脚本源码目录 该目录和源码目录不同：绝大多数情况下，该目录下的内容会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。 --&gt; &lt;scriptSourceDirectory /&gt; &lt;!-- 该元素设置了项目单元测试使用的源码目录 当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;testSourceDirectory /&gt; &lt;!-- 被编译过的应用程序class文件存放的目录。 --&gt; &lt;outputDirectory /&gt; &lt;!-- 被编译过的测试class文件存放的目录。 --&gt; &lt;testOutputDirectory /&gt; &lt;!-- 使用来自该项目的一系列构建扩展 --&gt; &lt;extensions&gt; &lt;!-- 描述使用到的构建扩展。 --&gt; &lt;extension&gt; &lt;!-- 构建扩展的groupId --&gt; &lt;groupId /&gt; &lt;!-- 构建扩展的artifactId --&gt; &lt;artifactId /&gt; &lt;!-- 构建扩展的版本 --&gt; &lt;version /&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;!-- 当项目没有规定目标(Maven2叫做阶段)时的默认值 --&gt; &lt;defaultGoal /&gt; &lt;!-- 这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。 --&gt; &lt;resources&gt; &lt;!-- 这个元素描述了项目相关或测试相关的所有资源路径 --&gt; &lt;resource&gt; &lt;!-- 描述了资源的目标路径。该路径相对target/classes目录(例如$&#123;project.build.outputDirectory&#125;)。 举个例子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为: org/apache/maven/messages。然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。 --&gt; &lt;targetPath /&gt; &lt;!-- 是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。 --&gt; &lt;filtering /&gt; &lt;!-- 描述存放资源的目录，该路径相对POM路径 --&gt; &lt;directory /&gt; &lt;!-- 包含的模式列表，例如：**/*.xml --&gt; &lt;includes /&gt; &lt;!-- 排除的模式列表，例如：**/*.xml --&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;!-- 这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。 --&gt; &lt;testResources&gt; &lt;!-- 这个元素描述了测试相关的所有资源路径，参见build/resources/resource元素的说明 --&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;!-- 构建产生的所有文件存放的目录 --&gt; &lt;directory /&gt; &lt;!-- 产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。 --&gt; &lt;finalName /&gt; &lt;!-- 当filtering开关打开时，使用到的过滤器属性文件列表。 --&gt; &lt;filters /&gt; &lt;!-- 子项目可以引用的默认插件信息。该插件配置项直到被引用时才会被解析或绑定到生命周期。 给定插件的任何本地配置都会覆盖这里的配置。 --&gt; &lt;pluginManagement&gt; &lt;!-- 使用的插件列表 --&gt; &lt;plugins&gt; &lt;!-- plugin元素包含描述插件所需要的信息。 --&gt; &lt;plugin&gt; &lt;!-- 插件在仓库里的groupID --&gt; &lt;groupId /&gt; &lt;!-- 插件在仓库里的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 被使用的插件的版本(或版本范围) --&gt; &lt;version /&gt; &lt;!-- 是否从该插件下载Maven扩展，例如打包和类型处理器。 由于性能原因，只有在真需要下载时，该元素才被设置成enabled。 --&gt; &lt;extensions /&gt; &lt;!-- 在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --&gt; &lt;executions&gt; &lt;!-- execution元素包含了插件执行需要的信息 --&gt; &lt;execution&gt; &lt;!-- 执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --&gt; &lt;id /&gt; &lt;!-- 绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --&gt; &lt;phase /&gt; &lt;!-- 配置的执行目标 --&gt; &lt;goals /&gt; &lt;!-- 配置是否被传播到子POM --&gt; &lt;inherited /&gt; &lt;!-- 作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!-- 项目引入插件所需要的额外依赖 --&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!-- 作为DOM对象的配置 --&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!-- 使用的插件列表 --&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!-- 在列的项目构建profile，如果被激活，会修改构建处理。 --&gt; &lt;profiles&gt; &lt;!-- 根据环境参数或命令行参数激活某个构建处理 --&gt; &lt;profile&gt; &lt;!-- 构建配置的唯一标识符。即用于命令行激活，也用于在继承时合并具有相同标识符的profile。 --&gt; &lt;id /&gt; &lt;!-- 自动触发profile的条件逻辑。Activation是profile的开启钥匙。profile的力量来自于它， 能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。 activation元素并不是激活profile的唯一方式。 --&gt; &lt;activation&gt; &lt;!-- profile默认是否激活的标志 --&gt; &lt;activeByDefault /&gt; &lt;!-- 当匹配的jdk被检测到，profile被激活。 例如，&quot;1.4&quot;激活JDK1.4，1.4.0_2，而&quot;!1.4&quot;激活所有版本不是以1.4开头的JDK。 --&gt; &lt;jdk /&gt; &lt;!-- 当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!-- 激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!-- 激活profile的操作系统所属家族(如&quot;windows&quot;) --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!-- 激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x64&lt;/arch&gt; &lt;!-- 激活profile的操作系统版本 --&gt; &lt;version&gt;6.1.7100&lt;/version&gt; &lt;/os&gt; &lt;!-- 如果Maven检测到某一个属性(其值可以在POM中通过$&#123;名称&#125;引用)，其拥有对应的名称和值，Profile就会被激活。 如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段。 --&gt; &lt;property&gt; &lt;!-- 激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!-- 激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!-- 提供一个文件名，通过检测该文件的存在或不存在来激活profile。 exists：检查文件是否存在，如果存在则激活profile。 missing：检查文件是否存在，如果不存在则激活profile。 --&gt; &lt;file&gt; &lt;!-- 如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/usr/local/xxxx/xxxx-home/tomcat/maven-guide-zh-to-production/workspace/&lt;/exists&gt; &lt;!-- 如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/usr/local/xxxx/xxxx-home/tomcat/maven-guide-zh-to-production/workspace/&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!-- 构建项目所需要的信息。参见build元素。 --&gt; &lt;build&gt; &lt;defaultGoal /&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;directory /&gt; &lt;finalName /&gt; &lt;filters /&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;!-- 参见build/pluginManagement/plugins/plugin元素 --&gt; &lt;plugin&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;extensions /&gt; &lt;executions&gt; &lt;execution&gt; &lt;id /&gt; &lt;phase /&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals /&gt; &lt;inherited /&gt; &lt;configuration /&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!-- 模块(有时称作子项目)被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径。 --&gt; &lt;modules /&gt; &lt;!-- 发现依赖和扩展的远程仓库列表 --&gt; &lt;repositories&gt; &lt;!-- 参见repositories/repository元素 --&gt; &lt;repository&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!-- 发现插件的远程仓库列表，这些插件用于构建和报表 --&gt; &lt;pluginRepositories&gt; &lt;!-- 包含需要连接到远程插件仓库的信息。参见repositories/repository元素。 --&gt; &lt;pluginRepository&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!-- 该元素描述了项目相关的所有依赖。这些依赖组成了项目构建过程中的一个个环节。 它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。 --&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 不赞成使用。现在Maven忽略该元素。 --&gt; &lt;reports /&gt; &lt;!-- 该元素包括使用报表插件产生报表的规范。当用户执行&quot;mvn site&quot;，这些报表就会运行。 在页面导航栏能看到所有报表的链接。参见reporting 元素。 --&gt; &lt;reporting&gt;......&lt;/reporting&gt; &lt;!-- 参见dependencyManagement元素 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 参见distributionManagement元素 --&gt; &lt;distributionManagement&gt;......&lt;/distributionManagement&gt; &lt;!-- 参见properties元素 --&gt; &lt;properties /&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!-- 模块(有时称作子项目)被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径。 --&gt; &lt;modules /&gt; &lt;!-- 发现依赖和扩展的远程仓库列表，配置多个repository时，按顺序依次查找。 --&gt; &lt;repositories&gt; &lt;!-- 包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!-- 远程仓库唯一标识符。可以用来匹配在settings.xml文件里配置的远程仓库。 --&gt; &lt;id&gt;banseon-repository-proxy&lt;/id&gt; &lt;!-- 远程仓库名称 --&gt; &lt;name&gt;banseon-repository-proxy&lt;/name&gt; &lt;!-- 远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://10.10.10.123:8080/repository/&lt;/url&gt; &lt;!-- 用于定位和排序构件的仓库布局类型-可以是default(默认)或者legacy(遗留)。 Maven2为其仓库提供了一个默认的布局；然而，Maven1.x有一种不同的布局。 我们可以使用该元素指定布局是default(默认)还是legacy(遗留)。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;!-- 如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!-- true或者false表示该仓库是否为下载某种类型构件(发布版，快照版)开启。 --&gt; &lt;enabled /&gt; &lt;!-- 该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。 选项：always(一直)，daily(默认，每日)，interval：X(这里X是以分钟为单位的时间间隔)，或者never(从不)。 --&gt; &lt;updatePolicy /&gt; &lt;!-- 当Maven验证构件校验文件失败时该怎么做：ignore(忽略)，fail(失败)，或者warn(警告)。 --&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载。 有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。 例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素。 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!-- 发现插件的远程仓库列表，这些插件用于构建和报表。 --&gt; &lt;pluginRepositories&gt; &lt;!-- 包含需要连接到远程插件仓库的信息。参见repositories/repository元素。 --&gt; &lt;pluginRepository&gt;......&lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!-- 该元素描述了项目相关的所有依赖。这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。 要获取更多信息，请看项目依赖机制。 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!-- 依赖的groupID --&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;!-- 依赖的artifactID --&gt; &lt;artifactId&gt;maven-artifact&lt;/artifactId&gt; &lt;!-- 依赖的版本号。在Maven2里，也可以配置成版本号的范围。 --&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;!-- 依赖类型，默认类型是jar。它通常表示依赖的文件的扩展名，但也有例外。 一个类型可以被映射成另外一个扩展名或分类器。类型经常和使用的打包方式对应，尽管这也有例外。 一些类型的例子：jar，war，ejb-client和test-jar。 如果设置extensions为true，就可以在plugin里定义新的类型。所以前面的类型的例子不完整。 --&gt; &lt;type&gt;jar&lt;/type&gt; &lt;!-- 依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面。 例如，如果你想要构建两个单独的构件成JAR，一个使用Java 1.4编译器，另一个使用Java 6编译器， 你就可以使用分类器来生成两个单独的JAR构件。 --&gt; &lt;classifier&gt;&lt;/classifier&gt; &lt;!-- 依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 - compile： 默认范围，用于编译 - provided： 类似于编译，但支持你期待jdk或者容器提供，类似于classpath - runtime： 在执行时需要使用 - test： 用于test任务时使用 - system： 需要外在提供相应的元素。通过systemPath来取得 - systemPath： 仅用于范围为system。提供相应的路径 - optional： 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 --&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;!-- 仅供system范围使用。注意，不鼓励使用这个元素，并且在新的版本中该元素可能被覆盖掉。 该元素为依赖规定了文件系统上的路径。需要绝对路径而不是相对路径。 推荐使用属性匹配绝对路径，例如$&#123;java.home&#125;。 --&gt; &lt;systemPath&gt;&lt;/systemPath&gt; &lt;!-- 当计算传递依赖时，从依赖构件列表里，列出被排除的依赖构件集。 即告诉maven你只依赖指定的项目，不依赖项目的依赖。此元素主要用于解决版本冲突问题。 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;!-- 可选依赖，如果你在项目B中把C依赖声明为可选，则要在依赖于B的项目(例如项目A)中显式的引用对C的依赖。 可选依赖阻断依赖的传递性。 --&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 不赞成使用，现在Maven忽略该元素 --&gt; &lt;reports&gt;&lt;/reports&gt; &lt;!-- 该元素描述使用报表插件产生报表的规范。当用户执行&quot;mvn site&quot;，这些报表就会运行。在页面导航栏能看到所有报表的链接。 --&gt; &lt;reporting&gt; &lt;!-- true，则网站不包括默认的报表。这包括&quot;项目信息&quot;菜单中的报表。 --&gt; &lt;excludeDefaults /&gt; &lt;!-- 所有产生的报表存放到哪里。默认值是$&#123;project.build.directory&#125;/site。 --&gt; &lt;outputDirectory /&gt; &lt;!-- 使用的报表插件和他们的配置 --&gt; &lt;plugins&gt; &lt;!-- plugin元素包含描述报表插件需要的信息 --&gt; &lt;plugin&gt; &lt;!-- 报表插件在仓库里的groupID --&gt; &lt;groupId /&gt; &lt;!-- 报表插件在仓库里的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 被使用的报表插件的版本(或版本范围) --&gt; &lt;version /&gt; &lt;!-- 任何配置是否被传播到子项目 --&gt; &lt;inherited /&gt; &lt;!-- 报表插件的配置 --&gt; &lt;configuration /&gt; &lt;!-- 一组报表的多重规范，每个规范可能有不同的配置。一个规范(报表集)对应一个执行目标。 例如，有 1，2，3，4，5，6，7，8，9 个报表， 1，2，5 构成A报表集，对应一个执行目标， 2，5，8 构成B报表集，对应另一个执行目标。 --&gt; &lt;reportSets&gt; &lt;!-- 表示报表的一个集合，以及产生该集合的配置。 --&gt; &lt;reportSet&gt; &lt;!-- 报表集合的唯一标识符，POM继承时用到。 --&gt; &lt;id /&gt; &lt;!-- 产生报表集合时，被使用的报表的配置。 --&gt; &lt;configuration /&gt; &lt;!-- 配置是否被继承到子POMs --&gt; &lt;inherited /&gt; &lt;!-- 这个集合里使用到哪些报表 --&gt; &lt;reports /&gt; &lt;/reportSet&gt; &lt;/reportSets&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt; &lt;!-- 继承自该项目的所有子项目的默认依赖信息。 这部分的依赖信息不会被立即解析，而是当子项目声明一个依赖(必须描述groupID和artifactID信息)时，如果groupID 和artifactID以外的一些信息没有描述，则通过groupID和artifactID匹配到这里的依赖，并使用这里的依赖信息。 比如锁定子项目的一些依赖的版本时，即可在父项目中定义。 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 参见dependencies/dependency元素 --&gt; &lt;dependency&gt;......&lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 项目分发信息，在执行&quot;mvn deploy&quot;后表示要发布的位置。 有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。 --&gt; &lt;distributionManagement&gt; &lt;!-- 部署项目产生的构件到远程仓库需要的信息 --&gt; &lt;repository&gt; &lt;!-- 是分配给快照一个唯一的版本号(由时间戳和构建流水号)？还是每次都使用相同的版本号？ 参见repositories/repository元素 --&gt; &lt;uniqueVersion /&gt; &lt;id&gt;xxx-maven2&lt;/id&gt; &lt;name&gt;xxx maven2&lt;/name&gt; &lt;url&gt;file://$&#123;basedir&#125;/target/deploy&lt;/url&gt; &lt;layout /&gt; &lt;/repository&gt; &lt;!-- 构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库。 参见distributionManagement/repository元素 --&gt; &lt;snapshotRepository&gt; &lt;uniqueVersion /&gt; &lt;id&gt;xxx-maven2&lt;/id&gt; &lt;name&gt;xxx-maven2 Snapshot Repository&lt;/name&gt; &lt;url&gt;scp://svn.xxxx.com/xxx:/usr/local/maven-snapshot&lt;/url&gt; &lt;layout /&gt; &lt;/snapshotRepository&gt; &lt;!-- 部署项目的网站需要的信息 --&gt; &lt;site&gt; &lt;!-- 部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置 --&gt; &lt;id&gt;banseon-site&lt;/id&gt; &lt;!-- 部署位置的名称 --&gt; &lt;name&gt;business api website&lt;/name&gt; &lt;!-- 部署位置的URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;scp://svn.baidu.com/xxx:/var/www/localhost/web&lt;/url&gt; &lt;/site&gt; &lt;!-- 项目下载页面的URL。如果没有该元素，用户应该参考主页。 使用该元素的原因是：帮助定位那些不在仓库里的构件(由于license限制)。 --&gt; &lt;downloadUrl /&gt; &lt;!-- 如果构件有了新的groupID和artifactID(构件移到了新的位置)，这里列出构件的重定位信息。 --&gt; &lt;relocation&gt; &lt;!-- 构件新的groupID --&gt; &lt;groupId /&gt; &lt;!-- 构件新的artifactID --&gt; &lt;artifactId /&gt; &lt;!-- 构件新的版本号 --&gt; &lt;version /&gt; &lt;!-- 显示给用户的，关于移动的额外信息，例如原因。 --&gt; &lt;message /&gt; &lt;/relocation&gt; &lt;!-- 给出该构件在远程仓库的状态。不得在本地项目中设置该元素，因为这是工具自动更新的。有效的值有： - none： 默认 - converted： 仓库管理员从Maven1 POM转换过来 - partner： 直接从伙伴Maven 2仓库同步过来 - deployed： 从Maven 2实例部署 - verified： 被核实时正确的和最终的 --&gt; &lt;status /&gt; &lt;/distributionManagement&gt; &lt;!-- 以值替代名称，Properties可以在整个POM中使用，也可以作为触发条件(见settings.xml配置文件里activation元素的说明)。 格式是：&lt;name&gt;value&lt;/name&gt;。 --&gt; &lt;properties /&gt;&lt;/project&gt; 本文参考https://www.cnblogs.com/iceJava/p/10356309.html https://www.cnblogs.com/hongmoshui/p/10762272.html https://www.cnblogs.com/cxzdy/p/5126087.html 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"maven 仓库的配置方式以及依赖的下载顺序","slug":"maven-repository","date":"2021-01-19T08:17:18.000Z","updated":"2021-04-09T07:51:24.008Z","comments":true,"path":"2021/01/19/maven-repository/","link":"","permalink":"http://example.com/2021/01/19/maven-repository/","excerpt":"","text":"maven 仓库分为本地仓库和远程仓库，而远程仓库又分为 maven 中央仓库、其他远程仓库和私服 (私有服务器)。 maven 项目使用的仓库一般有如下几种方式： maven 中央仓库，这是默认的仓库。 镜像仓库，通过 sttings.xml 中的 settings.mirrors.mirror 配置。 全局 profile 仓库，通过 settings.xml 中的 settings.repositories.repository 配置。 项目仓库，通过 pom.xml 中的 project.repositories.repository 配置。 项目 profile 仓库，通过 pom.xml 中的 project.profiles.profile.repositories.repository 配置。 本地仓库。 如果所有仓库的配置都存在，那么依赖的搜索顺序也会变得异常复杂。 仓库的配置方式本地仓库maven 缺省的本地仓库地址为 ${user.home}/.m2/repository，也就是说，一个用户会对应的拥有一个本地仓库。 可以通过修改 ${user.home}/.m2/settings.xml，在 节点下添加配置： 1&lt;localRepository&gt;D:\\java\\maven-repo&lt;/localRepository&gt; 如果想让所有的用户使用统一的配置，那么可以修改 maven 主目录下的 setting.xml：${M2_HOME}/conf/setting.xml。 maven 中央仓库在 maven 安装目录的 lib 目录下，有一个 maven-model-builder-3.6.1.jar，里面的 org/apache/maven/model/pom-4.0.0.xml 文件定义了 maven 默认中央仓库的地址：https://repo.maven.apache.org/maven2，如下图所示： 1234567891011121314151617181920212223242526&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;updatePolicy&gt;never&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 一般使用阿里云镜像仓库代替默认的 maven 中央仓库，配置方式有两种： 第一种，全局配置 修改 ${M2_HOME}/conf/setting.xml 文件，在 节点下添加配置： 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;mirrors&gt; 修改全局配置后，所有使用此 maven 的工程都会生效。 &lt; mirrorOf&gt; 可以设置为哪个中央仓库做镜像，为名为 “central” 的中央仓库做镜像，写作 &lt; mirrorOf&gt;central&lt; /mirrorOf&gt;；为所有中央仓库做镜像，写作 &lt; mirrorOf&gt;*&lt; /mirrorOf&gt; (不建议)。maven 默认中央仓库的 id 为 central。id是唯一的。 第二种，局部配置 在需要使用阿里云镜像仓库的 maven 工程的 pom.xml 文件中添加： 123456789101112131415161718192021222324&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;aliyun&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun-plugin&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 修改局部配置后，只对当前工程有效。 私服 第一种，全局配置 修改 ${M2_HOME}/conf/setting.xml 文件，在 节点下添加配置： 1234567891011121314151617181920212223242526272829&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;matgene-nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-plugin&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 然后，在 节点下添加激活配置 (通过配置的 profile 的 id 标识进行激活)： 123&lt;activeProfiles&gt; &lt;activeProfile&gt;matgene-nexus&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 第二种，局部配置 在需要使用私服的 maven 工程的 pom.xml 文件中添加。 上传： settings.xml： 12345678910&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; pom.xml： 12345678910&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 下载： pom.xml： 123456789101112131415161718192021222324&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-plugin&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 在项目中，将私服地址更改为自己公司的实际地址。 依赖的下载顺序准备测试环境安装 jdk 和 maven。 使用如下命令创建测试项目： 1yes | mvn archetype:generate -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-webapp -DinteractiveMode=true -DgroupId=com.pollyduan -DartifactId=myweb -Dversion=1.0 -Dpackage=com.pollyduan 创建完成后，为了避免后续测试干扰，先执行一次 compile。 12cd mywebmvn compile 最后，修改 pom.xml 文件，将 junit 版本号改为 4.12 。我们要使用这个 jar 来测试依赖的搜索顺序。 默认情况首先确保 junit 4.12 不存在： 1rm -rf ~/.m2/repository/junit/junit/4.12 默认情况下没有配置任何仓库，也就是说，既没更改 $M2_HOME/conf/settings.xml，也没有添加 ~/.m2/settings.xml。 执行编译，查看日志中拉取 junit 的仓库。 1234mvn compile...Downloaded from central: https://repo.maven.apache.org/maven2/junit/junit/4.12/junit-4.12.pom (24 kB at 11 kB/s) 从显示的仓库 id 可以看出：默认是从 maven 中央仓库拉取的 jar。 配置镜像仓库 settings_mirror创建 ~/.m2/setttings.xml，配置 maven 中央仓库的镜像，如下： 123456789&lt;settings&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;settings_mirror&lt;/id&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt; 重新测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile 在日志中查看下载依赖的仓库： 1Downloaded from settings_mirror: https://maven.aliyun.com/repository/public/junit/junit/4.12/junit-4.12.pom (24 kB at 35 kB/s) 从显示的仓库 id 可以看出：是从 settings_mirror 中下载的 jar。 结论：settings_mirror 的优先级高于 central。 配置项目仓库 pom_repositories在 project 中的 pom.xml 文件中，增加如下配置： 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;pom_repositories&lt;/id&gt; &lt;name&gt;local&lt;/name&gt; &lt;url&gt;http://10.18.29.128/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;sapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 由于改变了 id 的名字，所以仓库地址无所谓，使用相同的地址也不影响测试。 执行测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile 在日志中查看下载依赖的仓库： 1Downloaded from pom_repositories: http://10.18.29.128/nexus/content/groups/public/junit/junit/4.12/junit-4.12.pom (24 kB at 95 kB/s) 从显示的仓库 id 可以看出：jar 是从 pom_repositories 中下载的。 结论：pom_repositories 优先级高于 settings_mirror。 配置全局 profile 仓库 settings_profile_repo在 ~/.m2/settings.xml 中 settings 的节点内增加： 123456789101112131415161718&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;s_profile&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;settings_profile_repo&lt;/id&gt; &lt;name&gt;netease&lt;/name&gt; &lt;url&gt;http://mirrors.163.com/maven/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 执行测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Ps_profile 在日志中查看下载依赖的仓库： 1Downloaded from settings_profile_repo: http://mirrors.163.com/maven/repository/maven-public/junit/junit/4.12/junit-4.12.pom (24 kB at 63 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：settings_profile_repo 优先级高于 pom_repositories 和 settings_mirror。 配置项目 profile 仓库 pom_profile_repo在 project 中的 pom.xml 文件中，增加如下配置： 123456789101112131415161718&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;p_profile&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;pom_profile_repo&lt;/id&gt; &lt;name&gt;local&lt;/name&gt; &lt;url&gt;http://10.18.29.128/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt;&lt;/profiles&gt; 执行测试： 123rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Ps_profile,p_profilemvn compile -Pp_profile,s_profile 在日志中查看下载依赖的仓库： 1Downloaded from settings_profile_repo: http://mirrors.163.com/maven/repository/maven-public/junit/junit/4.12/junit-4.12.pom (24 kB at 68 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：settings_profile_repo 优先级高于 pom_profile_repo。 进一步测试： 12rm -rf ~/.m2/repository/junit/junit/4.12mvn compile -Pp_profile 在日志中查看下载依赖的仓库： 1Downloaded from pom_profile_repo: http://10.18.29.128/nexus/content/groups/public/junit/junit/4.12/junit-4.12.pom (24 kB at 106 kB/s) 从显示的仓库 id 可以看出：jar 是从 settings_profile_repo 中下载的。 结论：pom_profile_repo 优先级高于 pom_repositories。 本地仓库 local_repo这不算测试了，只是一个结论，可以任意测试：只要 ~/.m2/repository 中包含依赖，无论怎么配置，都会优先使用 local 本地仓库中的 jar。 最终结论 settings_mirror 的优先级高于 central settings_profile_repo 优先级高于 settings_mirror settings_profile_repo 优先级高于 pom_repositories settings_profile_repo 优先级高于 pom_profile_repo pom_repositories 优先级高于 settings_mirror pom_profile_repo 优先级高于 pom_repositories 通过上面的比较，可以得出各种仓库完整的搜索顺序链： local_repo &gt; settings_profile_repo &gt; pom_profile_repo &gt; pom_repositories &gt; settings_mirror &gt; central 简单来说，查找依赖的顺序大致如下： 在本地仓库中寻找，如果没有则进入下一步。 在全局配置的私服仓库 (settings.xml 中配置的并被激活) 中寻找，如果没有则进入下一步。 在项目自身配置的私服仓库 (pom.xml) 中寻找，如果没有则进入下一步。 在中央仓库中寻找，如果没有则终止寻找。 说明： 如果在找寻的过程中，发现该仓库有镜像设置，则用镜像的地址代替，即假设现在进行到要在 respository A 仓库中查找某个依赖，但 A 仓库配置了 mirror，则会转到从 A 的 mirror 中查找该依赖，不会再从 A 中查找。 settings.xml 中配置的 profile (激活的) 下的 respository 优先级高于项目中 pom.xml 文件配置的 respository。 如果仓库的 id 设置成 “central”，则该仓库会覆盖 maven 默认的中央仓库配置。 本文参考https://blog.csdn.net/asdfsfsdgdfgh/article/details/96576665 https://www.cnblogs.com/default/p/11856188.html https://my.oschina.net/polly/blog/2120650 https://blog.csdn.net/fengdayuan/article/details/93089136 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"maven 构建分模块项目","slug":"maven-modules","date":"2021-01-12T09:17:19.000Z","updated":"2021-01-20T07:37:52.805Z","comments":true,"path":"2021/01/12/maven-modules/","link":"","permalink":"http://example.com/2021/01/12/maven-modules/","excerpt":"","text":"分模块构建 maven 工程分析在现实生活中，汽车厂家进行汽车生产时，由于整个生产过程非常复杂和繁琐，工作量非常大，所以车场都会将整个汽车的部件分开生产，最终再将生产好的部件进行组装，形成一台完整的汽车： 类似的，随着项目功能的增加，项目本身会变得越来越庞大，这个时候，代码的良好管理和规划就会变得很重要。为了提高效率，根据业务的不同将揉作一团的业务代码分离出来，业务划分上分割清晰，提高代码复用率，例如： 上述功能的实现就是代码这一层级的变动，可以采用多项目模式和多模块模式： 多项目：每个业务单独新建项目并编写相应逻辑 多模块：业务聚合在一个项目中的不同模块中，然后通过依赖调用实现业务逻辑 maven 工程的继承在 java 语言中，类之间是可以继承的，通过继承，子类就可以引用父类中非 private 的属性和方法。同样，在 maven 工程之间也可以继承，子工程继承父工程后，就可以使用在父工程中引入的依赖。继承的目的是为了消除重复代码。 maven 工程的聚合在 maven 工程的 pom.xml 文件中，可以使用 标签将其他 maven 工程聚合到一起，聚合的目的是为了进行统一操作。 例如，拆分后的 maven 工程有多个，如果要进行打包，就需要针对每个工程分别执行打包命令，操作起来非常繁琐。这时就可以使用 标签将这些工程统一聚合到 maven 工程中，需要打包的时候，只需要在此工程中执行一次打包命令，其下被聚合的工程就都会被打包了。 分模块构建 maven 工程构建父模块新建 maven 项目： 保留 pom.xml 文件，删除 src 目录： 构建子模块新建 module 1： 新建 module 2： 如果需要更多的模块，重复上述步骤。 父模块 pom 配置公用的 pom 配置，可以放在父模块的 pom.xml 文件中： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.matgene.reaction-extractor-assistant&lt;/groupId&gt; &lt;artifactId&gt;reaction-extractor-assistant&lt;/artifactId&gt; &lt;!-- parent必须使用pom格式打包并上传到仓库 --&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;patent-loader&lt;/module&gt; &lt;module&gt;consumer-log&lt;/module&gt; &lt;module&gt;consumer-reaction&lt;/module&gt; &lt;module&gt;consumer-timeout&lt;/module&gt; &lt;/modules&gt; &lt;!-- 全局版本管理 --&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.version&gt;3.8.1&lt;/maven.compiler.version&gt; &lt;maven.compiler.source&gt;$&#123;java.version&#125;&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;$&#123;java.version&#125;&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;!-- 全局依赖管理 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.danielwegener&lt;/groupId&gt; &lt;artifactId&gt;logback-kafka-appender&lt;/artifactId&gt; &lt;version&gt;0.2.0-RC1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.json-lib&lt;/groupId&gt; &lt;artifactId&gt;json-lib&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;classifier&gt;jdk15&lt;/classifier&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt; &lt;version&gt;5.6.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven.compiler.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;maven.compiler.source&#125;&lt;/source&gt; &lt;target&gt;$&#123;maven.compiler.target&#125;&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 子模块 pom 配置子模块单独使用的 pom 配置，放在子模块自己的 pom.xml 文件中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;reaction-extractor-assistant&lt;/artifactId&gt; &lt;groupId&gt;cn.matgene.reaction-extractor-assistant&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;patent-loader&lt;/artifactId&gt; &lt;properties&gt; &lt;app.main.class&gt;cn.matgene.patent.cn.matgene.patent.loader.PatentLoaderJob&lt;/app.main.class&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;configuration&gt; &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;manifestEntries&gt; &lt;Main-Class&gt;$&#123;app.main.class&#125;&lt;/Main-Class&gt; &lt;X-Compile-Source-JDK&gt;$&#123;maven.compiler.source&#125;&lt;/X-Compile-Source-JDK&gt; &lt;X-Compile-Target-JDK&gt;$&#123;maven.compiler.target&#125;&lt;/X-Compile-Target-JDK&gt; &lt;/manifestEntries&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 上面构建的项目比较简单，各模块之间不存在依赖关系，同时，因为每个模块都需要打包，因此把打包的插件放在每一个子模块的 pom.xml 文件中。 一个 spring web 项目的实例 父工程 maven_parent 构建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394 &lt;properties&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt; &lt;springmvc.version&gt;5.0.5.RELEASE&lt;/springmvc.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt;&lt;/properties&gt;&lt;!--锁定jar版本--&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- Mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- springMVC --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;springmvc.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 子工程 maven_pojo 构建 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_dao 构建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 &lt;dependencies&gt; &lt;!-- maven_pojo的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_pojo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis和mybatis与spring的整合 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- druid数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- junit测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_service 构建 12345678&lt;dependencies&gt; &lt;!-- maven_dao的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_dao&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 子工程 maven_web 构建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- maven_service的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;finalName&gt;maven_web&lt;/finalName&gt; &lt;pluginManagement&gt;&lt;!-- lock down plugins versions to avoid using Maven defaults (may be moved to parent pom) --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/plugin&gt; &lt;!-- see http://maven.apache.org/ref/current/maven-core/default-bindings.html#Plugin_bindings_for_war_packaging --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.22.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 项目整体结构如下： maven_parent 为父工程，其余工程为子工程，都继承父工程 maven_parent； maven_parent 工程将其子工程都进行了聚合 ； 子工程之间存在依赖关系，比如 maven_dao 依赖 maven_pojo，maven_service 依赖 maven_dao，maven_web 依赖 maven_service。 本文参考https://juejin.cn/post/6844903970024980488 https://www.cnblogs.com/tianlong/p/10552848.html 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"linux 常见错误","slug":"linux-error","date":"2021-01-11T06:49:11.000Z","updated":"2021-01-11T08:00:48.503Z","comments":true,"path":"2021/01/11/linux-error/","link":"","permalink":"http://example.com/2021/01/11/linux-error/","excerpt":"","text":"No space left on device有时候，在创建新文件，或者往磁盘写内容时，会提示 No space left on device 异常。 一般来说，linux 空间占满有如两种情况： 空间占满通过 df -h 命令，查看空间的使用情况： 1234567891011121314$ df -hFilesystem Size Used Avail Use% Mounted onudev 3.9G 0 3.9G 0% /devtmpfs 799M 82M 718M 11% /run/dev/mapper/lin--vg-root 491G 195G 272G 42% /tmpfs 3.9G 8.0K 3.9G 1% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 472M 58M 390M 13% /boottmpfs 799M 0 799M 0% /run/user/1000192.168.1.152:/mnt/chenlei 10T 5.0T 4.6T 53% /home/lin/share/storage_server_1192.168.1.236:/mnt 10T 9.0T 520G 95% /home/lin/share/storage_server_2192.168.1.106:/mnt 40T 24T 15T 63% /home/lin/share/storage_server_3192.168.1.102:/home/lin/share 491G 195G 272G 42% /tmp/share 可以看出，各分区仍有较大的空间能够使用。如果某个分区的使用率达到了 100%，那也就无法再创建新文件，也无法再写入内容，需要删除一些文件。 inode 占满通过 df -i 命令，查看 inode 的使用情况。 1234567891011121314$ df -ihFilesystem Inodes IUsed IFree IUse% Mounted onudev 993K 421 993K 1% /devtmpfs 998K 749 998K 1% /run/dev/mapper/lin--vg-root 32M 1.6M 30M 6% /tmpfs 998K 2 998K 1% /dev/shmtmpfs 998K 3 998K 1% /run/locktmpfs 998K 16 998K 1% /sys/fs/cgroup/dev/sda1 122K 303 122K 1% /boottmpfs 998K 4 998K 1% /run/user/1000192.168.1.152:/mnt/chenlei 320M 42M 279M 13% /home/lin/share/storage_server_1192.168.1.236:/mnt 320M 69M 252M 22% /home/lin/share/storage_server_2192.168.1.106:/mnt 640M 641M 0M 100% /home/lin/share/storage_server_3192.168.1.102:/home/lin/share 32M 1.6M 30M 6% /tmp/share 可以看出，每个分区都有一定大小的 inode 空间，但 /home/lin/share/storage_server_3 分区的 inode 空间使用率达到 100%。因此，再往此分区创建新文件或写入内容时，会提示 No space left on device 异常。 解决方法：将 /home/lin/share/storage_server_3 分区上一些不必要的文件删除。 理解 inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做 “扇区” (Sector)，每个扇区储存 512 字节 (相当于 0.5KB)。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个 “块” (Block)。这种由多个扇区组成的 “块”，是文件存取的最小单位。”块” 的大小，最常见的是 4 KB，即连续八个 Sector 组成一个 Block。 文件数据都储存在 “块” 中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做 inode，中文译名为 “索引节点”。 每一个文件都有对应的 inode，里面包含了与该文件有关的一些信息。 某些时候，尽管一个分区的磁盘占用率未满，但是 inode 已经用完，可能是因为该分区的目录下存在大量小文件导致。尽管小文件占用的磁盘空间并不大，但是数量太多，也会导致 inode 用尽。本例中就是因为 /home/lin/share/storage_server_3 分区存在大量的小文件。","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"Java 的日志处理","slug":"java-log","date":"2021-01-06T09:14:19.000Z","updated":"2021-04-09T08:01:06.092Z","comments":true,"path":"2021/01/06/java-log/","link":"","permalink":"http://example.com/2021/01/06/java-log/","excerpt":"","text":"常用日志处理工具常见的 log 日志处理工具有：log4j、Logging、commons-logging、slf4j、logback。其中，commons-loggin、slf4j 是一种日志抽象门面，不是具体的日志框架；log4j、logback 是具体的日志实现框架。 一般使用 slf4j + logback 处理日志，也可以使用 slf4j + log4j、commons-logging + log4j 这两种日志组合框架。 日志级别日志的输出都是分级别的，不同的场合设置不同的级别，以打印不同的日志。下面拿最普遍用的 log4j 日志框架来做个日志级别的说明，这个比较奇全，其他的日志框架也都大同小异。 log4j 的级别类 org.apache.log4j.Level 里面定义了日志级别，日志输出优先级由高到底分别为以下 8 种： 日志级别 描述 OFF 关闭：最高级别，不输出日志。 FATAL 致命：输出非常严重的可能会导致应用程序终止的错误。 ERROR 错误：输出错误，但应用还能继续运行。 WARN 警告：输出可能潜在的危险状况。 INFO 信息：输出应用运行过程的详细信息。 DEBUG 调试：输出更细致的对调试应用有用的信息。 TRACE 跟踪：输出更细致的程序运行轨迹。 ALL 所有：输出所有级别信息。 所以，日志优先级别标准顺序为： ALL &lt; TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL &lt; OFF 如果日志设置为 L ，一个级别为 P 的输出日志只有当 P &gt;= L 时日志才会输出。 即如果日志级别 L 设置 INFO，只有 P 的输出级别为 INFO、WARN，后面的日志才会正常输出。 具体的输出关系可以参考下图： LombokLombok 是一种 java 实用工具，可用来帮助开发人员消除 java 的冗长代码，尤其是对于简单的 java 对象 (POJO)。它通过注释实现这一目的。 引入IntelliJ 安装： Lombok 是侵入性很高的一个 library。 maven 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt;&lt;/dependency&gt; 注解说明常用注解： @Getter 和 @Setter 自动生成 getter 和 setter 方法。 @ToString 自动重写 toString() 方法，打印所有变量。也可以加其他参数，例如 @ToString(exclude=”id”) 排除 id 属性，或者 @ToString(callSuper=true, includeFieldNames=true) 调用父类的 toString() 方法，包含所有属性。 @EqualsAndHashCode 自动生成 equals(Object other) 和 hashcode() 方法，包括所有非静态变量和非 transient 的变量。 如果某些变量不想要加进判断，可以通过 exclude 排除，也可以使用 of 指定某些字段： java 中规定，当两个 object equals 时，它们的 hashcode 一定要相同，反之，当 hashcode 相同时，object 不一定 equals。所以 equals 和 hashcode 要一起 implement，免得出现违反 java 规定的情形。 @NoArgsConstructor，@AllArgsConstructor，@RequiredArgsConstructor 这三个很像，都是自动生成该类的 constructor，差別只在生成的 constructor 的参数不一样而已。 @NoArgsConstructor：生成一个沒有参数的 constructor。 在 java 中，如果沒有指定类的 constructor，java compiler 会自动生成一个无参构造器，但是如果自己写了 constructor 之后，java 就不会再自动生成无参构造器。但是，很多时候，无参构造器是必须的，因此，为避免不必要的麻烦，应在类上至少加上 @NoArgsConstrcutor。 @AllArgsConstructor ：生成一个包含所有参数的 constructor。 @RequiredArgsConstructor：生成一个包含 “特定参数” 的 constructor，特定参数指的是那些有加上 final 修饰词的变量。 如果所有的变量都沒有用 final 修饰，@RequiredArgsConstructor 会生成一个沒有参数的 constructor。 @Data 等于同时添加了以下注解：@Getter，@Setter，@ToString，@EqualsAndHashCode 和 @RequiredArgsConstructor。 @Value 把所有的变量都设成 final，其他的就跟 @Data 类似，等于同时添加了以下注解：@Getter，@ToString，@EqualsAndHashCode 和 @RequiredArgsConstructor。 @Builder 自动生成流式 set 值写法。 注意，虽然只要加上 @Builder 注解，我们就能用流式写法快速设定 Object 的值，但是 setter 还是不应该舍弃的，因为 Spring 或是其他框架，有很多地方都会用到 Object 的 getter/setter 方法来对属性取值/赋值。 所以，通常是 @Data 和 @Builder 会一起用在同个类上，既方便流式写 code，也方便框架做事。比如： 123456@Data@Builderpublic class User &#123; private Integer id; private String name;&#125; @Slf4j 自动生成该类的 log 静态常量，要打日志就可以直接打，不用再手动 new log 静态常量了。 除了 @Slf4j 之外，Lombok 也提供其他日志框架的几种注解，像是 @Log，@Log4j 等，他们都可以创建一个静态常量 log，只是使用的 library 不一样而已。 12345@Log // 对应的log语句如下private static final java.util.logging.Logger log = java.util.logging.Logger.getLogger(LogExample.class.getName());@Log4j // 对应的log语句如下private static final org.apache.log4j.Logger log = org.apache.log4j.Logger.getLogger(LogExample.class); 更多的参考：https://juejin.cn/post/6844903557016076302 Logback引入12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; logback 依赖中，含有对 slf4j 的依赖。 节点configuration 为主节点，其主要字节点如下： property定义变量值的标签，有两个属性，name 和 value，定义变量后，可以使 “${name}” 来使用变量。 1&lt;property name=&quot;logging.level&quot; value=&quot;info&quot;/&gt; appender日志打印的组件，定义打印过滤的条件、打印输出方式、滚动策略、编码方式、打印格式等。 种类： ConsoleAppender：把日志添加到控制台。 12345&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder charset=&quot;utf-8&quot;&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-6level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt; FileAppender：把日志添加到文件。 12345678910111213&lt;appender name=&quot;ReactionExtractorAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/reaction-extractor.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; RollingFileAppender：FileAppender 的子类，滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。 1234567891011121314&lt;appender name=&quot;ReactionExtractorRollingAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/reaction-extractork-%d&#123;yyyy-MM-dd&#125;.log&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; 属性： name：指定 appender 的名称。 class：指定 appender 的全限定名。 子节点： append：默认为 true，表示日志被追加到文件结尾，如果是 false，清空现存文件。 filter：过滤器，执行完一个过滤器后返回 DENY，NEUTRAL，ACCEPT 三个枚举值中的一个。 filter 的返回值含义： DENY：日志将立即被抛弃不再经过其他过滤器。 NEUTRAL：有序列表里的下个过滤器过接着处理日志。 ACCEPT：日志会被立即处理，不再经过剩余过滤器。 filter 的两种类型： ThresholdFilter：临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器返回 NEUTRAL，当日志级别低于临界值时，日志会被拒绝。 123&lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;INFO&lt;/level&gt;&lt;/filter&gt; LevelFilter：级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据 onMath (用于配置符合过滤条件的操作) 和 onMismatch (用于配置不符合过滤条件的操作) 接收或拒绝日志。 12345&lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; file：指定被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 rollingPolicy：滚动策略，只有 appender 的 class 是 RollingFileAppender 时才需要配置。 TimeBasedRollingPolicy：根据时间来制定滚动策略，既负责滚动也负责触发滚动。 12345678&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!-- 日志文件输出的文件名：按天回滚 daily --&gt; &lt;FileNamePattern&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log.%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; &lt;/FileNamePattern&gt; &lt;!-- 日志文件保留天数 --&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt;&lt;/rollingPolicy&gt; 每天生成一个日志文件，日志文件保存 30 天。 FixedWindowRollingPolicy：根据固定窗口算法重命名文件的滚动策略。 encoder：对记录事件进行格式化。主要作用是：把日志信息转换成字节数组，以及把字节数组写入到输出流。 12345&lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;!-- 格式化输出：%d表示日期；%thread表示线程名；%-5level：级别从左显示5个字符宽度；%logger&#123;50&#125; 表示logger名字最长50个字符，否则按照句点分割；%msg：日志消息；%n是换行符 --&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt;&lt;/encoder&gt; logger用来设置某一个包或者具体的某一个类的日志打印级别以及指定 appender。 属性： name：指定受此 logger 约束的某一个包或者具体的某一个类。 level：设置打印级别 (TRACE，DEBUG，INFO，WARN，ERROR，ALL 和 OFF)，还有一个值 INHERITED 或者同义词 NULL，代表强制执行上级的级别。如果没有设置此属性，那么当前 logger 将会继承上级的级别。 addtivity：设置是否向上级 logger 传递打印信息，默认为 true。 123&lt;logger name=&quot;com.glmapper.spring.boot.controller&quot; level=&quot;$&#123;logging.level&#125;&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;GLMAPPER-LOGGERONE&quot; /&gt;&lt;/logger&gt; com.glmapper.spring.boot.controller 这个包下的 ${logging.level} 级别的日志将会使用 GLMAPPER-LOGGERONE 来打印。 root根 logger，也是一种 logger，但只有一个 level 属性。 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187&lt;!-- 使用说明： 1. logback核心jar包：logback-core-1.2.3.jar，logback-classic-1.2.3.jar，slf4j-api-1.7.25.jar 1) logback官方建议配合slf4j使用 2) logback手动下载地址：https://repo1.maven.org/maven2/ch/qos/logback/ 3) slf4j手动下载地址：https://www.mvnjar.com/org.slf4j/slf4j-api/1.7.25/detail.html 4) jar包可以从maven仓库快速获取 2. logback分为3个组件：logback-core，logback-classic和logback-access 1) 其中logback-core提供了logback的核心功能，是另外两个组件的基础 2) logback-classic实现了slf4j的API，所以当想配合slf4j使用时，需要将logback-classic加入classpath 3) logback-access是为了集成servlet环境而准备的，可提供HTTP-access的日志接口 3. 配置中KafkaAppender的jar包：logback-kafka-appender-0.2.0-RC1.jar--&gt;&lt;!-- 参考： https://juejin.im/post/5b51f85c5188251af91a7525 https://my.oschina.net/Declan/blog/1793444--&gt;&lt;!-- 说明：logback.xml配置文件，需放置在项目的resources路径下 --&gt;&lt;!-- configuration属性： scan：热加载，当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true scanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟 debug：当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false packagingData：是否打印包的信息。默认值为false--&gt;&lt;configuration debug=&quot;false&quot; xmlns=&quot;http://ch.qos.logback/xml/ns/logback&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://ch.qos.logback/xml/ns/logback https://raw.githubusercontent.com/enricopulatzo/logback-XSD/master/src/main/xsd/logback.xsd&quot;&gt; &lt;!-- property：定义变量值，两个属性，name和value --&gt; &lt;property name=&quot;logging.path&quot; value=&quot;./&quot;/&gt; &lt;property name=&quot;logging.level&quot; value=&quot;INFO&quot;/&gt; &lt;!-- 日志格式化： %d：日期 %thread：线程名 %-5level：日志级别，从左显示5个字符宽度 %logger&#123;50&#125;：logger名字最长50个字符，超过的按照句点分割 %msg：日志消息 %n：换行符 %ex&#123;full, DISPLAY_EX_EVAL&#125;：异常信息，full表示全输出，可以替换为异常信息指定输出的行数 --&gt; &lt;property name=&quot;message.format&quot; value=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n%ex&#123;full, DISPLAY_EX_EVAL&#125;&quot;/&gt; &lt;!-- kafka topic --&gt; &lt;property name=&quot;topic.name&quot; value=&quot;log-collect&quot;/&gt; &lt;!-- 本地地址 --&gt; &lt;property name=&quot;bootstrap.servers&quot; value=&quot;192.168.1.71:9092&quot;/&gt; &lt;!-- 集群地址 --&gt; &lt;!-- &lt;property name=&quot;bootstrap.servers&quot; value=&quot;hadoopdatanode1:9092,hadoopdatanode2:9092,hadoopdatanode3:9092&quot;/&gt; --&gt; &lt;!-- appender种类： ConsoleAppender：把日志添加到控制台 FileAppender：把日志添加到文件 RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。FileAppender的子类 --&gt; &lt;!-- 控制台输出日志 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义输出日志到文件 --&gt; &lt;appender name=&quot;FileAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;!-- append：true，日志被追加到文件结尾；false，清空现存文件；默认是true --&gt; &lt;append&gt;true&lt;/append&gt; &lt;!-- 级别过滤器： ThresholdFilter：临界值过滤器，过滤掉低于指定临界值的日志 LevelFilter：级别过滤器，需配置onMatch和onMismatch --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/base.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义异常输出日志文件 --&gt; &lt;appender name=&quot;ErrorFileAppender&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/error-file.log &lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 自定义输出日志：滚动记录日志 --&gt; &lt;appender name=&quot;RollingFileAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;!-- 滚动策略：每天生成一个日志文件，保存365天的日志文件 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!-- 日志文件输出的文件名：按天回滚 daily --&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/reaction-log-%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;.log&lt;/FileNamePattern&gt; &lt;!-- 日志文件保留天数 --&gt; &lt;MaxHistory&gt;365&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 日志文件最大的大小 --&gt; &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;MaxFileSize&gt;50MB&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 输出日志到kafka，参考：https://github.com/danielwegener/logback-kafka-appender --&gt; &lt;appender name=&quot;KafkaAppender&quot; class=&quot;com.github.danielwegener.logback.kafka.KafkaAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;message.format&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;topic&gt;$&#123;topic.name&#125;&lt;/topic&gt; &lt;keyingStrategy class=&quot;com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy&quot;/&gt; &lt;deliveryStrategy class=&quot;com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy&quot;/&gt; &lt;!-- Optional parameter to use a fixed partition --&gt; &lt;!-- &lt;partition&gt;0&lt;/partition&gt; --&gt; &lt;!-- Optional parameter to include log timestamps into the kafka message --&gt; &lt;!-- &lt;appendTimestamp&gt;true&lt;/appendTimestamp&gt; --&gt; &lt;!-- each &lt;producerConfig&gt; translates to regular kafka-client config (format: key=value) --&gt; &lt;!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs --&gt; &lt;!-- bootstrap.servers is the only mandatory producerConfig --&gt; &lt;producerConfig&gt;bootstrap.servers=$&#123;bootstrap.servers&#125;&lt;/producerConfig&gt; &lt;!-- this is the fallback appender if kafka is not available. --&gt; &lt;appender-ref ref=&quot;FileAppender&quot;/&gt; &lt;/appender&gt; &lt;!-- 异步输出日志 步骤：异步输出日志就是Logger.info负责往Queue(BlockingQueue)中放日志，然后再起个线程把Queue中的日志写到磁盘上 参考：https://blog.csdn.net/lkforce/article/details/76637071 --&gt; &lt;appender name=&quot;ASYNC&quot; class=&quot;ch.qos.logback.classic.AsyncAppender&quot;&gt; &lt;!-- 不丢失日志。默认的，如果队列的80%已满，则会丢弃TRACT、DEBUG、INFO级别的日志 --&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;!-- 更改默认的队列的深度，该值会影响性能。默认值为256 --&gt; &lt;queueSize&gt;100&lt;/queueSize&gt; &lt;!-- 添加附加的appender，最多只能添加一个，此处指定后，在root下不要再指定该appender，否则会输出两次 --&gt; &lt;appender-ref ref=&quot;KafkaAppender&quot;/&gt; &lt;/appender&gt; &lt;!--日志异步到数据库：未做测试，配置正确与否未知，先记录于此 --&gt; &lt;!--&lt;appender name=&quot;DB&quot; class=&quot;ch.qos.logback.classic.db.DBAppender&quot;&gt; &lt;connectionSource class=&quot;ch.qos.logback.core.db.DriverManagerConnectionSource&quot;&gt; &lt;dataSource class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;driverClass&gt;com.mysql.jdbc.Driver&lt;/driverClass&gt; &lt;url&gt;jdbc:mysql://127.0.0.1:3306/databaseName&lt;/url&gt; &lt;user&gt;root&lt;/user&gt; &lt;password&gt;root&lt;/password&gt; &lt;/dataSource&gt; &lt;/connectionSource&gt; &lt;/appender&gt;--&gt; &lt;!-- 关闭指定包下的日志输出，name里面的内容可以是包路径，或者具体要忽略的文件名称 --&gt; &lt;logger name=&quot;org.apache.flink&quot; level=&quot;OFF&quot;/&gt; &lt;!-- 将指定包下指定级别的日志，输出到指定的appender中 addtivity：是否向上级logger传递打印信息。默认是true。若此包下的日志单独输出到文件中，应设置为false，否则在root日志也会记录一遍 --&gt; &lt;logger name=&quot;org.apache.kafka&quot; level=&quot;ERROR&quot; addtivity=&quot;false&quot;&gt; &lt;!-- 指定此包下的error级别信息，输出到指定的收集文件 --&gt; &lt;appender-ref ref=&quot;ErrorFileAppender&quot;/&gt; &lt;/logger&gt; &lt;root level=&quot;$&#123;logging.level&#125;&quot;&gt; &lt;!--&lt;appender-ref ref=&quot;STDOUT&quot;/&gt;--&gt; &lt;!--&lt;appender-ref ref=&quot;FileAppender&quot;/&gt;--&gt; &lt;appender-ref ref=&quot;ASYNC&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt; 根据实际情况，对 appender 进行取舍，实际使用时不要所有的都添加到 logback.xml 配置文件中。 本文参考https://kucw.github.io/blog/2020/3/java-lombok/ https://juejin.cn/post/6844903641535479821 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"hadoop-hdfs","slug":"hadoop-hdfs","date":"2021-01-03T13:59:31.000Z","updated":"2021-01-05T07:30:50.217Z","comments":true,"path":"2021/01/03/hadoop-hdfs/","link":"","permalink":"http://example.com/2021/01/03/hadoop-hdfs/","excerpt":"","text":"HDFS 常用 shell 命令操作 HDFS 的 shell命令有三种： hadoop fs：适用于任何不同的文件系统，比如本地文件系统和 HDFS 文件系统。 hadoop dfs：只适用于 HDFS 文件系统。 hdfs dfs：只适用于 HDFS 文件系统。 官方不推荐使用第二种命令 hadoop dfs，有些 Hadoop 版本中已将这种命令弃用。 语法1hadoop fs [genericOptions] [commandOptions] 参数说明 HDFS 常用命令 说明 hadoop fs -ls 显示指定文件的详细信息 hadoop fs -cat 将指定文件的内容输出到标准输出 hadoop fs touchz 创建一个指定的空文件 hadoop fs -mkdir [-p] 创建指定的一个或多个文件夹，-p 选项用于递归创建 hadoop fs -cp 将文件从源路径复制到目标路径 hadoop fs -mv 将文件从源路径移动到目标路径 hadoop fs -rm 删除指定的文件，只删除非空目录和文件 hadoop fs -rm -r 删除指定的文件夹及其下的所有文件，-r 表示递归删除子目录 hadoop fs -chown 改变指定文件的所有者，该命令仅适用于超级用户 hadoop fs -chmod 将指定的文件权限更改为可执行文件，该命令仅适用于超级用户和文件所有者 hadoop fs -get 复制指定的文件到本地文件系统指定的文件或文件夹 hadoop fs -put 从本地文件系统中复制指定的单个或多个源文件到指定的目标文件系统 hadoop fs -moveFromLocal 与 -put 命令功能相同，但是文件上传结束后会删除源文件 hadoop fs -copyFromLocal 与 -put 命令功能相同，将本地源文件复制到路径指定的文件或文件夹中 hadoop fs -copyToLocal 与 -get命令功能相同，将目标文件复制到本地文件或文件夹中","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://example.com/tags/hadoop/"}]},{"title":"Java 线程池","slug":"java-threadpool","date":"2021-01-01T11:44:47.000Z","updated":"2021-04-09T08:01:39.739Z","comments":true,"path":"2021/01/01/java-threadpool/","link":"","permalink":"http://example.com/2021/01/01/java-threadpool/","excerpt":"","text":"线程池的理解线程池是预先创建线程的一种技术，线程池在还没有任务到来之前，事先创建一定数量的线程，放入空闲队列中，然后对这些资源进行复用，从而减少频繁的创建和销毁对象。 系统启动一个新线程的成本是比较高的，因为它涉及与操作系统交互。在这种情形下，使用线程池可以很好地提高性能，尤其是当程序中需要创建大量生存期很短暂的线程时，更应该考虑使用线程池。 与数据库连接池类似的是，线程池在系统启动时即创建大量空闲的线程，程序将一个 Runnable 对象或 Callable 对象传给线程池，线程池就会启动一个线程来执行它们的 run() 或 call() 方法， 当 run() 或 call() 方法执行结束后， 该线程并不会死亡，而是再次返回线程池中成为空闲状态，等待执行下一个 Runnable 对象的 run() 或 call() 方法。 总结：由于系统创建和销毁线程都是需要时间和系统资源开销，为了提高性能，才考虑使用线程池。线程池会在系统启动时就创建大量的空闲线程，然后等待新的线程调用，线程执行结束并不会销毁，而是重新进入线程池，等待再次被调用。这样子就可以减少系统创建启动和销毁线程的时间，提高系统的性能。 线程池的使用使用 Executors 创建线程池 Executor 是线程池的顶级接口，接口中只定义了一个方法 void execute(Runnable command);，线程池的操作方法都是定义在 ExecutorService 子接口中的，所以说 ExecutorService 是线程池真正的接口。 newSingleThreadExecutor创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; newFixedThreadPool创建固定大小的线程池，每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到是大值就会保持不变。如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 123456public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);&#125; newCachedThreadPool创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲 (60 秒不执行任务) 的线程。当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对钱程池大小做限制，线程池大小完全依赖于操作系统 (或者说 JVM) 能够创建的最大线程大小。 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 使用 ThreadPoolExecutor 创建线程池123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 构造函数参数说明corePoolSize：核心线程数大小，当线程数小于 corePoolSize 的时候，会创建线程执行 runnable。 maximumPoolSize：最大线程数， 当线程数大于等于 corePoolSize 的时候，会把 runnable 放入 workQueue 中。 keepAliveTime：保持存活时间，当线程数大于 corePoolSize 的时候，空闲线程能保持的最大时间。 unit：时间单位。 workQueue：保存任务的阻塞队列。 threadFactory：创建线程的工厂。 handler：拒绝策略。 任务执行顺序 当线程数小于 corePoolSize 时，创建线程执行新任务。 当线程数大于等于 corePoolSize，并且 workQueue 没有满时，新任务放入 workQueue 中。 当线程数大于等于 corePoolSize，并且 workQueue 满时，新任务创建新线程运行，但线程总数要小于 maximumPoolSize。 当线程总数等于 maximumPoolSize，并且 workQueue 满时，执行 handler 的 rejectedExecution，也就是拒绝策略。 阻塞队列阻塞队列是一个在队列基础上又支持了两个附加操作的队列： 支持阻塞的插入方法：队列满时，队列会阻塞插入元素的线程，直到队列不满。 支持阻塞的移除方法：队列空时，获取元素的线程会等待队列变为非空。 阻塞队列的应用场景阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。简而言之，阻塞队列是生产者用来存放元素、消费者获取元素的容器。 阻塞队列的方法在阻塞队列不可用的时候，上述两个附加操作提供了四种处理方法： 方法处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e,time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() 不可用 不可用 阻塞队列的类型jdk 7 提供了 7 个阻塞队列，如下： ArrayBlockingQueue：数组结构组成的有界阻塞队列。 此队列按照先进先出 (FIFO) 的原则对元素进行排序，但是默认情况下不保证线程公平的访问队列，即如果队列满了，那么被阻塞在外面的线程对队列访问的顺序是不能保证线程公平 (即先阻塞，先插入) 的。 LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。 此队列按照先出先进的原则对元素进行排序。 PriorityBlockingQueue：支持优先级的无界阻塞队列。 DelayQueue：支持延时获取元素的无界阻塞队列，即可以指定多久才能从队列中获取当前元素。 SynchronousQueue：不存储元素的阻塞队列，每一个 put 必须等待一个 take 操作，否则不能继续添加元素。并且他支持公平访问队列。 LinkedTransferQueue：由链表结构组成的无界阻塞 TransferQueue 队列。 相对于其他阻塞队列，多了 tryTransfer 和 transfer 方法： transfer方法：如果当前有消费者正在等待接收元素 (take 或者待时间限制的 poll 方法)，transfer 可以把生产者传入的元素立刻传给消费者。如果没有消费者等待接收元素，则将元素放在队列的 tail 节点，并等到该元素被消费者消费了才返回。 tryTransfer方法：用来试探生产者传入的元素能否直接传给消费者。如果没有消费者在等待，则返回 false。和上述方法的区别是该方法无论消费者是否接收，方法立即返回，而 transfer 方法是必须等到消费者消费了才返回。 LinkedBlockingDeque：链表结构的双向阻塞队列，优势在于多线程入队时，减少一半的竞争。 拒绝策略当队列和线程池都满了，说明线程池处于饱和的状态，那么必须采取一种策略处理提交的新任务。ThreadPoolExecutor 默认有四个拒绝策略： ThreadPoolExecutor.AbortPolicy()：默认策略，直接抛出异常 RejectedExecutionException。 java.util.concurrent.RejectedExecutionException： 当线程池 ThreadPoolExecutor 执行方法 shutdown () 之后，再向线程池提交任务的时候，如果配置的拒绝策略是 AbortPolicy ，这个异常就会抛出来。 当设置的任务缓存队列过小的时候，或者说，线程池里面所有的线程都在干活（线程数等于 maxPoolSize)，并且任务缓存队列也已经充满了等待的队列， 这个时候，再向它提交任务，也会抛出这个异常。 ThreadPoolExecutor.CallerRunsPolicy()：直接调用 run () 方法并且阻塞执行。 ThreadPoolExecutor.DiscardPolicy()：不处理，直接丢弃后来的任务。 ThreadPoolExecutor.DiscardOldestPolicy()：丢弃在队列中队首的任务，并执行当前任务。 当然可以继承 RejectedExecutionHandler 来自定义拒绝策略。 线程池参数选择CPU 密集型：线程池的大小推荐为 CPU 数量 +1。CPU 数量可以根据 Runtime.getRuntime().availableProcessors() 方法获取。 IO 密集型：CPU 数量 * CPU 利用率 * (1 + 线程等待时间 / 线程 CPU 时间)。 混合型：将任务分为 CPU 密集型和 IO 密集型，然后分别使用不同的线程池去处理，从而使每个线程池可以根据各自的工作负载来调整。 阻塞队列：推荐使用有界队列，有界队列有助于避免资源耗尽的情况发生。 拒绝策略：默认采用的是 AbortPolicy 拒绝策略，直接在程序中抛出 RejectedExecutionException 异常，因为是运行时异常，不强制 catch，但这种处理方式不够优雅。处理拒绝策略有以下几种比较推荐： 在程序中捕获 RejectedExecutionException 异常，在捕获异常中对任务进行处理。针对默认拒绝策略。 使用 CallerRunsPolicy 拒绝策略，该策略会将任务交给调用 execute 的线程执行 (一般为主线程)，此时主线程将在一段时间内不能提交任何任务，从而使工作线程处理正在执行的任务。此时提交的线程将被保存在 TCP 队列中，TCP 队列满将会影响客户端，这是一种平缓的性能降低。 自定义拒绝策略，只需要实现 RejectedExecutionHandler 接口即可。 如果任务不是特别重要，使用 DiscardPolicy 和 DiscardOldestPolicy 拒绝策略将任务丢弃也是可以的。 如果使用 Executors 的静态方法创建 ThreadPoolExecutor 对象，可以通过使用 Semaphore 对任务的执行进行限流也可以避免出现 OOM 异常。 示例1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class TestThreadPoolExecutor &#123; public static void main(String[] args) &#123; long startTimeMillis = System.currentTimeMillis(); // 构造一个线程池 ThreadPoolExecutor threadPool = new ThreadPoolExecutor(5, 6, 3, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3) ); for (int i = 1; i &lt;= 10; i++) &#123; try &#123; String task = &quot;task = &quot; + i; System.out.println(&quot;创建任务并提交到线程池中：&quot; + task); threadPool.execute(new ThreadPoolTask(task)); Thread.sleep(100); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; try &#123; // 等待所有线程执行完毕当前任务 threadPool.shutdown(); boolean loop = true; do &#123; // 等待所有线程执行完毕，当前任务结束 loop = !threadPool.awaitTermination(2, TimeUnit.SECONDS);// 等待2秒 &#125; while (loop); if (!loop) &#123; System.out.println(&quot;所有线程执行完毕&quot;); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(&quot;耗时：&quot; + (System.currentTimeMillis() - startTimeMillis)); &#125; &#125;&#125; 12345678910111213141516171819import java.io.Serializable;public class ThreadPoolTask implements Runnable, Serializable &#123; private String attachData; public ThreadPoolTask(String tasks) &#123; this.attachData = tasks; &#125; public void run() &#123; try &#123; System.out.println(&quot;开始执行：&quot; + attachData + &quot;任务，使用的线程池，线程名称：&quot; + Thread.currentThread().getName() + &quot;\\r\\n&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; attachData = null; &#125;&#125; 运行结果，可以看到线程 pool-1-thread-1 到 pool-1-thread-5 循环使用： 1234567891011121314151617181920212223242526272829303132创建任务并提交到线程池中：task = 1开始执行：task = 1任务，使用的线程池，线程名称：pool-1-thread-1创建任务并提交到线程池中：task = 2开始执行：task = 2任务，使用的线程池，线程名称：pool-1-thread-2创建任务并提交到线程池中：task = 3开始执行：task = 3任务，使用的线程池，线程名称：pool-1-thread-3创建任务并提交到线程池中：task = 4开始执行：task = 4任务，使用的线程池，线程名称：pool-1-thread-4创建任务并提交到线程池中：task = 5开始执行：task = 5任务，使用的线程池，线程名称：pool-1-thread-5创建任务并提交到线程池中：task = 6开始执行：task = 6任务，使用的线程池，线程名称：pool-1-thread-1创建任务并提交到线程池中：task = 7开始执行：task = 7任务，使用的线程池，线程名称：pool-1-thread-2创建任务并提交到线程池中：task = 8开始执行：task = 8任务，使用的线程池，线程名称：pool-1-thread-3创建任务并提交到线程池中：task = 9开始执行：task = 9任务，使用的线程池，线程名称：pool-1-thread-4创建任务并提交到线程池中：task = 10开始执行：task = 10任务，使用的线程池，线程名称：pool-1-thread-5所有线程执行完毕耗时：1014 本文参考https://segmentfault.com/a/1190000011527245 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java String 类","slug":"java-string","date":"2021-01-01T11:31:54.000Z","updated":"2021-04-09T08:01:30.594Z","comments":true,"path":"2021/01/01/java-string/","link":"","permalink":"http://example.com/2021/01/01/java-string/","excerpt":"","text":"String 的特性String：字符串，使用双引号引起来表示。 String 是一个 final 类，不可被继承。 String 继承了 Serializable、Comparable 和 CharSequence 接口。 1public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123;&#125; 实现 Serializable 接口：表示字符串是支持可序列化的。 实现 Comparable 接口 ：表示 String 可以比较大小。 String 内部定义了 final char value[] 用于存储字符串数据。 String 代表不可变的字符序列 — 不可变性。 体现在： 当对字符串重新赋值时，需要重新指定内存区域赋值，不能使用原有的 value 进行赋值。 当对现有的字符串进行连接操作时，也需要重新指定内存区域赋值。 当调用 String 的 replace() 修改原字符串中指定的字符或字符串时，也需要重新指定内存区域赋值。 通过字面量的定义 (区别于 new) 方式给一个字符串赋值，此时的字符串值声明在字符串常量池中。 字符串常量池中不会存储相同内容的字符串。 实例： 1234567891011121314151617181920212223public class Test &#123; public static void main(String[] args) &#123; String s1 = &quot;abc&quot;;// 字面量的定义方式 String s2 = &quot;abc&quot;; System.out.println(s1 == s2);// true s1 = &quot;hello&quot;; System.out.println(s1 == s2);// false System.out.println(s1);// hello System.out.println(s2);// abc System.out.println(&quot;**************************&quot;); String s3 = &quot;abc&quot;; s3 += &quot;def&quot;; System.out.println(s3);// abcdef System.out.println(s2);// abc ---&gt; 原abc没变 System.out.println(&quot;**************************&quot;); String s4 = &quot;abc&quot;; String s5 = s4.replace(&quot;a&quot;, &quot;m&quot;); System.out.println(s4);// abc ---&gt; 原abc没变 System.out.println(s5);// mbc &#125;&#125; String 对象的创建 方式一：通过字面量定义的方式，此时的字符串数据声明在方法区中的字符串常量池中。 方式二：通过 “new + 构造器” 的方式，此时变量保存的地址值，是字符串数据在堆空间中开辟空间以后所对应的地址值。 常用的几种创建方式： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Test &#123; public static void main(String[] args) &#123; // 通过字面量定义的方式：此时的s1和s2的数据javaEE，声明在方法区中的字符串常量池中 String s1 = &quot;javaEE&quot;; String s2 = &quot;javaEE&quot;; // 通过&quot;new+构造器&quot;的方式：此时的s3和s4保存的地址值，是数据在堆空间中开辟空间以后所对应的地址值 String s3 = new String(&quot;javaEE&quot;); String s4 = new String(&quot;javaEE&quot;); System.out.println(s1 == s2);// true System.out.println(s1 == s3);// false System.out.println(s1 == s4);// false System.out.println(s3 == s4);// false System.out.println(&quot;*************************&quot;); Person p1 = new Person(&quot;Tom&quot;, 12);// 通过字面量定义的方式定义的name Person p2 = new Person(&quot;Tom&quot;, 12); System.out.println(p1.name.equals(p2.name));// true System.out.println(p1.name == p2.name);// true p1.name = &quot;Jerry&quot;; System.out.println(p2.name);// Tom System.out.println(&quot;*************************&quot;); Person p3 = new Person(new String(&quot;Tom&quot;), 12);// 通过&quot;new+构造器&quot;的方式定义的name Person p4 = new Person(new String(&quot;Tom&quot;), 12); System.out.println(p3.name.equals(p4.name));// true System.out.println(p3.name == p4.name);// false &#125;&#125;class Person &#123; String name; int age; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125;&#125; 内存解析说明： 字符串初始化的过程： 字符串拼接： 如果是常量与常量的拼接，则结果在常量池，且常量池中不会存在相同内容的常量。 如果其中有一个是变量，则结果在堆中。 如果拼接的结果调用 intern() 方法，则返回值在常量池中。 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; String s1 = &quot;javaEE&quot;; String s2 = &quot;hadoop&quot;; String s3 = &quot;javaEEhadoop&quot;; String s4 = &quot;javaEE&quot; + &quot;hadoop&quot;; String s5 = s1 + &quot;hadoop&quot;; String s6 = &quot;javaEE&quot; + s2; String s7 = s1 + s2; String s8 = (s1 + s2).intern(); System.out.println(s3 == s4);// true System.out.println(s3 == s5);// false System.out.println(s3 == s6);// false System.out.println(s3 == s7);// false System.out.println(s3 == s8);// true System.out.println(s5 == s6);// false System.out.println(s5 == s7);// false System.out.println(s5 == s8);// false System.out.println(s6 == s7);// false System.out.println(s6 == s8);// false System.out.println(s7 == s8);// false &#125;&#125; 内存解析说明： 特殊的情况： 12345678910111213public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;javaEEhadoop&quot;; String s2 = &quot;javaEE&quot;; String s3 = s2 + &quot;hadoop&quot;; System.out.println(s1 == s3);// false final String s4 = &quot;javaEE&quot;; String s5 = s4 + &quot;hadoop&quot;; System.out.println(s1 == s5);// true System.out.println(s2 == s4);// true &#125;&#125; s4 变量被 final 修饰，实际上也就是常量，等同于 s2。 面试题 String s = new String(&quot;abc&quot;); 方式创建对象，在内存中创建了几个对象？ 两个：一个是堆空间中 new 的结构，另一个是 char[] 对应的常量池中的数据 “abc”。 String str1 = &quot;abc&quot;; 与 String str2 = new String(&quot;abc&quot;); 的区别？ 字符串常量存储在字符串常量池，目的是共享。 字符串非常量对象存储在堆中。 下面程序的运行结果是： 12345678910111213141516public class StringTest &#123; String str = new String(&quot;good&quot;); char[] ch = &#123;&#x27;t&#x27;, &#x27;e&#x27;, &#x27;s&#x27;, &#x27;t&#x27;&#125;; public void change(String str, char ch[]) &#123; str = &quot;test ok&quot;; ch[0] = &#x27;b&#x27;; &#125; public static void main(String[] args) &#123; StringTest ex = new StringTest(); ex.change(ex.str, ex.ch); System.out.println(ex.str);// good System.out.println(ex.ch);// best &#125;&#125; 值传递机制和 String 的不可变性。 String 的常用方法 int length()：返回字符串的长度，return value.length。 char charAt(int index)：返回某索引处的字符，return value[index]。 boolean isEmpty()：判断是否是空字符串，return value.length == 0。 String toLowerCase()：使用默认语言环境，将 String 中的所有字符转换为小写。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; String s2 = s1.toLowerCase();// s1不可变，仍未原来的字符串 System.out.println(s2);// helloworld &#125;&#125; String toUpperCase()：使用默认语言环境，将 String 中的所有字符转换为大写。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; String s2 = s1.toUpperCase();// s1不可变，仍未原来的字符串 System.out.println(s2);// HELLOWORLD &#125;&#125; String trim()：返回字符串的副本，忽略前导空白和尾部空白。 12345678public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot; he llo world &quot;; String s2 = s1.trim(); System.out.println(&quot;---&quot; + s1 + &quot;---&quot;);// --- he llo world --- System.out.println(&quot;---&quot; + s2 + &quot;---&quot;);// ---he llo world--- &#125;&#125; boolean equals(Object obj)：比较字符串的内容是否相同。 boolean equalsIgnoreCase(String anotherString)：比较字符串的内容是否相同，忽略大小写。 String concat(String str)：将指定字符串连接到此字符串的结尾，等价于用 “+”。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;abc&quot;; String s2 = s1.concat(&quot;def&quot;); System.out.println(s2);// abcdef &#125;&#125; int compareTo(String anotherString)：比较两个字符串的大小。 String substring(int beginIndex)：返回一个新的字符串，截取当前字符串从 beginIndex 开始到最后的一个子字符串。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; String s2 = s1.substring(2); System.out.println(s2);// lloWorld &#125;&#125; String substring(int beginIndex, int endIndex)：返回一个新字符串，截取当前字符串从 beginIndex 开始到 endIndex (不包含) 结束的一个子字符串 — 左闭右开，[beginIndex, endIndex)。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; String s2 = s1.substring(2, 6); System.out.println(s2);// lloW &#125;&#125; boolean endsWith(String suffix)：测试此字符串是否以指定的后缀结束。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.endsWith(&quot;ld&quot;));// true &#125;&#125; boolean startsWith(String prefix)：测试此字符串是否以指定的前缀开始。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.startsWith(&quot;ll&quot;));// false &#125;&#125; boolean startsWith(String prefix, int toffset)：测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.startsWith(&quot;ll&quot;, 2));// true &#125;&#125; boolean contains(CharSequence s)：当且仅当此字符串包含指定的 char 值序列时，返回 true。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.contains(&quot;wo&quot;));// false &#125;&#125; int indexOf(String str)：返回指定子字符串在此字符串中第一次出现处的索引，未找到返回 -1。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.indexOf(&quot;lo&quot;));// 3 &#125;&#125; int indexOf(String str, int fromIndex)：返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始，未找到返回 -1。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;HelloWorld&quot;; System.out.println(s1.indexOf(&quot;lo&quot;, 5));// -1 &#125;&#125; int lastIndexOf(String str)：返回指定子字符串在此字符串中最右边出现处的索引，未找到返回 -1。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;hellorworld&quot;; System.out.println(s1.lastIndexOf(&quot;or&quot;));// 7 &#125;&#125; 面试题：什么情况下，indexOf(str) 和 lastIndexOf(str) 返回值相同？ 情况一：存在唯一的一个 str。情况二：不存在 str。 int lastIndexOf(String str, int fromIndex)：返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索，未找到返回 -1。 123456public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;hellorworld&quot;; System.out.println(s1.lastIndexOf(&quot;or&quot;, 6));// 4 &#125;&#125; String replace(char oldChar, char newChar)：返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;hellorworld&quot;; String s2 = s1.replace(&quot;o&quot;, &quot;D&quot;); System.out.println(s2);// hellDrwDrld &#125;&#125; String replace(CharSequence target, CharSequence replacement)：使用指定的字面值替换序列替换此字符串所有匹配字面值目标序列的子字符串。 1234567public class StringTest &#123; public static void main(String[] args) &#123; String s1 = &quot;hellorworld&quot;; String s2 = s1.replace(&quot;or&quot;, &quot;DE&quot;); System.out.println(s2);// hellDEwDEld &#125;&#125; String replaceAll(String regex, String replacement)：使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串。 12345678910public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;12hello34world5java7891mysql456&quot;; // 把字符串中的数字替换成,，如果结果中开头和结尾有,的话去掉 String s1 = str.replaceAll(&quot;\\\\d+&quot;, &quot;,&quot;); System.out.println(s1);// ,hello,world,java,mysql, String s2 = s1.replaceAll(&quot;^,|,$&quot;, &quot;&quot;); System.out.println(s2);// hello,world,java,mysql &#125;&#125; String replaceFirst(String regex, String replacement)：使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串。 12345678public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;12hello34world5java7891mysql456&quot;; // 把字符串中的数字替换成,，如果结果中开头和结尾有，的话去掉 String s1 = str.replaceFirst(&quot;\\\\d+&quot;, &quot;,&quot;); System.out.println(s1);// ,hello34world5java7891mysql456 &#125;&#125; boolean matches(String regex)：告知此字符串是否匹配给定的正则表达式。 123456789101112public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;1234d5&quot;; // 判断str字符串中是否全部由数字组成，即有1-n个数字组成 boolean matches = str.matches(&quot;\\\\d+&quot;); System.out.println(matches);// false String tel = &quot;0571-4534289&quot;; // 判断这是否是一个杭州的固定电话 boolean result = tel.matches(&quot;0571-\\\\d&#123;7,8&#125;&quot;); System.out.println(result);// true &#125;&#125; String[] split(String regex)：根据匹配给定的正则表达式来拆分此字符串。 123456789101112131415161718192021222324252627282930313233public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;hello|world|java&quot;; String[] strs = str.split(&quot;\\\\|&quot;); for (String value : strs) &#123; System.out.println(value); &#125; System.out.println(); String str2 = &quot;hello.world.java&quot;; String[] strs2 = str2.split(&quot;\\\\.&quot;); for (String s : strs2) &#123; System.out.println(s); &#125; System.out.println(); String str3 = &quot;hello-world-java&quot;; String[] strs3 = str3.split(&quot;-&quot;); for (String s : strs3) &#123; System.out.println(s); &#125; &#125;&#125;输出结果：helloworldjavahelloworldjavahelloworldjava String[] split(String regex, int limit)：根据匹配给定的正则表达式来拆分此字符串，最多不超过 limit 个，如果超过了，剩下的全部都放到最后一个元素中。 123456789101112public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;hello|world|java&quot;; String[] strs = str.split(&quot;\\\\|&quot;,2); for (String value : strs) &#123; System.out.println(value); &#125; &#125;&#125;输出结果：helloworld|java substring() 与 indexOf() 结合使用： 截取第二个 “-“ 之前的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(0, s.indexOf(&quot;-&quot;, s.indexOf(&quot;-&quot;) + 1)); System.out.println(r); &#125;&#125;输出结果：application-2005 截取第二个 “-“ 之后的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(s.indexOf(&quot;-&quot;, s.indexOf(&quot;-&quot;) + 1) + 1); System.out.println(r); &#125;&#125;输出结果：US20050154023A1-20050714 截取倒数第二个 “-“ 之前的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(0, s.lastIndexOf(&quot;-&quot;, s.lastIndexOf(&quot;-&quot;) - 1)); System.out.println(r); &#125;&#125;输出结果：application-2005 截取倒数第二个 “-“ 之后的字符： 123456789public class Test &#123; public static void main(String[] args) &#123; String s = &quot;application-2005-US20050154023A1-20050714&quot;; String r = s.substring(s.lastIndexOf(&quot;-&quot;, s.lastIndexOf(&quot;-&quot;) - 1) + 1); System.out.println(r); &#125;&#125;输出结果：US20050154023A1-20050714 String 与其他结构之间的转换String 与基本数据类型/包装类之间的转换 字符串转换为基本数据类型/包装类 Integer 包装类的 public static int parseInt(String s)：可以将由 “数字” 字符组成的字符串，转换为整型。 类似地，使用 java.lang 包中的 Byte、Short、Long、Float、Double 类调相应的类方法可以将由 “数字” 字符组成的字符串，转换为相应的基本数据类型。 基本数据类型/包装类转换为字符串 调用 String 类的 public String valueOf(int n) 可将 int 型转换为字符串。 相应的 valueOf(byte b)、valueOf(long l)、valueOf(float f)、valueOf(doubled)、valueOf(boolean b) 可将参数的相应类型转换为字符串。 String 与字符数组 (char[]) 之间的转换 字符串转换为字符数组 public char[] toCharArray()：将字符串中的全部字符存放在一个字符数组中的方法。 123456789public class StringTest &#123; public static void main(String[] args) &#123; String str = &quot;hello|world|java&quot;; char[] chars = str.toCharArray(); for (char c : chars) &#123; System.out.println(c); &#125; &#125;&#125; public void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin)：提供了将指定索引范围内的字符串存放到数组中的方法。 字符数组转换为字符串 String 类的构造器：String(char[]) 和 String(char[], int offset, intlength) 分别用字符数组中的全部字符和部分字符创建字符串对象。 1234567public class StringTest &#123; public static void main(String[] args) &#123; char[] arr = new char[]&#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;; String str = new String(arr); System.out.println(str); &#125;&#125; String 与字节数组 (byte[]) 之间的转换 字符串转换为字节数组 编码：String —&gt; byte[]，字符串 —&gt; 字节，看得懂的 —&gt; 看不懂的二进制数据。 public byte[] getBytes()：使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到新的 byte 数组中。 public byte[] getBytes(String charsetName)：使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到新的 byte 数组中。 字节数组转换为字符串 解码：byte[] —&gt; String，字节 —&gt; 字符串，看不懂的二进制数据 —&gt; 看得懂的。编码的逆过程。 String(byte[])：通过使用平台的默认字符集解码指定的 byte 数组，构造一个新的 String。 String(byte[] ，int offset ，int length)： ：用指定的字节数组的一部分，即从数组起始位置 offset 开始，取 length 个字节构造一个字符串对象。 实例： 1234567891011121314151617181920212223242526272829public class StringTest &#123; public static void main(String[] args) &#123; String str1 = &quot;abc123ABC中国&quot;; byte[] bytes = str1.getBytes();// 使用默认的字符集进行编码，此处是UTF-8 System.out.println(Arrays.toString(bytes));// [97, 98, 99, 49, 50, 51, 65, 66, 67, -28, -72, -83, -27, -101, -67] byte[] gbks = null; try &#123; gbks = str1.getBytes(&quot;GBK&quot;);// 使用GBK进行编码 System.out.println(Arrays.toString(gbks));// [97, 98, 99, 49, 50, 51, 65, 66, 67, -42, -48, -71, -6] &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;*********************************&quot;); String str2 = new String(bytes);// 使用默认的字符集进行解码，此处是UTF-8 System.out.println(str2);// abc123ABC中国 String str4 = new String(gbks); System.out.println(str4);// abc123ABC�й�，出现乱码，原因：编码集和解码集不一致 try &#123; String gbk = new String(gbks, &quot;GBK&quot;); System.out.println(gbk);// abc123ABC中国，因为编码集和解码集一致，所以不会出现乱码 &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 解码时，要求解码使用的字符集必须与编码时使用的字符集一致，否则会出现乱码。 String 相关的算法题目 模拟一个 trim 方法，去除字符串两端的空格。 1234567891011121314151617181920212223242526272829public class Test &#123; public static String myTrim(String str) &#123; if (str != null) &#123; // 用于记录从前往后首次索引位置不是空格的位置的索引 int start = 0; // 用于记录从后往前首次索引位置不是空格的位置的索引 int end = str.length() - 1; while (start &lt; end &amp;&amp; str.charAt(start) == &#x27; &#x27;) &#123; start++; &#125; while (start &lt; end &amp;&amp; str.charAt(end) == &#x27; &#x27;) &#123; end--; &#125; if (str.charAt(start) == &#x27; &#x27;) &#123; return &quot;&quot;; &#125; return str.substring(start, end + 1); &#125; return null; &#125; public static void main(String[] args) &#123; String s = myTrim(&quot; abc 123 d &quot;); System.out.println(s); &#125;&#125; 将一个字符串中指定部分进行反转。比如 “abcdefg” 反转为 “abfedcg”。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Test &#123; // 方式一： public static String reverse1(String str, int start, int end) &#123; if (str != null) &#123; char[] charArray = str.toCharArray(); for (int i = start, j = end; i &lt; j; i++, j--) &#123; char temp = charArray[i]; charArray[i] = charArray[j]; charArray[j] = temp; &#125; return new String(charArray); &#125; return null; &#125; // 方式二： public static String reverse2(String str, int start, int end) &#123; String newStr = str.substring(0, start); for (int i = end; i &gt;= start; i--) &#123; newStr += str.charAt(i); &#125; newStr += str.substring(end + 1); return newStr; &#125; // 方式三：推荐(相较于方式二做的改进) public static String reverse3(String str, int start, int end) &#123; StringBuilder newStr = new StringBuilder(str.length()); newStr.append(str, 0, start); for (int i = end; i &gt;= start; i--) &#123; newStr.append(str.charAt(i)); &#125; newStr.append(str.substring(end + 1)); return newStr.toString(); &#125; public static void main(String[] args) &#123; String str = &quot;abcdefg&quot;; String str1 = reverse3(str, 2, 5); System.out.println(str1);// abfedcg &#125;&#125; 获取一个字符串在另一个字符串中出现的次数。比如：获取 “ab” 在 “abkkcadkabkebfkabkskab” 中出现的次数。 123456789101112131415161718192021222324252627public class Test &#123; public static int getCount(String mainStr, String subStr) &#123; if (mainStr.length() &gt;= subStr.length()) &#123; int count = 0; int index = 0; /*// 方式一： while ((index = mainStr.indexOf(subStr)) != -1) &#123; count++; mainStr = mainStr.substring(index + subStr.length()); &#125;*/ // 改进： while ((index = mainStr.indexOf(subStr, index)) != -1) &#123; index += subStr.length(); count++; &#125; return count; &#125; return 0; &#125; public static void main(String[] args) &#123; String str1 = &quot;cdabkkcadkabkebfkabkskab&quot;; String str2 = &quot;ab&quot;; int count = getCount(str1, str2); System.out.println(count); &#125;&#125; 获取两个字符串中最大相同子串。比如：str1 = &quot;abcwerthelloyuiodef&quot;; str2 = &quot;cvhellobnm&quot;;。提示：将短的那个串进行长度依次递减的子串与较长的串比较。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class Test &#123; // 如果只存在一个最大长度的相同子串 public static String getMaxSameSubString(String str1, String str2) &#123; if (str1 != null &amp;&amp; str2 != null) &#123; String maxStr = (str1.length() &gt; str2.length()) ? str1 : str2; String minStr = (str1.length() &gt; str2.length()) ? str2 : str1; int len = minStr.length(); for (int i = 0; i &lt; len; i++) &#123;// 此层循环决定要去几个字符 for (int x = 0, y = len - i; y &lt;= len; x++, y++) &#123; if (maxStr.contains(minStr.substring(x, y))) &#123; return minStr.substring(x, y); &#125; &#125; &#125; &#125; return null; &#125; // 如果存在多个长度相同的最大相同子串 // 此时先返回String[]，后面可以用集合中的ArrayList替换，较方便 public static String[] getMaxSameSubString1(String str1, String str2) &#123; if (str1 != null &amp;&amp; str2 != null) &#123; StringBuilder strs = new StringBuilder(); String maxString = (str1.length() &gt; str2.length()) ? str1 : str2; String minString = (str1.length() &gt; str2.length()) ? str2 : str1; int len = minString.length(); for (int i = 0; i &lt; len; i++) &#123; for (int x = 0, y = len - i; y &lt;= len; x++, y++) &#123; String subString = minString.substring(x, y); if (maxString.contains(subString)) &#123; strs.append(subString).append(&quot;,&quot;); &#125; &#125; if (strs.length() != 0) &#123; break; &#125; &#125; return strs.toString().replaceAll(&quot;,$&quot;, &quot;&quot;).split(&quot;,&quot;); &#125; return null; &#125; // 如果存在多个长度相同的最大相同子串：使用ArrayList public static List&lt;String&gt; getMaxSameSubString2(String str1, String str2) &#123; if (str1 != null &amp;&amp; str2 != null) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); String maxString = (str1.length() &gt; str2.length()) ? str1 : str2; String minString = (str1.length() &gt; str2.length()) ? str2 : str1; int len = minString.length(); for (int i = 0; i &lt; len; i++) &#123; for (int x = 0, y = len - i; y &lt;= len; x++, y++) &#123; String subString = minString.substring(x, y); if (maxString.contains(subString)) &#123; list.add(subString); &#125; &#125; if (list.size() != 0) &#123; break; &#125; &#125; return list; &#125; return null; &#125; public static void main(String[] args) &#123; String str1 = &quot;abcwerthelloyuiodef&quot;; String str2 = &quot;cvhellobnmiodef&quot;; String[] strs = getMaxSameSubString1(str1, str2); System.out.println(Arrays.toString(strs)); &#125;&#125; 对字符串中的字符进行自然顺序排序。提示：① 字符串变成字符数组；② 对数组排序，选择，冒泡，Arrays.sort();；③ 将排序后的数组变成字符串。 123456789public class Test &#123; public static void main(String[] args) &#123; String str = &quot;abcwerthelloyuiodef&quot;; char[] arr = str.toCharArray(); Arrays.sort(arr); String newStr = new String(arr); System.out.println(newStr); &#125;&#125; String、StringBuffer 和 StringBuilder java.lang.StringBuffer 代表可变的字符序列，jdk 1.0 中声明，可以对字符串内容进行增删，此时不会产生新的对象。作为参数传递时，方法内部可以改变值。 1public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence &#123;&#125; java.lang.StringBuilder 和 java.lang.StringBuffer 非常类似，也代表可变的字符序列，二者提供相关功能的方法也比较类似。 1public final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence &#123;&#125; StringBuffer 类和 StringBuilder 类不同于 String，其对象必须使用构造器生成。常用以下三个构造器： StringBuffer()/StringBuilder()：初始容量为 16 的字符串缓冲区。 1234567/** * Constructs a string buffer with no characters in it and an * initial capacity of 16 characters. */public StringBuffer() &#123; super(16);&#125; 1234567/** * Constructs a string builder with no characters in it and an * initial capacity of 16 characters. */public StringBuilder() &#123; super(16);&#125; StringBuffer(int capacity)/StringBuilder(int capacity)：构造指定容量的字符串缓冲区。 1234567891011/** * Constructs a string buffer with no characters in it and * the specified initial capacity. * * @param capacity the initial capacity. * @exception NegativeArraySizeException if the &#123;@code capacity&#125; * argument is less than &#123;@code 0&#125;. */public StringBuffer(int capacity) &#123; super(capacity);&#125; 1234567891011/** * Constructs a string builder with no characters in it and an * initial capacity specified by the &#123;@code capacity&#125; argument. * * @param capacity the initial capacity. * @throws NegativeArraySizeException if the &#123;@code capacity&#125; * argument is less than &#123;@code 0&#125;. */public StringBuilder(int capacity) &#123; super(capacity);&#125; StringBuffer(String str)/StringBuilder(String str)：将内容初始化为指定字符串内容。 1234567891011/** * Constructs a string buffer initialized to the contents of the * specified string. The initial capacity of the string buffer is * &#123;@code 16&#125; plus the length of the string argument. * * @param str the initial contents of the buffer. */public StringBuffer(String str) &#123; super(str.length() + 16); append(str);&#125; 1234567891011/** * Constructs a string builder initialized to the contents of the * specified string. The initial capacity of the string builder is * &#123;@code 16&#125; plus the length of the string argument. * * @param str the initial contents of the buffer. */public StringBuilder(String str) &#123; super(str.length() + 16); append(str);&#125; StringBuffer 类的常用方法，很多方法与 String 相同，StringBuilder 类的常用方法参考 StringBuffer。 StringBuffer 类和 StringBuilder 类的方法的主要区别，举例如下： StringBuffer 类 — 同步方法： 123456@Overridepublic synchronized StringBuffer append(String str) &#123; toStringCache = null; super.append(str); return this;&#125; StringBuilder 类 — 非同步方法： 12345@Overridepublic StringBuilder append(String str) &#123; super.append(str); return this;&#125; StringBuffer append(xxx)：提供了参数可为多种类型的 append() 方法，用于进行字符串拼接。 面试题，输出结果： 12345678910111213141516171819public class ExceptionTest &#123; public static void main(String[] args) &#123; String str = null; StringBuilder sb = new StringBuilder(); System.out.println(sb); sb.append(str); System.out.println(sb.length()); System.out.println(sb); StringBuilder sb1 = new StringBuilder(str); System.out.println(sb1); &#125;&#125;输出结果：4nullException in thread &quot;main&quot; java.lang.NullPointerException at java.lang.StringBuilder.&lt;init&gt;(StringBuilder.java:112) at cn.xisun.java.base.ExceptionTest.main(ExceptionTest.java:17) 原因： ① java 里面，null 不占字节。如果一个引用指向 null，该应用就不再指向堆内存中的任何对象。并且，这个对象引用的大小是 4 个字节。 ② append() 方法如果传入 null 参数，最终执行以下方法，因此上面第 7 行和第 8 行输出结果为 4 和 null (字符串 null)。 1234567891011private AbstractStringBuilder appendNull() &#123; int c = count; ensureCapacityInternal(c + 4); final char[] value = this.value; value[c++] = &#x27;n&#x27;; value[c++] = &#x27;u&#x27;; value[c++] = &#x27;l&#x27;; value[c++] = &#x27;l&#x27;; count = c; return this;&#125; ③ new StringBuilder(str) 的代码中：super(str.length() + 16);，调用 null 的 length() 方法，会发生空指针异常。 StringBuffer delete(int start,int end)：删除指定位置的内容。 StringBuffer replace(int start, int end, String str)：把 [start,end) 位置替换为 str。 StringBuffer insert(int offset, xxx)：在指定位置插入多种类型的参数。 1234567public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer(&quot;abc123&quot;); sb.insert(3, &quot;ABC&quot;); System.out.println(sb);// abcABC123 &#125;&#125; StringBuffer reverse()：把当前字符序列逆转。 1234567public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer(&quot;abc123&quot;); sb.reverse(); System.out.println(sb);// 321cba &#125;&#125; int indexOf(String str)：返回指定子字符串在此字符串中第一次出现处的索引，未找到返回 -1。 String substring(int start)：返回一个新的字符串，截取当前字符串从 start 开始到最后的一个子字符串。 String substring(int start,int end)：返回一个新字符串，截取当前字符串从 start 开始到 end (不包含) 结束的一个子字符串 — 左闭右开，[start, end)。 1234567public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer(&quot;abc123abc&quot;); String sb2 = sb.substring(1, 5); System.out.println(sb2);// bc12 &#125;&#125; int length()：返回字符串的长度。 1234@Overridepublic int length() &#123; return count;&#125; char charAt(int n )：返回某索引处的字符。 void setCharAt(int n ,char ch)：在指定索引处插入字符。 1234567public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer(&quot;abc123abc&quot;); sb.setCharAt(1, &#x27;中&#x27;); System.out.println(sb);// a中c123abc &#125;&#125; 方法总结： 增：append，删：delete，改：setCharAt/replace，查：charAt，插：insert，长度：length，遍历：for + charAt/toString。 append、delete、replace、insert 和 reverse 这些方法，支持方法链操作，方法链的原理为： String、StringBuffer 和 StringBuilder 的异同？ String：Since jdk 1.0，不可变的字符序列；底层使用 final char[] 存储。 StringBuffer：Since jdk 1.0，可变的字符序列；线程安全的，效率低；底层使用 char[] 存储。 StringBuilder：Since jdk 1.5，可变的字符序列；线程不安全的，效率高；底层使用 char[] 存储。 StringBuffer 和 StringBuilder 的扩容问题： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; String str = new String();// final char[] value = new char[0]; String str1 = new String(&quot;abc&quot;);// final char[] value = new char[]&#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;; StringBuffer sb = new StringBuffer();// char[] value = new char[16]; 底层创建了一个长度是16的char数组 System.out.println(sb.length());// 0 sb.append(&#x27;a&#x27;);// value[0] = &#x27;a&#x27;; sb.append(&#x27;b&#x27;);// value[1] = &#x27;b&#x27;; sb.append(&#x27;c&#x27;);// value[1] = &#x27;c&#x27;; System.out.println(sb.length());// 3 StringBuffer sb1 = new StringBuffer(&quot;abc&quot;);// char[] value = new char[&quot;abc&quot;.length() + 16]; System.out.println(sb1.length());// 3 &#125;&#125; 扩容问题：如果要添加的数据底层数组盛不下了，那就需要扩容底层的数组。 扩容方法： 1234567private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) &#123; value = Arrays.copyOf(value, newCapacity(minimumCapacity)); &#125;&#125; 123456789101112131415161718192021/** * Returns a capacity at least as large as the given minimum capacity. * Returns the current capacity increased by the same amount + 2 if * that suffices. * Will not return a capacity greater than &#123;@code MAX_ARRAY_SIZE&#125; * unless the given minimum capacity is greater than that. * * @param minCapacity the desired minimum capacity * @throws OutOfMemoryError if minCapacity is less than zero or * greater than Integer.MAX_VALUE */private int newCapacity(int minCapacity) &#123; // overflow-conscious code int newCapacity = (value.length &lt;&lt; 1) + 2; if (newCapacity - minCapacity &lt; 0) &#123; newCapacity = minCapacity; &#125; return (newCapacity &lt;= 0 || MAX_ARRAY_SIZE - newCapacity &lt; 0) ? hugeCapacity(minCapacity) : newCapacity;&#125; 默认情况下，扩容为原来容量的 2 倍 + 2，同时将原来数组中的元素复制到新的数组中。 指导意义：开发中，如果知道创建的字符串的长度，建议使用 StringBuffer(int capacity) 或 StringBuilder(int capacity)，即可能得避免扩容的发生，这样可以提高效率。 String、StringBuffer 和 StringBuilder 三者的效率测试： 123456789101112131415161718192021222324252627282930313233343536public class Test &#123; public static void main(String[] args) &#123; // 初始设置 String text = &quot;&quot;; StringBuffer buffer = new StringBuffer(&quot;&quot;); StringBuilder builder = new StringBuilder(&quot;&quot;); long startTime = 0L; long endTime = 0L; // 开始对比 startTime = System.currentTimeMillis(); for (int i = 0; i &lt; 20000; i++) &#123; text = text + i; &#125; endTime = System.currentTimeMillis(); System.out.println(&quot;String的执行时间：&quot; + (endTime - startTime)); startTime = System.currentTimeMillis(); for (int i = 0; i &lt; 20000; i++) &#123; buffer.append(String.valueOf(i)); &#125; endTime = System.currentTimeMillis(); System.out.println(&quot;StringBuffer的执行时间：&quot; + (endTime - startTime)); startTime = System.currentTimeMillis(); for (int i = 0; i &lt; 20000; i++) &#123; builder.append(String.valueOf(i)); &#125; endTime = System.currentTimeMillis(); System.out.println(&quot;StringBuilder的执行时间：&quot; + (endTime - startTime)); &#125;&#125;输出结果：StringBuffer的执行时间：6StringBuilder的执行时间：3String的执行时间：1713 效率从高到低排列：StringBuilder &gt; StringBuffer &gt; String。 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"redis 命令","slug":"redis-command","date":"2020-12-31T09:10:36.000Z","updated":"2020-12-31T09:12:10.982Z","comments":true,"path":"2020/12/31/redis-command/","link":"","permalink":"http://example.com/2020/12/31/redis-command/","excerpt":"","text":"查询当前库 key 的个数info可以看到所有库的key数量 dbsize则是当前库key的数量 keys *这种数据量小还可以，大的时候可以直接搞死生产环境。 dbsize和keys *统计的key数可能是不一样的，如果没记错的话，keys *统计的是当前db有效的key，而dbsize统计的是所有未被销毁的key（有效和未被销毁是不一样的，具体可以了解redis的过期策略）","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}]},{"title":"Kafka 命令行工具","slug":"kafka-command","date":"2020-12-30T02:19:35.000Z","updated":"2021-02-26T02:17:31.813Z","comments":true,"path":"2020/12/30/kafka-command/","link":"","permalink":"http://example.com/2020/12/30/kafka-command/","excerpt":"","text":"查看 Kafka topic 列表命令，返回 topic 名字列表 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --list 创建 Kafka topic 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181,hadoopdatanode2:2181,hadoopdatanode3:2181 --create --partitions 6 --replication-factor 2 --topic patent-grant 查看 Kafka 指定 topic 的详情命令，返回该 topic 的 parition 数量、replica 因子以及每个 partition 的 leader、replica 信息 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --describe --topic patent-grant 查看 Kafka 指定 topic 各 partition 的 offset 信息命令，–time 参数为 -1 时，表示各分区最大的 offset，为 -2 时，表示各分区最小的 offset 1$ ~/kafka_2.12-2.6.0/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list hadoopdatanode1:9092 --time -1 --topic patent-grant 删除 Kafka topic 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-topics.sh --zookeeper hadoopdatanode1:2181 --delete -topic patent-grant 只有 topic 不再被使用时，才能被删除。 修改 kafka topic 的数据保存时间： 1$ ~/kafka_2.12-2.6.0/bin/kafka-configs.sh --bootstrap-server hadoopdatanode1:9092 --alter --entity-type topics --entity-name extractor-patent --add-config retention.ms=2592000000 kafka 中默认消息的保留时间是 7 天，若想更改，需在配置文件 server.properties 里更改选项：log.retention.hours=168。 如果需要对某一个主题的消息存留的时间进行变更，但不影响其他主题，并且 kafka 集群不用重启，则使用上面的命令修改，该命令设置的是 30 天。 查看 kafka topic 配置信息： 1$ ~/kafka_2.12-2.6.0/bin/kafka-configs.sh --bootstrap-server hadoopdatanode1:9092 --describe --entity-type topics --entity-name extractor-patent 如果使用的是默认配置，显示： 1Dynamic configs for topic extractor-patent are: 如果更改了配置，显示： 12Dynamic configs for topic extractor-patent are: retention.ms=2592000000 sensitive=false synonyms=&#123;DYNAMIC_TOPIC_CONFIG:retention.ms=2592000000&#125; 查看 kafka consumer group 命令，返回 consumer group 名字列表 (新版信息保存在 broker 中，老版信息保存在 zookeeper 中，二者命令不同) 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --list 老版命令：~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --zookeeper hadoopdatanode1:2181 --list 查看 Kafka 指定 consumer group 的详情命令，返回 consumer group 对应的 topic 信息、当前消费的 offset、总 offset、剩余待消费 offset 等信息 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --describe --group log-consumer 重置 Kafka 指定 consumer group 消费的 topic 的 offset 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --reset-offsets -to-offset 0 --execute --topic patent-app --group log-consumer 删除 Kafka 指定 consumer group 命令 1$ ~/kafka_2.12-2.6.0/bin/kafka-consumer-groups.sh --bootstrap-server hadoopdatanode1:9092 --delete --group log-consumer 消费 Kafka 指定 topic 的内容命令 kafka-console-consumer.sh 脚本是一个简易的消费者控制台。该 shell 脚本的功能通过调用 kafka.tools 包下的 ConsoleConsumer 类，并将提供的命令行参数全部传给该类实现。 参数说明： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.shThis tool helps to read data from Kafka topics and outputs it to standard output.Option Description ------ ----------- --bootstrap-server &lt;String: server to REQUIRED: The server(s) to connect to. connect to&gt; --consumer-property &lt;String: A mechanism to pass user-defined consumer_prop&gt; properties in the form key=value to the consumer. --consumer.config &lt;String: config file&gt; Consumer config properties file. Note that [consumer-property] takes precedence over this config. --enable-systest-events Log lifecycle events of the consumer in addition to logging consumed messages. (This is specific for system tests.) --formatter &lt;String: class&gt; The name of a class to use for formatting kafka messages for display. (default: kafka.tools. DefaultMessageFormatter) --from-beginning If the consumer does not already have an established offset to consume from, start with the earliest message present in the log rather than the latest message. --group &lt;String: consumer group id&gt; The consumer group id of the consumer. --help Print usage information. --isolation-level &lt;String&gt; Set to read_committed in order to filter out transactional messages which are not committed. Set to read_uncommitted to read all messages. (default: read_uncommitted)--key-deserializer &lt;String: deserializer for key&gt; --max-messages &lt;Integer: num_messages&gt; The maximum number of messages to consume before exiting. If not set, consumption is continual. --offset &lt;String: consume offset&gt; The offset id to consume from (a non- negative number), or &#x27;earliest&#x27; which means from beginning, or &#x27;latest&#x27; which means from end (default: latest) --partition &lt;Integer: partition&gt; The partition to consume from. Consumption starts from the end of the partition unless &#x27;--offset&#x27; is specified. --property &lt;String: prop&gt; The properties to initialize the message formatter. Default properties include: print.timestamp=true|false print.key=true|false print.value=true|false key.separator=&lt;key.separator&gt; line.separator=&lt;line.separator&gt; key.deserializer=&lt;key.deserializer&gt; value.deserializer=&lt;value. deserializer&gt; Users can also pass in customized properties for their formatter; more specifically, users can pass in properties keyed with &#x27;key. deserializer.&#x27; and &#x27;value. deserializer.&#x27; prefixes to configure their deserializers. --skip-message-on-error If there is an error when processing a message, skip it instead of halt. --timeout-ms &lt;Integer: timeout_ms&gt; If specified, exit if no message is available for consumption for the specified interval. --topic &lt;String: topic&gt; The topic id to consume on. --value-deserializer &lt;String: deserializer for values&gt; --version Display Kafka version. --whitelist &lt;String: whitelist&gt; Regular expression specifying whitelist of topics to include for consumption. 参数说明参考：https://blog.csdn.net/qq_29116427/article/details/80206125 从头开始消费： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --from-beginning --topic log-collect 从头开始消费前 10 条消息，并显示 key： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --from-beginning --max-messages 10 --property print.key=true --topic log-collect 从指定分区、指定 offset 开始消费： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --partition 0 --offset 219000 --topic log-collect 从尾开始消费，必须指定分区： 1$ ~/kafka_2.12-2.6.0/bin/kafka-console-consumer.sh --bootstrap-server hadoopdatanode1:9092 --partition 0 --offset latest --topic log-collect","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"shell 脚本中的一些方法","slug":"shell","date":"2020-12-22T09:16:35.000Z","updated":"2021-01-01T08:13:35.647Z","comments":true,"path":"2020/12/22/shell/","link":"","permalink":"http://example.com/2020/12/22/shell/","excerpt":"","text":"字符串截取shell 截取字符串通常有两种方式：从指定位置开始截取和从指定字符 (子字符串) 开始截取。 从指定位置开始截取两个参数：起始位置，截取长度。 从字符串左边开始计数1$&#123;string: start :length&#125; 其中，string 是要截取的字符串，start 是起始位置 (从左边开始，从 0 开始计数)，length 是要截取的长度 (如果省略表示直到字符串的末尾)。 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 2: 9&#125;biancheng 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 2&#125;biancheng.net 从字符串右边开始计数1$&#123;string: 0-start :length&#125; 0- 是固定的写法，专门用来表示从字符串右边开始计数。 从左边开始计数时，起始数字是 0；从右边开始计数时，起始数字是 1。计数方向不同，起始数字也不同。 不管从哪边开始计数，截取方向都是从左到右。 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 0-13: 9&#125;biancheng 1234url=&quot;c.biancheng.net&quot;echo $&#123;url: 0-13&#125;biancheng.net 从右边数，b 是第 13 个字符。 从指定字符 (子字符串) 开始截取这种截取方式无法指定字符串长度，只能从指定字符 (子字符串) 截取到字符串末尾。shell 可以截取指定字符 (子字符串) 右边的所有字符，也可以截取左边的所有字符。 使用 # 截取指定字符 (子字符串)右边字符1$&#123;string#*chars&#125; 其中，string 表示要截取的字符，chars 是指定的字符 (子字符串)，* 是通配符的一种，表示任意长度的字符串。*chars 连起来使用的意思是：忽略左边的所有字符，直到遇见 chars (chars 不会被截取)。 从左往右看。 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*:&#125;//c.biancheng.net/index.html 以下写法也可以得到同样的结果： 12echo $&#123;url#*p:&#125;echo $&#123;url#*ttp:&#125; 如果不需要忽略 chars 左边的字符，那么也可以不写 *，例如： 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#http://&#125;c.biancheng.net/index.html 注意，以上写法遇到第一个匹配的字符 (子字符串) 就结束了。例如： 1234url=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*/&#125;/c.biancheng.net/index.html url 字符串中有三个 /，输出结果表明，shell 遇到第一个 / 就匹配结束了。 如果希望直到最后一个指定字符 (子字符串) 再匹配结束，那么可以使用 ##，具体格式为： 1$&#123;string##*chars&#125; 123456789#!/bin/bashurl=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url#*/&#125; # 结果为 /c.biancheng.net/index.htmlecho $&#123;url##*/&#125; # 结果为 index.htmlstr=&quot;---aa+++aa@@@&quot;echo $&#123;str#*aa&#125; # 结果为 +++aa@@@echo $&#123;str##*aa&#125; # 结果为 @@@ 使用 % 截取指定字符 (子字符串)左边字符1$&#123;string%chars*&#125; 注意 * 的位置，因为要截取 chars 左边的字符，而忽略 chars 右边的字符，所以 * 应该位于 chars 的右侧。其他方面 % 和 # 的用法相同。 从右往左看。 123456789#!/bin/bashurl=&quot;http://c.biancheng.net/index.html&quot;echo $&#123;url%/*&#125; # 结果为 http://c.biancheng.netecho $&#123;url%%/*&#125; # 结果为 http:str=&quot;---aa+++aa@@@&quot;echo $&#123;str%aa*&#125; # 结果为 ---aa+++echo $&#123;str%%aa*&#125; # 结果为 --- 汇总 格式 说明 ${string: start :length} 从 string 字符串的左边第 start 个字符开始，向右截取 length 个字符。 ${string: start} 从 string 字符串的左边第 start 个字符开始截取，直到最后。 ${string: 0-start :length} 从 string 字符串的右边第 start 个字符开始，向右截取 length 个字符。 ${string: 0-start} 从 string 字符串的右边第 start 个字符开始截取，直到最后。 ${string#*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string##*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 右边的所有字符。 ${string%*chars} 从 string 字符串第一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 ${string%%*chars} 从 string 字符串最后一次出现 *chars 的位置开始，截取 *chars 左边的所有字符。 参考： http://c.biancheng.net/view/1120.html 字符串拼接12345678910111213141516171819202122#!/bin/bashname=&quot;Shell&quot;url=&quot;http://c.biancheng.net/shell/&quot;str1=$name$url # 中间不能有空格str2=&quot;$name $url&quot; # 如果被双引号包围，那么中间可以有空格str3=$name&quot;: &quot;$url # 中间可以出现别的字符串str4=&quot;$name: $url&quot; # 这样写也可以str5=&quot;$&#123;name&#125;Script: $&#123;url&#125;index.html&quot; # 这个时候需要给变量名加上大括号echo $str1echo $str2echo $str3echo $str4echo $str5Shellhttp://c.biancheng.net/shell/Shell http://c.biancheng.net/shell/Shell: http://c.biancheng.net/shell/Shell: http://c.biancheng.net/shell/ShellScript: http://c.biancheng.net/shell/index.html 对于第 7 行代码，$name 和 $url 之间之所以不能出现空格，是因为当字符串不被任何一种引号包围时，遇到空格就认为字符串结束了，空格后边的内容会作为其他变量或者命令解析。 对于第 10 行代码，加 { } 是为了帮助解释器识别变量的边界。 字符串分割以空格为分隔符比如有一个变量 “123 456 789”，要求以空格为分隔符把这个变量分隔，并把分隔后的字段分别赋值给变量，即 a=123；b=456；c=789。共有3中方法：方法一：先定义一个数组，然后把分隔出来的字段赋值给数组中的每一个元素方法二：通过 eval+ 赋值的方式方法三：通过多次 awk 把每个字段赋值 1234567891011121314151617181920212223242526272829#!/bin/basha=&quot;123 456 789&quot;# 方法一：通过数组的方式declare -a arrindex=0for i in $(echo $a | awk &#x27;&#123;print $1,$3&#125;&#x27;)do arr[$index]=$i let &quot;index+=1&quot;doneecho $&#123;arr[0]&#125; # 结果为 123echo $&#123;arr[1]&#125; # 结果为 789# 方法二：通过eval+赋值的方式b=&quot;&quot;c=&quot;&quot;eval $(echo $a | awk &#x27;&#123; printf(&quot;b=%s;c=%s&quot;,$2,$1)&#125;&#x27;)echo $b # 结果为 456echo $c # 结果为 123# 方法三：通过多次awk赋值的方式m=&quot;&quot;n=&quot;&quot;m=`echo $a | awk &#x27;&#123;print $1&#125;&#x27;`n=`echo $a | awk &#x27;&#123;print $2&#125;&#x27;`echo $m # 结果为 123echo $n # 结果为 456 指定分隔符123456789101112131415161718192021222324252627#!/bin/bashstring=&quot;hello,shell,haha&quot;# 方法一 array=($&#123;string//,/ &#125;) for var in $&#123;array[@]&#125;do echo $vardone# 方法二IFS=&quot;,&quot;OLD_IFS=&quot;$IFS&quot;IFS=&quot;$OLD_IFS&quot;array2=($string)for var2 in $&#123;array2[@]&#125;do echo $var2donehelloshellhahahelloshellhaha 变量赋值反引号： 1var=`command` $()： 1var=$(command) 例如： 1234567$ A=`date`$ echo $AFri Dec 25 20:02:30 CST 2020 # 变量A存放了date命令的执行结果$ B=$(date)$ echo $BFri Dec 25 20:03:12 CST 2020 # 变量B存放了date命令的执行结果 注意：= 号前后不要有空格。 参考： https://book.51cto.com/art/201411/457601.htm 判断文件夹是否存在1234567#!/bin/bashif [ ! -d testgrid ];then mkdir testgridelse echo dir existfi 外部传参： 12345678910111213141516#!/bin/bash# 判断传入的参数的个数是不是一个if [ ! $# -eq 1 ];then echo param error! exit 1fi# 判断目录是不是已经存在，如果不存在则创建，存在则输出&quot;dir exist&quot; dirname=$1echo &quot;the dir name is $dirname&quot;if [ ! -d $dirname ];then mkdir $dirnameelse echo dir existfi 循环类 C 语言： 123456# !/bin/shfor ((i=1; i&lt;=100; i ++))do echo $idone in 使用： 123456# !/bin/shfor i in &#123;1..100&#125;do echo $idone seq 使用： 123456# !/bin/shfor i in `seq 1 100`do echo $idone 发送微信消息https://blog.csdn.net/whatday/article/details/105781861","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"linux 命令","slug":"linux-command","date":"2020-12-15T09:31:47.000Z","updated":"2021-01-01T11:35:41.779Z","comments":true,"path":"2020/12/15/linux-command/","link":"","permalink":"http://example.com/2020/12/15/linux-command/","excerpt":"","text":"find用于在指定目录下查找文件。默认列出当前目录及子目录下所有的文件和文件夹。 语法1find path -option [ -print ] [ -exec -ok command ] &#123;&#125; \\; -print： find 命令将匹配到的文件输出到标准输出。 -exec： find 命令对匹配的文件执行该参数所给出的 Shell 命令。 -ok： 和 -exec 的作用相同，只是更安全，在执行每个命令之前，都会给出提示，让用户来确定是否执行。 参数说明1234567891011121314151617181920212223242526272829303132333435363738394041424344-mount, -xdev 只检查和指定目录在同一个文件系统下的文件，避免列出其它文件系统中的文件。-amin n 在过去n分钟内被读取过。-anewer file 比文件file更晚被读取过的文件。-atime n 在过去n天内被读取过的文件。 -cmin n 在过去n分钟内被修改过。 -cnewer file 比文件file更新的文件。 -ctime n 在过去n天内被修改过的文件。 -empty 空的文件。 -gid n gid是n。 -group name group名称是name。 -path p | -ipath p 路径名称符合p的文件。ipath忽略大小写。 -name name | -iname name 文件名称符合name的文件。iname忽略大小写。 -size n 文件大小是n单位，b代表512位元组的区块，c表示字元数，k表示kilo bytes，w是二个位元组。 -type d|f 文件类型是d|f的文件。 -pid n process id是n的文件。 文件类型：d — 目录，f — 一般文件，c — 字型装置文件，b — 区块装置文件，p — 具名贮列，l — 符号连结，s — socket。 实例 查询当前路径下的所有目录|普通文件 12$ find ./ -type d$ find ./ -type f 查询权限为 777 的普通文件 1$ find ./ -type f -perm 777 查询 .XML 文件，且权限不为 777 1$ find ./ -type f -name &quot;*.XML&quot; ! -perm 777 查询 .XML 文件，并统计查询结果的条数 1$ find ./ -name &quot;*.XML&quot; | wc -l 查询 .XML 文件，并复制查询结果到指定路径 12$ find ./ -name &quot;*.XML&quot; | xargs -i cp &#123;&#125; ../111$ find ./ -name &quot;*.XML&quot; -exec cp &#123;&#125; ../111 \\; 此命令不同于 cp，cp *.XML ../111 命令复制的是当前路径下符合条件的所有文件，子路径的不会被复制。 查询 .XML 文件，并删除 12$ find ./ -name &quot;*.XML&quot; | xargs -i rm &#123;&#125;$ find ./ -name &quot;*.XML&quot; -exec rm &#123;&#125; \\; 此命令不同于 rm，rm *.XML 命令删除的是当前路径下符合条件的所有文件，子路径的不会被删除。 查询 .XML 文件，并将查询结果以 “File: 文件名” 的形式打印出来 12$ find ./ -name &quot;*.XML&quot; | xargs -i printf &quot;File: %s\\n&quot; &#123;&#125;$ find ./ -name &quot;*.XML&quot; -exec printf &quot;File: %s\\n&quot; &#123;&#125; \\; 将当前路径及子路径下所有 3 天前的 .XML 格式的文件复制一份到指定路径 1$ find ./ -name &quot;*.XML&quot; -mtime +3 -exec cp &#123;&#125; ../111 \\; 查询多个文件后缀类型的文件 12$ find ./ -regextype posix-extended -regex &quot;.*\\.(java|xml|XML)&quot; # 查找所有的.java、.xml和.XML文件$ find ./ -name &quot;*.java&quot; -o -name &quot;*.xml&quot; -o -name &quot;*.XML&quot; # -o选项，适用于查询少量文件后缀类型 组合查询，可以多次拼接查询条件 1234$ find ./ -name &quot;file1*&quot; -a -name &quot;*.xml&quot; # -a：与，查找以file1开头，且以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; -o -name &quot;*.xml&quot; # -o：或，查找以file1开头，或以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; -not -name &quot;*.xml&quot; # -not：非，查找以file1开头，且不以.xml 结尾的文件$ find ./ -name &quot;file1*&quot; ! -name &quot;file2*&quot; # !：同-not 查询当前目录下文件类型为 d 的文件，不包含子目录 1$ find ./ -maxdepth 1 -type d 和正则表达式的结合使用 12$ find ./ –name &quot;[^abc]*&quot; # 在当前路径中搜索不以a、b、c开头的所有文件$ find ./ -name &quot;[A-Z0-9]*&quot; # 在当前路径中搜索以大写字母或数字开头的所有文件 正则表达式符号含义： * 代表任意字符 (可以没有字符) ? 代表任意单个字符 [] 代表括号内的任意字符，如 [abc] 可以匹配 a\\b\\c 某个字符 [a-z] 可以匹配 a-z 的某个字母 [A-Z] 可以匹配 A-Z 的某个字符 [0-9] 可以匹配 0-9 的某个数字 ^ 用在 [] 内的前缀表示不匹配 [] 中的字符 [^a-z] 表示不匹配a-z的某个字符 参考： https://www.jianshu.com/p/b30a8aa4d1f1 https://www.oracle.com/cn/technical-resources/articles/linux-calish-find.html https://www.cnblogs.com/qmfsun/p/3811142.html https://www.cnblogs.com/ay-a/p/8017419.html cat用于连接文件或标准输入并打印。这个命令常用来显示文件内容，或者将几个文件连接起来显示，或者从标准输入读取内容并显示，它常与重定向符号配合使用。 语法1cat [-AbeEnstTuv] [--help] [--version] fileName 参数说明1234567891011121314151617181920212223242526-n | --number 由1开始对所有输出的行数编号。-b | --number-nonblank 和-n相似，只不过对于空白行不编号。-s | --squeeze-blank 当遇到有连续两行以上的空白行，就代换为一行的空白行。-v | --show-nonprinting 使用^和M-符号，除了LFD和TAB之外。-E | --show-ends 在每行结束处显示$。 -T | --show-tabs 将TAB字符显示为^I。 -A | --show-all 等价于&quot;-vET&quot;。 -e 等价于&quot;-vE&quot;。 -t 等价于&quot;-vT&quot;。 实例 把 textfile1 的文档内容加上行号后输入 textfile2 这个文档里 1$ cat -n textfile1 &gt; textfile2 清空 /etc/test.txt 文档内容 1$ cat /dev/null &gt; /etc/test.txt /dev/null：在类 Unix 系统中，/dev/null 称空设备，是一个特殊的设备文件，它丢弃一切写入其中的数据 (但报告写入操作成功)，读取它则会立即得到一个 EOF。 而使用 cat $filename &gt; /dev/null 则不会得到任何信息，因为我们将本来该通过标准输出显示的文件信息重定向到了 /dev/null 中。 使用 cat $filename 1 &gt; /dev/null 也会得到同样的效果，因为默认重定向的 1 就是标准输出。 如果你对 shell 脚本或者重定向比较熟悉的话，应该会联想到 2 ，也即标准错误输出。 如果我们不想看到错误输出呢？我们可以禁止标准错误 cat $badname 2 &gt; /dev/null。 合并多个文件内容 12$ cat b1.sql b2.sql b3.sql &gt; b_all.sql$ cat *.sql &gt; merge.sql headhttps://blog.csdn.net/zmx19951103/article/details/78575265 tailhttps://blog.csdn.net/luo200618/article/details/52510638 sedhttps://www.cnblogs.com/qmfsun/p/6626361.html jqhttps://www.jianshu.com/p/dde911234761 https://blog.csdn.net/whatday/article/details/105781861 https://blog.csdn.net/qq_26502245/article/details/100191694 https://blog.csdn.net/u011641885/article/details/45559031 basename用于去掉文件名的目录和后缀。 语法12 basename NAME [SUFFIX]or: basename OPTION... NAME... 去掉 NAME 中的目录部分和后缀 SUFFIX，如果输出结果没有，则输出 SUFFIX。 参数说明1234567891011121314-a | --multiple support multiple arguments and treat each as a NAME -s | --suffix=SUFFIX remove a trailing SUFFIX; implies -a -z | --zero end each output line with NUL, not newline(默认情况下，每条输出行以换行符结尾) --help display this help and exit --version output version information and exit 实例 去除目录 12$basename /usr/bin/sortsort 12$ basename /usr/include/stdio.h stdio.h 去除目录和后缀 12$ basename /usr/include/stdio.h .hstdio 12$ basename -s .h /usr/include/stdio.hstdio 12$ basename /usr/include/stdio.h stdio.h stdio.h 去除多个目录 1234$ basename -a any1/str1 any2/str2 any3/str3str1str2str3 dirname用于去除文件名中的非目录部分，删除最后一个 “\\“ 后面的路径，显示父目录。 语法1dirname [OPTION] NAME... 如果 NAME 中不包含 /，则输出 .，即当前目录。 参数说明12345678-z | --zero end each output line with NUL, not newline --help display this help and exit --version output version information and exit 实例1234567891011$ dirname /usr/bin//usr$ dirname /usr/bin/usr$ dirname /etc//$ dirname /etc/httpd/conf/httpd.conf/etc/httpd/conf 123$ dirname dir1/str dir2/strdir1dir2 12$ dirname stdio.h. xargsxargs 是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。 xargs 可以将管道或标准输入 (stdin) 数据转换成命令行参数，也能够从文件的输出中读取数据。 xargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。 xargs 默认的命令是 echo，这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。 xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。 之所以能用到这个命令，关键是由于很多命令不支持管道来传递参数，而日常工作中有有这个必要，所以就有了 xargs 命令，例如： 12find /sbin -perm +700 | ls -l #这个命令是错误的find /sbin -perm +700 | xargs ls -l #这样才是正确的 xargs 一般是和管道一起使用。 语法1some command | xargs -item command 参数说明1234567891011121314151617181920212223242526272829303132333435363738-a file 从文件中读入作为sdtin -e flag 注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。 -p 当每次执行一个argument的时候询问一次用户。 -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。-t 表示先打印命令，然后再执行。-i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给&#123;&#125;，可以用&#123;&#125;代替。-r no-run-if-empty，当xargs的输入为空的时候则停止xargs，不用再去执行了。-s num命令行的最大字符数，指的是xargs后面那个命令的最大命令行字符数。-L num 从标准输入一次读取num行送给command命令。-l 同 -L。-d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符。-x exit的意思，主要是配合-s使用。-P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。 实例 读取输入数据重新格式化后输出 定义一个测试文件，内有多行文本数据： 123456$ cat test.txta b c d e f gh i j k l m no p qr s tu v w x y z 单行输出： 12$ cat test.txt | xargsa b c d e f g h i j k l m n o p q r s t u v w x y z -n 选项自定义多行输出： 12345678910$ cat test.txt | xargs -n3a b cd e fg h ij k lm n op q rs t uv w xy z -d 选项自定义一个定界符： 12$ echo &quot;nameXnameXnameXname&quot; | xargs -dXname name name name 结合 -n 选项使用： 123$ echo &quot;nameXnameXnameXname&quot; | xargs -dX -n2name namename name 读取 stdin，将格式化后的参数传递给命令 假设一个命令为 sk.sh 和一个保存参数的文件 arg.txt： sk.sh 命令内容： 1234#!/bin/bash# 打印出所有参数。echo $* arg.txt 文件内容： 1234$ cat arg.txtaaabbbccc xargs 的一个选项 -I，使用 -I 指定一个替换字符串 {}，这个字符串在 xargs 扩展时会被替换掉，当 -I 与 xargs 结合使用，每一个参数命令都会被执行一次： 1234$ cat arg.txt | xargs -I &#123;&#125; ./sk.sh -p &#123;&#125; -l-p aaa -l-p bbb -l-p ccc -l 复制所有图片文件到 /data/images 目录下： 1ls *.jpg | xargs -n1 -I &#123;&#125; cp &#123;&#125; /data/images 结合 find 使用 用 rm 删除太多的文件时候，可能得到一个错误信息：**/bin/rm Argument list too long.**， 用 xargs 去避免这个问题： 1$ find . -type f -name &quot;*.log&quot; -print0 | xargs -0 rm -f xargs -0 将 \\0 作为定界符。 统计一个源代码目录中所有 php 文件的行数： 1$ find . -type f -name &quot;*.php&quot; -print0 | xargs -0 wc -l 查找所有的 jpg 文件，并且压缩它们： 1$ find . -type f -name &quot;*.jpg&quot; -print | xargs tar -zcvf images.tar.gz xargs 其他应用 假如你有一个文件包含了很多你希望下载的 URL，你能够使用 xargs 下载所有链接： 1$ cat url-list.txt | xargs wget -c 参考：？？？ https://www.cnblogs.com/wangqiguo/p/6464234.html crontablinux 内置的 cron 进程能实现定时任务需求。 crontab 命令是 cron table 的简写，它是 cron 的配置文件，也可以叫它作业列表。我们可以在以下文件夹内找到相关配置文件： **/var/spool/cron/**：该目录下存放的是每个用户包括 root 的 crontab 任务，每个任务以创建者的名字命名。 /etc/crontab：该文件负责调度各种管理和维护任务。 **/etc/cron.d/**：该目录用来存放任何要执行的crontab文件或脚本。 另外，还可以把脚本放在 /etc/cron.hourly、/etc/cron.daily、/etc/cron.weekly、/etc/cron.monthly 目录中，让它每小时/天/星期/月执行一次。 语法1crontab [-u user] [ -e | -l | -r ] 参数说明1234567891011121314151617-u user 用来设定某个用户的crontab服务，省略此参数表示操作当前用户的crontab。file file是命令文件的名字，表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入(键盘)上键入的命令，并将它们载入crontab。-e 编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。-l 显示某个用户的crontab文件内容。如果不指定用户，则表示显示当前用户的crontab文件内容。-r 从/var/spool/cron目录中删除某个用户的crontab文件。如果不指定用户，则默认删除当前用户的crontab文件。-i 在删除用户的crontab文件时给确认提示。 实例设置定时任务时，输入 crontab -e 命令，进入当前用户的工作表编辑，是常见的 vim 界面。每行是一条命令。 crontab 的命令格式： crontab 的命令构成： 时间 + 命令，其时间有分、时、日、月、周五种，时间的操作符有： *****：取值范围内的所有数字 /：每过多少个数字，间隔频率，例如：用在小时段的”*/2”表示每隔两小时 -：从X到Z，例如：”2-6”表示”2,3,4,5,6” ,：散列数字，例如：”1,2,5,7” crontab 的命令实例： 每 2 小时执行一次：0 */2 * * * command 上述命令的含义：能被2整除的整点的0分，执行命令，即 0、2、4、6、…、20、22、24。 crontab 的日志查看： 12$ tail -f /var/log/cron.log$ tail -f /var/spool/mail/[username] 注意事项： 环境变量问题 有时我们创建了一个 crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在 crontab 文件中没有配置环境变量引起的。 在 crontab 文件中定义多个调度任务时，需要特别注环境变量的设置，因为我们手动执行某个任务时，是在当前 shell 环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在 crontab 文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定 cron 知道所需要的特殊环境，它其实并不知道。所以你要保证在 shell 脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下 3 点： 脚本中涉及文件路径时写全局路径； 脚本执行要用到 java 或其他环境变量时，通过 source 命令引入环境变量，如： 12345$ cat start_cbp.sh!/bin/shsource /etc/profileexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; 或者通过在 crontab 中直接引入环境变量解决问题。如： 10 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 及时清理系统用户的邮件日志 每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。 例如，可以在 crontab 文件中设置如下形式，忽略日志输出： 10 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 “&gt;/dev/null 2&gt;&amp;1” 表示先将标准输出重定向到 /dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了 /dev/null，因此标准错误也会重定向到 /dev/null，这样日志输出问题就解决了。 其他注意事项 新创建的 cron job，不会马上执行，至少要过 2 分钟才执行。如果重启 cron 则马上执行。 当 crontab 失效时，可以尝试 /etc/init.d/crond restart 解决问题。或者查看日志看某个 job 有没有执行/报错 tail -f /var/log/cron。 **千万别乱运行 crontab -r**。它从 crontab 目录 (/var/spool/cron) 中删除用户的 crontab 文件，删除了该文件，则用户的所有 crontab 都没了。 在 crontab 中 % 是有特殊含义的，表示换行的意思。如果要用的话必须进行转义 %，如经常用的 date ‘+%Y%m%d’ 在 crontab 里是不会执行的，应该换成 date ‘+%Y%m%d’。 参考： https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html https://segmentfault.com/a/1190000021815907 mv用于为文件或目录改名、或将文件或目录移入其它位置。 语法123 mv [OPTION]... [-T] SOURCE DESTor: mv [OPTION]... SOURCE... DIRECTORYor: mv [OPTION]... -t DIRECTORY SOURCE... 参数说明1234567891011121314-b 当目标文件或目录sh存在时，在执行覆盖前，会为其创建一个备份。-i 如果指定移动的源目录或文件与目标的目录或文件同名，则会先询问是否覆盖旧文件，输入y表示直接覆盖，输入n表示取消该操作。-f 如果指定移动的源目录或文件与目标的目录或文件同名，不会询问，直接覆盖旧文件。-n 不要覆盖任何已存在的文件或目录。-u 当源文件比目标文件新或者目标文件不存在时，才执行移动操作。 实例 将源文件名 source_file 改为目标文件名 dest_file 1$ mv source_file(文件) dest_file(文件) 将文件 source_file 移动到目标目录 dest_directory 中 1$ mv source_file(文件) dest_directory(目录) 将源目录 source_directory 移动到 目标目录 dest_directory中 1$ mv source_directory(目录) dest_directory(目录) 若目录名 dest_directory 已存在，则 source_directory 移动到目录名 dest_directory 中； 若目录名 dest_directory 不存在，则 source_directory 改名为目录名 dest_directory。 rename用于实现文件或批量文件重命名。在不同的 linux 版本，命令的语法格式可能不同。 语法1rename [ -h|-m|-V ] [ -v ] [ -n ] [ -f ] [ -e|-E *perlexpr*]*|*perlexpr* [ *files* ] linux 版本： Linux version 4.4.0-116-generic (buildd@lgw01-amd64-021) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) ) #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 或者 1rename [options] expression replacement file... linux 版本： Linux version 3.10.0-1062.el7.x86_64 (&#x6d;&#x6f;&#x63;&#x6b;&#x62;&#x75;&#105;&#x6c;&#x64;&#x40;&#x6b;&#98;&#x75;&#105;&#108;&#x64;&#101;&#114;&#46;&#98;&#x73;&#x79;&#x73;&#x2e;&#99;&#x65;&#110;&#116;&#x6f;&#x73;&#x2e;&#x6f;&#x72;&#x67;) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) ) #1 SMP Wed Aug 7 18:08:02 UTC 2019 参数说明123456789101112131415161718192021222324-v, -verbose Verbose: print names of files successfully renamed.-n, -nono No action: print names of files to be renamed, but don&#x27;t rename.-f, -force Over write: allow existing files to be over-written.-h, -help Help: print SYNOPSIS and OPTIONS.-m, -man Manual: print manual page.-V, -version Version: show version number.-e Expression: code to act on files name. May be repeated to build up code (like &quot;perl -e&quot;). If no -e, the first argument is used as code.-E Statement: code to act on files name, as -e but terminated by &#x27;;&#x27;. 或者 12345-v, --verbose explain what is being done-s, --symlink act on symlink target-h, --help display this help and exit-V, --version output version information and exit 实例 替换文件名中特定字段 1$ rename -v &quot;s/20/patent-application/&quot; *.tar.gz 1234567891011121314151617181920212223242526# lin @ lin in ~/share/storage_server_3/Download/test [14:52:08] $ lltotal 76G-rw-rw-r-- 1 lin lin 4.3G Dec 4 16:54 2005.tar.gz-rw-rw-r-- 1 lin lin 4.3G Dec 5 21:50 2006.tar.gz-rw-rw-r-- 1 lin lin 4.4G Dec 5 21:52 2007.tar.gz-rw-rw-r-- 1 lin lin 4.7G Dec 5 21:53 2008.tar.gz-rw-rw-r-- 1 lin lin 5.0G Dec 7 22:10 2009.tar.gz# lin @ lin in ~/share/storage_server_3/Download/test [14:52:08] $ rename -v &quot;s/20/patent-application/&quot; *.tar.gz2005.tar.gz renamed as patent-application05.tar.gz2006.tar.gz renamed as patent-application06.tar.gz2007.tar.gz renamed as patent-application07.tar.gz2008.tar.gz renamed as patent-application08.tar.gz2009.tar.gz renamed as patent-application09.tar.gz# lin @ lin in ~/share/storage_server_3/Download/test [14:53:55] $ lltotal 76G-rw-rw-r-- 1 lin lin 4.3G Dec 4 16:54 patent-application05.tar.gz-rw-rw-r-- 1 lin lin 4.3G Dec 5 21:50 patent-application06.tar.gz-rw-rw-r-- 1 lin lin 4.4G Dec 5 21:52 patent-application07.tar.gz-rw-rw-r-- 1 lin lin 4.7G Dec 5 21:53 patent-application08.tar.gz-rw-rw-r-- 1 lin lin 5.0G Dec 7 22:10 patent-application09.tar.gz 或者 1$ rename 20 patent-application-20 *.tar.gz 12345678910111213141516(base) [hadoop@client version-1.0]$ lltotal 79555796-rw-rw-r-- 1 hadoop hadoop 4527645498 Dec 4 16:54 2005.tar.gz-rw-rw-r-- 1 hadoop hadoop 4550889304 Dec 5 21:50 2006.tar.gz-rw-rw-r-- 1 hadoop hadoop 4712276001 Dec 5 21:52 2007.tar.gz-rw-rw-r-- 1 hadoop hadoop 4986740725 Dec 5 21:53 2008.tar.gz-rw-rw-r-- 1 hadoop hadoop 5311490484 Dec 7 22:10 2009.tar.gz(base) [hadoop@client version-1.0]$ rename 20 patent-application-20 *.tar.gz(base) [hadoop@client version-1.0]$ lltotal 79555796-rw-rw-r-- 1 hadoop hadoop 1372 Dec 16 09:15 hash_calculate.txt-rw-rw-r-- 1 hadoop hadoop 4527645498 Dec 4 16:54 patent-application-2005.tar.gz-rw-rw-r-- 1 hadoop hadoop 4550889304 Dec 5 21:50 patent-application-2006.tar.gz-rw-rw-r-- 1 hadoop hadoop 4712276001 Dec 5 21:52 patent-application-2007.tar.gz-rw-rw-r-- 1 hadoop hadoop 4986740725 Dec 5 21:53 patent-application-2008.tar.gz-rw-rw-r-- 1 hadoop hadoop 5311490484 Dec 7 22:10 patent-application-2009.tar.gz 参考： http://einverne.github.io/post/2018/01/rename-files-batch.html unzip用于解压缩 .zip 文件。 语法12 unzip [-cflptuvz][-agCjLMnoqsVX][-P &lt;密码&gt;][.zip文件][文件][-d &lt;目录&gt;][-x &lt;文件&gt;]or: unzip [-Z] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778-c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。-f 更新现有的文件。-l 显示压缩文件内所包含的文件。-p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。-t 检查压缩文件是否正确。-u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。-v 查看压缩文件目录信息，但是不解压该文件。-z 仅显示压缩文件的备注文字。-a 对文本文件进行必要的字符转换。-b 不要对文本文件进行字符转换。-C 压缩文件中的文件名称区分大小写。-j 不处理压缩文件中原有的目录路径。-L 将压缩文件中的全部文件名改为小写。-M 将输出结果送到more程序处理。-n 解压缩时不要覆盖原有的文件。-o 不必先询问用户，unzip执行后覆盖原有文件。-q 执行时不显示任何信息。-s 将文件名中的空白字符转换为底线字符。-V 保留VMS的文件版本信息。-X 解压缩时同时回存文件原来的UID/GID。-P &lt;密码&gt; 使用zip的密码选项。[.zip文件] 指定.zip压缩文件。[文件] 指定要处理.zip压缩文件中的哪些文件。-d &lt;目录&gt; 指定文件解压缩后所要存储的目录。-x &lt;文件&gt; 指定不要处理.zip压缩文件中的哪些文件。-Z &#x27;unzip -Z&#x27;等于执行zipinfo指令。 实例 查看压缩文件目录信息，但不解压 123456789101112131415$ unzip -v I20090212-SUPP.ZIP Archive: I20090212-SUPP.ZIP Length Method Size Cmpr Date Time CRC-32 Name-------- ------ ------- ---- ---------- ----- -------- ---- 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/ 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/DTDS/ 88199 Stored 88199 0% 2007-01-22 00:07 d5e3060f project/pdds/ICEApplication/I20090212-SUPP/DTDS/DTDS.zip 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/ 15664 Stored 15664 0% 2009-01-28 22:45 3dfa6c1c project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/US20090041797A1-20090212-SUPP.ZIP 0 Stored 0 0% 2009-01-31 04:56 00000000 project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/ 901714 Stored 901714 0% 2009-01-28 22:45 75ce3ca6 project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044288A1-20090212-SUPP.ZIP 1911858 Stored 1911858 0% 2009-01-28 22:45 cbc1d0bd project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044297A1-20090212-SUPP.ZIP-------- ------- --- ------- 2917435 2917435 0% 8 files 解压 .zip 文件 12345678910$ unzip I20090212-SUPP.ZIP Archive: I20090212-SUPP.ZIP creating: project/pdds/ICEApplication/I20090212-SUPP/ creating: project/pdds/ICEApplication/I20090212-SUPP/DTDS/ extracting: project/pdds/ICEApplication/I20090212-SUPP/DTDS/DTDS.zip creating: project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/ extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0041/US20090041797A1-20090212-SUPP.ZIP creating: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/ extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044288A1-20090212-SUPP.ZIP extracting: project/pdds/ICEApplication/I20090212-SUPP/UTIL0044/US20090044297A1-20090212-SUPP.ZIP 解压 .zip 文件，但不显示信息 1$ unzip -q I20090212-SUPP.ZIP 注意：如果压缩文件 .zip 是大于 2G 的，那 unzip 就无法使用，此时可以使用 7zip 解压。 参考： https://www.bbsmax.com/A/lk5aMEAP51/ zip用于压缩文件，压缩后的文件后缀名为 .zip。 语法1zip [-AcdDfFghjJKlLmoqrSTuvVwXyz$] [-b &lt;工作目录&gt;] [-ll] [-n &lt;字尾字符串&gt;] [-t &lt;日期时间&gt;] [-&lt;压缩效率&gt;] [压缩文件] [文件...] [-i &lt;范本样式&gt;] [-x &lt;范本样式&gt;] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101-A 调整可执行的自动解压缩文件。 -c 替每个被压缩的文件加上注释。-d 从压缩文件内删除指定的文件。-D 压缩文件内不建立目录名称。-f 更新现有的文件。-F 尝试修复已损坏的压缩文件。-g 将文件压缩后附加在既有的压缩文件之后，而非另行建立新的压缩文件。-h 在线帮助。-j 只保存文件名称及其内容，而不存放任何目录名称。-J 删除压缩文件前面不必要的数据。 -k 使用MS-DOS兼容格式的文件名称。 -l 压缩文件时，把LF字符置换成LF+CR字符。-L 显示版权信息。-m 将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中。-o 以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同。-q 不显示指令执行过程。-r 递归处理，将指定目录下的所有文件和子目录一并处理。-S 包含系统和隐藏文件。-T 检查备份文件内的每个文件是否正确无误。-u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。-v 显示指令执行过程或显示版本信息。-V 保存VMS操作系统的文件属性。-w 在文件名称里假如版本编号，本参数仅在VMS操作系统下有效。-X 不保存额外的文件属性。-y 直接保存符号连接，而非该连接所指向的文件，本参数仅在UNIX之类的系统下有效。-z 替压缩文件加上注释。-$ 保存第一个被压缩文件所在磁盘的卷册名称。 -b &lt;工作目录&gt; 指定暂时存放文件的目录。 -ll 压缩文件时，把LF+CR字符置换成LF字符。-n &lt;字尾字符串&gt; 不压缩具有特定字尾字符串的文件。-t &lt;日期时间&gt; 把压缩文件的日期设成指定的日期。-&lt;压缩效率&gt; 压缩效率是一个介于1-9的数值。-i &lt;范本样式&gt; 只压缩符合条件的文件。-x &lt;范本样式&gt; 压缩时排除符合条件的文件。 实例 将 /home/html/ 目录下所有文件和文件夹打包为当前目录下的 html.zip 1$ zip -q -r html.zip /home/html 如果当前在 /home/html 目录下，可以执行以下命令 1$ zip -q -r html.zip * 从压缩文件 cp.zip 中删除文件 a.c 1zip -dv cp.zip a.c tar用于打包、解包文件。 tar 本身不具有压缩功能，可以通过参数调用其他压缩工具实现压缩功能。 语法1tar [-ABcdgGhiklmMoOpPrRsStuUvwWxzZ] [-b &lt;区块数目&gt;] [-C &lt;目的目录&gt;] [-f &lt;备份文件&gt;] [-F &lt;Script文件&gt;] [-K &lt;文件&gt;] [-L &lt;媒体容量&gt;] [-N &lt;日期时间&gt;] [-T &lt;范本文件&gt;] [-V &lt;卷册名称&gt;] [-X &lt;范本文件&gt;] [-&lt;设备编号&gt;&lt;存储密度&gt;] [--after-date=&lt;日期时间&gt;] [--atime-preserve] [--backuup=&lt;备份方式&gt;] [--checkpoint] [--concatenate] [--confirmation] [--delete] [--exclude=&lt;范本样式&gt;] [--force-local] [--group=&lt;群组名称&gt;] [--help] [--ignore-failed-read] [--new-volume-script=&lt;Script文件&gt;] [--newer-mtime] [--no-recursion] [--null] [--numeric-owner] [--owner=&lt;用户名称&gt;] [--posix] [--erve] [--preserve-order] [--preserve-permissions] [--record-size=&lt;区块数目&gt;] [--recursive-unlink] [--remove-files] [--rsh-command=&lt;执行指令&gt;] [--same-owner] [--suffix=&lt;备份字尾字符串&gt;] [--totals] [--use-compress-program=&lt;执行指令&gt;] [--version] [--volno-file=&lt;编号文件&gt;] [文件或目录...] 语法结构：tar [必要参数] [可选参数] [文件] 参数说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212-A | --catenate 新增文件到已存在的备份文件。 -B | --read-full-records 读取数据时重设区块大小。 -c | --create 建立新的备份文件。 -d | --diff | --compare 对比备份文件内和文件系统上的文件的差异。 -g | --listed-incremental 处理GNU格式的大量备份。 -G | --incremental 处理旧的GNU格式的大量备份。 -h | --dereference 不建立符号连接，直接复制该连接所指向的原始文件。 -i | --ignore-zeros 忽略备份文件中的0 Byte区块，也就是EOF。 -k | --keep-old-files 解开备份文件时，不覆盖已有的文件。 -l | --one-file-system 复制的文件或目录存放的文件系统，必须与tar指令执行时所处的文件系统相同，否则不予复制。 -m | --modification-time 还原文件时，不变更文件的更改时间。 -M | --multi-volume 在建立，还原备份文件或列出其中的内容时，采用多卷册模式。 -o | --old-archive | --portability 将资料写入备份文件时使用V7格式。 -O | --stdout 把从备份文件里还原的文件输出到标准输出设备。 -p | --same-permissions 用原来的文件权限还原文件。 -P | --absolute-names 文件名使用绝对名称，不移除文件名称前的&quot;/&quot;号。 -r | --append 新增文件到已存在的备份文件的结尾部分。 -R | --block-number 列出每个信息在备份文件中的区块编号。 -s | --same-order 还原文件的顺序和备份文件内的存放顺序相同。 -S | --sparse 倘若一个文件内含大量的连续0字节，则将此文件存成稀疏文件。 -t | --list 列出备份文件的内容。 -u | --update 仅置换较备份文件内的文件更新的文件。 -U | --unlink-first 解开压缩文件还原文件之前，先解除文件的连接。 -v | --verbose 显示指令执行过程。 -w | --interactive 遭遇问题时先询问用户。 -W | --verify 写入备份文件后，确认文件正确无误。 -x | --extract | --get 从备份文件中还原文件。 -z | --gzip | --ungzip 通过gzip指令处理备份文件。 -Z | --compress | --uncompress 通过compress指令处理备份文件。-b &lt;区块数目&gt; | --blocking-factor=&lt;区块数目&gt; 设置每笔记录的区块数目，每个区块大小为12Bytes。-C &lt;目的目录&gt; | --directory=&lt;目的目录&gt; 切换到指定的目录。-f &lt;备份文件&gt; | --file=&lt;备份文件&gt; 指定备份文件。多个命令时需要放在最后面。 -F &lt;Script文件&gt; | --info-script=&lt;Script文件&gt; 每次更换磁带时，就执行指定的Script文件。-K &lt;文件&gt; | --starting-file=&lt;文件&gt; 从指定的文件开始还原。-L &lt;媒体容量&gt; | -tape-length=&lt;媒体容量&gt; 设置存放每体的容量，单位以1024Bytes计算。-N &lt;日期格式&gt; | --newer=&lt;日期时间&gt; 只将较指定日期更新的文件保存到备份文件里。-T &lt;范本文件&gt; | --files-from=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让tar解开或建立符合设置条件的文件。-V&lt;卷册名称&gt; | --label=&lt;卷册名称&gt; 建立使用指定的卷册名称的备份文件。-X &lt;范本文件&gt; | --exclude-from=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让tar排除符合设置条件的文件。-&lt;设备编号&gt;&lt;存储密度&gt; 设置备份用的外围设备编号及存放数据的密度。 --after-date=&lt;日期时间&gt; 此参数的效果和指定&quot;-N&quot;参数相同。--atime-preserve 不变更文件的存取时间。--backup=&lt;备份方式&gt; | --backup 移除文件前先进行备份。--checkpoint 读取备份文件时列出目录名称。--concatenate 此参数的效果和指定&quot;-A&quot;参数相同。--confirmation 此参数的效果和指定&quot;-w&quot;参数相同。--delete 从备份文件中删除指定的文件。--exclude=&lt;范本样式&gt; 排除符合范本样式的文件。--group=&lt;群组名称&gt; 把加入设备文件中的文件的所属群组设成指定的群组。--help 在线帮助。--ignore-failed-read 忽略数据读取错误，不中断程序的执行。--new-volume-script=&lt;Script文件&gt; 此参数的效果和指定&quot;-F&quot;参数相同。--newer-mtime 只保存更改过的文件。--no-recursion 不做递归处理，也就是指定目录下的所有文件及子目录不予处理。--null 从null设备读取文件名称。--numeric-owner 以用户识别码及群组识别码取代用户名称和群组名称。--owner=&lt;用户名称&gt; 把加入备份文件中的文件的拥有者设成指定的用户。--posix 将数据写入备份文件时使用POSIX格式。--preserve 此参数的效果和指定&quot;-ps&quot;参数相同。--preserve-order 此参数的效果和指定&quot;-A&quot;参数相同。--preserve-permissions 此参数的效果和指定&quot;-p&quot;参数相同。--record-size=&lt;区块数目&gt; 此参数的效果和指定&quot;-b&quot;参数相同。--recursive-unlink 解开压缩文件还原目录之前，先解除整个目录下所有文件的连接。--remove-files 文件加入备份文件后，就将其删除。--rsh-command=&lt;执行指令&gt; 设置要在远端主机上执行的指令，以取代rsh指令。--same-owner 尝试以相同的文件拥有者还原文件。--suffix=&lt;备份字尾字符串&gt; 移除文件前先行备份。--totals 备份文件建立后，列出文件大小。--use-compress-program=&lt;执行指令&gt; 通过指定的指令处理备份文件。--version 显示版本信息。--volno-file=&lt;编号文件&gt; 使用指定文件内的编号取代预设的卷册编号。 实例 打包，不压缩 1234567$ tar -cvf test.tar testtest/test/3test/1test/2test/5test/4 解包 1234567$ tar -xvf test.tar test/test/3test/1test/2test/5test/4 打包，并以 gzip 压缩 1234567$ tar -zcvf test.tar.gz testtest/test/3test/1test/2test/5test/4 在参数 f 之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加 z 参数，则以 .tar.gz 或 .tgz 来代表 gzip 压缩过的 tar 包。 解压 .tar.gz 1234567$ tar -zxvf test.tar.gz test/test/3test/1test/2test/5test/4 打包，以 bzip2 压缩 1234567$ tar -zcvf test.tar.bz2 testtest/test/3test/1test/2test/5test/4 解压 .tar.bz2 1234567$ tar -zxvf test.tar.bz2 test/test/3test/1test/2test/5test/4 查看 .tar.gz 或 .tar.bz2 压缩包内的文件，但不解压 123456789101112131415$ tar -ztvf test.tar.gz drwxrwxr-x lin/lin 0 2020-12-21 11:38 test/-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/3-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/1-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/2-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/5-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/4$ tar -ztvf test.tar.bz2 drwxrwxr-x lin/lin 0 2020-12-21 11:38 test/-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/3-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/1-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/2-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/5-rw-rw-r-- lin/lin 0 2020-12-21 11:38 test/4 解压 .tar.gz 压缩包内的部分文件 123$ tar -zxvf test.tar.gz test/2 test/3test/3test/2 解压 .tar.gz 到指定目录 1234567$ tar -zxvf test.tar.gz -C ../Download test/test/3test/1test/2test/5test/4 使用绝对路径打包压缩和解压 12345# 压缩$ tar -zcPf /home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0/patent-grant-2019.tar.gz /home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0/2019# 解压$ tar -zxPf patent-grant-2019.tar.gz tar 对文件打包时，一般不建议使用绝对路径。 如果使用绝对路径，需要加 -P 参数。如果不添加，会发出警告：tar: Removing leading &#39;/&#39; from member names。 对于使用绝对路径打包压缩的文件，解压时 tar 会在当前目录下创建压缩时的绝对路径所对应的目录，在上面例子中，即为在当前目录下创建一个子目录 home/lin/share/storage_server_3/patent/grant-patent/patent_version-1.0。 如果在解压时使用 -P 参数，需要保证系统存在压缩时的绝对路径。 使用 pigz 并发压缩和解压 安装 pigz： 1$ sudo apt install pigz 打包： 1$ tar --use-compress-program=pigz -cvpf package.tgz ./package 解包： 1$ tar --use-compress-program=pigz -xvpf package.tgz -C ./package pigz 是支持并行的 gzip，默认用当前逻辑 cpu 个数来并发压缩，无法检测个数的话，则并发 8 个线程。 另一种方式： 12345# 语法$ tar -cvpf - $Dir | pigz -9 -p 6 $target-name# 实例$ tar -cvpf - /usr/bin | pigz -9 -p 6 bin.tgz -9：代表压缩率-p ：代表 cpu 数量 time用于检测特定指令执行时所需消耗的时间及系统资源 (内存和 I/O) 等资讯。 语法1time [options] COMMAND [arguments] 参数说明12345678-o | --output=FILE 设定结果输出档。这个选项会将time的输出写入所指定的档案中。如果档案已经存在，系统将覆写其内容。 -a | --append 配合-o使用，会将结果写到档案的末端，而不会覆盖掉原来的内容。 -f FORMAT | --format=FORMAT 以FORMAT字串设定显示方式。当这个选项没有被设定的时候，会用系统预设的格式。不过你可以用环境变数time来设定这个格式，如此一来就不必每次登入系统都要设定一次。 实例 date 命令的运行时间 123$ time dateTue Dec 22 12:01:50 CST 2020date 0.00s user 0.01s system 8% cpu 0.092 total 查找文件并复制的运行时间 123$ time find /home/lin/share/storage_server_3/patent/application/unzip_version-1.0/2019 -iname &quot;*.xml&quot; | xargs -P 6 -i cp &#123;&#125; /home/lin/share/storage_server_3/patent/application-patent/patent_version-1.0/2019 find -iname &quot;*.xml&quot; 24.00s user 114.39s system 1% cpu 2:08:02.95 totalxargs -P 6 -i cp &#123;&#125; 4.35s user 28.35s system 0% cpu 2:08:02.99 total 参考： http://c.biancheng.net/linux/time.html mkdir语法参数说明实例 创建多级目录 1$ mkdir -p Project/a/src 创建多层次、多维度的目录树 1$ mkdir -p Project/&#123;a,b,c,d&#125;/src sh -csh -c 命令，可以让 bash 将一个字串作为完整的命令来执行。 比如，向 test.asc 文件中随便写入点内容，可以： 1$ echo &quot;信息&quot; &gt; test.asc 或者 1$ echo &quot;信息&quot; &gt;&gt; test.asc 下面，如果将 test.asc 权限设置为只有 root 用户才有权限进行写操作： 1$ sudo chown root.root test.asc 然后，我们使用 sudo 并配合 echo 命令再次向修改权限之后的 test.asc 文件中写入信息： 12$ sudo echo &quot;又一行信息&quot; &gt;&gt; test.asc-bash: test.asc: Permission denied 这时，可以看到 bash 拒绝这么做，说是权限不够。这是因为重定向符号 &gt; 和 &gt;&gt; 也是 bash 的命令。我们使用 sudo 只是让 echo 命令具有了 root 权限，但是没有让 &gt; 和 &gt;&gt; 命令也具有 root 权限，所以 bash 会认为这两个命令都没有向 test.asc 文件写入信息的权限。解决这一问题的途径有两种。 第一种是利用 sh -c 命令，它可以让 bash 将一个字串作为完整的命令来执行，这样就可以将 sudo 的影响范围扩展到整条命令。具体用法如下： 1$ sudo sh -c &#x27;echo &quot;又一行信息&quot; &gt;&gt; test.asc&#x27; 另一种方法是利用管道和 tee 命令，该命令可以从标准输入中读入信息并将其写入标准输出或文件中，具体用法如下： 1$ echo &quot;第三条信息&quot; | sudo tee -a test.asc 注意，tee 命令的 -a 选项的作用等同于 &gt;&gt; 命令，如果去除该选项，那么 tee 命令的作用就等同于 &gt; 命令。 1&gt;/dev/null 2&gt;&amp;1https://blog.csdn.net/ithomer/article/details/9288353 tophttps://www.jianshu.com/p/e9e0ce23a152 freepsmd5sumsha1sum用来为给定的文件或文件夹计算单个哈希，以校验文件或文件夹的完整性。 给文件： 12$ sha1sum patent-grant-2005.tar.gz 77b6416501d34b904bd25f9aa32ca60d3e14659a patent-grant-2005.tar.gz https://www.itranslater.com/qa/details/2326085750774825984 parallelhttps://linux.cn/article-9718-1.html https://www.myfreax.com/gnu-parallel/ https://www.hi-linux.com/posts/32794.html https://www.jianshu.com/p/c5a2369fa613 https://www.aqee.net/post/use-multiple-cpu-cores-with-your-linux-commands.html https://blog.csdn.net/orangefly0214/article/details/103701600","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"Java 关闭 IO 流","slug":"java-io-close","date":"2020-12-14T13:54:17.000Z","updated":"2021-04-09T08:00:50.216Z","comments":true,"path":"2020/12/14/java-io-close/","link":"","permalink":"http://example.com/2020/12/14/java-io-close/","excerpt":"","text":"在操作 java 流对象后要将流关闭，但实际编写代码时，可能会出现一些误区，导致不能正确关闭流。 在 try 中关流，而没在 finally 中关流错误： 1234567try &#123; OutputStream out = new FileOutputStream(&quot;&quot;); // ...操作流代码 out.close();&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 修正： 12345678910111213141516OutputStream out = null;try &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 在一个 try 中关闭多个流错误： 1234567891011121314151617181920OutputStream out = null;OutputStream out2 = null;try &#123; out = new FileOutputStream(&quot;&quot;); out2 = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close();// 如果此处出现异常，则out2流没有被关闭 &#125; if (out2 != null) &#123; out2.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 修正： 123456789101112131415161718192021222324OutputStream out = null;OutputStream out2 = null;try &#123; out = new FileOutputStream(&quot;&quot;); out2 = new FileOutputStream(&quot;&quot;); // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close();// 如果此处出现异常，则out2流也会被关闭 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; try &#123; if (out2 != null) &#123; out2.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 在循环中创建流，在循环外关闭，导致关闭的是最后一个流错误： 1234567891011121314151617OutputStream out = null;try &#123; for (int i = 0; i &lt; 10; i++) &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码 &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 修正： 1234567891011121314151617for (int i = 0; i &lt; 10; i++) &#123; OutputStream out = null; try &#123; out = new FileOutputStream(&quot;&quot;); // ...操作流代码 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (out != null) &#123; out.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 在 java 7 中，关闭流的方式得到很大的简化12345try (OutputStream out = new FileOutputStream(&quot;&quot;))&#123; // ...操作流代码&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 只要实现的自动关闭接口 (Closeable) 的类都可以在 try 结构体上定义，java 会自动帮我们关闭，即使在发生异常的情况下也会。 可以在 try 结构体上定义多个，用分号隔开即可，如： 123456try (OutputStream out = new FileOutputStream(&quot;&quot;); OutputStream out2 = new FileOutputStream(&quot;&quot;))&#123; // ...操作流代码&#125; catch (Exception e) &#123; throw e;&#125; Android SDK 20 版本对应 java 7，低于 20 版本无法使用 try-catch-resources 自动关流。 内存流的关闭内存流可以不用关闭。 ByteArrayOutputStream 和 ByteArrayInputStream 其实是伪装成流的字节数组 (把它们当成字节数据来看就好了)，他们不会锁定任何文件句柄和端口，如果不再被使用，字节数组会被垃圾回收掉，所以不需要关闭。 装饰流的关闭装饰流是指通过装饰模式实现的 java 流，又称为包装流，装饰流只是为原生流附加额外的功能或效果，java 中的缓冲流、桥接流也是属于装饰流。 例如： 123456789101112InputStream is = new FileInputStream(&quot;C:\\\\Users\\\\tang\\\\Desktop\\\\test.txt&quot;);InputStreamReader isr = new InputStreamReader(is);BufferedReader br = new BufferedReader(isr);String string = br.readLine();System.out.println(string);// 只需要关闭最后的br即可try &#123; br.close();&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 装饰流关闭时会调用原生流关闭。 BufferedReader.java 源码如下： 12345678910111213public void close() throws IOException &#123; synchronized (lock) &#123; if (in == null) return; try &#123; // 这里的in就是原生流 in.close(); &#125; finally &#123; in = null; cb = null; &#125; &#125;&#125; InputStreamReader.java 源码如下： 1234public void close() throws IOException &#123; // 这里的sd就是原生流的解码器(StreamDecoder)，解码器的close会调用原生流的close sd.close();&#125; 如上所示，有这样层层关闭的机制，我们就只需要关闭最外层的流就行了。 关闭流的顺序问题两个不相干的流的关闭顺序没有任何影响，如： 123// 这里的out1和out2谁先关谁后关都一样，没有任何影响out1 = new FileOutputStream(&quot;&quot;);out2 = new FileOutputStream(&quot;&quot;); 如果两个流有依赖关系，那么可以像上面说的，只关闭最外层的即可。 如果不嫌麻烦，非得一个个关闭，那么需要先关闭最里层，从里往外一层层进行关闭。 为什么不能从外层往里层逐步关闭？原因上面讲装饰流已经讲的很清楚了，关闭外层时，内层的流其实已经同时关闭了，你再去关内层的流，就会报错。 至于网上说的先声明先关闭，就是这个道理，先声明的是内层，最先申明的是最内层，最后声明的是最外层。 一定要关闭流的原因一个流绑定了一个文件句柄 (或网络端口)，如果流不关闭，该文件 (或端口) 将始终处于被锁定 (不能读取、写入、删除和重命名) 状态，占用大量系统资源却没有释放。 本文参考https://blog.csdn.net/u012643122/article/details/38540721 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"Java 的 IO 流","slug":"java-io","date":"2020-11-27T03:33:09.000Z","updated":"2021-04-13T07:21:31.070Z","comments":true,"path":"2020/11/27/java-io/","link":"","permalink":"http://example.com/2020/11/27/java-io/","excerpt":"","text":"File 类 java.io.File 类：文件和文件目录路径的抽象表示形式，与平台无关。 File 主要表示类似 D:\\\\文件目录1 与 D:\\\\文件目录1\\\\文件.txt，前者是文件夹 (directory)，后者则是文件 (file)，而 File 类就是操作这两者的类。 File 能新建、删除、重命名文件和目录，但 File 不能访问文件内容本身。如果需要访问文件内容本身，则需要使用输入/输出流。 File 跟流无关，File 类不能对文件进行读和写也就是输入和输出。 想要在 Java 程序中表示一个真实存在的文件或目录，那么必须有一个 File 对象，但是 Java 程序中的一个 File 对象，可能不对应一个真实存在的文件或目录。 File 对象可以作为参数传递给流的构造器，指明读取或写入的 “终点”。 在 Java 中，一切皆是对象，File 类也不例外，不论是哪个对象都应该从该对象的构造方法说起： public File(String pathname) ：以 pathname 为路径创建 File 对象，可以是绝对路径或者相对路径，如果 pathname 是相对路径，则默认的当前路径在系统属性 user.dir 中存储。 绝对路径：是一个固定的路径，从盘符开始。 相对路径：是相对于某个位置开始。 IDEA 中的路径说明，main() 和 Test 中，相对路径不一样： 123456789101112public class Test &#123; public static void main(String[] args) &#123; File file = new File(&quot;hello.txt&quot;);// 相较于当前工程 System.out.println(file.getAbsolutePath());// D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-projects\\hello.txt &#125; @Test public void testFileReader() &#123; File file = new File(&quot;hello.txt&quot;);// 相较于当前Module System.out.println(file.getAbsolutePath());// D:\\JetBrainsWorkSpace\\IDEAProjects\\xisun-projects\\xisun-java_base\\hello.txt &#125;&#125; public File(String parent, String child) ：以 parent 为父路径，child 为子路径创建 File 对象。 public File(File parent, String child) ：根据一个父 File 对象和子文件路径创建 File 对象。 实例： 1234567891011121314151617// 通过文件路径名 String path1 = &quot;D:\\\\123.txt&quot;;File file1 = new File(path1); // 通过文件路径名String path2 = &quot;D:\\\\1\\\\2.txt&quot;;File file2 = new File(path2); -------------相当于d:\\\\1\\\\2.txt// 通过父路径和子路径字符串 String parent = &quot;F:\\\\aaa&quot;; String child = &quot;bbb.txt&quot;; File file3 = new File(parent, child); --------相当于f:\\\\aaa\\\\bbb.txt// 通过父级File对象和子路径字符串File parentDir = new File(&quot;F:\\\\aaa&quot;);String child = &quot;bbb.txt&quot;;File file4 = new File(parentDir, child); --------相当于f:\\\\aaa\\\\bbb.txt 路径分隔符： 路径中的每级目录之间用一个路径分隔符隔开。 路径分隔符和系统有关： windows 和 DOS 系统默认使用 “\\“ 来表示。 UNIX 和 URL 使用 “/“ 来表示。 Java 程序支持跨平台运行，因此路径分隔符要慎用。为了解决这个隐患，File 类提供了一个常量 public static final String separator，能够根据操作系统，动态的提供分隔符。 实例： 123File file1 = new File(&quot;d:\\\\test\\\\info.txt&quot;);File file2 = new File(&quot;d:/test/info.txt&quot;);File file3 = new File(&quot;d:&quot; + File.separator + &quot;test&quot; + File.separator + &quot;info.txt&quot;); 获取功能的方法： public String getAbsolutePath()：获取绝对路径。 public String getPath()：获取路径。 public String getName()：获取名称。 public String getParent()：获取上层文件目录路径。若无，返回 null。 public long length()：获取文件长度，即：字节数。不能获取目录的长度。 public long lastModified()：获取最后一次的修改时间，毫秒值。 实例： 12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; File file1 = new File(&quot;hello.txt&quot;); File file2 = new File(&quot;d:\\\\io\\\\hi.txt&quot;); System.out.println(file1.getAbsolutePath()); System.out.println(file1.getPath()); System.out.println(file1.getName()); System.out.println(file1.getParent()); System.out.println(file1.length()); System.out.println(new Date(file1.lastModified())); System.out.println(); System.out.println(file2.getAbsolutePath()); System.out.println(file2.getPath()); System.out.println(file2.getName()); System.out.println(file2.getParent()); System.out.println(file2.length()); System.out.println(file2.lastModified()); &#125;&#125; public String[] list()：获取指定目录下的所有文件或者文件目录的名称数组，如果指定目录不存在，返回 null。 public File[] listFiles()：获取指定目录下的所有文件或者文件目录的 File 数组，如果指定目录不存在，返回 null。 实例： 1234567891011121314151617181920212223public class Test &#123; public static void main(String[] args) &#123; File file = new File(&quot;D:\\\\workspace_idea1\\\\JavaSenior&quot;); String[] list = file.list(); System.out.println(list); if (list != null) &#123; for (String s : list) &#123; System.out.println(s); &#125; &#125; System.out.println(); File[] files = file.listFiles(); System.out.println(files); if (files != null) &#123; for (File f : files) &#123; System.out.println(f); &#125; &#125; &#125;&#125; public String[] list(FilenameFilter filter)：指定文件过滤器。 public File[] listFiles(FilenameFilter filter)：指定文件过滤器。 public File[] listFiles(FileFilter filter)：指定文件过滤器。 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041public class Test &#123; public static void main(String[] args) &#123; File srcFile = new File(&quot;d:\\\\code&quot;); String[] subFiles1 = srcFile.list(new FilenameFilter() &#123; @Override public boolean accept(File dir, String name) &#123; return name.endsWith(&quot;.jpg&quot;); &#125; &#125;); if (subFiles1 != null) &#123; for (String fileName : subFiles1) &#123; System.out.println(fileName); &#125; &#125; File[] subFiles2 = srcFile.listFiles(new FilenameFilter() &#123; @Override public boolean accept(File dir, String name) &#123; return name.endsWith(&quot;.jpg&quot;); &#125; &#125;); if (subFiles2 != null) &#123; for (File file : subFiles2) &#123; System.out.println(file.getAbsolutePath()); &#125; &#125; File[] subFiles3 = srcFile.listFiles(new FileFilter() &#123; @Override public boolean accept(File pathname) &#123; return pathname.getName().endsWith(&quot;.jpg&quot;); &#125; &#125;); if (subFiles3 != null) &#123; for (File file : subFiles3) &#123; System.out.println(file.getAbsolutePath()); &#125; &#125; &#125;&#125; 重命名功能的方法 public boolean renameTo(File dest)：把文件重命名为指定的文件路径。以 file1.renameTo(file2) 为例：要想保证返回 true，需要 file1 在硬盘中是存在的，且 file2 在硬盘中不能存在。 实例： 123456789public class Test &#123; public static void main(String[] args) &#123; File file1 = new File(&quot;hello.txt&quot;); File file2 = new File(&quot;D:\\\\io\\\\hi.txt&quot;); boolean renameTo = file2.renameTo(file1); System.out.println(renameTo); &#125;&#125; 判断功能的方法 public boolean exists()：判断是否存在。 public boolean isDirectory()：判断是否是文件目录。 public boolean isFile()：判断是否是文件。 public boolean canRead()：判断是否可读。 public boolean canWrite()：判断是否可写。 public boolean isHidden()：判断是否隐藏。 实例： 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) &#123; File file1 = new File(&quot;hello.txt&quot;); file1 = new File(&quot;hello1.txt&quot;); System.out.println(file1.isDirectory()); System.out.println(file1.isFile()); System.out.println(file1.exists()); System.out.println(file1.canRead()); System.out.println(file1.canWrite()); System.out.println(file1.isHidden()); System.out.println(); File file2 = new File(&quot;d:\\\\io&quot;); file2 = new File(&quot;d:\\\\io1&quot;); System.out.println(file2.isDirectory()); System.out.println(file2.isFile()); System.out.println(file2.exists()); System.out.println(file2.canRead()); System.out.println(file2.canWrite()); System.out.println(file2.isHidden()); &#125;&#125; 创建功能的方法 public boolean createNewFile()：创建文件。若文件不存在，则创建一个新的空文件并返回 true；若文件存在，则不创建文件并返回 false。 public boolean mkdir()：创建文件目录。如果此文件目录存在，则不创建；如果此文件目录的上层目录不存在，也不创建。 public boolean mkdirs()：创建文件目录。如果上层文件目录不存在，也一并创建。 如果创建文件或者文件目录时，没有写盘符路径，那么，默认在项目路径下。 实例： 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; File file1 = new File(&quot;hi.txt&quot;); if (!file1.exists()) &#123; // 文件不存在 try &#123; boolean newFile = file1.createNewFile(); System.out.println(&quot;创建成功？&quot; + newFile); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; else &#123; // 文件存在 boolean delete = file1.delete(); System.out.println(&quot;删除成功？&quot; + delete); &#125; &#125;&#125; 删除功能的方法 public boolean delete()：删除文件或者文件夹。 Java 中的删除不走回收站。要删除一个文件目录，请注意该文件目录内不能包含文件或者文件目录，即只能删除空的文件目录。 实例： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) &#123; // 文件目录的创建 File file1 = new File(&quot;d:\\\\io\\\\io1\\\\io3&quot;); boolean mkdir = file1.mkdir(); if (mkdir) &#123; System.out.println(&quot;创建成功1&quot;); &#125; File file2 = new File(&quot;d:\\\\io\\\\io1\\\\io4&quot;); boolean mkdir1 = file2.mkdirs(); if (mkdir1) &#123; System.out.println(&quot;创建成功2&quot;); &#125; // 要想删除成功，io4文件目录下不能有子目录或文件 File file3 = new File(&quot;D:\\\\io\\\\io1\\\\io4&quot;); file3 = new File(&quot;D:\\\\io\\\\io1&quot;); System.out.println(file3.delete()); &#125;&#125; 递归遍历文件夹下所有文件以及子文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class Test &#123; public static void main(String[] args) &#123; // 递归:文件目录 /** 打印出指定目录所有文件名称，包括子文件目录中的文件 */ // 1.创建目录对象 File dir = new File(&quot;E:\\\\teach\\\\01_javaSE\\\\_尚硅谷Java编程语言\\\\3_软件&quot;); // 2.打印目录的子文件 printSubFile(dir); &#125; // 方式一： public static void printSubFile(File dir) &#123; // 判断传入的是否是目录 if (!dir.isDirectory()) &#123; // 不是目录直接退出 return; &#125; // 打印目录的子文件 File[] subfiles = dir.listFiles(); if (subfiles != null) &#123; for (File f : subfiles) &#123; if (f.isDirectory()) &#123; // 文件目录 printSubFile(f); &#125; else &#123; // 文件 System.out.println(f.getAbsolutePath()); &#125; &#125; &#125; &#125; // 方式二：循环实现 // 列出file目录的下级内容，仅列出一级的话，使用File类的String[] list()比较简单 public void listSubFiles(File file) &#123; if (file.isDirectory()) &#123; String[] all = file.list(); if (all != null) &#123; for (String s : all) &#123; System.out.println(s); &#125; &#125; &#125; else &#123; System.out.println(file + &quot;是文件！&quot;); &#125; &#125; // 方式三：列出file目录的下级，如果它的下级还是目录，接着列出下级的下级，依次类推 // 建议使用File类的File[] listFiles() public void listAllSubFiles(File file) &#123; if (file.isFile()) &#123; System.out.println(file); &#125; else &#123; File[] all = file.listFiles(); // 如果all[i]是文件，直接打印 // 如果all[i]是目录，接着再获取它的下一级 if (all != null) &#123; for (File f : all) &#123; // 递归调用：自己调用自己就叫递归 listAllSubFiles(f); &#125; &#125; &#125; &#125; // 拓展1：计算指定目录所在空间的大小 // 求任意一个目录的总大小 public long getDirectorySize(File file) &#123; // file是文件，那么直接返回file.length() // file是目录，把它的下一级的所有大小加起来就是它的总大小 long size = 0; if (file.isFile()) &#123; size += file.length(); &#125; else &#123; // 获取file的下一级 File[] all = file.listFiles(); if (all != null) &#123; // 累加all[i]的大小 for (File f : all) &#123; // f的大小 size += getDirectorySize(f); &#125; &#125; &#125; return size; &#125; // 拓展2：删除指定文件目录及其下的所有文件 public void deleteDirectory(File file) &#123; // 如果file是文件，直接delete // 如果file是目录，先把它的下一级干掉，然后删除自己 if (file.isDirectory()) &#123; File[] all = file.listFiles(); // 循环删除的是file的下一级 if (all != null) &#123; // f代表file的每一个下级 for (File f : all) &#123; deleteDirectory(f); &#125; &#125; &#125; // 删除自己 file.delete(); &#125;&#125; 字符编码 字符集 Charset：也叫编码表。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等。 编码表的由来：计算机只能识别二进制数据，早期由来是电信号。为了方便应用计算机，让它可以识别各个国家的文字。就将各个国家的文字用数字来表示，并一一对应，形成一张表。这就是编码表。 常见的编码表： ASCII：美国标准信息交换码。用一个字节的 7 位可以表示。 ISO8859-1：拉丁码表，欧洲码表。用一个字节的 8 位表示。 GB2312：中国的中文编码表。最多两个字节编码所有字符。 GBK：中国的中文编码表升级，融合了更多的中文文字符号。最多两个字节编码。 Unicode：国际标准码，融合了目前人类使用的所有字符。为每个字符分配唯一的字符码。所有的文字都用两个字节来表示。 UTF-8：变长的编码方式，可用 1 ~ 4 个字节来表示一个字符。 在 Unicode 出现之前，所有的字符集都是和具体编码方案绑定在一起的，即字符集 ≈ 编码方式，都是直接将字符和最终字节流绑定死了。 GBK 等双字节编码方式，用最高位是 1 或 0 表示两个字节和一个字节。 Unicode 不完美，这里就有三个问题，一个是，我们已经知道，英文字母只用一个字节表示就够了，第二个问题是如何才能区别 Unicode 和 ASCII，计算机怎么知道是两个字节表示一个符号，而不是分别表示两个符号呢？第三个，如果和 GBK 等双字节编码方式一样，用最高位是 1 或 0 表示两个字节和一个字节，就少了很多值无法用于表示字符，不够表示所有字符。Unicode 在很长一段时间内无法推广，直到互联网的出现。 面向传输的众多 UTF (UCS Transfer Format) 标准出现了，顾名思义，UTF-8 就是每次 8 个位传输数据，而 UTF-16 就是每次 16 个位。这是为传输而设计的编码，并使编码无国界，这样就可以显示全世界上所有文化的字符了。 Unicode 只是定义了一个庞大的、全球通用的字符集，并为每个字符规定了唯一确定的编号，具体存储成什么样的字节流，取决于字符编码方案。推荐的 Unicode 编码是 UTF-8 和 UTF-16。 计算机中储存的信息都是用二进制数表示的，而能在屏幕上看到的数字、英文、标点符号、汉字等字符是二进制数转换之后的结果。按照某种规则，将字符存储到计算机中，称为编码 。反之，将存储在计算机中的二进制数按照某种规则解析显示出来，称为解码 。 编码规则和解码规则要对应，否则会导致乱码。比如说，按照 A 规则存储，同样按照 A 规则解析，那么就能显示正确的文本符号。反之，按照 A 规则存储，再按照 B 规则解析，就会导致乱码现象。 编码： 字符串 —&gt; 字节数组。(能看懂的 —&gt; 看不懂的) 解码： 字节数组 —&gt; 字符串。(看不懂的 —&gt; 能看懂的) 启示：客户端/浏览器端 &lt;——&gt; 后台 (Java，GO，Python，Node.js，php…) &lt;——&gt; 数据库，要求前前后后使用的字符集要统一，都使用 UTF-8，这样才不会乱码。 IO 流原理 I/O 是 Input/Output 的缩写， I/O 技术是非常实用的技术，用于处理设备之间的数据传输。如读/写文件，网络通讯等。 Java 程序中，对于数据的输入/输出操作以 “流 (stream)” 的方式进行。 java.io 包下提供了各种 “流” 类和接口，用以获取不同种类的数据，并通过标准的方法输入或输出数据。 输入 input：读取外部数据 (磁盘、光盘等存储设备的数据) 到程序 (内存) 中。 输出 output：将程序 (内存) 数据输出到磁盘、光盘等外部存储设备中。 IO 流的分类 按操作数据单位不同分为：字节流 (8 bit)，字符流 (16 bit)。 字节流：以字节为单位，读写数据的流。 字符流：以字符为单位，读写数据的流。 按数据流的流向不同分为：输入流，输出流。 输入流：把数据从其他设备上读取到内存中的流。 输出流：把数据从内存中写出到其他设备上的流。 按流的角色的不同分为：节点流，处理流。 节点流：直接从数据源或目的地读写数据。也叫文件流。 处理流：不直接连接到数据源或目的地，而是连接在已存在的流 (节点流或处理流) 之上，通过对数据的处理为程序提供更为强大的读写功能。 Java 的 IO 流共涉及 40 多个类，实际上非常规则，都是从如下四个抽象基类派生的。同时，由这四个类派生出来的子类名称都是以其父类名作为子类名后缀： IO 流体系： 四个抽象基类InputStream &amp; Reader： InputStream 和 Reader 是所有输入流的基类。 InputStream 的典型实现：FileInputStream。 int read() int read(byte[] b) int read(byte[] b, int off, int len) Reader 的典型实现：FileReader。 int read() int read(char [] c) int read(char [] c, int off, int len) 程序中打开的文件 IO 资源不属于内存里的资源，垃圾回收机制无法回收该资源，所以应该显式关闭文件 IO 资源。 FileInputStream 从文件系统中的某个文件中获得输入字节。FileInputStream 用于读取非文本数据之类的原始字节流。如果要读取文本数据的字符流，需要使用 FileReader。 InputStream： int read()：从输入流中读取数据的下一个字节。返回 0 到 255 范围内的 int 字节值。如果因为已经到达流末尾而没有可用的字节，则返回值 -1。 int read(byte[] b)：从输入流中将最多 b.length() 个字节的数据读入一个 byte 数组中。以整数形式返回实际读取的字节数。如果因为已经到达流末尾而没有可用的字节，则返回值 -1。 int read(byte[] b, int off,int len)：将输入流中最多 len 个数据字节读入 byte 数组。尝试读取 len 个字节，但读取的字节也可能小于该值。以整数形式返回实际读取的字节数。如果因为已经到达流末尾而没有可用的字节，则返回值 -1。 public void close() throws IOException：关闭输入流并释放与该流关联的所有系统资源。 Reader： int read()：读取单个字符。作为整数读取的字符，范围在 0 到 65535 之间 (0x00-0xffff) (2 个字节的 Unicode 码)，如果已到达流的末尾，则返回 -1。 int read(char[] cbuf)：将字符读入数组。如果已到达流的末尾，则返回 -1。否则返回本次读取的字符数。 int read(char[] cbuf,int off,int len)：将字符读入数组的某一部分。存到数组 cbuf 中，从 off 处开始存储，最多读 len 个字符。如果已到达流的末尾，则返回 -1。否则返回本次读取的字符数。 public void close() throws IOException：关闭此输入流并释放与该流关联的所有系统资源。 OutputStream &amp; Writer： OutputStream 和 Writer 是所有输入流的基类。 OutputStream 的典型实现：FileOutStream。 void write(int b) void write(byte[] b) void write(byte[] b, int off, int len) public void flush() throws IOException public void close() throws IOException Writer 的典型实现：FileWriter。 void write(int c) void write(char[] cbuf) void write(char[] buff, int off, int len) public void flush() throws IOException public void close() throws IOException 因为字符流直接以字符作为操作单位，所以 Writer 还可以用字符串来替换字符数组，即以 String 对象作为参数。 void write(String str) void write(String str, int off, int len) FileOutputStream 从文件系统中的某个文件中获得输出字节。FileOutputStream 用于写出非文本数据之类的原始字节流。如果要要写出文本数据的字符流，需要使用 FileWriter。 OutputStream： void write(int b)：将指定的字节写入此输出流。write 的常规协定是：向输出流写入一个字节。要写入的字节是参数 b 的八个低位。b 的 24 个高位将被忽略，即写入 0 ~ 255 范围的。 void write(byte[] b)：将 b.length() 个字节从指定的 byte 数组写入此输出流。write(b) 的常规协定是：应该与调用 write(b, 0, b.length) 的效果完全相同。 void write(byte[] b,int off,int len)：将指定 byte 数组中从偏移量 off 开始的 len 个字节写入此输出流。 public void flush() throws IOException：刷新此输出流并强制写出所有缓冲的输出字节，调用此方法指示应将这些字节立即写入它们预期的目标。 public void close() throws IOException：关闭此输出流并释放与该流关联的所有系统资源。 Writer： void write(int c)：写入单个字符。要写入的字符包含在给定整数值的 16 个低位中，16 高位被忽略。 即写入0 到 65535 之间的 Unicode 码。 void write(char[] cbuf)：写入字符数组。 void write(char[] cbuf,int off,int len)：写入字符数组的某一部分。从 off 开始，写入 len 个字符。 void write(String str)：写入字符串。 void write(String str,int off,int len)：写入字符串的某一部分。 public void flush() throws IOException：刷新该流的缓冲，则立即将它们写入预期目标。 public void close() throws IOException：关闭此输出流并释放与该流关联的所有系统资源。 节点流 (或文件流) 读取文件流程： 实例化 File 类的对象，指明要操作的文件。 提供具体的流对象。 数据的读入。 流的关闭操作。 写入文件流程： 实例化 File 类的对象，指明写出到的文件。 提供具体的流对象。 数据的写入。 流的关闭操作。 定义文件路径时，可以用 / 或者 \\。 在写入一个文件时，如果使用构造器 FileOutputStream(file)，则目录下有同名文件将被覆盖。 如果使用构造器 FileOutputStream(file,true)，则目录下的同名文件不会被覆盖，而是在文件内容末尾追加内容。 在读取文件时，必须保证该文件已存在，否则报异常。 对于非文本文件 (.jpg，.mp3，.mp4，.avi，.rmvb，.doc，.ppt 等)，使用字节流处理。如果使用字节流操作文本文件，在输出到控制台时，可能会出现乱码。 如果只是将文本文件复制到其他地方，也可以使用字节流。 对于文本文件 (.txt，.java，.c，.cpp 等)，使用字符流处理。 FileReader 和 FileWriter 操作的实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216/** * 一、流的分类： * 1.操作数据单位：字节流、字符流 * 2.数据的流向：输入流、输出流 * 3.流的角色：节点流、处理流 * * 二、流的体系结构 * 抽象基类 节点流（或文件流） 缓冲流（处理流的一种） * InputStream FileInputStream (read(byte[] buffer)) BufferedInputStream (read(byte[] buffer)) * OutputStream FileOutputStream (write(byte[] buffer,0,len) BufferedOutputStream (write(byte[] buffer,0,len)/flush() * Reader FileReader (read(char[] cbuf)) BufferedReader (read(char[] cbuf)/readLine()) * Writer FileWriter (write(char[] cbuf,0,len) BufferedWriter (write(char[] cbuf,0,len)/flush() */public class FileReaderWriterTest &#123; /* 将当前Module下的hello.txt文件内容读入程序中，并输出到控制台 说明点： 1. read()的理解：返回读入的一个字符。如果达到文件末尾，返回-1 2. 异常的处理：为了保证流资源一定可以执行关闭操作。需要使用try-catch-finally处理 3. 读入的文件一定要存在，否则就会报FileNotFoundException。 */ // read(): 返回读入的一个字符。如果达到文件末尾，返回-1 @Test public void testFileReader() &#123; FileReader fr = null; try &#123; // 1.实例化File类的对象，指明要操作的文件 File file = new File(&quot;hello.txt&quot;);// 相较于当前Module // 2.提供具体的流 fr = new FileReader(file); // 3.数据的读入 // 方式一： /*int data = fr.read(); while (data != -1) &#123; System.out.print((char) data); data = fr.read(); &#125;*/ // 方式二：语法上针对于方式一的修改 int data; while ((data = fr.read()) != -1) &#123; System.out.print((char) data); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.流的关闭操作 // 方式一： /*try &#123; if (fr != null) fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;*/ // 方式二： if (fr != null) &#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; // 对read()操作升级：使用read的重载方法read(char[] cbuf) @Test public void testFileReader1() &#123; FileReader fr = null; try &#123; // 1.File类的实例化 File file = new File(&quot;hello.txt&quot;); // 2.FileReader流的实例化 fr = new FileReader(file); // 3.读入的操作 // read(char[] cbuf)：返回每次读入cbuf数组中的字符的个数。如果达到文件末尾，返回-1 char[] cbuf = new char[5]; int len; while ((len = fr.read(cbuf)) != -1) &#123; // 方式一： // 错误的写法，如果以cubf的length为基准，可能会造成多输出内容 /*for (int i = 0; i &lt; cbuf.length; i++) &#123; System.out.print(cbuf[i]); &#125;*/ // 正确的写法 /*for (int i = 0; i &lt; len; i++) &#123; System.out.print(cbuf[i]); &#125;*/ //方式二： // 错误的写法，对应着方式一的错误的写法 /*String str = new String(cbuf); System.out.print(str);*/ // 正确的写法，对应着方式一的正确的写法 String str = new String(cbuf, 0, len); System.out.print(str); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fr != null) &#123; // 4.资源的关闭 try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 从内存中写出数据到硬盘的文件里。 说明： 1. 输出操作，对应的File可以不存在的。并不会报异常 2. File对应的硬盘中的文件如果不存在，在输出的过程中，会自动创建此文件。 File对应的硬盘中的文件如果存在： 如果流使用的构造器是：FileWriter(file,false) / FileWriter(file)---&gt;对原有文件的覆盖 如果流使用的构造器是：FileWriter(file,true)---&gt;不会对原有文件覆盖，而是在原有文件基础上追加内容 */ @Test public void testFileWriter() &#123; FileWriter fw = null; try &#123; // 1.提供File类的对象，指明写出到的文件 File file = new File(&quot;hello1.txt&quot;); // 2.提供FileWriter的对象，用于数据的写出 fw = new FileWriter(file, false); // 3.写出的操作 fw.write(&quot;I have a dream!\\n&quot;); fw.write(&quot;you need to have a dream!&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.流资源的关闭 if (fw != null) &#123; try &#123; fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 实现对已存在文件的复制 */ @Test public void testFileReaderFileWriter() &#123; FileReader fr = null; FileWriter fw = null; try &#123; // 1.创建File类的对象，指明读入和写出的文件 File srcFile = new File(&quot;hello.txt&quot;); File destFile = new File(&quot;hello2.txt&quot;); // 不能使用字符流来处理图片等字节数据 /*File srcFile = new File(&quot;爱情与友情.jpg&quot;); File destFile = new File(&quot;爱情与友情1.jpg&quot;);*/ // 2.创建输入流和输出流的对象 fr = new FileReader(srcFile); fw = new FileWriter(destFile); // 3.数据的读入和写出操作 char[] cbuf = new char[5]; // 记录每次读入到cbuf数组中的字符的个数 int len; while ((len = fr.read(cbuf)) != -1) &#123; // 每次写出len个字符 fw.write(cbuf, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭流资源 // 方式一： /*try &#123; if (fw != null) fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fr != null) fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;*/ // 方式二： try &#123; if (fw != null) fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; if (fr != null) fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; FileInputStream 和 FileOutputStream 操作的实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147/** * 测试FileInputStream和FileOutputStream的使用 * * 结论： * 1. 对于文本文件(.txt,.java,.c,.cpp)，使用字符流处理 * 2. 对于非文本文件(.jpg,.mp3,.mp4,.avi,.doc,.ppt,...)，使用字节流处理 */public class FileInputOutputStreamTest &#123; /* 使用字节流FileInputStream处理文本文件，可能出现乱码。 */ @Test public void testFileInputStream() &#123; FileInputStream fis = null; try &#123; // 1. 造文件 File file = new File(&quot;hello.txt&quot;); // 2.造流 fis = new FileInputStream(file); // 3.读数据 byte[] buffer = new byte[5]; // 记录每次读取的字节的个数 int len; while ((len = fis.read(buffer)) != -1) &#123; String str = new String(buffer, 0, len); System.out.print(str); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fis != null) &#123; // 4.关闭资源 try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 实现对图片的复制操作 */ @Test public void testFileInputOutputStream() &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; // 1.获取文件 File srcFile = new File(&quot;爱情与友情.jpg&quot;); File destFile = new File(&quot;爱情与友情2.jpg&quot;); // 2.获取流 fis = new FileInputStream(srcFile); fos = new FileOutputStream(destFile); // 3.复制的过程 byte[] buffer = new byte[5]; int len; while ((len = fis.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭流 if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 指定路径下文件的复制 */ public void copyFile(String srcPath, String destPath) &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; // 1.获取文件 File srcFile = new File(srcPath); File destFile = new File(destPath); // 2.获取流 fis = new FileInputStream(srcFile); fos = new FileOutputStream(destFile); // 3.复制的过程 byte[] buffer = new byte[1024]; int len; while ((len = fis.read(buffer)) != -1) &#123; fos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭流 if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; @Test public void testCopyFile() &#123; long start = System.currentTimeMillis(); String srcPath = &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\01-视频.avi&quot;; String destPath = &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\02-视频.avi&quot;; /*String srcPath = &quot;hello.txt&quot;; String destPath = &quot;hello3.txt&quot;;*/ copyFile(srcPath, destPath); long end = System.currentTimeMillis(); System.out.println(&quot;复制操作花费的时间为：&quot; + (end - start));// 618 &#125;&#125; 处理流之一：缓冲流 为了提高数据读写的速度，Java API 提供了带缓冲功能的流类，在使用这些流类时，会创建一个内部缓冲区数组，缺省使用 8192 个字节 (8Kb) 的缓冲区。 123public class BufferedInputStream extends FilterInputStream &#123; private static int DEFAULT_BUFFER_SIZE = 8192;&#125; 123public class BufferedReader extends Reader &#123; private static int defaultCharBufferSize = 8192;&#125; 123public class BufferedWriter extends Writer &#123; private static int defaultCharBufferSize = 8192;&#125; 缓冲流要 “套接” 在相应的节点流之上，根据数据操作单位可以把缓冲流分为： BufferedInputStream 和 和 BufferedOutputStream public BufferedInputStream(InputStream in) ：创建一个新的缓冲输入流，注意参数类型为 InputStream。 public BufferedOutputStream(OutputStream out)： 创建一个新的缓冲输出流，注意参数类型为 OutputStream。 BufferedReader 和 BufferedWriter public BufferedReader(Reader in) ：创建一个新的缓冲输入流，注意参数类型为 Reader。 public BufferedWriter(Writer out)： 创建一个新的缓冲输出流，注意参数类型为 Writer。 当读取数据时，数据按块读入缓冲区，其后的读操作则直接访问缓冲区。 当使用 BufferedInputStream 读取字节文件时，BufferedInputStream 会一次性从文件中读取 8192 个字节 (8Kb) 存在缓冲区中，直到缓冲区装满了，才重新从文件中读取下一个 8192 个字节数组。 向流中写入字节时，不会直接写到文件，先写到缓冲区中直到缓冲区写满，BufferedOutputStream 才会把缓冲区中的数据一次性写到文件里。使用 flush() 可以强制将缓冲区的内容全部写入输出流。 flush() 的使用：手动将 buffer 中内容写入文件。 如果使用带缓冲区的流对象的 close()，不但会关闭流，还会在关闭流之前刷新缓冲区，但关闭流后不能再写出。 关闭流的顺序和打开流的顺序相反。一般只需关闭最外层流即可，关闭最外层流也会相应关闭内层节点流。 流程示意图： 实现非文本文件及文本文件的复制： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183/** * 处理流之一：缓冲流的使用 * * 1.缓冲流： * BufferedInputStream * BufferedOutputStream * BufferedReader * BufferedWriter * * 2.作用：提高流的读取、写入的速度 * 提高读写速度的原因：内部提供了一个缓冲区 * * 3. 处理流，就是&quot;套接&quot;在已有的流的基础上。(不一定必须是套接在节点流之上) */public class BufferedStreamTest &#123; /* 使用BufferedInputStream和BufferedOutputStream实现非文本文件的复制 */ @Test public void BufferedStreamTest() throws FileNotFoundException &#123; BufferedInputStream bis = null; BufferedOutputStream bos = null; try &#123; // 1.造文件 File srcFile = new File(&quot;爱情与友情.jpg&quot;); File destFile = new File(&quot;爱情与友情3.jpg&quot;); // 2.造流 // 2.1 造节点流 FileInputStream fis = new FileInputStream((srcFile)); FileOutputStream fos = new FileOutputStream(destFile); // 2.2 造缓冲流 bis = new BufferedInputStream(fis); bos = new BufferedOutputStream(fos); // 3.复制的细节：读取、写入 byte[] buffer = new byte[10]; int len; while ((len = bis.read(buffer)) != -1) &#123; bos.write(buffer, 0, len); // bos.flush();// 显示的刷新缓冲区，一般不需要 &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.资源关闭 // 要求：先关闭外层的流，再关闭内层的流 if (bos != null) &#123; try &#123; bos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (bis != null) &#123; try &#123; bis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 说明：关闭外层流的同时，内层流也会自动的进行关闭。关于内层流的关闭，我们可以省略. // fos.close(); // fis.close(); &#125; &#125; /* 使用BufferedInputStream和BufferedOutputStream实现文件复制的方法 */ public void copyFileWithBuffered(String srcPath, String destPath) &#123; BufferedInputStream bis = null; BufferedOutputStream bos = null; try &#123; // 1.造文件 File srcFile = new File(srcPath); File destFile = new File(destPath); // 2.造流 // 2.1 造节点流 FileInputStream fis = new FileInputStream((srcFile)); FileOutputStream fos = new FileOutputStream(destFile); // 2.2 造缓冲流 bis = new BufferedInputStream(fis); bos = new BufferedOutputStream(fos); // 3.复制的细节：读取、写入 byte[] buffer = new byte[1024]; int len; while ((len = bis.read(buffer)) != -1) &#123; bos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.资源关闭 // 要求：先关闭外层的流，再关闭内层的流 if (bos != null) &#123; try &#123; bos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (bis != null) &#123; try &#123; bis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 说明：关闭外层流的同时，内层流也会自动的进行关闭。关于内层流的关闭，我们可以省略. // fos.close(); // fis.close(); &#125; &#125; @Test public void testCopyFileWithBuffered() &#123; long start = System.currentTimeMillis(); String srcPath = &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\01-视频.avi&quot;; String destPath = &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\03-视频.avi&quot;; copyFileWithBuffered(srcPath, destPath); long end = System.currentTimeMillis(); System.out.println(&quot;复制操作花费的时间为：&quot; + (end - start));//618 - 176 &#125; /* 使用BufferedReader和BufferedWriter实现文本文件的复制 */ @Test public void testBufferedReaderBufferedWriter() &#123; BufferedReader br = null; BufferedWriter bw = null; try &#123; // 1.创建文件和相应的流 br = new BufferedReader(new FileReader(new File(&quot;dbcp.txt&quot;))); bw = new BufferedWriter(new FileWriter(new File(&quot;dbcp1.txt&quot;))); // 2.读写操作 // 方式一：使用char[]数组 /*char[] cbuf = new char[1024]; int len; while ((len = br.read(cbuf)) != -1) &#123;// 读到文件末尾时返回-1 bw.write(cbuf, 0, len); // bw.flush(); &#125;*/ // 方式二：使用String String data; while ((data = br.readLine()) != null) &#123;// 读到文件末尾时返回null // 方法一： // bw.write(data + &quot;\\n&quot;);// data中不包含换行符 // 方法二： bw.write(data);// data中不包含换行符 bw.newLine();// 提供换行的操作 &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 3.关闭资源 if (bw != null) &#123; try &#123; bw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (br != null) &#123; try &#123; br.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 实现图片加密： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class ImageEncryption &#123; /* 图片的加密 */ @Test public void test1() &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; fis = new FileInputStream(&quot;爱情与友情.jpg&quot;); fos = new FileOutputStream(&quot;爱情与友情secret.jpg&quot;); byte[] buffer = new byte[20]; int len; while ((len = fis.read(buffer)) != -1) &#123; // 加密：对字节数组进行修改，异或操作 // 错误的写法，buffer数组中的数据没有改变，只是重新复制给了变量b /*for (byte b : buffer) &#123; b = (byte) (b ^ 5); &#125;*/ // 正确的写法 for (int i = 0; i &lt; len; i++) &#123; buffer[i] = (byte) (buffer[i] ^ 5); &#125; fos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 图片的解密 */ @Test public void test2() &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; fis = new FileInputStream(&quot;爱情与友情secret.jpg&quot;); fos = new FileOutputStream(&quot;爱情与友情4.jpg&quot;); byte[] buffer = new byte[20]; int len; while ((len = fis.read(buffer)) != -1) &#123; // 解密：对字节数组进行修改，异或操作之后再异或，返回的是自己本身 // 错误的写法 /*for (byte b : buffer) &#123; b = (byte) (b ^ 5); &#125;*/ // 正确的写法 for (int i = 0; i &lt; len; i++) &#123; buffer[i] = (byte) (buffer[i] ^ 5); &#125; fos.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (fis != null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 获取文本上每个字符出现的次数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class WordCount &#123; /* 说明：如果使用单元测试，文件相对路径为当前module 如果使用main()测试，文件相对路径为当前工程 */ @Test public void testWordCount() &#123; FileReader fr = null; BufferedWriter bw = null; try &#123; // 1.创建Map集合 Map&lt;Character, Integer&gt; map = new HashMap&lt;Character, Integer&gt;(); // 2.遍历每一个字符，每一个字符出现的次数放到map中 fr = new FileReader(&quot;dbcp.txt&quot;); int c; while ((c = fr.read()) != -1) &#123; // int 还原 char char ch = (char) c; // 判断char是否在map中第一次出现 if (map.get(ch) == null) &#123; map.put(ch, 1); &#125; else &#123; map.put(ch, map.get(ch) + 1); &#125; &#125; // 3.把map中数据存在文件count.txt // 3.1 创建Writer bw = new BufferedWriter(new FileWriter(&quot;wordcount.txt&quot;)); // 3.2 遍历map，再写入数据 Set&lt;Map.Entry&lt;Character, Integer&gt;&gt; entrySet = map.entrySet(); for (Map.Entry&lt;Character, Integer&gt; entry : entrySet) &#123; switch (entry.getKey()) &#123; case &#x27; &#x27;: bw.write(&quot;空格 = &quot; + entry.getValue()); break; case &#x27;\\t&#x27;://\\t表示tab 键字符 bw.write(&quot;tab键 = &quot; + entry.getValue()); break; case &#x27;\\r&#x27;:// bw.write(&quot;回车 = &quot; + entry.getValue()); break; case &#x27;\\n&#x27;:// bw.write(&quot;换行 = &quot; + entry.getValue()); break; default: bw.write(entry.getKey() + &quot; = &quot; + entry.getValue()); break; &#125; bw.newLine(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 4.关闭流 if (fr != null) &#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (bw != null) &#123; try &#123; bw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 处理流之二：转换流 转换流提供了在字节流和字符流之间的转换。 Java API 提供了两个转换流： InputStreamReader：将 InputStream 转换为 Reader。 InputStreamReader(InputStream in)：创建一个使用默认字符集的字符流。 InputStreamReader(InputStream in, String charsetName)：创建一个指定字符集的字符流。 OutputStreamWriter：将 Writer 转换为 OutputStream。 OutputStreamWriter(OutputStream in)：创建一个使用默认字符集的字符流。 OutputStreamWriter(OutputStream in, String charsetName)：创建一个指定字符集的字符流。 字节流中的数据都是字符时，转成字符流操作更高效。 很多时候我们使用转换流来处理文件乱码问题，实现编码和解码的功能。 InputStreamReader： 实现将字节的输入流按指定字符集转换为字符的输入流。 需要和 InputStream 套接。 构造器 public InputStreamReader(InputStream in) public InputSreamReader(InputStream in,String charsetName) 比如：Reader isr = new InputStreamReader(System.in,&quot;gbk&quot;);，指定字符集为 gbk。 OutputStreamWriter： 实现将字符的输出流按指定字符集转换为字节的输出流。 需要和 OutputStream 套接。 构造器 public OutputStreamWriter(OutputStream out) public OutputSreamWriter(OutputStream out,String charsetName) 使用 InputStreamReader 解码时，使用的字符集取决于 OutputStreamWriter 编码时使用的字符集。 流程示意图： 转换流的编码应用： 可以将字符按指定编码格式存储。 可以对文本数据按指定编码格式来解读。 指定编码表的动作由构造器完成。 为了达到最高效率，可以考虑在 BufferedReader 内包装 InputStreamReader： 1BufferedReader in = new BufferedReader(new InputStreamReader(System.in))； 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101/** * 处理流之二：转换流的使用 * 1.转换流：属于字符流 * InputStreamReader：将一个字节的输入流转换为字符的输入流 * OutputStreamWriter：将一个字符的输出流转换为字节的输出流 * * 2.作用：提供字节流与字符流之间的转换 * * 3. 解码：字节、字节数组 ---&gt;字符数组、字符串 * 编码：字符数组、字符串 ---&gt; 字节、字节数组 * * * 4.字符集 * ASCII：美国标准信息交换码。 * 用一个字节的7位可以表示。 * ISO8859-1：拉丁码表。欧洲码表 * 用一个字节的8位表示。 * GB2312：中国的中文编码表。最多两个字节编码所有字符 * GBK：中国的中文编码表升级，融合了更多的中文文字符号。最多两个字节编码 * Unicode：国际标准码，融合了目前人类使用的所有字符。为每个字符分配唯一的字符码。所有的文字都用两个字节来表示。 * UTF-8：变长的编码方式，可用1-4个字节来表示一个字符。 */public class InputStreamReaderTest &#123; /* 此时处理异常的话，仍然应该使用try-catch-finally InputStreamReader的使用，实现字节的输入流到字符的输入流的转换 */ @Test public void test1() &#123; InputStreamReader isr = null; try &#123; FileInputStream fis = new FileInputStream(&quot;dbcp.txt&quot;); // InputStreamReader isr = new InputStreamReader(fis);// 使用系统默认的字符集，如果在IDEA中，就是看IDEA设置的默认字符集 // 参数2指明了字符集，具体使用哪个字符集，取决于文件dbcp.txt保存时使用的字符集 isr = new InputStreamReader(fis, StandardCharsets.UTF_8);// 指定字符集 char[] cbuf = new char[20]; int len; while ((len = isr.read(cbuf)) != -1) &#123; String str = new String(cbuf, 0, len); System.out.print(str); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (isr != null) &#123; try &#123; isr.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125; /* 此时处理异常的话，仍然应该使用try-catch-finally 综合使用InputStreamReader和OutputStreamWriter */ @Test public void test2() &#123; InputStreamReader isr = null; OutputStreamWriter osw = null; try &#123; // 1.造文件、造流 File file1 = new File(&quot;dbcp.txt&quot;); File file2 = new File(&quot;dbcp_gbk.txt&quot;); FileInputStream fis = new FileInputStream(file1); FileOutputStream fos = new FileOutputStream(file2); isr = new InputStreamReader(fis, StandardCharsets.UTF_8); osw = new OutputStreamWriter(fos, &quot;gbk&quot;); // 2.读写过程 char[] cbuf = new char[20]; int len; while ((len = isr.read(cbuf)) != -1) &#123; osw.write(cbuf, 0, len); &#125; &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; // 3.关闭资源 if (isr != null) &#123; try &#123; isr.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; if (osw != null) &#123; try &#123; osw.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 处理流之三：标准输入、输出流 System.in 和 System.out 分别代表了系统标准的输入和输出设备。 默认输入设备是：键盘，输出设备是：显示器。 System.in 的类型是 InputStream。 System.out 的类型是 PrintStream，其是 OutputStream 的子类 FilterOutputStream 的子类。 重定向：通过 System 类的 setIn() 和 setOut() 对默认设备进行改变。 public static void setIn(InputStream in) public static void setOut(PrintStream out) 实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class OtherStreamTest &#123; /* 1.标准的输入、输出流 1.1 System.in: 标准的输入流，默认从键盘输入 System.out: 标准的输出流，默认从控制台输出 1.2 System类的setIn(InputStream is) / setOut(PrintStream ps)方式重新指定输入和输出的流。 1.3练习： 从键盘输入字符串，要求将读取到的整行字符串转成大写输出。然后继续进行输入操作， 直至当输入“e”或者“exit”时，退出程序。 方法一：使用Scanner实现，调用next()返回一个字符串 方法二：使用System.in实现。System.in ---&gt; 转换流 ---&gt; BufferedReader的readLine() */ // IDEA的单元测试不支持从键盘输入，更改为main() public static void main(String[] args) &#123; BufferedReader br = null; try &#123; InputStreamReader isr = new InputStreamReader(System.in); br = new BufferedReader(isr); while (true) &#123; System.out.println(&quot;请输入字符串：&quot;); String data = br.readLine(); if (&quot;e&quot;.equalsIgnoreCase(data) || &quot;exit&quot;.equalsIgnoreCase(data)) &#123; System.out.println(&quot;程序结束&quot;); break; &#125; String upperCase = data.toUpperCase(); System.out.println(upperCase); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (br != null) &#123; try &#123; br.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 模拟 Scanner： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * MyInput.java: Contain the methods for reading int, double, float, boolean, short, byte and * string values from the keyboard */public class MyInput &#123; // Read a string from the keyboard public static String readString() &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); // Declare and initialize the string String string = &quot;&quot;; // Get the string from the keyboard try &#123; string = br.readLine(); &#125; catch (IOException ex) &#123; System.out.println(ex); &#125; // Return the string obtained from the keyboard return string; &#125; // Read an int value from the keyboard public static int readInt() &#123; return Integer.parseInt(readString()); &#125; // Read a double value from the keyboard public static double readDouble() &#123; return Double.parseDouble(readString()); &#125; // Read a byte value from the keyboard public static double readByte() &#123; return Byte.parseByte(readString()); &#125; // Read a short value from the keyboard public static double readShort() &#123; return Short.parseShort(readString()); &#125; // Read a long value from the keyboard public static double readLong() &#123; return Long.parseLong(readString()); &#125; // Read a float value from the keyboard public static double readFloat() &#123; return Float.parseFloat(readString()); &#125; public static void main(String[] args) &#123; int i = readInt(); System.out.println(&quot;输出的数为：&quot; + i); &#125;&#125; 处理流之四：打印流 实现将基本数据类型的数据格式转化为字符串输出。 打印流：PrintStream 和 PrintWriter。 提供了一系列重载的 print() 和 println()，用于多种数据类型的输出。 PrintStream 和 PrintWriter 的输出不会抛出 IOException 异常。 PrintStream 和 PrintWriter 有自动 flush 功能。 PrintStream 打印的所有字符都使用平台的默认字符编码转换为字节。在需要写入字符而不是写入字节的情况下，应该使用 PrintWriter 类。 System.out 返回的是 PrintStream 的实例。 把标准输出流 (控制台输出) 改成文件： 123456789101112131415161718192021222324252627282930313233343536public class OtherStreamTest &#123; /* 2. 打印流：PrintStream 和PrintWriter 2.1 提供了一系列重载的print()和println() 2.2 练习：将ASCII字符输出到自定义的外部文件 */ @Test public void test2() &#123; PrintStream ps = null; try &#123; FileOutputStream fos = new FileOutputStream(new File(&quot;D:\\\\text.txt&quot;)); // 创建打印输出流，设置为自动刷新模式(写入换行符或字节 &#x27;\\n&#x27; 时都会刷新输出缓冲区) ps = new PrintStream(fos, true); // 把标准输出流(控制台输出)改成输出到本地文件 if (ps != null) &#123; // 如果不设置，下面的循环输出是在控制台 // 设置之后，控制台不再输出，而是输出到D:\\text.txt System.setOut(ps); &#125; // 开始输出ASCII字符 for (int i = 0; i &lt;= 255; i++) &#123; System.out.print((char) i); if (i % 50 == 0) &#123;// 每50个数据一行 System.out.println();// 换行 &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (ps != null) &#123; ps.close(); &#125; &#125; &#125;&#125; 处理流之五：数据流 为了方便地操作 Java 语言的基本数据类型和 String 类型的数据，可以使用数据流。(不能操作内存中的对象) 数据流有两个类：分别用于读取和写出基本数据类型、String类的数据。 DataInputStream 和 DataOutputStream。 分别套接在 InputStream 和 和 OutputStream 子类的流上。 用 DataOutputStream 输出的文件需要用 DataInputStream 来读取。 DataInputStream 读取不同类型的数据的顺序，要与当初 DataOutputStream 写入文件时，保存的数据的顺序一致。 DataInputStream 中的方法： boolean readBoolean()，byte readByte() char readChar()，float readFloat() double readDouble()，short readShort() long readLong()，int readInt() String readUTF()，void readFully(byte[] b) DataOutputStream 中的方法： 将上述的方法的 read 改为相应的 write 即可。 将内存中的字符串、基本数据类型的变量写出到文件中，再读取到内存中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class OtherStreamTest &#123; /* 3. 数据流 3.1 DataInputStream 和 DataOutputStream 3.2 作用：用于读取或写出基本数据类型的变量或字符串 练习：将内存中的字符串、基本数据类型的变量写出到文件中。 注意：处理异常的话，仍然应该使用try-catch-finally。 */ @Test public void test3() &#123; DataOutputStream dos = null; try &#123; // 1.造流 dos = new DataOutputStream(new FileOutputStream(&quot;data.txt&quot;)); // 2.写入操作 dos.writeUTF(&quot;刘建辰&quot;);// 写入String dos.flush();// 刷新操作，将内存中的数据立即写入文件，也可以在关闭流时自动刷新 dos.writeInt(23);// 写入int dos.flush(); dos.writeBoolean(true);// 写入boolean dos.flush(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; // 3.关闭流 if (dos != null) &#123; try &#123; dos.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125; /* 将文件中存储的基本数据类型变量和字符串读取到内存中，保存在变量中。 注意点：读取不同类型的数据的顺序要与当初写入文件时，保存的数据的顺序一致！ */ @Test public void test4() &#123; DataInputStream dis = null; try &#123; // 1.造流 dis = new DataInputStream(new FileInputStream(&quot;data.txt&quot;)); // 2.读取操作 String name = dis.readUTF();// 读取String int age = dis.readInt();// 读取int boolean isMale = dis.readBoolean();// 读取boolean System.out.println(&quot;name = &quot; + name); System.out.println(&quot;age = &quot; + age); System.out.println(&quot;isMale = &quot; + isMale); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; // 3.关闭流 if (dis!=null) &#123; try &#123; dis.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 处理流之六：对象流 ObjectInputStream 和 OjbectOutputSteam：用于存储和读取基本数据类型数据或对象的处理流。它的强大之处就是可以把 Java 中的对象写入到数据源中，也能把对象从数据源中还原回来。 一般情况下，会把对象转换为 Json 字符串，然后进行序列化和反序列化操作，而不是直接操作对象。 序列化：用 ObjectOutputStream 类保存基本类型数据或对象的机制。 反序列化：用 ObjectInputStream 类读取基本类型数据或对象的机制。 ObjectOutputStream 和 ObjectInputStream 不能序列化 static 和 transient 修饰的成员变量。 在序列化一个类的对象时，如果类中含有 static 和 transient 修饰的成员变量，则在反序列化时，这些成员变量的值会变成默认值，而不是序列化时这个对象赋予的值。比如，Person 类含有一个 static 修饰的 String name 属性，序列化时，对象把 name 赋值为张三，在反序列化时，name 会变为 null 对象序列化机制允许把内存中的 Java 对象转换成平台无关的二进制流 (序列化操作)，从而允许把这种二进制流持久地保存在磁盘上，或通过网络将这种二进制流传输到另一个网络节点。当其它程序获取了这种二进制流，就可以恢复成原来的 Java 对象 (反序列化操作)。 序列化的好处在于可将任何实现了 Serializable 接口的对象转化为字节数据，使其在保存和传输时可被还原。 序列化是 RMI (Remote Method Invoke – 远程方法调用) 过程的参数和返回值都必须实现的机制，而 RMI 是 JavaEE 的基础，因此序列化机制是 JavaEE 平台的基础。 如果需要让某个对象支持序列化机制，则必须让对象所属的类及其属性是可序列化的，为了让某个类是可序列化的，该类必须实现如下两个接口之一。否则，会抛出 NotSerializableException 异常。 Serializable Externalizable 凡是实现 Serializable 接口的类都有一个表示序列化版本标识符的静态变量： private static final long serialVersionUID; serialVersionUID 用来表明类的不同版本间的兼容性。 简言之，其目的是以序列化对象进行版本控制，有关各版本反序列化时是否兼容。 如果类没有显示定义这个静态常量，它的值是 Java 运行时环境根据类的内部细节自动生成的。此时，若类的实例变量做了修改，serialVersionUID 可能发生变化，则再对修改之前被序列化的类进行反序列化操作时，会操作失败。因此，建议显式声明 serialVersionUID。 在某些场合，希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有相同的 serialVersionUID；在某些场合，不希望类的不同版本对序列化兼容，因此需要确保类的不同版本具有不同的 serialVersionUID。 当序列化了一个类实例后，后续可能更改一个字段或添加一个字段。如果不设置 serialVersionUID，所做的任何更改都将导致无法反序化旧有实例，并在反序列化时抛出一个异常；如果你添加了 serialVersionUID，在反序列旧有实例时，新添加或更改的字段值将设为初始化值 (对象为 null，基本类型为相应的初始默认值)，字段被删除将不设置。 简单来说，Java 的序列化机制是通过在运行时判断类的 serialVersionUID 来验证版本一致性的。在进行反序列化时，JVM 会把传来的字节流中的 serialVersionUID 与本地相应实体类的 serialVersionUID 进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常，即 InvalidCastException。 若某个类实现了 Serializable 接口，该类的对象就是可序列化的： 创建一个 ObjectOutputStream。 public ObjectOutputStream(OutputStream out)： 创建一个指定 OutputStream 的 ObjectOutputStream。 调用 ObjectOutputStream 对象的 writeObject(Object obj) 输出可序列化对象。 注意写出一次，操作 flush() 一次。 反序列化： 创建一个 ObjectInputStream。 public ObjectInputStream(InputStream in)： 创建一个指定 InputStream 的 ObjectInputStream。 调用 readObject() 读取流中的对象。 强调：如果某个类的属性不是基本数据类型或 String 类型，而是另一个引用类型，那么这个引用类型必须是可序列化的，否则拥有该类型的 Field 的类也不能序列化。 默认情况下，基本数据类型是可序列化的。String 实现了 Serializable 接口。 流程示意图： 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * Person需要满足如下的要求，方可序列化 * 1.需要实现接口：Serializable * 2.当前类提供一个全局常量：serialVersionUID * 3.除了当前Person类需要实现Serializable接口之外，还必须保证其内部所有属性 * 也必须是可序列化的。（默认情况下，基本数据类型可序列化） * * 补充：ObjectOutputStream和ObjectInputStream不能序列化static和transient修饰的成员变量 */public class Person implements Serializable &#123; public static final long serialVersionUID = 475463534532L; private String name; private int age; private int id; private Account acct; public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public Person(String name, int age, int id) &#123; this.name = name; this.age = age; this.id = id; &#125; public Person(String name, int age, int id, Account acct) &#123; this.name = name; this.age = age; this.id = id; this.acct = acct; &#125; @Override public String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, id=&quot; + id + &quot;, acct=&quot; + acct + &#x27;&#125;&#x27;; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125;class Account implements Serializable &#123; public static final long serialVersionUID = 4754534532L; private double balance; public Account(double balance) &#123; this.balance = balance; &#125; @Override public String toString() &#123; return &quot;Account&#123;&quot; + &quot;balance=&quot; + balance + &#x27;&#125;&#x27;; &#125; public double getBalance() &#123; return balance; &#125; public void setBalance(double balance) &#123; this.balance = balance; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * 对象流的使用 * 1.ObjectInputStream 和 ObjectOutputStream * 2.作用：用于存储和读取基本数据类型数据或对象的处理流。它的强大之处就是可以把Java中的对象写入到数据源中，也能把对象从数据源中还原回来。 * * 3.要想一个java对象是可序列化的，需要满足相应的要求。见Person.java * * 4.序列化机制： * 对象序列化机制允许把内存中的Java对象转换成平台无关的二进制流，从而允许把这种 * 二进制流持久地保存在磁盘上，或通过网络将这种二进制流传输到另一个网络节点。 * 当其它程序获取了这种二进制流，就可以恢复成原来的Java对象。 */public class ObjectInputOutputStreamTest &#123; /* 序列化过程：将内存中的java对象保存到磁盘中或通过网络传输出去 使用ObjectOutputStream实现 */ @Test public void testObjectOutputStream() &#123; ObjectOutputStream oos = null; try &#123; // 1.造流 oos = new ObjectOutputStream(new FileOutputStream(&quot;object.dat&quot;)); // 2.序列化：写操作 oos.writeObject(new String(&quot;我爱北京天安门&quot;)); oos.flush();// 刷新操作 oos.writeObject(new Person(&quot;王铭&quot;, 23)); oos.flush(); oos.writeObject(new Person(&quot;张学&quot;, 23, 1001, new Account(5000))); oos.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (oos != null) &#123; // 3.关闭流 try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 反序列化：将磁盘文件中的对象还原为内存中的一个java对象 使用ObjectInputStream来实现 */ @Test public void testObjectInputStream() &#123; ObjectInputStream ois = null; try &#123; // 1.造流 ois = new ObjectInputStream(new FileInputStream(&quot;object.dat&quot;)); // 2.反序列化：读操作 // 文件中保存的是不同类型的对象，反序列化时，需要与序列化时的顺序一致 Object obj = ois.readObject(); String str = (String) obj; System.out.println(str); Person p = (Person) ois.readObject(); System.out.println(p); Person p1 = (Person) ois.readObject(); System.out.println(p1); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (ois != null) &#123; try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 面试题：谈谈你对 java.io.Serializable 接口的理解，我们知道它用于序列化，是空方法接口，还有其它认识吗？ 实现了 Serializable 接口的对象，可将它们转换成一系列字节，并可在以后完全恢复回原来的样子。 这一过程亦可通过网络进行。这意味着序列化机制能自动补偿操作系统间的差异。换句话说，可以先在 Windows 机器上创台 建一个对象，对其序列化，然后通过网络发给一台 Unix 机器，然后在那里准确无误地重新“装配”。不必关心数据在不同机器上如何表示，也不必关心字节的顺序或者其他任何细节。 由于大部分作为参数的类如 String 、Integer 等都实现了 java.io.Serializable 接口，也可以利用多态的性质，作为参数使接口更灵活。 随机存取文件流 RandomAccessFile 声明在 java.io 包下，但直接继承于 java.lang.Object 类。并且它实现了 DataInput、DataOutput 这两个接口，也就意味着这个类既可以读也可以写。 RandomAccessFile 类支持 “随机访问” 的方式，程序可以直接跳到文件的任意地方来读、写文件。 支持只访问文件的部分内容。 可以向已存在的文件后追加内容。 RandomAccessFile 对象包含一个记录指针，用以标示当前读写处的位置。RandomAccessFile 类对象可以自由移动记录指针： long getFilePointer()：获取文件记录指针的当前位置。 void seek(long pos)：将文件记录指针定位到 pos 位置。 构造器 public RandomAccessFile(File file, String mode) public RandomAccessFile(String name, String mode) 创建 RandomAccessFile 类实例需要指定一个 mode 参数，该参数指定 RandomAccessFile 的访问模式： r：以只读方式打开。 rw：打开以便读取和写入。 rwd：打开以便读取和写入；同步文件内容的更新。 rws：打开以便读取和写入；同步文件内容和元数据的更新。 JDK 1.6 上面写的每次 write 数据时，rw 模式，数据不会立即写到硬盘中，而 rwd 模式，数据会被立即写入硬盘。如果写数据过程发生异常，rwd 模式中已被 write 的数据会被保存到硬盘，而 rw 模式的数据会全部丢失。 如果模式为只读 r，则不会创建文件，而是会去读取一个已经存在的文件，如果读取的文件不存在则会出现异常。 如果模式为读写 rw，如果文件不存在则会去创建文件，如果存在则不会创建。 RandomAccessFile 的应用：我们可以用 RandomAccessFile 这个类，来实现一个多线程断点下载的功能，用过下载工具的朋友们都知道，下载前都会建立两个临时文件，一个是与被下载文件大小相同的空文件，另一个是记录文件指针的位置文件，每次暂停的时候，都会保存上一次的指针，然后断点下载的时候，会继续从上一次的地方下载，从而实现断点下载或上传的功能，有兴趣的朋友们可以自己实现下。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132/** * RandomAccessFile的使用 * 1.RandomAccessFile直接继承于java.lang.Object类，实现了DataInput和DataOutput接口 * 2.RandomAccessFile既可以作为一个输入流，又可以作为一个输出流 * * 3.如果RandomAccessFile作为输出流时，写出到的文件如果不存在，则在执行过程中自动创建。 * 如果写出到的文件存在，则会对原有文件内容进行覆盖。（默认情况下，从头覆盖） * * 4. 可以通过相关的操作，实现RandomAccessFile“插入”数据的效果 */public class RandomAccessFileTest &#123; /* 使用RandomAccessFile实现文件的复制 */ @Test public void test1() &#123; RandomAccessFile raf1 = null; RandomAccessFile raf2 = null; try &#123; // 1.造流 raf1 = new RandomAccessFile(new File(&quot;爱情与友情.jpg&quot;), &quot;r&quot;); raf2 = new RandomAccessFile(new File(&quot;爱情与友情1.jpg&quot;), &quot;rw&quot;); // 2.读写操作 byte[] buffer = new byte[1024]; int len; while ((len = raf1.read(buffer)) != -1) &#123; raf2.write(buffer, 0, len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 3.关闭流 if (raf1 != null) &#123; try &#123; raf1.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (raf2 != null) &#123; try &#123; raf2.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /* 使用RandomAccessFile实现文件内容的覆盖和追加 */ @Test public void test2() &#123; RandomAccessFile raf1 = null; try &#123; // hello.txt内容为：abcdefghijklmn File file = new File(&quot;hello.txt&quot;); raf1 = new RandomAccessFile(file, &quot;rw&quot;); raf1.write(&quot;123&quot;.getBytes());// 从头开始覆盖：123defghijklmn raf1.seek(5);// 将指针调到角标为5的位置，角标从0开始 raf1.write(&quot;456&quot;.getBytes());// 从角标为5处开始覆盖：123de456ijklmn raf1.seek(file.length());// 将指针调到文件末尾 raf1.write(&quot;789&quot;.getBytes());// 在文件末尾追加：123de456ijklmn789 &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (raf1 != null) &#123; try &#123; raf1.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125; /* 使用RandomAccessFile实现数据的插入效果 */ @Test public void test3() &#123; RandomAccessFile raf1 = null; try &#123; // hello.txt内容为：abcdefghijklmn File file = new File(&quot;hello.txt&quot;); raf1 = new RandomAccessFile(file, &quot;rw&quot;); // 将指针调到角标为3的位置，从此处开始读入文件的数据 raf1.seek(3); // 方法一：保存指针3后面的所有数据到StringBuilder中 /*StringBuilder builder = new StringBuilder((int) file.length()); byte[] buffer = new byte[20]; int len; while ((len = raf1.read(buffer)) != -1) &#123; builder.append(new String(buffer, 0, len)); &#125;*/ // 方法二：保存指针3后面的所有数据到ByteArrayOutputStream中 ByteArrayOutputStream baos = new ByteArrayOutputStream(); byte[] buffer = new byte[10]; int len; while ((len = raf1.read(buffer)) != -1) &#123; baos.write(buffer, 0, len); &#125; // 经过上面的读操作后，指针位置移到了文件的末尾处 // 调回指针，写入&quot;123&quot;，实际上是覆盖原文件内容 raf1.seek(3); raf1.write(&quot;123&quot;.getBytes());// abc123ghijklmn // 经过上面的写入操作，指针位置已到了123后，紧接着： // 方法一：将StringBuilder中的数据写入到文件中，实际上是覆盖123后的内容 // raf1.write(builder.toString().getBytes());// abc123defghijklmn // 方法二：将ByteArrayOutputStream中的数据写入到文件中 raf1.write(baos.toString().getBytes());// abc123defghijklmn &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; finally &#123; if (raf1 != null) &#123; try &#123; raf1.close(); &#125; catch (IOException exception) &#123; exception.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 流的基本应用小结 流是用来处理数据的。 处理数据时，一定要先明确数据源，与数据目的地： 数据源可以是文件，可以是键盘。 数据目的地可以是文件、显示器或者其他设备。 流只是在帮助数据进行传输，并对传输的数据进行处理，比如过滤处理、转换处理等。 NIO.2 中 Path 、Paths 、Files Java NIO (New IO 或 Non-Blocking IO) 是从 Java 1.4 版本开始引入的一套新的 IO API，可以替代标准的 Java IO API。NIO 与原来的 IO 有同样的作用和目的，但是使用的方式完全不同，NIO 支持面向缓冲区的 (IO是面向流的)、基于通道的 IO 操作，NIO 也会以更加高效的方式进行文件的读写操作。 Java API 中提供了两套 NIO，一套是针对标准输入输出 NIO，另一套就是网络编程 NIO。 |—– java.nio.channels.Channel |—– FileChannel：处理本地文件。 |—– SocketChannel：TCP 网络编程的客户端的 Channel。 |—– ServerSocketChannel：TCP 网络编程的服务器端的 Channel。 |—– DatagramChannel：UDP 网络编程中发送端和接收端的 Channel。 随着 JDK 7 的发布，Java 对 NIO 进行了极大的扩展，增强了对文件处理和文件系统特性的支持，以至于我们称他们为 NIO.2。因为 NIO 提供的一些功能，NIO 已经成为文件处理中越来越重要的部分。 早期的 Java 只提供了一个 File 类来访问文件系统，但 File 类的功能比较有限，所提供的方法性能也不高。而且，大多数方法在出错时仅返回失败，并不会提供异常信息。 NIO. 2 为了弥补这种不足，引入了 Path 接口，代表一个平台无关的平台路径，描述了目录结构中文件的位置。Path 可以看成是 File 类的升级版本，实际引用的资源也可以不存在。 在以前 IO 操作是类似如下写法的： 123import java.io.File;File file = new File(&quot;index.html&quot;); 但在 Java 7 中，我们可以这样写： 1234import java.nio.file.Path;import java.nio.file.Paths;Path path = Paths.get(&quot;index.html&quot;); 同时，NIO.2 在 java.nio.file 包下还提供了 Files、Paths 工具类，Files 包含了大量静态的工具方法来操作文件；Paths 则包含了两个返回 Path 的静态工厂方法。 Paths 类提供的获取 Path 对象的方法： static Path get(String first, String … more)：用于将多个字符串串连成路径。 static Path get(URI uri)：返回指定 uri 对应的 Path 路径。 12345678910111213141516public class PathTest &#123; /* 如何使用Paths实例化Path */ @Test public void test1() &#123; Path path1 = Paths.get(&quot;d:\\\\nio\\\\hello.txt&quot;);// = new File(String filepath) System.out.println(path1); Path path2 = Paths.get(&quot;d:\\\\&quot;, &quot;nio\\\\hello.txt&quot;);// = new File(String parent,String filename); System.out.println(path2); Path path3 = Paths.get(&quot;d:\\\\&quot;, &quot;nio&quot;); System.out.println(path3); &#125;&#125; Path 类常用方法： String toString()：返回调用 Path 对象的字符串表示形式。 boolean startsWith(String path)：判断是否以 path 路径开始。 boolean endsWith(String path)：判断是否以 path 路径结束。 boolean isAbsolute()：判断是否是绝对路径。 Path getParent()：返回 Path 对象包含整个路径，不包含 Path 对象指定的文件路径。 Path getRoot()：返回调用 Path 对象的根路径。 Path getFileName()：返回与调用 Path 对象关联的文件名。 int getNameCount()：返回 Path 根目录后面元素的数量。 Path getName(int idx)：返回指定索引位置 idx 的路径名称。 Path toAbsolutePath()：作为绝对路径返回调用 Path 对象。 Path resolve(Path p)：合并两个路径，返回合并后的路径对应的 Path 对象。 File toFile()：将 Path 转化为 File 类的对象。File 类转化为 Path 对象的方法是：Path toPath()。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class PathTest &#123; /* Path中的常用方法 */ @Test public void test2() &#123; Path path1 = Paths.get(&quot;d:\\\\&quot;, &quot;nio\\\\nio1\\\\nio2\\\\hello.txt&quot;); Path path2 = Paths.get(&quot;hello1.txt&quot;);// 相对当前Module的路径 // String toString()：返回调用Path对象的字符串表示形式 System.out.println(path1);// d:\\nio\\nio1\\nio2\\hello.txt // boolean startsWith(String path): 判断是否以path路径开始 System.out.println(path1.startsWith(&quot;d:\\\\nio&quot;));// true // boolean endsWith(String path): 判断是否以path路径结束 System.out.println(path1.endsWith(&quot;hello.txt&quot;));// true // boolean isAbsolute(): 判断是否是绝对路径 System.out.println(path1.isAbsolute() + &quot;~&quot;);// true~ System.out.println(path2.isAbsolute() + &quot;~&quot;);// false~ // Path getParent()：返回Path对象包含整个路径，不包含Path对象指定的文件路径 System.out.println(path1.getParent());// d:\\nio\\nio1\\nio2 System.out.println(path2.getParent());// null // Path getRoot()：返回调用Path对象的根路径 System.out.println(path1.getRoot());// d:\\ System.out.println(path2.getRoot());// null // Path getFileName(): 返回与调用Path对象关联的文件名 System.out.println(path1.getFileName() + &quot;~&quot;);// hello.txt~ System.out.println(path2.getFileName() + &quot;~&quot;);// hello1.txt~ // int getNameCount(): 返回Path根目录后面元素的数量 // Path getName(int idx): 返回指定索引位置idx的路径名称 for (int i = 0; i &lt; path1.getNameCount(); i++) &#123; // nio*****nio1*****nio2*****hello.txt***** System.out.print(path1.getName(i) + &quot;*****&quot;); &#125; System.out.println(); // Path toAbsolutePath(): 作为绝对路径返回调用Path对象 System.out.println(path1.toAbsolutePath());// d:\\nio\\nio1\\nio2\\hello.txt System.out.println(path2.toAbsolutePath());// D:\\xisun-projects\\java_base\\hello1.txt // Path resolve(Path p): 合并两个路径，返回合并后的路径对应的Path对象 Path path3 = Paths.get(&quot;d:\\\\&quot;, &quot;nio&quot;); Path path4 = Paths.get(&quot;nioo\\\\hi.txt&quot;); path3 = path3.resolve(path4); System.out.println(path3);// d:\\nio\\nioo\\hi.txt // File toFile(): 将Path转化为File类的对象 File file = path1.toFile();// Path---&gt;File的转换 // Path toPath(): 将File转化为Path类的对象 Path newPath = file.toPath();// File---&gt;Path的转换 &#125;&#125; java.nio.file.Files：用于操作文件或目录的工具类。 Files 常用方法： Path copy(Path src, Path dest, CopyOption … how)：文件的复制。 Path createDirectory(Path path, FileAttribute&lt;?&gt; … attr)：创建一个目录。 Path createFile(Path path, FileAttribute&lt;?&gt; … arr)：创建一个文件。 void delete(Path path)：删除一个文件/目录，如果不存在，执行报错。 void deleteIfExists(Path path)：Path 对应的文件/目录如果存在，执行删除。 Path move(Path src, Path dest, CopyOption…how)：将 src 移动到 dest 位置。 long size(Path path)：返回 path 指定文件的大小。 boolean exists(Path path, LinkOption … opts)：判断文件是否存在。 boolean isDirectory(Path path, LinkOption … opts)：判断是否是目录。 boolean isRegularFile(Path path, LinkOption … opts)：判断是否是文件。 boolean isHidden(Path path)：判断是否是隐藏文件。 boolean isReadable(Path path)：判断文件是否可读。 boolean isWritable(Path path)：判断文件是否可写。 boolean notExists(Path path, LinkOption … opts)：判断文件是否不存在。 SeekableByteChannel newByteChannel(Path path, OpenOption…how)：获取与指定文件的连接，how 指定打开方式。 DirectoryStream\\&lt;Path&gt; newDirectoryStream(Path path)：打开 path 指定的目录。 InputStream newInputStream(Path path, OpenOption…how)：获取 InputStream 对象。 OutputStream newOutputStream(Path path, OpenOption…how)：获取 OutputStream 对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class FilesTest &#123; @Test public void test1() throws IOException &#123; Path path1 = Paths.get(&quot;d:\\\\nio&quot;, &quot;hello.txt&quot;); Path path2 = Paths.get(&quot;atguigu.txt&quot;); // Path copy(Path src, Path dest, CopyOption … how): 文件的复制 // 要想复制成功，要求path1对应的物理上的文件存在。path2 对应的文件没有要求。 // Files.copy(path1, path2, StandardCopyOption.REPLACE_EXISTING); // Path createDirectory(Path path, FileAttribute&lt;?&gt; … attr): 创建一个目录 // 要想执行成功，要求path对应的物理上的文件目录不存在。一旦存在，抛出异常。 Path path3 = Paths.get(&quot;d:\\\\nio\\\\nio1&quot;); // Files.createDirectory(path3); // Path createFile(Path path, FileAttribute&lt;?&gt; … arr): 创建一个文件 // 要想执行成功，要求path对应的物理上的文件不存在。一旦存在，抛出异常。 Path path4 = Paths.get(&quot;d:\\\\nio\\\\hi.txt&quot;); // Files.createFile(path4); // void delete(Path path): 删除一个文件/目录，如果不存在，执行报错 // Files.delete(path4); // void deleteIfExists(Path path): Path对应的文件/目录如果存在，执行删除。如果不存在，正常执行结束 Files.deleteIfExists(path3); // Path move(Path src, Path dest, CopyOption…how): 将src移动到dest位置 // 要想执行成功，src对应的物理上的文件需要存在，dest对应的文件没有要求。 // Files.move(path1, path2, StandardCopyOption.ATOMIC_MOVE); // long size(Path path): 返回path指定文件的大小 long size = Files.size(path2); System.out.println(size); &#125; @Test public void test2() throws IOException &#123; Path path1 = Paths.get(&quot;d:\\\\nio&quot;, &quot;hello.txt&quot;); Path path2 = Paths.get(&quot;atguigu.txt&quot;); // boolean exists(Path path, LinkOption … opts): 判断文件是否存在 System.out.println(Files.exists(path2, LinkOption.NOFOLLOW_LINKS)); // boolean isDirectory(Path path, LinkOption … opts): 判断是否是目录 // 不要求此path对应的物理文件存在。 System.out.println(Files.isDirectory(path1, LinkOption.NOFOLLOW_LINKS)); // boolean isRegularFile(Path path, LinkOption … opts): 判断是否是文件 // /boolean isHidden(Path path): 判断是否是隐藏文件 // 要求此path对应的物理上的文件需要存在。才可判断是否隐藏。否则，抛异常。 System.out.println(Files.isHidden(path1)); // /boolean isReadable(Path path): 判断文件是否可读 System.out.println(Files.isReadable(path1)); // boolean isWritable(Path path): 判断文件是否可写 System.out.println(Files.isWritable(path1)); // boolean notExists(Path path, LinkOption … opts): 判断文件是否不存在 System.out.println(Files.notExists(path1, LinkOption.NOFOLLOW_LINKS)); &#125; /** * StandardOpenOption.READ: 表示对应的Channel是可读的。 * StandardOpenOption.WRITE：表示对应的Channel是可写的。 * StandardOpenOption.CREATE：如果要写出的文件不存在，则创建。如果存在，忽略 * StandardOpenOption.CREATE_NEW：如果要写出的文件不存在，则创建。如果存在，抛异常 * * @throws IOException */ @Test public void test3() throws IOException &#123; Path path1 = Paths.get(&quot;d:\\\\nio&quot;, &quot;hello.txt&quot;); // InputStream newInputStream(Path path, OpenOption…how): 获取InputStream对象 InputStream inputStream = Files.newInputStream(path1, StandardOpenOption.READ); // OutputStream newOutputStream(Path path, OpenOption…how): 获取OutputStream对象 OutputStream outputStream = Files.newOutputStream(path1, StandardOpenOption.WRITE, StandardOpenOption.CREATE); // SeekableByteChannel newByteChannel(Path path, OpenOption…how): 获取与指定文件的连接，how指定打开方式 SeekableByteChannel channel = Files.newByteChannel(path1, StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE); // DirectoryStream&lt;Path&gt; newDirectoryStream(Path path): 打开path指定的目录 Path path2 = Paths.get(&quot;e:\\\\teach&quot;); DirectoryStream&lt;Path&gt; directoryStream = Files.newDirectoryStream(path2); Iterator&lt;Path&gt; iterator = directoryStream.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125;&#125; FileUtils 工具类 Maven 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt;&lt;/dependency&gt; 复制功能： 123456789101112public class FileUtilsTest &#123; public static void main(String[] args) &#123; File srcFile = new File(&quot;day10\\\\爱情与友情.jpg&quot;); File destFile = new File(&quot;day10\\\\爱情与友情2.jpg&quot;); try &#123; FileUtils.copyFile(srcFile, destFile); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 遍历文件夹和文件的每一行： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class FileUtilsMethod &#123; /** * 常规方法：若文件路径内的文件比较少，可以采用此方法 * * @param filePath 文件路径 */ public static void common(String filePath) &#123; File file = new File(filePath); if (file.exists()) &#123; // 获取子文件夹内所有文件，放到文件数组里，如果含有大量文件，会创建一个很大的数组，占用空间 File[] fileList = file.listFiles(); for (File currentFile : fileList) &#123; // 当前文件是普通文件（排除文件夹），且不是隐藏文件 if (currentFile.isFile() &amp;&amp; !currentFile.isHidden()) &#123; // 当前文件的完整路径，含文件名 String currentFilePath = currentFile.getPath(); if (currentFilePath.endsWith(&quot;xml&quot;) || currentFilePath.endsWith(&quot;XML&quot;)) &#123; // 当前文件的文件名，含后缀 String fileName = currentFile.getName(); System.out.println(&quot;文件名：&quot; + fileName); &#125; &#125; &#125; System.out.println(&quot;=======================================&quot;); // list方法返回的是文件名的String数组 String[] fileNameList = file.list(); for (String fileName : fileNameList) &#123; System.out.println(&quot;文件名：&quot; + fileName); &#125; &#125; &#125; /** * 根据文件路径，迭代获取该路径下指定文件后缀类型的文件：若文件路径内含有大量文件，建议采用此方法 * * @param filePath 文件路径 */ public static void iterateFiles(String filePath) &#123; File file = FileUtils.getFile(filePath); if (file.isDirectory()) &#123; Iterator&lt;File&gt; fileIterator = FileUtils.iterateFiles(file, new String[]&#123;&quot;xml&quot;, &quot;XML&quot;&#125;, false); while (fileIterator.hasNext()) &#123; File currentFile = fileIterator.next(); if (currentFile.isFile() &amp;&amp; !currentFile.isHidden()) &#123; // 绝对路径 String currentFilePath = currentFile.getAbsolutePath(); System.out.println(&quot;绝对路径：&quot; + currentFilePath); // 文件名，含文件后缀 String fileName = currentFilePath.substring(currentFilePath.lastIndexOf(&quot;\\\\&quot;) + 1); System.out.println(&quot;含后缀文件名：&quot; + fileName); // 文件名，不含文件后缀 fileName = fileName.substring(0, fileName.lastIndexOf(&quot;.&quot;)); System.out.println(&quot;不含后缀文件名：&quot; + fileName); &#125; &#125; &#125; &#125; /** * 读取目标文件每一行数据，返回List：若文件内容较少，可以采用此方法 * * @param filePath 文件路径 * @throws IOException */ public static void readLinesForList(String filePath) throws IOException &#123; List&lt;String&gt; linesList = FileUtils.readLines(new File(filePath), &quot;utf-8&quot;); for (String line : linesList) &#123; System.out.println(line); &#125; &#125; /** * 读取目标文件每一行数据，返回迭代器：若文件内容较多，建议采用此方法 * * @param filePath 文件路径 * @throws IOException */ public static void readLinesForIterator(String filePath) throws IOException &#123; LineIterator lineIterator = FileUtils.lineIterator(new File(filePath), &quot;utf-8&quot;); while (lineIterator.hasNext()) &#123; System.out.println(lineIterator.next()); &#125; &#125;&#125; 本文参考https://www.gulixueyuan.com/goods/show/203?targetId=309&amp;preview=0 https://juejin.cn/post/6844903985078337550#heading-55 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"KafkaConsumer 源码之 consumer 的 offset commit 机制和 partition 分配机制","slug":"kafka-consumer-commitandpartition","date":"2020-11-24T07:17:06.000Z","updated":"2021-01-05T07:32:30.026Z","comments":true,"path":"2020/11/24/kafka-consumer-commitandpartition/","link":"","permalink":"http://example.com/2020/11/24/kafka-consumer-commitandpartition/","excerpt":"","text":"紧接着上篇文章，这篇文章讲述 consumer 提供的 offset commit 机制和 partition 分配机制，具体如何使用是需要用户结合具体的场景进行选择，本文讲述一下其底层实现。 自动 offset commit 机制1234// 自动提交，默认trueprops.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// 设置自动每1s提交一次，默认为5sprops.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 通过上面设置，启动自动提交 offset 以及设置自动提交间隔时间。 手动 offset commit 机制先看下两种不同的手动 offset commit 机制，一种是同步 commit，一种是异步 commit，既然其作用都是 offset commit，应该不难猜到它们底层使用接口都是一样的，其调用流程如下图所示： 同步 commit1234567// 对poll()中返回的所有topics和partition列表进行commit// 这个方法只能将offset提交Kafka中，Kafka将会在每次rebalance之后的第一次拉取或启动时使用同步commit// 这是同步commit，它将会阻塞进程，直到commit成功或者遇到一些错误public void commitSync() &#123;&#125;// 只对指定的topic-partition列表进行commitpublic void commitSync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) &#123;&#125; 其实，从上图中，就已经可以看出，同步 commit 的实现方式，client.poll () 方法会阻塞直到这个 request 完成或超时才会返回。 异步 commit12345public void commitAsync() &#123;&#125;public void commitAsync(OffsetCommitCallback callback) &#123;&#125;public void commitAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback) &#123;&#125; 对于异步的 commit，最后调用的都是 doCommitOffsetsAsync () 方法，其具体实现如下： 12345678910111213141516171819202122232425262728private void doCommitOffsetsAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, final OffsetCommitCallback callback) &#123; // 发送offset-commit请求 RequestFuture&lt;Void&gt; future = sendOffsetCommitRequest(offsets); final OffsetCommitCallback cb = callback == null ? defaultOffsetCommitCallback : callback; future.addListener(new RequestFutureListener&lt;Void&gt;() &#123; @Override public void onSuccess(Void value) &#123; if (interceptors != null) interceptors.onCommit(offsets); // 添加成功的请求，以唤醒相应的回调函数 completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, null)); &#125; @Override public void onFailure(RuntimeException e) &#123; Exception commitException = e; if (e instanceof RetriableException) &#123; commitException = new RetriableCommitFailedException(e); &#125; // 添加失败的请求，以唤醒相应的回调函数 completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, commitException)); if (commitException instanceof FencedInstanceIdException) &#123; asyncCommitFenced.set(true); &#125; &#125; &#125;);&#125; 在异步 commit 中，可以添加相应的回调函数，如果 request 处理成功或处理失败，ConsumerCoordinator 会通过 invokeCompletedOffsetCommitCallbacks () 方法唤醒相应的回调函数。 上面简单的介绍了同步 commit 和异步 commit，更详细的分析参考：Kafka consumer 的 offset 的提交方式。 注意：手动 commit 时，提交的是下一次要读取的 offset。举例如下： 1234567891011121314151617181920try &#123; while(running) &#123; // 取得消息 ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE); // 根据分区来遍历数据 for (TopicPartition partition : records.partitions()) &#123; List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition); // 数据处理 for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123; System.out.println(record.offset() + &quot;: &quot; + record.value()); &#125; // 取得当前读取到的最后一条记录的offset long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); // 提交offset，记得要 + 1 consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1))); &#125; &#125;&#125; finally &#123; consumer.close();&#125; commit offset 请求的处理当 Kafka Server 端接收到来自 client 端的 offset commit 请求时，对于提交的 offset，GroupCoordinator 会记录在 GroupMetadata 对象中，至于其实现的逻辑细节，此处不再赘述。 partition 分配机制consumer 提供了三种不同的 partition 分配策略，可以通过 partition.assignment.strategy 参数进行配置，默认情况下使用的是 org.apache.kafka.clients.consumer.RangeAssignor，Kafka 中提供了另外两种 partition 的分配策略 org.apache.kafka.clients.consumer.RoundRobinAssignor 和 org.apache.kafka.clients.consumer.StickyAssignor，它们关系如下图所示： 通过上图可以看出，用户可以自定义相应的 partition 分配机制，只需要继承这个 AbstractPartitionAssignor 抽象类即可。 partition 分配策略，其实也就是 reblance 策略。 AbstractPartitionAssignorAbstractPartitionAssignor 有一个抽象方法，如下所示： 12345678910/** * Perform the group assignment given the partition counts and member subscriptions * @param partitionsPerTopic The number of partitions for each subscribed topic. Topics not in metadata will be excluded * from this map. * @param subscriptions Map from the memberId to their respective topic subscription * @return Map from each member to the list of partitions assigned to them. */// 根据partitionsPerTopic和subscriptions进行分配，具体的实现会在子类中实现(不同的子类，其实现方法不相同)public abstract Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions); assign () 这个方法，有两个参数： partitionsPerTopic：所订阅的每个 topic 与其 partition 数的对应关系，metadata 没有的 topic 将会被移除； subscriptions：每个 consumerId 与其所订阅的 topic 列表的关系。 继承 AbstractPartitionAssignor 的子类，通过实现 assign () 方法，来进行相应的 partition 分配。 RangeAssignor 分配模式assign () 方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; // 1.参数含义:(topic, List&lt;consumerId&gt;)，获取每个topic被多少个consumer订阅了 Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions); // 2.存储最终的分配方案 Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) assignment.put(memberId, new ArrayList&lt;&gt;()); for (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123; String topic = topicEntry.getKey(); List&lt;String&gt; consumersForTopic = topicEntry.getValue(); // 3.每个topic的partition数量 Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic == null) continue; Collections.sort(consumersForTopic); // 4.取商，表示平均每个consumer会分配到多少个partition int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size(); // 5.取余，表示平均分配后还剩下多少个partition未被分配 int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size(); List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic); // 6.这里是关键点，分配原则是将未能被平均分配的partition分配到前consumersWithExtraPartition个consumer for (int i = 0, n = consumersForTopic.size(); i &lt; n; i++) &#123; // 假设partition有7个，consumer有5个，则numPartitionsPerConsumer=1，consumersWithExtraPartition=2 // i=0, start: 0, length: 2, topic-partition: p0, p1 // i=1, start: 2, length: 2, topic-partition: p2, p3 // i=2, start: 4, length: 1, topic-partition: p4 // i=3, start: 5, length: 1, topic-partition: p5 // i=4, start: 6, length: 1, topic-partition: p6 int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition); int length = numPartitionsPerConsumer + (i + 1 &gt; consumersWithExtraPartition ? 0 : 1); assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length)); &#125; &#125; return assignment;&#125; 假设 topic 的 partition 数为 numPartitionsForTopic，group 中订阅这个 topic 的 member 数为 consumersForTopic.size()，首先需要算出两个值： numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size()：表示平均每个 consumer 会分配到几个 partition； consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size()：表示平均分配后还剩下多少个 partition 未分配。 分配的规则是：对于剩下的那些 partition 分配到前 consumersWithExtraPartition 个 consumer 上，也就是前 consumersWithExtraPartition 个 consumer 获得 topic-partition 列表会比后面多一个。 在上述的程序中，举了一个例子，假设有一个 topic 有 7 个 partition，group 有5个 consumer，这个5个 consumer 都订阅这个 topic，那么 range 的分配方式如下： 消费者 分配方案 consumer 0 start: 0, length: 2, topic-partition: p0, p1 consumer 1 start: 2, length: 2, topic-partition: p2, p3 consumer 2 start: 4, length: 1, topic-partition: p4 consumer 3 start: 5, length: 1, topic-partition: p5 consumer 4 start: 6, length: 1, topic-partition: p6 而如果 group 中有 consumer 没有订阅这个 topic，那么这个 consumer 将不会参与分配。下面再举个例子，假设有 2 个 topic，一个有 5 个 partition，另一个有 7 个 partition，group 中有 5 个 consumer，但是只有前 3 个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下： consumer 订阅 topic1 的列表 订阅 topic2 的列表 consumer 0 t1 p0, t1 p1 t2 p0, t2 p1 consumer 1 t1 p2, t1 p3 t2 p2, t2 p3 consumer 2 t1 p4 t2 p4 consumer 3 t2 p5 consumer 4 t2 p6 RoundRobinAssignorassign () 方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536@Overridepublic Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) assignment.put(memberId, new ArrayList&lt;&gt;()); // 环状链表，存储所有的consumer，一次迭代完之后又会回到原点 CircularIterator&lt;String&gt; assigner = new CircularIterator&lt;&gt;(Utils.sorted(subscriptions.keySet())); // 获取所有订阅的topic的partition总数 for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) &#123; final String topic = partition.topic(); while (!subscriptions.get(assigner.peek()).topics().contains(topic)) assigner.next(); assignment.get(assigner.next()).add(partition); &#125; return assignment;&#125;public List&lt;TopicPartition&gt; allPartitionsSorted(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; // 所有的topics(有序) SortedSet&lt;String&gt; topics = new TreeSet&lt;&gt;(); for (Subscription subscription : subscriptions.values()) topics.addAll(subscription.topics()); // 订阅的Topic的所有的TopicPartition集合 List&lt;TopicPartition&gt; allPartitions = new ArrayList&lt;&gt;(); for (String topic : topics) &#123; Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic != null) // topic的所有partition都添加进去 allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic)); &#125; return allPartitions;&#125; Round Robin 的实现原则，简单来说就是：列出所有 topic-partition 和列出所有的 consumer member，然后开始分配，一轮之后继续下一轮，假设有一个 topic，它有7个 partition，group 中有 3 个 consumer 都订阅了这个 topic，那么其分配方式为： 消费者 分配列表 consumer 0 p0, p3, p6 consumer 1 p1, p4 consumer 2 p2, p5 对于多个 topic 的订阅，假设有 2 个 topic，一个有 5 个 partition，一个有 7 个 partition，group 中有 5 个 consumer，但是只有前 3 个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下： 消费者 订阅 topic1 的列表 订阅的 topic2 的列表 consumer 0 t1 p0, t1 p3 t2 p0, t2 p5 consumer 1 t1 p1, t1 p4 t2 p1, t2 p6 consumer 2 t1 p2 t2 p2 consumer 3 t2 p3 consumer 4 t2 p4 StickyAssignorassign () 方法的实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;TopicPartition&gt;&gt; currentAssignment = new HashMap&lt;&gt;(); Map&lt;TopicPartition, ConsumerGenerationPair&gt; prevAssignment = new HashMap&lt;&gt;(); partitionMovements = new PartitionMovements(); prepopulateCurrentAssignments(subscriptions, currentAssignment, prevAssignment); boolean isFreshAssignment = currentAssignment.isEmpty(); // a mapping of all topic partitions to all consumers that can be assigned to them final Map&lt;TopicPartition, List&lt;String&gt;&gt; partition2AllPotentialConsumers = new HashMap&lt;&gt;(); // a mapping of all consumers to all potential topic partitions that can be assigned to them final Map&lt;String, List&lt;TopicPartition&gt;&gt; consumer2AllPotentialPartitions = new HashMap&lt;&gt;(); // initialize partition2AllPotentialConsumers and consumer2AllPotentialPartitions in the following two for loops for (Entry&lt;String, Integer&gt; entry: partitionsPerTopic.entrySet()) &#123; for (int i = 0; i &lt; entry.getValue(); ++i) partition2AllPotentialConsumers.put(new TopicPartition(entry.getKey(), i), new ArrayList&lt;&gt;()); &#125; for (Entry&lt;String, Subscription&gt; entry: subscriptions.entrySet()) &#123; String consumer = entry.getKey(); consumer2AllPotentialPartitions.put(consumer, new ArrayList&lt;&gt;()); entry.getValue().topics().stream().filter(topic -&gt; partitionsPerTopic.get(topic) != null).forEach(topic -&gt; &#123; for (int i = 0; i &lt; partitionsPerTopic.get(topic); ++i) &#123; TopicPartition topicPartition = new TopicPartition(topic, i); consumer2AllPotentialPartitions.get(consumer).add(topicPartition); partition2AllPotentialConsumers.get(topicPartition).add(consumer); &#125; &#125;); // add this consumer to currentAssignment (with an empty topic partition assignment) if it does not already exist if (!currentAssignment.containsKey(consumer)) currentAssignment.put(consumer, new ArrayList&lt;&gt;()); &#125; // a mapping of partition to current consumer Map&lt;TopicPartition, String&gt; currentPartitionConsumer = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry: currentAssignment.entrySet()) for (TopicPartition topicPartition: entry.getValue()) currentPartitionConsumer.put(topicPartition, entry.getKey()); List&lt;TopicPartition&gt; sortedPartitions = sortPartitions( currentAssignment, prevAssignment.keySet(), isFreshAssignment, partition2AllPotentialConsumers, consumer2AllPotentialPartitions); // all partitions that need to be assigned (initially set to all partitions but adjusted in the following loop) List&lt;TopicPartition&gt; unassignedPartitions = new ArrayList&lt;&gt;(sortedPartitions); for (Iterator&lt;Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt;&gt; it = currentAssignment.entrySet().iterator(); it.hasNext();) &#123; Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry = it.next(); if (!subscriptions.containsKey(entry.getKey())) &#123; // if a consumer that existed before (and had some partition assignments) is now removed, remove it from currentAssignment for (TopicPartition topicPartition: entry.getValue()) currentPartitionConsumer.remove(topicPartition); it.remove(); &#125; else &#123; // otherwise (the consumer still exists) for (Iterator&lt;TopicPartition&gt; partitionIter = entry.getValue().iterator(); partitionIter.hasNext();) &#123; TopicPartition partition = partitionIter.next(); if (!partition2AllPotentialConsumers.containsKey(partition)) &#123; // if this topic partition of this consumer no longer exists remove it from currentAssignment of the consumer partitionIter.remove(); currentPartitionConsumer.remove(partition); &#125; else if (!subscriptions.get(entry.getKey()).topics().contains(partition.topic())) &#123; // if this partition cannot remain assigned to its current consumer because the consumer // is no longer subscribed to its topic remove it from currentAssignment of the consumer partitionIter.remove(); &#125; else // otherwise, remove the topic partition from those that need to be assigned only if // its current consumer is still subscribed to its topic (because it is already assigned // and we would want to preserve that assignment as much as possible) unassignedPartitions.remove(partition); &#125; &#125; &#125; // at this point we have preserved all valid topic partition to consumer assignments and removed // all invalid topic partitions and invalid consumers. Now we need to assign unassignedPartitions // to consumers so that the topic partition assignments are as balanced as possible. // an ascending sorted set of consumers based on how many topic partitions are already assigned to them TreeSet&lt;String&gt; sortedCurrentSubscriptions = new TreeSet&lt;&gt;(new SubscriptionComparator(currentAssignment)); sortedCurrentSubscriptions.addAll(currentAssignment.keySet()); balance(currentAssignment, prevAssignment, sortedPartitions, unassignedPartitions, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer); return currentAssignment;&#125; sticky 分区策略是从 0.11 版本才开始引入的，它主要有两个目的： 分区的分配要尽可能均匀 分区的分配要尽可能与上次分配的保持相同 当两者冲突的时候，第一个目标优先于第二个目标。 sticky 的分区方式作用发生分区重分配的时候，尽可能地让前后两次分配相同，进而减少系统资源的损耗及其他异常情况的发生。因为 sticky 分区策略的代码，要比 range 和 roundrobin 复杂很多，此处不做具体的细节分析，只简单举例如下： 假设有 3 个 topic，一个有 2 个 partition，一个有 3 个 partition，另外一个有 4 个 partition，group 中有 3 个 consumer，第一个 consumer 订阅了第一个 topic，第二个 consumer 订阅了前两个 topic，第三个 consumer 订阅了三个 topic，那么它们的分配方案如下： 消费者 订阅 topic1 的列表 订阅 topic2 的列表 订阅 topic3 的列表 consumer1 t1 p0 consumer2 t1 p1, t2 p1 t2 p0, t2 p3 consumer3 t3 p0, t3 p1, t3 p2, t3 p3 上面三个分区策略有着不同的分配方式，在实际使用过程中，需要根据自己的需求选择合适的策略，但是如果你只有一个 consumer，那么选择哪个方式都是一样的，但是如果是多个 consumer 不在同一台设备上进行消费，那么 sticky 方式应该更加合适。 自定义分区策略如之前所说，只需要继承 AbstractPartitionAssignor 并复写其中方法即可 (当然也可以直接实现 PartitionAssignor 接口) 自定义分区策略，其中有两个方法需要复写： 1234public String name();public abstract Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions); 其中 assign () 方法表示的是分区分配方案的实现，而 name () 方法则表示了这个分配策略的唯一名称，比如之前提到的 range，roundrobin 和 sticky, 这个名字会在和 GroupCoordinator 的通信中返回，通过它 consumer leader 来确定整个 group 的分区方案 (分区策略是由 group 中的 consumer 共同投票决定的，谁使用的多，就使用哪个策略)。 本文参考http://generalthink.github.io/2019/06/06/kafka-consumer-partition-assign/ https://matt33.com/2017/11/19/consumer-two-summary/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。 至此，关于 Kafka 的学习暂时告一段落，未来有需要时，会继续学习。更多关于 Kafka 原理等知识的介绍，参考： http://generalthink.github.io/tags/Kafka/ https://matt33.com/tags/kafka/","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaConsumer 源码之 consumer 的两种订阅模式","slug":"kafka-consumer-subscribeandassign","date":"2020-11-24T02:18:47.000Z","updated":"2021-01-05T07:33:01.339Z","comments":true,"path":"2020/11/24/kafka-consumer-subscribeandassign/","link":"","permalink":"http://example.com/2020/11/24/kafka-consumer-subscribeandassign/","excerpt":"","text":"在前面的文章中，有简单的介绍了 KafkaConsumer 的两种订阅模式，本篇文章对此进行扩展说明一下。 KafkaConsumer 的两种订阅模式， subscribe () 模式和 assign () 模式，前者是 topic 粒度 (使用 group 管理)，后者是 topic-partition 粒度 (用户自己去管理)。 订阅模式KafkaConsumer 为订阅模式提供了 4 种 API，如下： 12345678910111213// 订阅指定的topic列表，并且会自动进行动态partition订阅// 当发生以下情况时，会进行rebalance:1.订阅的topic列表改变；2.topic被创建或删除；3.consumer线程die；4.加一个新的consumer线程// 当发生rebalance时，会唤醒ConsumerRebalanceListener线程public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123;&#125;// 同上，但是这里没有设置listenerpublic void subscribe(Collection&lt;String&gt; topics) &#123;&#125;// 订阅那些满足一定规则(pattern)的topicpublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123;&#125;// 同上，但是这里没有设置listenerpublic void subscribe(Pattern pattern) &#123;&#125; 以上 4 种 API 都是按照 topic 级别去订阅，可以动态地获取其分配的 topic-partition，这是使用 Group 动态管理，它不能与手动 partition 管理一起使用。当监控到发生下面的事件时，Group 将会触发 rebalance 操作： 订阅的 topic 列表变化； topic 被创建或删除； consumer group 的某个 consumer 实例挂掉； 一个新的 consumer 实例通过 join 方法加入到一个 group 中。 在这种模式下，当 KafkaConsumer 调用 poll () 方法时，第一步会首先加入到一个 group 中，并获取其分配的 topic-partition 列表，具体细节在前面的文章中已经分析过了。 这里介绍一下当调用 subscribe () 方法之后，consumer 所做的事情，分两种情况介绍，一种按 topic 列表订阅，一种是按 pattern 模式订阅： topic 列表订阅 topic 列表订阅，最终调用如下方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Subscribe to the given list of topics to get dynamically * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * As part of group management, the consumer will keep track of the list of consumers that belong to a particular * group and will trigger a rebalance operation if any one of the following events are triggered: * &lt;ul&gt; * &lt;li&gt;Number of partitions change for any of the subscribed topics * &lt;li&gt;A subscribed topic is created or deleted * &lt;li&gt;An existing member of the consumer group is shutdown or fails * &lt;li&gt;A new member is added to the consumer group * &lt;/ul&gt; * &lt;p&gt; * When any of these events are triggered, the provided listener will be invoked first to indicate that * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received. * Note that rebalances will only occur during an active call to &#123;@link #poll(Duration)&#125;, so callbacks will * also only be invoked during that time. * * The provided listener will immediately override any listener set in a previous call to subscribe. * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics * subscribed in this call. See &#123;@link ConsumerRebalanceListener&#125; for more details. * * @param topics The list of topics to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; acquireAndEnsureOpen(); try &#123; maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot be null&quot;); if (topics.isEmpty()) &#123; // treat subscribing to empty topic list as the same as unsubscribing this.unsubscribe(); &#125; else &#123; for (String topic : topics) &#123; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;); &#125; throwIfNoAssignorsConfigured(); fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(&quot;Subscribed to topic(s): &#123;&#125;&quot;, Utils.join(topics, &quot;, &quot;)); // 核心步骤在此处执行 if (this.subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; 将 SubscriptionType 类型设置为 AUTO_TOPICS，并更新 SubscriptionState 中记录的 subscription 属性 (记录的是订阅的 topic 列表)； 12345678910111213141516171819public synchronized boolean subscribe(Set&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_TOPICS); return changeSubscription(topics);&#125;private boolean changeSubscription(Set&lt;String&gt; topicsToSubscribe) &#123; if (subscription.equals(topicsToSubscribe)) return false; subscription = topicsToSubscribe; if (subscriptionType != SubscriptionType.USER_ASSIGNED) &#123; groupSubscription = new HashSet&lt;&gt;(groupSubscription); groupSubscription.addAll(topicsToSubscribe); &#125; else &#123; groupSubscription = new HashSet&lt;&gt;(topicsToSubscribe); &#125; return true;&#125; 请求更新 metadata。 1234567891011121314public synchronized void requestUpdateForNewTopics() &#123; // Override the timestamp of last refresh to let immediate update. this.lastRefreshMs = 0; this.requestVersion++; requestUpdate();&#125;/** * Request an update of the current cluster metadata info, return the current updateVersion before the update */public synchronized int requestUpdate() &#123; this.needUpdate = true; return this.updateVersion;&#125; pattern 模式订阅 pattern 模式订阅，最终调用如下方法： 123456789101112131415161718192021222324252627282930313233343536/** * Subscribe to all topics matching specified pattern to get dynamically assigned partitions. * The pattern matching will be done periodically against all topics existing at the time of check. * This can be controlled through the &#123;@code metadata.max.age.ms&#125; configuration: by lowering * the max metadata age, the consumer will refresh metadata more often and check for matching topics. * &lt;p&gt; * See &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125; for details on the * use of the &#123;@link ConsumerRebalanceListener&#125;. Generally rebalances are triggered when there * is a change to the topics matching the provided pattern and when consumer group membership changes. * Group rebalances only take place during an active call to &#123;@link #poll(Duration)&#125;. * * @param pattern Pattern to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If pattern or listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with topics, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; maybeThrowInvalidGroupIdException(); if (pattern == null) throw new IllegalArgumentException(&quot;Topic pattern to subscribe to cannot be null&quot;); acquireAndEnsureOpen(); try &#123; throwIfNoAssignorsConfigured(); log.info(&quot;Subscribed to pattern: &#x27;&#123;&#125;&#x27;&quot;, pattern); this.subscriptions.subscribe(pattern, listener); this.coordinator.updatePatternSubscription(metadata.fetch()); this.metadata.requestUpdateForNewTopics(); &#125; finally &#123; release(); &#125;&#125; 将 SubscriptionType 类型设置为 AUTO_PATTERN，并更新 SubscriptionState 中记录的 subscribedPattern 属性，设置为 pattern； 12345public synchronized void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_PATTERN); this.subscribedPattern = pattern;&#125; 调用 coordinator 的 updatePatternSubscription () 方法，遍历所有 topic 的 metadata，找到所有满足 pattern 的 topic 列表，更新到 SubscriptionState 的 subscriptions 属性，并请求更新 Metadata； 1234567891011121314151617181920212223242526272829public void updatePatternSubscription(Cluster cluster) &#123; final Set&lt;String&gt; topicsToSubscribe = cluster.topics().stream() .filter(subscriptions::matchesSubscribedPattern) .collect(Collectors.toSet()); if (subscriptions.subscribeFromPattern(topicsToSubscribe)) metadata.requestUpdateForNewTopics();&#125;public synchronized boolean subscribeFromPattern(Set&lt;String&gt; topics) &#123; if (subscriptionType != SubscriptionType.AUTO_PATTERN) throw new IllegalArgumentException(&quot;Attempt to subscribe from pattern while subscription type set to &quot; + subscriptionType); return changeSubscription(topics);&#125;private boolean changeSubscription(Set&lt;String&gt; topicsToSubscribe) &#123; if (subscription.equals(topicsToSubscribe)) return false; subscription = topicsToSubscribe; if (subscriptionType != SubscriptionType.USER_ASSIGNED) &#123; groupSubscription = new HashSet&lt;&gt;(groupSubscription); groupSubscription.addAll(topicsToSubscribe); &#125; else &#123; groupSubscription = new HashSet&lt;&gt;(topicsToSubscribe); &#125; return true;&#125; 1234567891011121314public synchronized void requestUpdateForNewTopics() &#123; // Override the timestamp of last refresh to let immediate update. this.lastRefreshMs = 0; this.requestVersion++; requestUpdate();&#125;/** * Request an update of the current cluster metadata info, return the current updateVersion before the update */public synchronized int requestUpdate() &#123; this.needUpdate = true; return this.updateVersion;&#125; 其他部分，两者基本一样，只是 pattern 模型在每次更新 topic-metadata 时，获取全局的 topic 列表，如果发现有新加入的符合条件的 topic，就立马去订阅，其他的地方，包括 group 管理、topic-partition 的分配都是一样的。 分配模式当调用 assign () 方法手动分配 topic-partition 列表时，不会使用 consumer 的 Group 管理机制，也即是当 consumer group member 变化或 topic 的 metadata 信息变化时，不会触发 rebalance 操作。比如：当 topic 的 partition 增加时，这里无法感知，需要用户进行相应的处理，Apache Flink 就是使用的这种方式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Manually assign a list of partitions to this consumer. This interface does not allow for incremental assignment * and will replace the previous assignment (if there is one). * &lt;p&gt; * If the given list of topic partitions is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * &lt;p&gt; * Manual topic assignment through this method does not use the consumer&#x27;s group management * functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic * metadata change. Note that it is not possible to use both manual partition assignment with &#123;@link #assign(Collection)&#125; * and group assignment with &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;. * &lt;p&gt; * If auto-commit is enabled, an async commit (based on the old assignment) will be triggered before the new * assignment replaces the old one. * * @param partitions The list of partitions to assign this consumer * @throws IllegalArgumentException If partitions is null or contains null or empty topics * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with topics or pattern * (without a subsequent call to &#123;@link #unsubscribe()&#125;) */@Overridepublic void assign(Collection&lt;TopicPartition&gt; partitions) &#123; acquireAndEnsureOpen(); try &#123; if (partitions == null) &#123; throw new IllegalArgumentException(&quot;Topic partition collection to assign to cannot be null&quot;); &#125; else if (partitions.isEmpty()) &#123; this.unsubscribe(); &#125; else &#123; for (TopicPartition tp : partitions) &#123; String topic = (tp != null) ? tp.topic() : null; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic partitions to assign to cannot have null or empty topic&quot;); &#125; fetcher.clearBufferedDataForUnassignedPartitions(partitions); // make sure the offsets of topic partitions the consumer is unsubscribing from // are committed since there will be no following rebalance if (coordinator != null) this.coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds()); log.info(&quot;Subscribed to partition(s): &#123;&#125;&quot;, Utils.join(partitions, &quot;, &quot;)); if (this.subscriptions.assignFromUser(new HashSet&lt;&gt;(partitions))) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; assign () 方法是手动向 consumer 分配一些 topic-partition 列表，并且这个接口不允许增加分配的 topic-partition 列表，将会覆盖之前分配的 topic-partition 列表，如果给定的 topic-partition 列表为空，它的作用将会与 unsubscribe () 方法一样。 这种手动 topic 分配也不会使用 consumer 的 group 管理，当 group 的 member 变化或 topic 的 metadata 变化时，也不会触发 rebalance 操作。 这里所说的 consumer 的 group 管理，就是前面所说的 consumer 如何加入 group 的管理过程。如果使用的是 assign 模式，也即是非 AUTO_TOPICS 或 AUTO_PATTERN 模式时，consumer 实例在调用 poll () 方法时，不会向 GroupCoordinator 发送 join-group、sync-group、heartbeat 请求，也就是说 GroupCoordinator 拿不到这个 consumer 实例的相关信息，也不会去维护这个 member 是否存活，这种情况下就需要用户自己管理自己的处理程序。但是这种模式可以进行 offset commit，这将在下一篇文章进行分析。 小结根据上面的讲述，这里做一下小结，两种模式对比如下图所示： 简单说明如下： 模式 不同之处 相同之处 subscribe () 使用 Kafka group 管理，自动进行 rebalance 操作 可以在 Kafka 保存 offset assign () 用户自己进行相关的处理 也可以进行 offset commit，但是尽量保证 group.id 唯一性，如果使用一个与上面模式一样的 group，offset commit 请求将会被拒绝 本文参考https://matt33.com/2017/11/18/consumer-subscribe/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"maven 的基础使用","slug":"maven","date":"2020-11-18T07:18:48.000Z","updated":"2021-04-09T07:50:19.362Z","comments":true,"path":"2020/11/18/maven/","link":"","permalink":"http://example.com/2020/11/18/maven/","excerpt":"","text":"maven 的功能maven 是一个项目管理工具，主要作用是在项目开发阶段对项目进行依赖管理和项目构建。 依赖管理：仅仅通过 jar 包的几个属性，就能确定唯一的 jar 包，在指定的文件 pom.xml 中，只要写入这些依赖属性，就会自动下载并管理 jar 包。 项目构建：内置很多的插件与生命周期，支持多种任务，比如校验、编译、测试、打包、部署、发布… 项目的知识管理：管理项目相关的其他内容，比如开发者信息，版本等等。 maven 的安装与配置 下载，地址：http://maven.apache.org/download.cgi 注意：安装 maven 之前，必须先确保你的机器中已经安装了 jdk，如果是 maven 3 则必须 jdk 1.7 以上。 解压，添加环境变量 MAVEN_HOME，值为解压后的 maven 路径。 在 Path 环境变量的变量值末尾添加 %MAVEN_HOME%\\bin; 。 在 cmd 窗口输入 mvn –version，显示 maven 版本信息，说明安装配置成功。 在 IDEA 中使用 maven maven 的仓库maven 仓库分为本地仓库和远程仓库，而远程仓库又分为 maven 中央仓库、其他远程仓库和私服 (私有服务器)。其中，中央仓库是由 maven 官方提供的，而私服就需要我们自己搭建。 本地仓库默认情况下，不管 linux 还是 windows，每个用户在自己的用户目录下都有一个路径名为 .m2\\repository 的仓库目录，如：C:\\Users\\XiSun\\.m2\\repository。如果不自定义本地仓库的地址，则会将下载的构件放到该目录下。 修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，可以自定义本地仓库地址，例如： 1&lt;localRepository&gt;D:\\Program Files\\Maven\\apache-maven-3.6.3-maven-repository&lt;/localRepository&gt; 运行 maven 的时候，maven 所需要的任何构件都是直接从本地仓库获取的。如果本地仓库没有，它会首先尝试从远程仓库下载构件至本地仓库，然后再使用本地仓库的构件。 远程仓库maven 中央仓库maven 中央仓库，是由 maven 社区提供的仓库，其中包含了大量常用的库。一般来说，简单的 java 项目依赖的构件都可以在这里下载到。 在 maven 安装目录的 lib 目录下，有一个 maven-model-builder-3.6.1.jar，里面的 org/apache/maven/model/pom-4.0.0.xml 文件定义了 maven 默认中央仓库的地址：https://repo.maven.apache.org/maven2 因为 maven 中央仓库默认在国外，国内使用难免很慢，推荐将其更换为阿里云的镜像。 全局配置：修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，在 mirrors 节点上，添加如下内容。 1234567891011121314151617181920&lt;mirrors&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 局部配置：修改项目的 pom.xml 文件，在 repositories 上，添加如下内容。 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 更多中央仓库地址参考：https://blog.csdn.net/Hello_World_QWP/article/details/82463799。 私服maven 私服就是公司局域网内的 maven 远程仓库，每个员工的电脑上安装 maven 软件并且连接 maven 私服，程序员可以将自己开发的项目打成 jar 并发布到私服，其它项目组成员就可以从私服下载所依赖的 jar。 私服还充当一个代理服务器的角色，当私服上没有 jar 包时，会从 maven 中央仓库自动下载。 nexus 是一个 maven 仓库管理器 (其实就是一个软件)，nexus 可以充当 maven 私服，同时 nexus 还提供强大的仓库管理、构件搜索等功能。 如果 maven 在中央仓库中也找不到依赖的文件，它会停止构建过程并输出错误信息到控制台。为避免这种情况，maven 提供了远程仓库的概念，它是开发人员自己定制的仓库，包含了所需要的代码库或者其他工程中用到的 jar 文件。 搭建 maven 私服 下载 nexus，地址：https://help.sonatype.com/repomanager2/download/download-archives---repository-manager-oss 安装 nexus 将下载的压缩包进行解压，进入 bin 目录： 打开 cmd 窗口并进入上面 bin 目录下，执行 nexus.bat install 命令安装服务 (注意需要以管理员身份运行 cmd 命令)： 启动 nexus 经过前面命令已经完成 nexus 的安装，可以通过如下两种方式启动 nexus 服务。 在 Windows 系统服务中启动 nexus 在命令行执行 nexus.bat start 命令启动 nexus 访问 nexus 启动 nexus 服务后，访问 http://localhost:8081/nexus，点击右上角 LogIn 按钮，使用默认用户名 admin 和密码 admin123 登录系统。 登录成功后，点击左侧菜单 Repositories，可以看到 nexus 内置的仓库列表，如下图： nexus 仓库类型通过前面的仓库列表可以看到，nexus 默认内置了很多仓库，这些仓库可以划分为 4 种类型，每种类型的仓库用于存放特定的 jar 包，具体说明如下。 hosted：宿主仓库，部署自己的 jar 到这个类型的仓库，包括 Releases 和 Snapshots 两部分，Releases 为公司内部发布版本仓库，Snapshots 为公司内部测试版本仓库。 proxy：代理仓库，用于代理远程的公共仓库，如 maven 中央仓库，用户连接私服，私服自动去中央仓库下载 jar 包或者插件。 group：仓库组，用来合并多个 hosted 或 proxy 仓库，通常我们配置自己的 maven 连接仓库组。 virtual (虚拟)：兼容 Maven1 版本的 jar 或者插件。 nexus 仓库类型与安装目录对应关系 将项目发布到 maven 私服maven 私服是搭建在公司局域网内的 maven 仓库，公司内的所有开发团队都可以使用。例如技术研发团队开发了一个基础组件，就可以将这个基础组件打成 jar 包发布到私服，其他团队成员就可以从私服下载这个 jar 包到本地仓库并在项目中使用。 具体操作步骤如下： 配置 maven 的 settings.xml 文件 12345678910&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 注意：一定要在 idea 工具中引入的 maven 的 settings.xml 文件中配置。 配置项目的 pom.xml 文件 12345678910&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 执行 mvn clean deploy 命令 从私服下载 jar 到本地仓库前面我们已经完成了将本地项目打成 jar 包发布到 maven 私服，下面我们就需要从 maven 私服下载 jar 包到本地仓库。 具体操作步骤如下： 在 maven 的 settings.xml 文件中配置下载模板 1234567891011121314151617181920212223242526&lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;!--仓库地址，即nexus仓库组的地址--&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;!--是否下载releases构件--&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;!--是否下载snapshots构件--&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;!-- 插件仓库，maven的运行依赖插件，也需要从私服下载插件 --&gt; &lt;pluginRepository&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/profile&gt; 在 maven 的 settings.xml 文件中配置激活下载模板 123&lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 将第三方 jar 安装到本地仓库和 maven 私服在 maven 工程的 pom.xml 文件中配置某个 jar 包的坐标后，如果本地的 maven 仓库不存在这个 jar 包，maven 工具会自动到配置的 maven 私服下载，如果私服中也不存在，maven 私服就会从 maven 中央仓库进行下载。 但是并不是所有的 jar 包都可以从中央仓库下载到，比如常用的 Oracle 数据库驱动的 jar 包在中央仓库就不存在。此时需要到 Oracle 的官网下载驱动 jar 包，然后将此 jar 包通过 maven 命令安装到我们本地的 maven 仓库或者 maven 私服中，这样在 maven 项目中就可以使用 maven 坐标引用到此 jar 包了。 将第三方 jar 安装到本地仓库 下载 Oracle 的 jar 包 mvn install 命令进行安装 1mvn install:install-file -Dfile=ojdbc14-10.2.0.4.0.jar -DgroupId=com.oracle -DartifactId=ojdbc14 –Dversion=10.2.0.4.0 -Dpackaging=jar 查看本地 maven 仓库，确认安装是否成功 再比如安装 Classifier4J-0.6.jar，打开 cmd 窗口，切换到 jar 包所在目录，输入 mvn 命令，命令格式如下： 1mvn install:install-file -DgroupId=net.sf(自定义，需要与pom.xml文件中的groupId一致) -DartifactId=classifier4j(自定义，需要与pom.xml文件中的artifaceId一致) -Dversion=0.6(自定义，需要与pom.xml文件中的version一致) -Dpackaging=jar -Dfile=Classifier4J-0.6.jar(本地jar包) -DgroupId、-DartifactId、-Dversion、-Dpackaging、-Dfile 前面均有一个空格。 使用示例如下： 之后，在 maven 的本地仓库，根据 groupId —— artifactId —— version，即可找到打包进来的本地 jar 包，也可以在项目中的 pom.xml 文件引入： 12345&lt;dependency&gt; &lt;groupId&gt;net.sf&lt;/groupId&gt; &lt;artifactId&gt;classifier4j&lt;/artifactId&gt; &lt;version&gt;0.6&lt;/version&gt;&lt;/dependency&gt; 将第三方 jar 安装到 maven 私服 下载 Oracle 的 jar 包 在 maven 的 settings.xml 配置文件中配置第三方仓库的 server 信息 12345&lt;server&gt; &lt;id&gt;thirdparty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 执行 mvn deploy 命令进行安装 1mvn deploy:deploy-file -Dfile=ojdbc14-10.2.0.4.0.jar -DgroupId=com.oracle -DartifactId=ojdbc14 –Dversion=10.2.0.4.0 -Dpackaging=jar –Durl=http://localhost:8081/nexus/content/repositories/thirdparty/ -DrepositoryId=thirdparty maven 的依赖搜索顺序一般情况下，当执行 maven 构建命令时，maven 按照以下顺序查找依赖的库： 步骤 1：在本地仓库中搜索，如果找不到，执行步骤 2，如果找到了则执行其他操作。 步骤 2：在中央仓库中搜索，如果找不到，并且有一个或多个远程仓库已经设置，则执行步骤 4，如果找到了则下载到本地仓库中以备将来引用。 步骤 3：如果远程仓库没有被设置，maven 将简单的停滞处理并抛出错误 (无法找到依赖的文件)。 步骤 4：在一个或多个远程仓库中搜索依赖的文件，如果找到则下载到本地仓库以备将来引用，否则 maven 将停止处理并抛出错误 (无法找到依赖的文件)。 maven 的常用命令 clean： 清理 compile：编译 test： 测试 package：打包 install： 安装 maven 的坐标书写规范12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt;&lt;/dependency&gt; maven 的依赖范围 依赖范围 对于编译 classpath 有效 对于测试 classpath 有效 对于运行 classpath 有效 例子 compile Y Y Y spring-core test - Y - Junit provided Y Y - servlet-api runtime - Y Y JDBC 驱动 system Y Y - 本地的，maven 仓库之外的类库 默认使用 compile 依赖范围。 使用 system 依赖范围的依赖时，必须通过 systemPath 元素显示地指定依赖文件的路径。由于此类依赖不是通过 maven 仓库解析的，而且往往与本机系统绑定，可能构成构建的不可移植，因此应该谨慎使用。systemPath 元素可以引用环境变量，例如： 1234567&lt;dependency&gt; &lt;groupId&gt;javax.sql&lt;/groupId&gt; &lt;artifactId&gt;jdbc-stdext&lt;/artifactId&gt; &lt;Version&gt;2.0&lt;/Version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;java.home&#125;/lib/rt.jar&lt;/systemPath&gt;&lt;/dependency&gt; maven 的依赖传递什么是依赖传递在 maven 中，依赖是可以传递的，假设存在三个项目，分别是项目 A，项目 B 以及项目 C。假设 C 依赖 B，B 依赖 A，那么根据 maven 项目依赖的特征，不难推出项目 C 也依赖 A。如图所示： ​ 通过上面的图可以看到， 在一个 web 项目中，直接依赖了 spring-webmvc，而 spring-webmvc 依赖了 spring-aop、spring-beans 等。最终的结果就是在这个 web 项目中，间接依赖了 spring-aop、spring-beans 等。 什么是依赖冲突由于依赖传递现象的存在，如图所示，spring-webmvc 依赖 spirng-beans-4.2.4，spring-aop 依赖 spring-beans-5.0.2，现在 spirng-beans-4.2.4 已经加入到了工程中，而我们希望 spring-beans-5.0.2 加入工程。这就造成了依赖冲突。 如何解决依赖冲突 使用 maven 提供的依赖调节原则 排除依赖 锁定版本 依赖调节原则路径近者优先原则当依赖声明不在同一个 pom.xml 文件中时，或者说存在依赖传递时，路径最短的 jar 包将被选为最终依赖。 上图中，Jar2.0 将被选为最终依赖。 第一声明者优先原则当依赖声明不在同一个 pom.xml 文件中时，或者说存在依赖传递时，并且依赖传递长度相同时，最先声明的依赖将被选为最终依赖。 上图中，spring-aop 和 spring-webmvc 都依赖了 spring-beans，但是因为 spring-aop 在前面，所以最终使用的 spring-beans 是由 spring-aop 传递过来的，而 spring-webmvc 传递过来的 spring-beans 则被忽略了。 覆盖优先当依赖声明在同一个 pom.xml 文件中时，后面声明的依赖将覆盖前面声明的依赖。 排除依赖使用 exclusions 标签将传递过来的依赖排除出去。 版本锁定采用直接锁定版本的方法确定依赖 jar 包的版本，版本锁定后则不考虑依赖的声明顺序或依赖的路径，以锁定的版本为准添加到工程中，此方法在企业开发中经常使用。 版本锁定的使用方式： 第一步：在 dependencyManagement 标签中锁定依赖的版本 第二步：在 dependencies 标签中声明需要导入的 maven 坐标 备注能查找依赖的网站：https://mvnrepository.com/ 本文参考https://juejin.cn/post/6844903543711907848 https://www.jianshu.com/p/a1d9fd97f568 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"KafkaConsumer 源码之 consumer 如何拉取 offset 和数据","slug":"kafka-consumer-offsetandfetcher","date":"2020-11-10T03:32:06.000Z","updated":"2021-01-05T07:32:53.038Z","comments":true,"path":"2020/11/10/kafka-consumer-offsetandfetcher/","link":"","permalink":"http://example.com/2020/11/10/kafka-consumer-offsetandfetcher/","excerpt":"","text":"上一篇文章讲了 consumer 如何加入 consumer group，现在加入 group 成功之后，就要准备开始消费。 kafkaConsumer.poll () 方法的核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private ConsumerRecords&lt;K, V&gt; poll(final Timer timer, final boolean includeMetadataInTimeout) &#123; // Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭 acquireAndEnsureOpen(); try &#123; if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123; throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;); &#125; // poll for new data until the timeout expires do &#123; client.maybeTriggerWakeup(); if (includeMetadataInTimeout) &#123; // Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息 if (!updateAssignmentMetadataIfNeeded(timer)) &#123; return ConsumerRecords.empty(); &#125; &#125; else &#123; while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123; log.warn(&quot;Still waiting for metadata&quot;); &#125; &#125; // Step3:拉取数据，核心步骤 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer); if (!records.isEmpty()) &#123; // before returning the fetched records, we can send off the next round of fetches // and avoid block waiting for their responses to enable pipelining while the user // is handling the fetched records. // // NOTE: since the consumed position has already been updated, we must not allow // wakeups or any other errors to be triggered prior to returning the fetched records. // 在返回数据之前，发送下次的fetch请求，避免用户在下次获取数据时线程block if (fetcher.sendFetches() &gt; 0 || client.hasPendingRequests()) &#123; client.pollNoWakeup(); &#125; return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records)); &#125; &#125; while (timer.notExpired()); return ConsumerRecords.empty(); &#125; finally &#123; release(); &#125;&#125; 紧跟上一篇文章，我们继续分析 consumer 加入 group 后的行为： 123456789101112/** * Visible for testing */boolean updateAssignmentMetadataIfNeeded(final Timer timer) &#123; // 1.上一篇主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group) if (coordinator != null &amp;&amp; !coordinator.poll(timer)) &#123; return false; &#125; // 2.本篇文章从updateFetchPositions(timer)方法开始继续分析(主要功能是:consumer获得partition的offset) return updateFetchPositions(timer);&#125; KafkaConsumer 的消费策略首先，我们应该知道，KafkaConsumer 关于如何消费的 2 种策略： 手动指定：调用 consumer.seek(TopicPartition, offset)，然后开始 poll ()。 自动指定：poll () 之前给集群发送请求，让集群告知客户端，当前该 TopicPartition 的 offset 是多少，这也是我们此次分析的重点。 在讲如何拉取 offset 之前，先认识下下面这个类 (SubscriptionState 的内部类)： 12345678910111213private static class TopicPartitionState &#123; private FetchState fetchState; private FetchPosition position; // last consumed position private Long highWatermark; // the high watermark from last fetch private Long logStartOffset; // the log start offset private Long lastStableOffset; private boolean paused; // whether this partition has been paused by the user private OffsetResetStrategy resetStrategy; // the strategy to use if the offset needs resetting private Long nextRetryTimeMs; private Integer preferredReadReplica; private Long preferredReadReplicaExpireTimeMs; ...&#125; consumer 实例订阅的每个 topic-partition 都会有一个对应的 TopicPartitionState 对象，在这个对象中会记录上面内容，最需要关注的就是 position 这个属性，它表示上一次消费的位置。通过 consumer.seek () 方式指定消费 offset 的时候，其实设置的就是这个 position 值。 updateFetchPositions - 拉取 offset在 consumer 成功加入 group 并开始消费之前，我们还需要知道 consumer 是从 offset 为多少的位置开始消费。consumer 加入 group 之后，就得去获取 offset 了，下面的方法，就是开始更新 position (offset)： 1234567891011121314151617181920212223242526272829303132333435363738/** * Set the fetch position to the committed position (if there is one) * or reset it using the offset reset policy the user has configured. * * @throws org.apache.kafka.common.errors.AuthenticationException if authentication fails. See the exception for more details * @throws NoOffsetForPartitionException If no offset is stored for a given partition and no offset reset policy is * defined * @return true iff the operation completed without timing out */private boolean updateFetchPositions(final Timer timer) &#123; // If any partitions have been truncated due to a leader change, we need to validate the offsets fetcher.validateOffsetsIfNeeded(); // Step1:查看TopicPartitionState的position是否为空，第一次消费肯定为空 cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions(); if (cachedSubscriptionHashAllFetchPositions) return true; // If there are any partitions which do not have a valid position and are not // awaiting reset, then we need to fetch committed offsets. We will only do a // coordinator lookup if there are partitions which have missing positions, so // a consumer with manually assigned partitions can avoid a coordinator dependence // by always ensuring that assigned partitions have an initial position. // Step2:如果没有有效的offset，那么需要从GroupCoordinator中获取 if (coordinator != null &amp;&amp; !coordinator.refreshCommittedOffsetsIfNeeded(timer)) return false; // If there are partitions still needing a position and a reset policy is defined, // request reset using the default policy. If no reset strategy is defined and there // are partitions with a missing position, then we will raise an exception. // Step3:如果还存在partition不知道position，并且设置了offsetreset策略，那么就等待重置，不然就抛出异常 subscriptions.resetMissingPositions(); // Finally send an asynchronous request to lookup and update the positions of any // partitions which are awaiting reset. // Step4:向PartitionLeader(GroupCoordinator所在机器)发送ListOffsetRequest重置position fetcher.resetOffsetsIfNeeded(); return true;&#125; 上面的代码主要分为 4 个步骤，具体如下： 首先，查看当前 TopicPartition 的 position 是否为空，如果不为空，表示知道下次 fetch position (即拉取数据时从哪个位置开始拉取)，但如果是第一次消费，这个 TopicPartitionState.position 肯定为空。 然后，通过 GroupCoordinator 为缺少 fetch position 的 partition 拉取 position (即 last committed offset)。 继而，仍不知道 partition 的 position (_consumer_offsets 中未保存位移信息)，且设置了 offsetreset 策略，那么就等待重置，如果没有设置重置策略，就抛出 NoOffsetForPartitionException 异常。 最后，为那些需要重置 fetch position 的 partition 发送 ListOffsetRequest 重置 position (consumer.beginningOffsets ()，consumer.endOffsets ()，consumer.offsetsForTimes ()，consumer.seek () 都会发送 ListOffRequest 请求)。 上面说的几个方法相当于都是用户自己自定义消费的 offset，所以可能出现越界 (消费位置无法在实际分区中查到) 的情况，所以也是会发送 ListOffsetRequest 请求的，即触发 auto.offset.reset 参数的执行。比如现在某个 partition 的可拉取 offset 最大值为 100，如果你指定消费 offset=200 的位置，那肯定拉取不到，此时就会根据 auto.offset.reset 策略将拉取位置重置为 100 (默认的 auto.offset.reset 为 latest)。 refreshCommittedOffsetsIfNeeded我们先看下 Setp 2 中 GroupCoordinator 是如何 fetch position 的： 1234567891011121314151617181920212223242526272829/** * Refresh the committed offsets for provided partitions. * * @param timer Timer bounding how long this method can block * @return true iff the operation completed within the timeout */public boolean refreshCommittedOffsetsIfNeeded(Timer timer) &#123; final Set&lt;TopicPartition&gt; missingFetchPositions = subscriptions.missingFetchPositions(); // 1.发送获取offset的请求，核心步骤 final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = fetchCommittedOffsets(missingFetchPositions, timer); if (offsets == null) return false; for (final Map.Entry&lt;TopicPartition, OffsetAndMetadata&gt; entry : offsets.entrySet()) &#123; final TopicPartition tp = entry.getKey(); // 2.获取response中的offset final OffsetAndMetadata offsetAndMetadata = entry.getValue(); final ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.leaderAndEpoch(tp); final SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(), leaderAndEpoch); log.info(&quot;Setting offset for partition &#123;&#125; to the committed offset &#123;&#125;&quot;, tp, position); entry.getValue().leaderEpoch().ifPresent(epoch -&gt; this.metadata.updateLastSeenEpochIfNewer(entry.getKey(), epoch)); // 3.实际就是设置SubscriptionState的position值 this.subscriptions.seekUnvalidated(tp, position); &#125; return true;&#125; fetchCommittedOffsets () 方法的核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Fetch the current committed offsets from the coordinator for a set of partitions. * * @param partitions The partitions to fetch offsets for * @return A map from partition to the committed offset or null if the operation timed out */public Map&lt;TopicPartition, OffsetAndMetadata&gt; fetchCommittedOffsets(final Set&lt;TopicPartition&gt; partitions, final Timer timer) &#123; if (partitions.isEmpty()) return Collections.emptyMap(); final Generation generation = generation(); if (pendingCommittedOffsetRequest != null &amp;&amp; !pendingCommittedOffsetRequest.sameRequest(partitions, generation)) &#123; // if we were waiting for a different request, then just clear it. pendingCommittedOffsetRequest = null; &#125; do &#123; if (!ensureCoordinatorReady(timer)) return null; // contact coordinator to fetch committed offsets final RequestFuture&lt;Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt; future; if (pendingCommittedOffsetRequest != null) &#123; future = pendingCommittedOffsetRequest.response; &#125; else &#123; // 1.封装FetchRequest请求 future = sendOffsetFetchRequest(partitions); pendingCommittedOffsetRequest = new PendingCommittedOffsetRequest(partitions, generation, future); &#125; // 2.通过KafkaClient发送请求 client.poll(future, timer); if (future.isDone()) &#123; pendingCommittedOffsetRequest = null; if (future.succeeded()) &#123; // 3.请求成功，获取请求的响应数据 return future.value(); &#125; else if (!future.isRetriable()) &#123; throw future.exception(); &#125; else &#123; timer.sleep(retryBackoffMs); &#125; &#125; else &#123; return null; &#125; &#125; while (timer.notExpired()); return null;&#125; 上面的步骤和我们之前提到的发送其他请求毫无区别，基本就是这三个套路。 在获取到响应之后，会通过 subscriptions.seekUnvalidated () 方法为每个 TopicPartition 设置 position 值后，就知道从哪里开始消费订阅 topic 下的 partition 了。 resetMissingPositions在 Step 3 中，什么时候发起 FetchRequest 拿不到 position 呢？ 我们知道消费位移 (consume offset) 是保存在 _consumer_offsets 这个 topic 里面的，当我们进行消费的时候需要知道上次消费到了什么位置。那么就会发起请求去看上次消费到了 topic 的 partition 的哪个位置，但是这个消费位移是有保存时长的，默认为 7 天 (broker 端通过 offsets.retention.minutes 设置)。 当隔了一段时间再进行消费，如果这个间隔时间超过了参数的配置值，那么原先的位移信息就会丢失，最后只能通过客户端参数 auto.offset.reset 来确定开始消费的位置。 如果我们第一次消费 topic，那么在 _consumer_offsets 中也是找不到消费位移的，所以就会执行第四个步骤，发起 ListOffsetRequest 请求根据配置的 reset 策略 (即 auto.offset.reset) 来决定开始消费的位置。 resetOffsetsIfNeeded在 Step 4 中，发起 ListOffsetRequest 请求和处理 response 的核心代码如下： 123456789101112131415161718192021222324252627/** * Reset offsets for all assigned partitions that require it. * * @throws org.apache.kafka.clients.consumer.NoOffsetForPartitionException If no offset reset strategy is defined * and one or more partitions aren&#x27;t awaiting a seekToBeginning() or seekToEnd(). */public void resetOffsetsIfNeeded() &#123; // Raise exception from previous offset fetch if there is one RuntimeException exception = cachedListOffsetsException.getAndSet(null); if (exception != null) throw exception; // 1.需要执行reset策略的partition Set&lt;TopicPartition&gt; partitions = subscriptions.partitionsNeedingReset(time.milliseconds()); if (partitions.isEmpty()) return; final Map&lt;TopicPartition, Long&gt; offsetResetTimestamps = new HashMap&lt;&gt;(); for (final TopicPartition partition : partitions) &#123; Long timestamp = offsetResetStrategyTimestamp(partition); if (timestamp != null) offsetResetTimestamps.put(partition, timestamp); &#125; // 2.执行reset策略 resetOffsetsAsync(offsetResetTimestamps);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940private void resetOffsetsAsync(Map&lt;TopicPartition, Long&gt; partitionResetTimestamps) &#123; Map&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; timestampsToSearchByNode = groupListOffsetRequests(partitionResetTimestamps, new HashSet&lt;&gt;()); for (Map.Entry&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; entry : timestampsToSearchByNode.entrySet()) &#123; Node node = entry.getKey(); final Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; resetTimestamps = entry.getValue(); subscriptions.setNextAllowedRetry(resetTimestamps.keySet(), time.milliseconds() + requestTimeoutMs); // 1.发送ListOffsetRequest请求 RequestFuture&lt;ListOffsetResult&gt; future = sendListOffsetRequest(node, resetTimestamps, false); // 2.为ListOffsetRequest请求添加监听器 future.addListener(new RequestFutureListener&lt;ListOffsetResult&gt;() &#123; @Override public void onSuccess(ListOffsetResult result) &#123; if (!result.partitionsToRetry.isEmpty()) &#123; subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs); metadata.requestUpdate(); &#125; for (Map.Entry&lt;TopicPartition, ListOffsetData&gt; fetchedOffset : result.fetchedOffsets.entrySet()) &#123; TopicPartition partition = fetchedOffset.getKey(); ListOffsetData offsetData = fetchedOffset.getValue(); ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition); // 3.发送ListOffsetRequest请求成功，对结果reset，如果reset策略设置的是latest，那么requestedReset.timestamp = -1，如果是earliest，requestedReset.timestamp = -2 resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData); &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; subscriptions.requestFailed(resetTimestamps.keySet(), time.milliseconds() + retryBackoffMs); metadata.requestUpdate(); if (!(e instanceof RetriableException) &amp;&amp; !cachedListOffsetsException.compareAndSet(null, e)) log.error(&quot;Discarding error in ListOffsetResponse because another error is pending&quot;, e); &#125; &#125;); &#125;&#125; sendListOffsetRequest () 方法的核心代码如下： 1234567891011121314151617181920212223242526/** * Send the ListOffsetRequest to a specific broker for the partitions and target timestamps. * * @param node The node to send the ListOffsetRequest to. * @param timestampsToSearch The mapping from partitions to the target timestamps. * @param requireTimestamp True if we require a timestamp in the response. * @return A response which can be polled to obtain the corresponding timestamps and offsets. */private RequestFuture&lt;ListOffsetResult&gt; sendListOffsetRequest(final Node node, final Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; timestampsToSearch, boolean requireTimestamp) &#123; ListOffsetRequest.Builder builder = ListOffsetRequest.Builder .forConsumer(requireTimestamp, isolationLevel) .setTargetTimes(timestampsToSearch); log.debug(&quot;Sending ListOffsetRequest &#123;&#125; to broker &#123;&#125;&quot;, builder, node); return client.send(node, builder) .compose(new RequestFutureAdapter&lt;ClientResponse, ListOffsetResult&gt;() &#123; @Override public void onSuccess(ClientResponse response, RequestFuture&lt;ListOffsetResult&gt; future) &#123; ListOffsetResponse lor = (ListOffsetResponse) response.responseBody(); log.trace(&quot;Received ListOffsetResponse &#123;&#125; from broker &#123;&#125;&quot;, lor, node); handleListOffsetResponse(timestampsToSearch, lor, future); &#125; &#125;);&#125; resetOffsetIfNeeded () 方法的核心代码如下： 1234567private void resetOffsetIfNeeded(TopicPartition partition, OffsetResetStrategy requestedResetStrategy, ListOffsetData offsetData) &#123; SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetData.offset, offsetData.leaderEpoch, metadata.leaderAndEpoch(partition)); offsetData.leaderEpoch.ifPresent(epoch -&gt; metadata.updateLastSeenEpochIfNewer(partition, epoch)); // reset对应的TopicPartition fetch的position subscriptions.maybeSeekUnvalidated(partition, position.offset, requestedResetStrategy);&#125; 这里解释下 auto.offset.reset 的两个值 (latest 和 earliest) 的区别： 假设我们现在要消费 MyConsumerTopic 的数据，它有 3 个分区，生产者往这个 topic 发送了 10 条数据，然后分区数据按照 MyConsumerTopic-0 (3 条数据)，MyConsumerTopic-1 (3 条数据)，MyConsumerTopic-2 (4 条数据) 这样分配。 当设置为 latest 的时候，返回的 offset 具体到每个 partition 就是 HW 值 (partition 0 是 3，partition 1 是 3，partition 2 是 4)。 当设置为 earliest 的时候，就会从起始处 (即 LogStartOffset，注意不是 LSO) 开始消费，这里就是从 0 开始。 Log Start Offset：表示 partition 的起始位置，初始值为 0，由于消息的增加以及日志清除策略影响，这个值会阶段性增大。尤其注意这个不能缩写为 LSO，LSO 代表的是 LastStableOffset，和事务有关。 Consumer Offset：消费位移，表示 partition 的某个消费者消费到的位移位置。 High Watermark：简称 HW，代表消费端能看到的 partition 的最高日志位移，HW 大于等于 ConsumerOffset 的值。 Log End Offset：简称 LEO，代表 partition 的最高日志位移，对消费者不可见，HW 到 LEO 这之间的数据未被 follwer 完全同步。 至此，我们成功的知道 consumer 消费的 partition 的 offset 位置在哪里，下面就开始拉取 partition 里的数据。 pollForFetches - 拉取数据现在万事俱备只欠东风了，consumer 成功加入 group，也确定了需要拉取的 topic partition 的 offset，那么现在就应该去拉取数据了，其核心源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) &#123; long pollTimeout = coordinator == null ? timer.remainingMs() : Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs()); // if data is available already, return it immediately // 1.获取fetcher已经拉取到的数据 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords(); if (!records.isEmpty()) &#123; return records; &#125; // 到此，说明上次fetch到的数据已经全部拉取了，需要再次发送fetch请求，从broker拉取新的数据 // send any new fetches (won&#x27;t resend pending fetches) // 2.发送fetch请求，会从多个topic-partition拉取数据(只要对应的topic-partition没有未完成的请求) fetcher.sendFetches(); // We do not want to be stuck blocking in poll if we are missing some positions // since the offset lookup may be backing off after a failure // NOTE: the use of cachedSubscriptionHashAllFetchPositions means we MUST call // updateAssignmentMetadataIfNeeded before this method. if (!cachedSubscriptionHashAllFetchPositions &amp;&amp; pollTimeout &gt; retryBackoffMs) &#123; pollTimeout = retryBackoffMs; &#125; Timer pollTimer = time.timer(pollTimeout); // 3.真正开始发送，底层同样使用NIO client.poll(pollTimer, () -&gt; &#123; // since a fetch might be completed by the background thread, we need this poll condition // to ensure that we do not block unnecessarily in poll() return !fetcher.hasCompletedFetches(); &#125;); timer.update(pollTimer.currentTimeMs()); // after the long poll, we should check whether the group needs to rebalance // prior to returning data so that the group can stabilize faster // 4.如果group需要rebalance，直接返回空数据，这样更快地让group进入稳定状态 if (coordinator != null &amp;&amp; coordinator.rejoinNeededOrPending()) &#123; return Collections.emptyMap(); &#125; // 5.返回拉取到的新数据 return fetcher.fetchedRecords();&#125; fetcher.sendFetches这里需要注意的是 fetcher.sendFetches () 方法，在发送请求的同时会注册回调函数，当有 response 的时候，会解析 response，将返回的数据放到 Fetcher 的成员变量中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/** * Set-up a fetch request for any node that we have assigned partitions for which doesn&#x27;t already have * an in-flight fetch or pending fetch data. * @return number of fetches sent */public synchronized int sendFetches() &#123; // Update metrics in case there was an assignment change sensors.maybeUpdateAssignment(subscriptions); // 1.创建FetchRequest Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; fetchRequestMap = prepareFetchRequests(); for (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData&gt; entry : fetchRequestMap.entrySet()) &#123; final Node fetchTarget = entry.getKey(); final FetchSessionHandler.FetchRequestData data = entry.getValue(); final FetchRequest.Builder request = FetchRequest.Builder .forConsumer(this.maxWaitMs, this.minBytes, data.toSend()) .isolationLevel(isolationLevel) .setMaxBytes(this.maxBytes) .metadata(data.metadata()) .toForget(data.toForget()) .rackId(clientRackId); if (log.isDebugEnabled()) &#123; log.debug(&quot;Sending &#123;&#125; &#123;&#125; to broker &#123;&#125;&quot;, isolationLevel, data.toString(), fetchTarget); &#125; // 2.发送FetchRequest RequestFuture&lt;ClientResponse&gt; future = client.send(fetchTarget, request); // We add the node to the set of nodes with pending fetch requests before adding the // listener because the future may have been fulfilled on another thread (e.g. during a // disconnection being handled by the heartbeat thread) which will mean the listener // will be invoked synchronously. this.nodesWithPendingFetchRequests.add(entry.getKey().id()); future.addListener(new RequestFutureListener&lt;ClientResponse&gt;() &#123; @Override public void onSuccess(ClientResponse resp) &#123; synchronized (Fetcher.this) &#123; try &#123; @SuppressWarnings(&quot;unchecked&quot;) FetchResponse&lt;Records&gt; response = (FetchResponse&lt;Records&gt;) resp.responseBody(); FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler == null) &#123; log.error(&quot;Unable to find FetchSessionHandler for node &#123;&#125;. Ignoring fetch response.&quot;, fetchTarget.id()); return; &#125; if (!handler.handleResponse(response)) &#123; return; &#125; Set&lt;TopicPartition&gt; partitions = new HashSet&lt;&gt;(response.responseData().keySet()); FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions); for (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&lt;Records&gt;&gt; entry : response.responseData().entrySet()) &#123; TopicPartition partition = entry.getKey(); FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition); if (requestData == null) &#123; String message; if (data.metadata().isFull()) &#123; message = MessageFormatter.arrayFormat( &quot;Response for missing full request partition: partition=&#123;&#125;; metadata=&#123;&#125;&quot;, new Object[]&#123;partition, data.metadata()&#125;).getMessage(); &#125; else &#123; message = MessageFormatter.arrayFormat( &quot;Response for missing session request partition: partition=&#123;&#125;; metadata=&#123;&#125;; toSend=&#123;&#125;; toForget=&#123;&#125;&quot;, new Object[]&#123;partition, data.metadata(), data.toSend(), data.toForget()&#125;).getMessage(); &#125; // Received fetch response for missing session partition throw new IllegalStateException(message); &#125; else &#123; long fetchOffset = requestData.fetchOffset; FetchResponse.PartitionData&lt;Records&gt; fetchData = entry.getValue(); log.debug(&quot;Fetch &#123;&#125; at offset &#123;&#125; for partition &#123;&#125; returned fetch data &#123;&#125;&quot;, isolationLevel, fetchOffset, partition, fetchData); // 3.发送FetchRequest请求成功，将返回的数据放到ConcurrentLinkedQueue&lt;CompletedFetch&gt;中 completedFetches.add(new CompletedFetch(partition, fetchOffset, fetchData, metricAggregator, resp.requestHeader().apiVersion())); &#125; &#125; sensors.fetchLatency.record(resp.requestLatencyMs()); &#125; finally &#123; nodesWithPendingFetchRequests.remove(fetchTarget.id()); &#125; &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; synchronized (Fetcher.this) &#123; try &#123; FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler != null) &#123; handler.handleError(e); &#125; &#125; finally &#123; nodesWithPendingFetchRequests.remove(fetchTarget.id()); &#125; &#125; &#125; &#125;); &#125; return fetchRequestMap.size();&#125; 该方法主要分为以下两步： prepareFetchRequests ()：为订阅的所有 topic-partition list 创建 fetch 请求 (只要该 topic-partition 没有还在处理的请求)，创建的 fetch 请求依然是按照 node 级别创建的； client.send ()：发送 fetch 请求，并设置相应的 Listener，请求处理成功的话，就加入到 completedFetches 中，在加入这个 completedFetches 队列时，是按照 topic-partition 级别去加入，这样也就方便了后续的处理。 从这里可以看出，在每次发送 fetch 请求时，都会向所有可发送的 topic-partition 发送 fetch 请求，调用一次 fetcher.sendFetches，拉取到的数据，可能需要多次 pollForFetches 循环才能处理完，因为 Fetcher 线程是在后台运行，这也保证了尽可能少地阻塞用户的处理线程，因为如果 Fetcher 中没有可处理的数据，用户的线程是会阻塞在 poll 方法中的。 fetcher.fetchedRecords这个方法的作用就是获取已经从 server 拉取到的 Records，其核心源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117/** * Return the fetched records, empty the record buffer and update the consumed position. * * NOTE: returning empty records guarantees the consumed position are NOT updated. * * @return The fetched records per partition * @throws OffsetOutOfRangeException If there is OffsetOutOfRange error in fetchResponse and * the defaultResetPolicy is NONE * @throws TopicAuthorizationException If there is TopicAuthorization error in fetchResponse. */public Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords() &#123; Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetched = new HashMap&lt;&gt;(); // 在max.poll.records中设置单词最大的拉取条数，默认500条 int recordsRemaining = maxPollRecords; try &#123; while (recordsRemaining &gt; 0) &#123; if (nextInLineRecords == null || nextInLineRecords.isFetched) &#123;// nextInLineRecords为空时 // Step1:当一个nextInLineRecords处理完，就从completedFetches处理下一个完成的Fetch请求 CompletedFetch completedFetch = completedFetches.peek(); if (completedFetch == null) break; try &#123; // Step2:获取下一个要处理的nextInLineRecords nextInLineRecords = parseCompletedFetch(completedFetch); &#125; catch (Exception e) &#123; // Remove a completedFetch upon a parse with exception if (1) it contains no records, and // (2) there are no fetched records with actual content preceding this exception. // The first condition ensures that the completedFetches is not stuck with the same completedFetch // in cases such as the TopicAuthorizationException, and the second condition ensures that no // potential data loss due to an exception in a following record. FetchResponse.PartitionData partition = completedFetch.partitionData; if (fetched.isEmpty() &amp;&amp; (partition.records == null || partition.records.sizeInBytes() == 0)) &#123; completedFetches.poll(); &#125; throw e; &#125; completedFetches.poll(); &#125; else &#123; // Step3:拉取records，更新position List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = fetchRecords(nextInLineRecords, recordsRemaining); TopicPartition partition = nextInLineRecords.partition; if (!records.isEmpty()) &#123; List&lt;ConsumerRecord&lt;K, V&gt;&gt; currentRecords = fetched.get(partition); if (currentRecords == null) &#123;// 正常情况下，一个node只会发送一个request，一般只会有一个 fetched.put(partition, records); &#125; else &#123; // this case shouldn&#x27;t usually happen because we only send one fetch at a time per partition, // but it might conceivably happen in some rare cases (such as partition leader changes). // we have to copy to a new list because the old one may be immutable List&lt;ConsumerRecord&lt;K, V&gt;&gt; newRecords = new ArrayList&lt;&gt;(records.size() + currentRecords.size()); newRecords.addAll(currentRecords); newRecords.addAll(records); fetched.put(partition, newRecords); &#125; recordsRemaining -= records.size(); &#125; &#125; &#125; &#125; catch (KafkaException e) &#123; if (fetched.isEmpty()) throw e; &#125; // Step4:返回相应的Records数据 return fetched;&#125;private List&lt;ConsumerRecord&lt;K, V&gt;&gt; fetchRecords(PartitionRecords partitionRecords, int maxRecords) &#123; if (!subscriptions.isAssigned(partitionRecords.partition)) &#123; // this can happen when a rebalance happened before fetched records are returned to the consumer&#x27;s poll call log.debug(&quot;Not returning fetched records for partition &#123;&#125; since it is no longer assigned&quot;, partitionRecords.partition); &#125; else if (!subscriptions.isFetchable(partitionRecords.partition)) &#123; // this can happen when a partition is paused before fetched records are returned to the consumer&#x27;s // poll call or if the offset is being reset // 这个topic-partition不能被消费了，比如调用了pause log.debug(&quot;Not returning fetched records for assigned partition &#123;&#125; since it is no longer fetchable&quot;, partitionRecords.partition); &#125; else &#123; SubscriptionState.FetchPosition position = subscriptions.position(partitionRecords.partition); if (partitionRecords.nextFetchOffset == position.offset) &#123;// offset对的上，也就是拉取是按顺序拉的 // 获取该topic-partition对应的records，并更新partitionRecords的fetchOffset(用于判断是否顺序) List&lt;ConsumerRecord&lt;K, V&gt;&gt; partRecords = partitionRecords.fetchRecords(maxRecords); if (partitionRecords.nextFetchOffset &gt; position.offset) &#123; SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition( partitionRecords.nextFetchOffset, partitionRecords.lastEpoch, position.currentLeader); log.trace(&quot;Returning fetched records at offset &#123;&#125; for assigned partition &#123;&#125; and update &quot; + &quot;position to &#123;&#125;&quot;, position, partitionRecords.partition, nextPosition); // 更新消费到的offset(the fetch position) subscriptions.position(partitionRecords.partition, nextPosition); &#125; // 获取Lag(即position与hw之间差值)，hw为null时，才返回null Long partitionLag = subscriptions.partitionLag(partitionRecords.partition, isolationLevel); if (partitionLag != null) this.sensors.recordPartitionLag(partitionRecords.partition, partitionLag); Long lead = subscriptions.partitionLead(partitionRecords.partition); if (lead != null) &#123; this.sensors.recordPartitionLead(partitionRecords.partition, lead); &#125; return partRecords; &#125; else &#123; // these records aren&#x27;t next in line based on the last consumed position, ignore them // they must be from an obsolete request log.debug(&quot;Ignoring fetched records for &#123;&#125; at offset &#123;&#125; since the current position is &#123;&#125;&quot;, partitionRecords.partition, partitionRecords.nextFetchOffset, position); &#125; &#125; partitionRecords.drain(); return emptyList();&#125; consumer 的 Fetcher 处理从 server 获取的 fetch response 大致分为以下几个过程： 通过 completedFetches.peek() 获取已经成功的 fetch response (在 fetcher.sendFetches () 方法中会把发送FetchRequest请求成功后的结果放在这个集合中，是拆分为 topic-partition 的粒度放进去的)； parseCompletedFetch() 处理上面获取的 completedFetch，构造成 PartitionRecords 类型； 通过 fetchRecords() 方法处理 PartitionRecords 对象，在这个里面会去验证 fetchOffset 是否能对得上，只有 fetchOffset 是一致的情况下才会去处理相应的数据，并更新 the fetch offset 的信息，如果 fetchOffset 不一致，这里就不会处理，the fetch offset 就不会更新，下次 fetch 请求时是会接着 the fetch offset 的位置去请求相应的数据； 返回相应的 Records 数据。 至此，KafkaConsumer 如何拉取消息的整体流程也分析完毕。 本文参考http://generalthink.github.io/2019/05/31/kafka-consumer-offset/ https://matt33.com/2017/11/11/consumer-pollonce/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"欧阳修","slug":"ouyangxiu","date":"2020-11-10T00:51:06.000Z","updated":"2020-11-23T08:48:11.442Z","comments":true,"path":"2020/11/10/ouyangxiu/","link":"","permalink":"http://example.com/2020/11/10/ouyangxiu/","excerpt":"","text":"伐树记署之东园，久茀不治。修至始辟之，粪瘠溉枯，为蔬圃十数畦，又植花果桐竹凡百本。 春阳既浮，萌者将动。园之守启曰：“园有樗焉，其根壮而叶大。根壮则梗地脉，耗阳气，而新植者不得滋；叶大则阴翳蒙碍，而新植者不得畅以茂。又其材拳曲臃肿，疏轻而不坚，不足养，是宜伐。”因尽薪之。明日，圃之守又曰：“圃之南有杏焉，凡其根庇之广可六七尺，其下之地最壤腴，以杏故，特不得蔬，是亦宜薪。”修曰：“噫！今杏方春且华，将待其实，若独不能损数畦之广为杏地邪？“因勿伐。 既而悟且叹曰：“吁！庄周之说曰：樗、栎以不材终其天年，桂、漆以有用而见伤夭。今樗诚不材矣，然一旦悉翦弃；杏之体最坚密，美泽可用，反见存。岂才不才各遭其时之可否邪？” 他日，客有过修者。仆夫曳薪过堂下，因指而语客以所疑。客曰： “是何怪邪？夫以无用处无用，庄周之贵也。以无用而贼有用，乌能免哉！彼杏之有华实也，以有生之具而庇其根，幸矣。若桂、漆之不能逃乎斤斧者，盖有利之者在死，势不得以生也，与乎杏实异矣。今樗之臃肿不材，而以壮大害物，其见伐，诚宜尔。与夫‘才者死、不才者生’之说，又异矣。凡物幸之与不幸，视其处之而已。”客既去，修善其言而记之。 非非堂记权衡之平物，动则轻重差，其于静也，锱铢不失。水之鉴物，动则不能有睹，其于静也，毫发可辨。在乎人，耳司听，目司视，动则乱于聪明，其于静也，闻见必审。处身者不为外物眩晃而动，则其心静，心静则智识明，是是非非，无所施而不中。夫是是近乎谄，非非近乎讪，不幸而过，宁讪无谄。是者，君子之常，是之何加？一以视之，未若非非之为正也。 予居洛之明年，既新厅事，有文纪于壁末。营其西偏作堂，户北向，植丛竹，辟户于其南，纳日月之光。设一几一榻，架书数百卷，朝夕居其中。以其静也，闭目澄心，览今照古，思虑无所不至焉。故其堂以非非为名云。 浪淘沙 · 把酒祝东风把酒祝东风，且共从容。垂杨紫陌洛城东。总是当时携手处，游遍芳丛。 聚散苦匆匆，此恨无穷。今年花胜去年红。可惜明年花更好，知与谁同？ 玉楼春 · 尊前拟把归期说尊前拟把归期说，欲语春容先惨咽。人生自是有情痴，此恨不关风与月。 离歌且莫翻新阕，一曲能教肠寸结。直须看尽洛城花，始共春风容易别。 生查子 · 元夕去年元夜时，花市灯如昼。 月上柳梢头，人约黄昏后。 今年元夜时，月与灯依旧。 不见去年人，泪满春衫袖。 读李翱文予始读翱《复性书》三篇，曰：此《中庸》之义疏尔。智者诚其性，当读《中庸》；愚者虽读此不晓也，不作可焉。又读《与韩侍郎荐贤书》，以谓翱特穷时愤世无荐己者，故丁宁如此；使其得志，亦未必。然以韩为秦汉间好侠行义之一豪俊，亦善论人者也。最后读《幽怀赋》，然后置书而叹，叹已复读，不自休。恨翱不生于今，不得与之交；又恨予不得生翱时，与翱上下其论也。 凡昔翱一时人，有道而能文者，莫若韩愈。愈尝有赋矣，不过羡二鸟之光荣，叹一饱之无时尔。此其心使光荣而饱，则不复云矣。若翱独不然，其赋曰：“众嚣嚣而杂处兮，咸叹老而嗟卑；视予心之不然兮，虑行道之犹非。”又怪神尧以一旅取天下，后世子孙不能以天下取河北，以为忧。呜呼！使当时君子皆易其叹老嗟卑之心为翱所忧之心，则唐之天下岂有乱与亡哉！ 然翱幸不生今时，见今之事，则其忧又甚矣。奈何今之人不忧也？余行天下，见人多矣，脱有一人能如翱忧者，又皆贱远，与翱无异；其余光荣而饱者，一闻忧世之言，不以为狂人，则以为病痴子，不怒则笑之矣。呜呼，在位而不肯自忧，又禁他人使皆不得忧，可叹也夫! 景祐三年十月十七日，欧阳修书。 答吴充秀才书修顿首白，先辈吴君足下。前辱示书及文三篇，发而读之，浩乎若千万言之多，及少定而视焉，才数百言尔。非夫辞丰意雄，沛然有不可御之势，何以至此！然犹自患伥伥莫有开之使前者，此好学之谦言也。 修材不足用于时，仕不足荣于世，其毁誉不足轻重，气力不足动人。世之欲假誉以为重，借力而后进者，奚取于修焉？先辈学精文雄，其施于时，又非待修誉而为重，力而后进者也。然而惠然见临，若有所责，得非急于谋道，不择其人而问焉者欤？ 夫学者未始不为道，而至者鲜焉；非道之于人远也，学者有所溺焉尔。盖文之为言，难工而可喜，易悦而自足。世之学者往往溺之，一有工焉，则曰：“吾学足矣。“甚者至弃百事不关于心，曰：“吾文士也，职于文而已。”此其所以至之鲜也。 昔孔子老而归鲁，六经之作，数年之顷尔。然读《易》者如无《春秋》，读《书》者如无《诗》，何其用功少而至于至也！圣人之文虽不可及，然大抵道胜者，文不难而自至也。故孟子皇皇不暇著书，荀卿盖亦晚而有作。若子云、仲淹，方勉焉以模言语，此道未足而强言者也。后之惑者，徒见前世之文传，以为学者文而已，故愈力愈勤而愈不至。此足下所谓”终日不出于轩序，不能纵横高下皆如意“者，道未足也。若道之充焉，虽行乎天地，入于渊泉，无不之也。 先辈之文浩乎沛然，可谓善矣。而又志于为道，犹自以为未广。若不止焉，孟、荀可至而不难也。修，学道而不至者，然幸不甘于所悦而溺于所止。因吾子之能不自止，又以励修之少进焉。幸甚！幸甚！修白。 答祖择之书修启。秀才人至，蒙示书一通，并诗赋杂文两策，谕之曰：“一览以为如何？”某既陋，不足以辱好学者之问；又其少贱而长穷，其素所为未有足称以取信于人。亦尝有人问者，以不足问之愚，而未尝答人之问。足下卒然及之，是以愧惧不知所言。虽然，不远数百里走使者以及门，意厚礼勤，何敢不报。 某闻古之学者必严其师，师严然后道尊，道尊然后笃敬，笃敬然后能自守，能自守然后果于用，果于用然后不畏而不迁。三代之衰，学校废。至两汉，师道尚存，故其学者各守其经以自用。是以汉之政理文章与其当时之事，后世莫及者，其所从来深矣。后世师，法渐坏，而今世无师，则学者不尊严，故自轻其道。轻之则不能至，不至则不能笃信，信不笃则不知所守，守不固则有所畏而物可移。是故学者惟俯仰徇时，以希禄利为急，至于忘本趋末，流而不返。夫以不信不固之心，守不至之学，虽欲果于自用，而莫知其所以用之之道，又况有禄利之诱、刑祸之惧以迁之哉！此足下所谓志古知道之士世所鲜，而未有合者，由此也。 足下所为文，用意甚高，卓然有不顾世俗之心，直欲自到于古人。今世之人用心如足下者有几？是则乡曲之中能为足下之师者谓谁，交游之间能发足下之议论者谓谁？学不师则守不一，议论不博则无所发明而究其深。足下之言高趣远，甚善，然所守未一而议论未精，此其病也。窃惟足下之交游能为足下称才誉美者不少，今皆舍之，远而见及，乃知足下是欲求其不至。此古君子之用心也，是以言之不敢隐。 夫世无师矣，学者当师经，师经必先求其意，意得则心定，心定则道纯，道纯则充于中者实，中充实则发为文者辉光，施于世者果致。三代、两汉之学，不过此也。足下患世未有合者，而不弃其愚，将某以为合，故敢道此。未知足下之意合否？ 与荆南乐秀才书修顿首白秀才足下。前者舟行往来，屡辱见过。又辱以所业一编，先之启事，及门而贽。田秀才西来，辱书；其后予家奴自府还县，比又辱书。仆有罪之人，人所共弃，而足下见礼如此，何以当之？当之未暇答，宜遂绝，而再辱书；再而未答，益宜绝，而又辱之。何其勤之甚也！如修者，天下穷贱之人尔，安能使足下之切切如是邪？盖足下力学好问，急于自为谋而然也。然蒙索仆所为文字者，此似有所过听也。 仆少从进士举于有司，学为诗赋，以备程试，凡三举而得第。与士君子相识者多，故往往能道仆名字，而又以游从相爱之私，或过称其文字。故使足下闻仆虚名，而欲见其所为者，由此也。 仆少孤贫，贪禄仕以养亲，不暇就师穷经，以学圣人之遗业。而涉猎书史，姑随世俗作所谓时文者，皆穿蠹经传，移此俪彼，以为浮薄，惟恐不悦于时人，非有卓然自立之言如古人者。然有司过采，屡以先多士。及得第已来，自以前所为不足以称有司之举而当长者之知，始大改其为，庶几有立。然言出而罪至，学成而身辱，为彼则获誉，为此则受祸，此明效也。 夫时文虽曰浮巧，然其为功，亦不易也。仆天姿不好而强为之，故比时人之为者尤不工，然已足以取禄仕而窃名誉者，顺时故也。先辈少年志盛，方欲取荣誉于世，则莫若顺时。天圣中，天子下诏书，敕学者去浮华，其后风俗大变。今时之士大夫所为，彬彬有两汉之风矣。先辈往学之，非徒足以顺时取誉而已，如其至之，是直齐肩于两汉之士也。若仆者，其前所为既不足学，其后所为慎不可学，是以徘徊不敢出其所为者，为此也。 在《易》之《困》曰：“有言不信。”谓夫人方困时，其言不为人所信也。今可谓困矣，安足为足下所取信哉？辱书既多且切，不敢不答。幸察。 答李诩第一书修白。人至，辱书及《性诠》三篇，曰以质其果是。夫自信笃者，无所待于人；有质于人者，自疑者也。今吾子自谓“夫子与孟、荀、扬、韩复生，不能夺吾言”，其可谓自信不疑者矣。而返以质于修。使修有过于夫子者，乃可为吾子辩，况修未及孟、荀、扬、韩之一二也。修非知道者，好学而未至者也。世无师久矣，尚赖朋友切磋之益，苟不自满而中止，庶几终身而有成。固常乐与学者论议往来，非敢以益于人，盖求益于人者也。况如吾子之文章论议，岂易得哉？固乐为吾子辩也。苟尚有所疑，敢不尽其所学以告，既吾子自信如是，虽夫子不能夺，使修何所说焉？人还索书，未知所答，惭惕惭惕。修再拜。 答李诩第二书修白。前辱示书及《性诠》三篇，见吾子好学善辩，而文能尽其意之详。令世之言性者多矣，有所不及也，故思与吾子卒其说。 修患世之学者多言性，故常为说曰“夫性，非学者之所急，而圣人之所罕言也。《易》六十四卦不言性，其言者动静得失吉凶之常理也；《春秋》二百四十二年不言性，其言者善恶是非之实录也；《诗》三百五篇不言性，其言者政教兴衰之美刺也；《书》五十九篇不言性，其言者尧、舜、三代之治乱也；《礼》、《乐》之书虽不完，而杂出于诸儒之记，然其大要，治国修身之法也。六经之所载，皆人事之切于世者，是以言之甚详。至于性也，百不一二言之，或因言而及焉，非为性而言也，故虽言而不究。 予之所谓不言者，非谓绝而无言，盖其言者鲜，而又不主于性而言也。《论语》所载七十二子之问于孔子者，问孝、问忠、问仁义、问礼乐、问修身、问为政、问朋友、问鬼神者有矣，未尝有问性者。孔子之告其弟子者，凡数千言，其及于性者一言而已。予故曰：非学者之所急，而圣人之罕言也。 《书》曰“习与性成”，《语》曰“性相近，习相远”者，戒人慎所习而言也。《中庸》曰“天命之谓性，率性之谓道”者，明性无常，必有以率之也。《乐记》亦曰“感物而动，性之欲”者，明物之感人无不至也。然终不言性果善果恶，但戒人慎所习与所感，而勤其所以率之者尔。予故曰“因言以及之，而不究也。 修少好学，知学之难。凡所谓六经之所载，七十二子之所问者，学之终身，有不能达者矣；于其所达，行之终身，有不能至者矣。以予之汲汲于此而不暇乎其他，因以知七十二子亦以是汲汲而不暇也，又以知圣人所以教人垂世，亦皇皇而不暇也。今之学者于古圣贤所皇皇汲汲者，学之行之，或未至其一二，而好为性说，以穷圣贤之所罕言而不究者，执后儒之偏说，事无用之空言，此予之所不暇也。 或有问曰：性果不足学乎？予曰：性者，与身俱生而人之所皆有也。为君子者，修身治人而已，性之善恶不必究也。使性果善邪，身不可以不修，人不可以不治；使性果恶邪，身不可以不修，人不可以不治。不修其身，虽君子而为小人，《书》曰“惟圣罔念作狂”是也；能修其身，虽小人而为君子，《书》曰“惟狂克念作圣”是也。治道备，人斯为善矣，《书》曰“黎民于变时雍”是也；治道失，人斯为恶矣，《书》曰“殷顽民”，又曰“旧染污俗”是也。故为君子者，以修身治人为急，而不穷性以为言。夫七十二子之不问，六经之不主言，或虽言而不究，岂略之哉，盖有意也。 或又问曰：然则三子言性，过欤？曰：不过也。其不同何也？曰：始异而终同也。使孟子曰人性善矣，遂怠而不教，则是过也；使荀子曰人性恶矣，遂弃而不教，则是过也；使扬子曰人性混矣，遂肆而不教，则是过也。然三子者，或身奔走诸侯以行其道，或著书累千万言以告于后世，未尝不区区以仁义礼乐为急。盖其意以谓善者一日不教，则失而入于恶；恶者勤而教之，则可使至于善；混者驱而率之，则可使去恶而就善也。其说与《书》之“习与性成”，《语》之“性近习远”，《中庸》之“有以率之”，《乐记》之“慎物所感”皆合。夫三子者，推其言则殊，察其用心则一，故予以为推其言不过始异而终同也。凡论三子者，以予言而一之，则譊譊者可以息矣。 予之所说如此，吾子其择焉。 醉翁亭记环滁皆山也。其西南诸峰，林壑尤美，望之蔚然而深秀者，琅琊也。山行六七里，渐闻水声潺潺，而泻出于两峰之间者，酿泉也。峰回路转，有亭翼然临于泉上者，醉翁亭也。作亭者谁？山之僧智仙也。名之者谁？太守自谓也。太守与客来饮于此，饮少辄醉，而年又最高，故自号曰醉翁也。醉翁之意不在酒，在乎山水之间也。山水之乐，得之心而寓之酒也。 若夫日出而林霏开，云归而岩穴暝，晦明变化者，山间之朝暮也。野芳发而幽香，佳木秀而繁阴，风霜高洁，水落而石出者，山间之四时也。朝而往，暮而归，四时之景不同，而乐亦无穷也。 至于负者歌于途，行者休于树，前者呼，后者应，伛偻提携，往来而不绝者，滁人游也。临溪而渔，溪深而鱼肥。酿泉为酒，泉香而酒洌；山肴野蔌，杂然而前陈者，太守宴也。宴酣之乐，非丝非竹，射者中，弈者胜，觥筹交错，起坐而喧哗者，众宾欢也。苍颜白发，颓然乎其间者，太守醉也。 已而夕阳在山，人影散乱，太守归而宾客从也。树林阴翳，鸣声上下，游人去而禽鸟乐也。然而禽鸟知山林之乐，而不知人之乐；人知从太守游而乐，而不知太守之乐其乐也。醉能同其乐，醒能述以文者，太守也。太守谓谁？庐陵欧阳修也。 朝中措 · 送刘仲原甫出守维扬平山阑槛倚晴空，山色有无中。手种堂前垂柳，别来几度春风？ 文章太守，挥毫万字，一饮千钟。行乐直须年少，尊前看取衰翁。 夜行船 · 忆昔西都欢纵忆昔西都欢纵。自别后、有谁能共。伊川山水洛川花，细寻思、旧游如梦。 今日相逢情愈重。愁闻唱、画楼钟动。白发天涯逢此景，倒金尊、殢谁相送。 伶官传序呜呼！盛衰之理，虽曰天命，岂非人事哉！原庄宗之所以得天下，与其所以失之者，可以知之矣。 世言晋王之将终也，以三矢赐庄宗而告之曰：“梁，吾仇也；燕王，吾所立，契丹，与吾约为兄弟，而皆背晋以归梁。此三者，吾遗恨也。与尔三矢，尔其无忘乃父之志！”庄宗受而藏之于庙。其后用兵，则遣从事以一少牢告庙，请其矢，盛以锦囊，负而前驱，及凯旋而纳之。 方其系燕父子以组，函梁君臣之首，入于太庙，还矢先王，而告以成功，其意气之盛，可谓壮哉！及仇雠已灭，天下已定，一夫夜呼，乱者四应，仓皇东出，未及见贼而士卒离散，君臣相顾，不知所归。至于誓天断发，泣下沾襟，何其衰也！岂得之难而失之易欤？抑本其成败之迹，而皆自于人欤？ 《书》曰：“满招损，谦得益。”忧劳可以兴国，逸豫可以亡身，自然之理也。故方其盛也，举天下之豪杰莫能与之争；及其衰也，数十伶人困之，而身死国灭，为天下笑。夫祸患常积于忽微，而智勇多困于所溺，岂独伶人也哉！作《伶官传》。","categories":[],"tags":[{"name":"Poetry and Prose","slug":"Poetry-and-Prose","permalink":"http://example.com/tags/Poetry-and-Prose/"}]},{"title":"KafkaConsumer 源码之 consumer 如何加入 consumer group","slug":"kafka-consumer-group","date":"2020-11-04T01:35:04.000Z","updated":"2021-01-05T07:32:45.777Z","comments":true,"path":"2020/11/04/kafka-consumer-group/","link":"","permalink":"http://example.com/2020/11/04/kafka-consumer-group/","excerpt":"","text":"Kafka 的 consumer 比 producer 要复杂许多，producer 没有 group 的概念，也不需要关注 offset，而 consumer 不一样，它有组织 (consumer group)，有纪律 (offset)。这些对 consumer 的要求就会很高，这篇文章就主要介绍 consumer 是如何加入 consumer group 的。 在这之前，我们需要先了解一下什么是 GroupCoordinator。简单地说，GroupCoordinator 是运行在服务器上的一个服务，Kafka 集群上的每一个 broker 节点启动的时候，都会启动一个 GroupCoordinator 服务，其功能是负责进行 consumer 的 group 成员与 offset 管理 (但每个 GroupCoordinator 只是管理一部分的 consumer group member 和 offset 信息)。 consumer group 对应的 GroupCoordinator 节点的确定，会通过如下方式： 将 consumer group 的 group.id 进行 hash，把得到的值的绝对值，对 _consumer_offsets 的 partition 总数取余，然后得到其对应的 partition 值，该 partition 的 leader 所在的 broker 即为该 consumer group 所对应的 GroupCoordinator 节点，GroupCoordinator 会存储与该 consumer group 相关的所有的 Meta 信息。 1._consumer_offsets 这个 topic 是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有 50 个 partition，每个 partition 默认有三个副本。 2.partition 计算方式：abs(GroupId.hashCode()) % NumPartitions，其中，NumPartitions 是 _consumer_offsets 的 partition 数，默认是 50 个。 3.比如，现在通过计算 abs(GroupId.hashCode()) % NumPartitions 的值为 35，然后就找第 35 个 partition 的 leader 在哪个 broker 上 (假设在 192.168.1.12)，那么 GroupCoordinator 节点就在这个 broker 上。 同时，这个 consumer group 所提交的消费 offset 信息也会发送给这个 partition 的 leader 所对应的 broker 节点，因此，这个节点不仅是 GroupCoordinator，而且还保存分区分配方案和组内消费者 offset 信息。 更多关于 GroupCoordinator 的解析，参考：Kafka 源码解析之 GroupCoordinator 详解。 KafkaConsumer 消费消息的主体流程接下来，我们回顾下 KafkaConsumer 消费消息的主体流程： 12345678// 创建消费者KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props);// 订阅主题kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;))// 从服务器拉取数据ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(100)); 创建 KafkaConsumer创建 KafkaConsumer 的时候，会创建一个 ConsumerCoordinator 服务，由它来负责和 GroupCoordinator 通信： 1234567891011121314151617181920// no coordinator will be constructed for the default (null) group idthis.coordinator = groupId == null ? null : new ConsumerCoordinator(logContext, this.client, groupId, this.groupInstanceId, maxPollIntervalMs, sessionTimeoutMs, new Heartbeat(time, sessionTimeoutMs, heartbeatIntervalMs, maxPollIntervalMs, retryBackoffMs), assignors, this.metadata, this.subscriptions, metrics, metricGrpPrefix, this.time, retryBackoffMs, enableAutoCommit, config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG), this.interceptors, config.getBoolean(ConsumerConfig.LEAVE_GROUP_ON_CLOSE_CONFIG)); 订阅 topicKafkaConsumer 订阅 topic 的方式有好几种，这在前面的文章有提到过。订阅的时候，会根据订阅的方式，设置其对应的订阅类型，默认存在四种订阅类型： 12345678910private enum SubscriptionType &#123; // 默认 NONE, // subscribe方式订阅 AUTO_TOPICS, // subscribe方式订阅 AUTO_PATTERN, // assign方式订阅 USER_ASSIGNED&#125; 比如，采用 kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;)) 方式订阅 topic 时，会将订阅类型设置为 SubscriptionType.AUTO_TOPICS，其核心代码如下： 12345678910111213141516171819202122232425/** * Subscribe to the given list of topics to get dynamically assigned partitions. * &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; It is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * This is a short-hand for &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;, which * uses a no-op listener. If you need the ability to seek to particular offsets, you should prefer * &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;, since group rebalances will cause partition offsets * to be reset. You should also provide your own listener if you are doing your own offset * management since the listener gives you an opportunity to commit offsets before a rebalance finishes. * * @param topics The list of topics to subscribe to * @throws IllegalArgumentException If topics is null or contains null or empty elements * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics) &#123; subscribe(topics, new NoOpConsumerRebalanceListener());&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Subscribe to the given list of topics to get dynamically * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * As part of group management, the consumer will keep track of the list of consumers that belong to a particular * group and will trigger a rebalance operation if any one of the following events are triggered: * &lt;ul&gt; * &lt;li&gt;Number of partitions change for any of the subscribed topics * &lt;li&gt;A subscribed topic is created or deleted * &lt;li&gt;An existing member of the consumer group is shutdown or fails * &lt;li&gt;A new member is added to the consumer group * &lt;/ul&gt; * &lt;p&gt; * When any of these events are triggered, the provided listener will be invoked first to indicate that * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received. * Note that rebalances will only occur during an active call to &#123;@link #poll(Duration)&#125;, so callbacks will * also only be invoked during that time. * * The provided listener will immediately override any listener set in a previous call to subscribe. * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics * subscribed in this call. See &#123;@link ConsumerRebalanceListener&#125; for more details. * * @param topics The list of topics to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; acquireAndEnsureOpen(); try &#123; maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot be null&quot;); if (topics.isEmpty()) &#123; // treat subscribing to empty topic list as the same as unsubscribing this.unsubscribe(); &#125; else &#123; for (String topic : topics) &#123; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;); &#125; throwIfNoAssignorsConfigured(); fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(&quot;Subscribed to topic(s): &#123;&#125;&quot;, Utils.join(topics, &quot;, &quot;)); if (this.subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; 12345public synchronized boolean subscribe(Set&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_TOPICS); return changeSubscription(topics);&#125; 123456789101112/** * This method sets the subscription type if it is not already set (i.e. when it is NONE), * or verifies that the subscription type is equal to the give type when it is set (i.e. * when it is not NONE) * @param type The given subscription type */private void setSubscriptionType(SubscriptionType type) &#123; if (this.subscriptionType == SubscriptionType.NONE) this.subscriptionType = type; else if (this.subscriptionType != type) throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);&#125; 从服务器拉取数据订阅完成后，就可以从服务器拉取数据了，应该注意的是，KafkaConsumer 没有后台线程默默的拉取数据，它的所有行为都集中在 poll () 方法中，KafkaConsumer 是线程不安全的，同时只能允许一个线程运行。 kafkaConsumer.poll () 方法的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445private ConsumerRecords&lt;K, V&gt; poll(final Timer timer, final boolean includeMetadataInTimeout) &#123; // Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭 acquireAndEnsureOpen(); try &#123; if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123; throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;); &#125; // poll for new data until the timeout expires do &#123; client.maybeTriggerWakeup(); if (includeMetadataInTimeout) &#123; // Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息 if (!updateAssignmentMetadataIfNeeded(timer)) &#123; return ConsumerRecords.empty(); &#125; &#125; else &#123; while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123; log.warn(&quot;Still waiting for metadata&quot;); &#125; &#125; // Step3:拉取数据 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer); if (!records.isEmpty()) &#123; // before returning the fetched records, we can send off the next round of fetches // and avoid block waiting for their responses to enable pipelining while the user // is handling the fetched records. // // NOTE: since the consumed position has already been updated, we must not allow // wakeups or any other errors to be triggered prior to returning the fetched records. if (fetcher.sendFetches() &gt; 0 || client.hasPendingRequests()) &#123; client.pollNoWakeup(); &#125; return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records)); &#125; &#125; while (timer.notExpired()); return ConsumerRecords.empty(); &#125; finally &#123; release(); &#125;&#125; 可以看出，在 Step 1 阶段， poll () 方法会先进行判定，如果有多个线程同时使用一个 KafkaConsumer 则会抛出异常： 1234567891011/** * Acquire the light lock and ensure that the consumer hasn&#x27;t been closed. * @throws IllegalStateException If the consumer has been closed */private void acquireAndEnsureOpen() &#123; acquire(); if (this.closed) &#123; release(); throw new IllegalStateException(&quot;This consumer has already been closed.&quot;); &#125;&#125; 123456789101112/** * Acquire the light lock protecting this consumer from multi-threaded access. Instead of blocking * when the lock is not available, however, we just throw an exception (since multi-threaded usage is not * supported). * @throws ConcurrentModificationException if another thread already has the lock */private void acquire() &#123; long threadId = Thread.currentThread().getId(); if (threadId != currentThread.get() &amp;&amp; !currentThread.compareAndSet(NO_CURRENT_THREAD, threadId)) throw new ConcurrentModificationException(&quot;KafkaConsumer is not safe for multi-threaded access&quot;); refcount.incrementAndGet();&#125; KafkaConsumer 如何加入 consumer group一个 KafkaConsumer 实例消费数据的前提是能够加入一个 consumer group 成功，并获取其要订阅的 tp（topic-partition）列表，因此首先要做的就是和 GroupCoordinator 建立连接，加入组织。 consumer 加入 group 的过程，也就是 reblance 的过程。如果出现了频繁 reblance 的问题，可能和 max.poll.interval.ms 和 max.poll.records 两个参数有关。 因此，我们先把目光集中在 ConsumerCoordinator 上，这个过程主要发生在 Step 2 阶段： 123456789101112/** * Visible for testing */boolean updateAssignmentMetadataIfNeeded(final Timer timer) &#123; // 1.本篇文章的内容主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group) if (coordinator != null &amp;&amp; !coordinator.poll(timer)) &#123; return false; &#125; // 2.updateFetchPositions(timer)方法留待下一篇文章分析(主要功能是:consumer获得partition的offset) return updateFetchPositions(timer);&#125; 关于对 ConsumerCoordinator 的处理都集中在 coordinator.poll () 方法中。其主要逻辑如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * Poll for coordinator events. This ensures that the coordinator is known and that the consumer * has joined the group (if it is using group management). This also handles periodic offset commits * if they are enabled. * (确保group的coordinator是已知的，并且这个consumer是已经加入到了group中，也用于offset周期性的commit) * &lt;p&gt; * Returns early if the timeout expires * * @param timer Timer bounding how long this method can block * @return true iff the operation succeeded */public boolean poll(Timer timer) &#123; maybeUpdateSubscriptionMetadata(); invokeCompletedOffsetCommitCallbacks(); // 如果是subscribe方式订阅的topic if (subscriptions.partitionsAutoAssigned()) &#123; // Always update the heartbeat last poll time so that the heartbeat thread does not leave the // group proactively due to application inactivity even if (say) the coordinator cannot be found. // 1.检查心跳线程运行是否正常，如果心跳线程失败则抛出异常，反之则更新poll调用的时间 pollHeartbeat(timer.currentTimeMs()); // 2.如果coordinator未知，则初始化ConsumeCoordinator if (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) &#123; return false; &#125; // 判断是否需要重新加入group，如果订阅的partition变化或者分配的partition变化，都可能需要重新加入group if (rejoinNeededOrPending()) &#123; // due to a race condition between the initial metadata fetch and the initial rebalance, // we need to ensure that the metadata is fresh before joining initially. This ensures // that we have matched the pattern against the cluster&#x27;s topics at least once before joining. if (subscriptions.hasPatternSubscription()) &#123; // For consumer group that uses pattern-based subscription, after a topic is created, // any consumer that discovers the topic after metadata refresh can trigger rebalance // across the entire consumer group. Multiple rebalances can be triggered after one topic // creation if consumers refresh metadata at vastly different times. We can significantly // reduce the number of rebalances caused by single topic creation by asking consumer to // refresh metadata before re-joining the group as long as the refresh backoff time has // passed. if (this.metadata.timeToAllowUpdate(timer.currentTimeMs()) == 0) &#123; this.metadata.requestUpdate(); &#125; if (!client.ensureFreshMetadata(timer)) &#123; return false; &#125; maybeUpdateSubscriptionMetadata(); &#125; // 3.确保group是active的，重新加入group，分配订阅的partition if (!ensureActiveGroup(timer)) &#123; return false; &#125; &#125; &#125; else &#123; // For manually assigned partitions, if there are no ready nodes, await metadata. // If connections to all nodes fail, wakeups triggered while attempting to send fetch // requests result in polls returning immediately, causing a tight loop of polls. Without // the wakeup, poll() with no channels would block for the timeout, delaying re-connection. // awaitMetadataUpdate() initiates new connections with configured backoff and avoids the busy loop. // When group management is used, metadata wait is already performed for this scenario as // coordinator is unknown, hence this check is not required. if (metadata.updateRequested() &amp;&amp; !client.hasReadyNodes(timer.currentTimeMs())) &#123; client.awaitMetadataUpdate(timer); &#125; &#125; // 4.如果设置的是自动commit,如果定时达到则自动commit maybeAutoCommitOffsetsAsync(timer.currentTimeMs()); return true;&#125; coordinator.poll () 方法中，具体实现可以分为四个步骤： pollHeartbeat ()：检测心跳线程运行是否正常，需要定时向 GroupCoordinator 发送心跳，如果超时未发送心跳，consumer 会离开 consumer group。 ensureCoordinatorReady ()：当通过 subscribe () 方法订阅 topic 时，如果 coordinator 未知，则初始化 ConsumerCoordinator (在 ensureCoordinatorReady () 中实现，该方法主要的作用是发送 FindCoordinatorRequest 请求，并建立连接)。 ensureActiveGroup ()：判断是否需要重新加入 group，如果订阅的 partition 变化或者分配的 partition 变化时，需要 rejoin，则通过 ensureActiveGroup () 发送 join-group、sync-group 请求，加入 group 并获取其 assign 的 TopicPartition list。 maybeAutoCommitOffsetsAsync ()：如果设置的是自动 commit，并且达到了发送时限则自动 commit offset。 关于 rejoin，下列几种情况会触发再均衡 (reblance) 操作： 订阅的 topic 列表变化 topic 被创建或删除 新的消费者加入消费者组 (第一次进行消费也属于这种情况) 消费者宕机下线 (长时间未发送心跳包) 消费者主动退出消费组，比如调用 unsubscrible () 方法取消对主题的订阅 消费者组对应的 GroupCoorinator 节点发生了变化 消费者组内所订阅的任一主题或者主题的分区数量发生了变化 取消 topic 订阅，consumer 心跳线程超时以及在 Server 端给定的时间内未收到心跳请求，这三个都是触发的 LEAVE_GROUP 请求。 下面重点介绍下第二步中的 ensureCoordinatorReady () 方法和第三步中的 ensureActiveGroup () 方法。 ensureCoordinatorReadyensureCoordinatorReady ()这个方法主要作用：选择一个连接数最少的 broker (还未响应请求最少的 broker)，发送 FindCoordinator 请求，找到 GroupCoordinator 后，建立对应的 TCP 连接。 方法调用流程是 ensureCoordinatorReady () → lookupCoordinator () → sendFindCoordinatorRequest ()。 如果 client 收到 server response，那么就与 GroupCoordinator 建立连接。 1234567891011121314151617181920212223242526272829303132333435363738/** * Visible for testing. * * Ensure that the coordinator is ready to receive requests. * * @param timer Timer bounding how long this method can block * @return true If coordinator discovery and initial connection succeeded, false otherwise */protected synchronized boolean ensureCoordinatorReady(final Timer timer) &#123; if (!coordinatorUnknown()) return true; do &#123; // 找到GroupCoordinator，并建立连接 final RequestFuture&lt;Void&gt; future = lookupCoordinator(); client.poll(future, timer); if (!future.isDone()) &#123; // ran out of time break; &#125; if (future.failed()) &#123; if (future.isRetriable()) &#123; log.debug(&quot;Coordinator discovery failed, refreshing metadata&quot;); client.awaitMetadataUpdate(timer); &#125; else throw future.exception(); &#125; else if (coordinator != null &amp;&amp; client.isUnavailable(coordinator)) &#123; // we found the coordinator, but the connection has failed, so mark // it dead and backoff before retrying discovery markCoordinatorUnknown(); timer.sleep(retryBackoffMs); &#125; &#125; while (coordinatorUnknown() &amp;&amp; timer.notExpired()); return !coordinatorUnknown();&#125; 12345678910111213protected synchronized RequestFuture&lt;Void&gt; lookupCoordinator() &#123; if (findCoordinatorFuture == null) &#123; // find a node to ask about the coordinator(找一个最少连接的broker，此处对应的应该就是文章开头处确定GroupCoordinator节点的发发) Node node = this.client.leastLoadedNode(); if (node == null) &#123; log.debug(&quot;No broker available to send FindCoordinator request&quot;); return RequestFuture.noBrokersAvailable(); &#125; else // 对找到的broker发送FindCoordinator请求，并对response进行处理 findCoordinatorFuture = sendFindCoordinatorRequest(node); &#125; return findCoordinatorFuture;&#125; 12345678910111213141516171819/** * Discover the current coordinator for the group. Sends a GroupMetadata request to * one of the brokers. The returned future should be polled to get the result of the request. * @return A request future which indicates the completion of the metadata request */private RequestFuture&lt;Void&gt; sendFindCoordinatorRequest(Node node) &#123; // initiate the group metadata request log.debug(&quot;Sending FindCoordinator request to broker &#123;&#125;&quot;, node); FindCoordinatorRequest.Builder requestBuilder = new FindCoordinatorRequest.Builder( new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.groupId)); // 发送请求，并将response转换为RequestFuture // compose的作用是将FindCoordinatorResponseHandler类转换为RequestFuture // 实际上就是为返回的Future类重置onSuccess()和onFailure()方法 return client.send(node, requestBuilder) .compose(new FindCoordinatorResponseHandler());&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142// 根据response返回的ip以及端口信息，和该broke上开启的GroupCoordinator建立连接private class FindCoordinatorResponseHandler extends RequestFutureAdapter&lt;ClientResponse, Void&gt; &#123; @Override public void onSuccess(ClientResponse resp, RequestFuture&lt;Void&gt; future) &#123; log.debug(&quot;Received FindCoordinator response &#123;&#125;&quot;, resp); clearFindCoordinatorFuture(); FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) resp.responseBody(); Errors error = findCoordinatorResponse.error(); if (error == Errors.NONE) &#123; // 如果正确获取broker上的GroupCoordinator，建立连接，并更新心跳时间 synchronized (AbstractCoordinator.this) &#123; // use MAX_VALUE - node.id as the coordinator id to allow separate connections // for the coordinator in the underlying network client layer int coordinatorConnectionId = Integer.MAX_VALUE - findCoordinatorResponse.data().nodeId(); AbstractCoordinator.this.coordinator = new Node( coordinatorConnectionId, findCoordinatorResponse.data().host(), findCoordinatorResponse.data().port()); log.info(&quot;Discovered group coordinator &#123;&#125;&quot;, coordinator); // 初始化tcp连接 client.tryConnect(coordinator); // 更新心跳时间 heartbeat.resetSessionTimeout(); &#125; future.complete(null); &#125; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else &#123; log.debug(&quot;Group coordinator lookup failed: &#123;&#125;&quot;, findCoordinatorResponse.data().errorMessage()); future.raise(error); &#125; &#125; @Override public void onFailure(RuntimeException e, RequestFuture&lt;Void&gt; future) &#123; clearFindCoordinatorFuture(); super.onFailure(e, future); &#125;&#125; 上面代码主要作用就是：往一个负载最小的 broker 节点发起 FindCoordinator 请求，Kafka 在走到这个请求后会根据 group_id 查找对应的 GroupCoordinator 节点 (文章开头处介绍的方法)，如果找到对应的 GroupCoordinator 则会返回其对应的 node_id，host 和 port 信息，并建立连接。 这里的 GroupCoordinator 节点的确定在文章开头提到过，是通过 group.id 和 partitionCount 来确定的。 ensureActiveGroup现在已经知道了 GroupCoordinator 节点，并建立了连接。ensureActiveGroup () 这个方法的主要作用：向 GroupCoordinator 发送 join-group、sync-group 请求，获取 assign 的 tp list。 调用过程是 ensureActiveGroup () → ensureCoordinatorReady () → startHeartbeatThreadIfNeeded () → joinGroupIfNeeded ()。 joinGroupIfNeeded () 方法中最重要的方法是 initiateJoinGroup ()，它的的调用流程是 disableHeartbeatThread () → sendJoinGroupRequest () → JoinGroupResponseHandler::handle () → onJoinLeader ()，onJoinFollower () → sendSyncGroupRequest ()。 12345678910111213141516171819/** * Ensure the group is active (i.e., joined and synced) * * @param timer Timer bounding how long this method can block * @return true iff the group is active */boolean ensureActiveGroup(final Timer timer) &#123; // always ensure that the coordinator is ready because we may have been disconnected // when sending heartbeats and does not necessarily require us to rejoin the group. // 1.确保GroupCoordinator已经连接 if (!ensureCoordinatorReady(timer)) &#123; return false; &#125; // 2.启动心跳线程，但是并不一定发送心跳，满足条件后才会发送心跳 startHeartbeatThreadIfNeeded(); // 3.发送joinGroup请求，并对返回的信息进行处理，核心步骤 return joinGroupIfNeeded(timer);&#125; 心跳线程就是在这里启动的，但是并不一定马上发送心跳包，会在满足条件之后才会开始发送。后面最主要的逻辑就集中在 joinGroupIfNeeded () 方法，它的核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Joins the group without starting the heartbeat thread. * * Visible for testing. * * @param timer Timer bounding how long this method can block * @return true iff the operation succeeded */boolean joinGroupIfNeeded(final Timer timer) &#123; while (rejoinNeededOrPending()) &#123; if (!ensureCoordinatorReady(timer)) &#123; return false; &#125; // call onJoinPrepare if needed. We set a flag to make sure that we do not call it a second // time if the client is woken up before a pending rebalance completes. This must be called // on each iteration of the loop because an event requiring a rebalance (such as a metadata // refresh which changes the matched subscription set) can occur while another rebalance is // still in progress. // 触发onJoinPrepare，包括offset commit和rebalance listener if (needsJoinPrepare) &#123; // 如果是自动提交，则要开始提交offset以及在join group之前回调reblance listener接口 onJoinPrepare(generation.generationId, generation.memberId); needsJoinPrepare = false; &#125; // 初始化joinGroup请求，并发送joinGroup请求，核心步骤 final RequestFuture&lt;ByteBuffer&gt; future = initiateJoinGroup(); client.poll(future, timer); if (!future.isDone()) &#123; // we ran out of time return false; &#125; // join succeed，这一步时，时间上sync-group已经成功了 if (future.succeeded()) &#123; // Duplicate the buffer in case `onJoinComplete` does not complete and needs to be retried. ByteBuffer memberAssignment = future.value().duplicate(); // 发送完成，consumer加入group成功，触发onJoinComplete()方法 onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment); // We reset the join group future only after the completion callback returns. This ensures // that if the callback is woken up, we will retry it on the next joinGroupIfNeeded. // 重置joinFuture为空 resetJoinGroupFuture(); needsJoinPrepare = true; &#125; else &#123; resetJoinGroupFuture(); final RuntimeException exception = future.exception(); if (exception instanceof UnknownMemberIdException || exception instanceof RebalanceInProgressException || exception instanceof IllegalGenerationException || exception instanceof MemberIdRequiredException) continue; else if (!future.isRetriable()) throw exception; timer.sleep(retryBackoffMs); &#125; &#125; return true;&#125; initiateJoinGroup () 方法的核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private synchronized RequestFuture&lt;ByteBuffer&gt; initiateJoinGroup() &#123; // we store the join future in case we are woken up by the user after beginning the // rebalance in the call to poll below. This ensures that we do not mistakenly attempt // to rejoin before the pending rebalance has completed. if (joinFuture == null) &#123; // fence off the heartbeat thread explicitly so that it cannot interfere with the join group. // Note that this must come after the call to onJoinPrepare since we must be able to continue // sending heartbeats if that callback takes some time. // Step1:rebalance期间，心跳线程停止运行 disableHeartbeatThread(); // 设置当前状态为rebalance state = MemberState.REBALANCING; // Step2:发送joinGroup请求，核心步骤 joinFuture = sendJoinGroupRequest(); // Step3:为joinGroup请求添加监听器，监听joinGroup请求的结果并做相应的处理 joinFuture.addListener(new RequestFutureListener&lt;ByteBuffer&gt;() &#123; @Override public void onSuccess(ByteBuffer value) &#123; // handle join completion in the callback so that the callback will be invoked // even if the consumer is woken up before finishing the rebalance synchronized (AbstractCoordinator.this) &#123; log.info(&quot;Successfully joined group with generation &#123;&#125;&quot;, generation.generationId); // 如果joinGroup成功，设置状态为stable state = MemberState.STABLE; rejoinNeeded = false; if (heartbeatThread != null) // Step4:允许心跳线程继续运行 heartbeatThread.enable(); &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; // we handle failures below after the request finishes. if the join completes // after having been woken up, the exception is ignored and we will rejoin synchronized (AbstractCoordinator.this) &#123; // 如果joinGroup失败，设置状态为unjoined state = MemberState.UNJOINED; &#125; &#125; &#125;); &#125; return joinFuture;&#125; 可以看到在 joinGroup 之前会让心跳线程暂时停下来，此时会将 ConsumerCoordinator 的状态设置为 rebalance 状态，当 joinGroup 成功之后会将状态设置为 stable 状态，同时让之前停下来的心跳线程继续运行。 sendJoinGroupRequest () 方法的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536/** * Join the group and return the assignment for the next generation. This function handles both * JoinGroup and SyncGroup, delegating to &#123;@link #performAssignment(String, String, List)&#125; if * elected leader by the coordinator. * * NOTE: This is visible only for testing * * @return A request future which wraps the assignment returned from the group leader */// 发送joinGroup请求RequestFuture&lt;ByteBuffer&gt; sendJoinGroupRequest() &#123; if (coordinatorUnknown()) return RequestFuture.coordinatorNotAvailable(); // send a join group request to the coordinator log.info(&quot;(Re-)joining group&quot;); JoinGroupRequest.Builder requestBuilder = new JoinGroupRequest.Builder( new JoinGroupRequestData() .setGroupId(groupId) .setSessionTimeoutMs(this.sessionTimeoutMs) .setMemberId(this.generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setProtocolType(protocolType()) .setProtocols(metadata()) .setRebalanceTimeoutMs(this.rebalanceTimeoutMs) ); log.debug(&quot;Sending JoinGroup (&#123;&#125;) to coordinator &#123;&#125;&quot;, requestBuilder, this.coordinator); // Note that we override the request timeout using the rebalance timeout since that is the // maximum time that it may block on the coordinator. We add an extra 5 seconds for small delays. int joinGroupTimeoutMs = Math.max(rebalanceTimeoutMs, rebalanceTimeoutMs + 5000); return client.send(coordinator, requestBuilder, joinGroupTimeoutMs) .compose(new JoinGroupResponseHandler());// Step5:处理joinGroup请求后的response&#125; 在发送 joinGroup 请求之后，会收到来自服务器的响应，然后针对这个响应再做一些重要的事情： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// 处理发送joinGroup请求后的response的handler(同步group信息)private class JoinGroupResponseHandler extends CoordinatorResponseHandler&lt;JoinGroupResponse, ByteBuffer&gt; &#123; @Override public void handle(JoinGroupResponse joinResponse, RequestFuture&lt;ByteBuffer&gt; future) &#123; Errors error = joinResponse.error(); if (error == Errors.NONE) &#123; log.debug(&quot;Received successful JoinGroup response: &#123;&#125;&quot;, joinResponse); sensors.joinLatency.record(response.requestLatencyMs()); synchronized (AbstractCoordinator.this) &#123; if (state != MemberState.REBALANCING) &#123; // if the consumer was woken up before a rebalance completes, we may have already left // the group. In this case, we do not want to continue with the sync group. future.raise(new UnjoinedGroupException()); &#125; else &#123; AbstractCoordinator.this.generation = new Generation(joinResponse.data().generationId(), joinResponse.data().memberId(), joinResponse.data().protocolName()); // Step6:joinGroup成功，下面需要进行sync-group，获取分配的tp列表 if (joinResponse.isLeader()) &#123; // 当前consumer是leader onJoinLeader(joinResponse).chain(future); &#125; else &#123; // 当前consumer是follower onJoinFollower().chain(future); &#125; &#125; &#125; &#125; else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS) &#123; log.debug(&quot;Attempt to join group rejected since coordinator &#123;&#125; is loading the group.&quot;, coordinator()); // backoff and retry future.raise(error); &#125; else if (error == Errors.UNKNOWN_MEMBER_ID) &#123; // reset the member id and retry immediately resetGeneration(); log.debug(&quot;Attempt to join group failed due to unknown member id.&quot;); future.raise(Errors.UNKNOWN_MEMBER_ID); &#125; else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123; // re-discover the coordinator and retry with backoff markCoordinatorUnknown(); log.debug(&quot;Attempt to join group failed due to obsolete coordinator information: &#123;&#125;&quot;, error.message()); future.raise(error); &#125; else if (error == Errors.FENCED_INSTANCE_ID) &#123; log.error(&quot;Received fatal exception: group.instance.id gets fenced&quot;); future.raise(error); &#125; else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL || error == Errors.INVALID_SESSION_TIMEOUT || error == Errors.INVALID_GROUP_ID || error == Errors.GROUP_AUTHORIZATION_FAILED || error == Errors.GROUP_MAX_SIZE_REACHED) &#123; // log the error and re-throw the exception log.error(&quot;Attempt to join group failed due to fatal error: &#123;&#125;&quot;, error.message()); if (error == Errors.GROUP_MAX_SIZE_REACHED) &#123; future.raise(new GroupMaxSizeReachedException(groupId)); &#125; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else &#123; future.raise(error); &#125; &#125; else if (error == Errors.UNSUPPORTED_VERSION) &#123; log.error(&quot;Attempt to join group failed due to unsupported version error. Please unset field group.instance.id and retry&quot; + &quot;to see if the problem resolves&quot;); future.raise(error); &#125; else if (error == Errors.MEMBER_ID_REQUIRED) &#123; // Broker requires a concrete member id to be allowed to join the group. Update member id // and send another join group request in next cycle. synchronized (AbstractCoordinator.this) &#123; AbstractCoordinator.this.generation = new Generation(OffsetCommitRequest.DEFAULT_GENERATION_ID, joinResponse.data().memberId(), null); AbstractCoordinator.this.rejoinNeeded = true; AbstractCoordinator.this.state = MemberState.UNJOINED; &#125; future.raise(Errors.MEMBER_ID_REQUIRED); &#125; else &#123; // unexpected error, throw the exception log.error(&quot;Attempt to join group failed due to unexpected error: &#123;&#125;&quot;, error.message()); future.raise(new KafkaException(&quot;Unexpected error in join group response: &quot; + error.message())); &#125; &#125;&#125; 上面代码的主要过程如下： 如果 group 是新的 group.id，那么此时 group 初始化的状态为 Empty； 当 GroupCoordinator 接收到 consumer 的 join-group 请求后，由于此时这个 group 的 member 列表还是空 (group 是新建的，每个 consumer 实例被称为这个 group 的一个 member)，第一个加入的 member 将被选为 leader，也就是说，对于一个新的 consumer group 而言，当第一个 consumer 实例加入后将会被选为 leader。如果后面 leader 挂了，会从其他 member 里面随机选择一个 member 成为新的 leader； 如果 GroupCoordinator 接收到 leader 发送 join-group 请求，将会触发 rebalance，group 的状态变为 PreparingRebalance； 此时，GroupCoordinator 将会等待一定的时间，如果在一定时间内，接收到 join-group 请求的 consumer 将被认为是依然存活的，此时 group 会变为 AwaitSync 状态，并且 GroupCoordinator 会向这个 group 的所有 member 返回其 response； consumer 在接收到 GroupCoordinator 的 response 后，如果这个 consumer 是 group 的 leader，那么这个 consumer 将会负责为整个 group assign partition 订阅安排（默认是按 range 的策略，目前也可选 roundrobin），然后 leader 将分配后的信息以 sendSyncGroupRequest () 请求的方式发给 GroupCoordinator，而作为 follower 的 consumer 实例会发送一个空列表； GroupCoordinator 在接收到 leader 发来的请求后，会将 assign 的结果返回给所有已经发送 sync-group 请求的 consumer 实例，并且 group 的状态将会转变为 Stable，如果后续再收到 sync-group 请求，由于 group 的状态已经是 Stable，将会直接返回其分配结果。 sync-group 发送请求核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 当consumer为follower时，从GroupCoordinator拉取分配结果private RequestFuture&lt;ByteBuffer&gt; onJoinFollower() &#123; // send follower&#x27;s sync group with an empty assignment SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder( new SyncGroupRequestData() .setGroupId(groupId) .setMemberId(generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setGenerationId(generation.generationId) .setAssignments(Collections.emptyList()) ); log.debug(&quot;Sending follower SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;, this.coordinator, requestBuilder); // 发送sync-group请求 return sendSyncGroupRequest(requestBuilder);&#125;// 当consumer客户端为leader时，对group下的所有实例进行分配，将assign的结果发送到GroupCoordinatorprivate RequestFuture&lt;ByteBuffer&gt; onJoinLeader(JoinGroupResponse joinResponse) &#123; try &#123; // perform the leader synchronization and send back the assignment for the group(assign 操作) Map&lt;String, ByteBuffer&gt; groupAssignment = performAssignment(joinResponse.data().leader(), joinResponse.data().protocolName(), joinResponse.data().members()); List&lt;SyncGroupRequestData.SyncGroupRequestAssignment&gt; groupAssignmentList = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, ByteBuffer&gt; assignment : groupAssignment.entrySet()) &#123; groupAssignmentList.add(new SyncGroupRequestData.SyncGroupRequestAssignment() .setMemberId(assignment.getKey()) .setAssignment(Utils.toArray(assignment.getValue())) ); &#125; SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder( new SyncGroupRequestData() .setGroupId(groupId) .setMemberId(generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setGenerationId(generation.generationId) .setAssignments(groupAssignmentList) ); log.debug(&quot;Sending leader SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;, this.coordinator, requestBuilder); // 发送sync-group请求 return sendSyncGroupRequest(requestBuilder); &#125; catch (RuntimeException e) &#123; return RequestFuture.failure(e); &#125;&#125;// 发送SyncGroup请求，获取对partition分配的安排private RequestFuture&lt;ByteBuffer&gt; sendSyncGroupRequest(SyncGroupRequest.Builder requestBuilder) &#123; if (coordinatorUnknown()) return RequestFuture.coordinatorNotAvailable(); return client.send(coordinator, requestBuilder) .compose(new SyncGroupResponseHandler());&#125;private class SyncGroupResponseHandler extends CoordinatorResponseHandler&lt;SyncGroupResponse, ByteBuffer&gt; &#123; @Override public void handle(SyncGroupResponse syncResponse, RequestFuture&lt;ByteBuffer&gt; future) &#123; Errors error = syncResponse.error(); if (error == Errors.NONE) &#123; // sync-group成功 sensors.syncLatency.record(response.requestLatencyMs()); future.complete(ByteBuffer.wrap(syncResponse.data.assignment())); &#125; else &#123; // join的标志位设置为true requestRejoin(); if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else if (error == Errors.REBALANCE_IN_PROGRESS) &#123; // group正在进行rebalance，任务失败 log.debug(&quot;SyncGroup failed because the group began another rebalance&quot;); future.raise(error); &#125; else if (error == Errors.FENCED_INSTANCE_ID) &#123; log.error(&quot;Received fatal exception: group.instance.id gets fenced&quot;); future.raise(error); &#125; else if (error == Errors.UNKNOWN_MEMBER_ID || error == Errors.ILLEGAL_GENERATION) &#123; log.debug(&quot;SyncGroup failed: &#123;&#125;&quot;, error.message()); resetGeneration(); future.raise(error); &#125; else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123; log.debug(&quot;SyncGroup failed: &#123;&#125;&quot;, error.message()); markCoordinatorUnknown(); future.raise(error); &#125; else &#123; future.raise(new KafkaException(&quot;Unexpected error from SyncGroup: &quot; + error.message())); &#125; &#125; &#125;&#125; 这个阶段主要是将分区分配方案同步给各个消费者，这个同步仍然是通过 GroupCoordinator 来转发的。 分区策略并非由 leader 消费者来决定，而是各个消费者投票决定的，谁的票多就采用什么分区策略。这里的分区策略是通过 partition.assignment.strategy 参数设置的，可以设置多个。如果选举出了消费者不支持的策略，那么就会抛出异常 IllegalArgumentException: Member does not support protocol。 经过上面的步骤，一个 consumer 实例就已经加入 group 成功了，加入 group 成功后，将会触发 ConsumerCoordinator 的 onJoinComplete () 方法，其作用就是：更新订阅的 tp 列表、更新其对应的 metadata 及触发注册的 listener。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 加入group成功@Overrideprotected void onJoinComplete(int generation, String memberId, String assignmentStrategy, ByteBuffer assignmentBuffer) &#123; // only the leader is responsible for monitoring for metadata changes (i.e. partition changes) if (!isLeader) assignmentSnapshot = null; PartitionAssignor assignor = lookupAssignor(assignmentStrategy); if (assignor == null) throw new IllegalStateException(&quot;Coordinator selected invalid assignment protocol: &quot; + assignmentStrategy); Assignment assignment = ConsumerProtocol.deserializeAssignment(assignmentBuffer); if (!subscriptions.assignFromSubscribed(assignment.partitions())) &#123; handleAssignmentMismatch(assignment); return; &#125; Set&lt;TopicPartition&gt; assignedPartitions = subscriptions.assignedPartitions(); // The leader may have assigned partitions which match our subscription pattern, but which // were not explicitly requested, so we update the joined subscription here. maybeUpdateJoinedSubscription(assignedPartitions); // give the assignor a chance to update internal state based on the received assignment assignor.onAssignment(assignment, generation); // reschedule the auto commit starting from now if (autoCommitEnabled) this.nextAutoCommitTimer.updateAndReset(autoCommitIntervalMs); // execute the user&#x27;s callback after rebalance ConsumerRebalanceListener listener = subscriptions.rebalanceListener(); log.info(&quot;Setting newly assigned partitions: &#123;&#125;&quot;, Utils.join(assignedPartitions, &quot;, &quot;)); try &#123; listener.onPartitionsAssigned(assignedPartitions); &#125; catch (WakeupException | InterruptException e) &#123; throw e; &#125; catch (Exception e) &#123; log.error(&quot;User provided listener &#123;&#125; failed on partition assignment&quot;, listener.getClass().getName(), e); &#125;&#125; 至此，一个 consumer 实例算是真正上意义上加入 group 成功。 然后 consumer 就进入正常工作状态，同时 consumer 也通过向 GroupCoordinator 发送心跳来维持它们与消费者组的从属关系，以及它们对分区的所有权关系。只要以正常的间隔发送心跳，就被认为是活跃的，但是如果 GroupCoordinator 没有响应，那么就会发送 LeaveGroup 请求退出消费者组。 本文参考http://generalthink.github.io/2019/05/15/how-to-join-kafka-consumer-group/ https://matt33.com/2017/10/22/consumer-join-group/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"Kafka 的核心配置参数","slug":"kafka-properties","date":"2020-10-30T07:48:10.000Z","updated":"2021-01-05T07:33:28.544Z","comments":true,"path":"2020/10/30/kafka-properties/","link":"","permalink":"http://example.com/2020/10/30/kafka-properties/","excerpt":"","text":"Kafka Producer 核心配置参数bootstrap.servers broke 服务器地址，多个服务器，用逗号隔开。 acks 发送应答，默认：1。 acks 参数指定了生产者希望 leader 返回的用于确认请求完成的确认数量，即必须要有多少个分区副本收到该消息，生产者才会认为消息写入是成功的。 允许以下设置： acks=0：生产者将完全不等待来自服务器的任何确认。记录将立即添加到 socket 缓冲区，并被认为已发送。在这种情况下，不能保证服务器已经收到记录，重试配置将不会生效 (因为客户机通常不会知道任何失败)。响应里来自服务端的 offset 总是-1。同时，由于不需要等待响应，所以可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量。 acks=1：只需要集群的 leader 收到消息，生产者就会收到一个来自服务器的成功响应。leader 会将记录写到本地日志中，但不会等待所有 follower 的完全确认。在这种情况下，如果 follower 复制数据之前，leader 挂掉，数据就会丢失。 acks=all / -1：当所有参与复制的节点全部收到消息的时候，生产者才会收到一个来自服务器的成功响应，最安全不过延迟比较高。如果需要保证消息不丢失, 需要使用该设置，同时需要设置 broke端 unclean.leader.election.enable 为 true，保证当 ISR 列表为空时，选择其他存活的副本作为新的 leader。 batch.size 批量发送大小，默认：16384，即 16 K。 当有多个消息需要被发送到同一个 partition 的时候，生产者会把他们放到同一个批次里面 (Deque)，该参数指定了一个批次可以使用的内存大小，按照字节数计算，当批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定会等到批次被填满才发送，半满甚至只包含一个消息的批次也有可能被发送。 生产者产生的消息缓存到本地，每次批量发送 batch.size 大小到服务器。太小的 batch 会降低吞吐，太大则会浪费内存。 linger.ms 发送延迟时间，默认：0。 指定了生产者在发送批次之前等待更多消息加入批次的时间。生产者会在批次填满或 linger.ms 达到上限时把批次发送出去。把 linger.ms 设置成比0大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次，虽然这样会增加延迟，但也会提升吞吐量。 说明：batch.size 和 linger.ms 满足任何一个条件都会发送。 buffer.memory 生产者最大可用缓存，默认：33554432，即 32 M。 生产者可以用来缓冲等待发送到服务器的记录的总内存字节。如果应用程序发送消息的速度超过生产者发送消息到服务器的速度，即超出 max.block.ms，将会抛出异常。 该设置应该大致与生产者将使用的总内存相对应，但不是硬绑定，因为生产者使用的内存并非全部都用于缓冲。一些额外的内存将用于压缩 (如果启用了压缩) 以及维护飞行中的请求。 max.block.ms 阻塞时间，默认：60000，即 1 分钟。 指定了在调用 send () 方法或者 partitionsFor () 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 max.block.ms 时，就会抛出 new TimeoutException(“Failed to allocate memory within the configured max blocking time “ + maxTimeToBlockMs + “ ms.”);。 用户提供的序列化器或分区程序中的阻塞将不计入此超时。 client.id 生产者 ID，默认：空。 请求时传递给服务器的 id 字符串，用来标识消息来源，后台线程会根据它命名。这样做的目的是通过允许在服务器端请求日志中包含逻辑应用程序名称，从而能够跟踪 ip/端口之外的请求源。 compression.type 生产者数据被发送到服务器之前被压缩的压缩类型，默认：none，即不压缩。 指定给定主题的最终压缩类型。此配置接受标准压缩编解码器 (“gzip”、“snappy”、“lz4”、“zstd”)。 “gzip”：压缩效率高，适合高内存、CPU。 “snappy”：适合带宽敏感性，压缩力度大。 retries 失败重试次数，默认：2147483647。 异常是 RetriableException 类型，或者 TransactionManager 允许重试 (transactionManager.canRetry () )。 RetriableException 类型异常如下： retry.backoff.ms 失败请求重试的间隔时间，默认：100。 这避免了在某些失败场景下以紧密循环的方式重复发送请求。 max.in.flight.requests.per.connection 单个连接上发送的未确认请求的最大数量，默认：5。 阻塞前客户端在单个连接上发送的未确认请求的最大数量。即指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。 如果设置为 1，可以保证消息是按照发送的顺序写入服务器的，即便发生了重试。 如果设置大于 1，在 retries 不为0的情况下可能会出现消息发送顺序的错误。例如将两个批发送到同一个分区，第一个批处理失败并重试，但是第二个批处理成功，那么第二个批处理中的记录可能会先出现。 delivery.timeout.ms 传输时间，默认：120000，即 2 分钟。 生产者发送完请求接受服务器 ACK 的时间，该时间允许重试 ，该配置应该大于 request.timeout.ms + linger.ms。 request.timeout.ms 请求超时时间，默认：30000，即30秒。 配置控制客户端等待请求响应的最长时间。 如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽，则该请求将失败。 这应该大于replica.lag.time.max.ms (broker 端配置)，以减少由于不必要的生产者重试引起的消息重复的可能性。 connections.max.idle.ms 连接空闲超时时间，默认：540000，即 9 分钟。 在此配置指定的毫秒数之后关闭空闲连接。 enable.idempotence 开启幂等，默认：false。 如果设置为 true ，将开启 exactly-once 模式，生产者将确保在流中准确地写入每个消息的副本。如果设置为 false，则由于代理失败而导致生产者重试，等等，可能会在流中写入重试消息的副本。 注意，启用幂等需要以下条件 ：max.in.flight.requests.per.connection 小于或等于 5，retries 大于 0， acks 必须为 all 或者 -1。如果用户没有显式地设置这些值，将选择合适的值。如果设置了不兼容的值，就会抛出 ConfigException。 key.serializer key 序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Serializer 。Kafka 提供以下几个默认的 key 序列化器： String：org.apache.kafka.common.serialization.StringSerializer。 value.serializer value 序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Serializer。Kafka 提供以下几个默认的 value 序列化器： byte[]：org.apache.kafka.common.serialization.ByteArraySerializer。 String：org.apache.kafka.common.serialization.StringSerializer。 max.request.size 请求的最大字节大小，默认：1048576，即 1 M。 该参数用于控制生产者发送的请求大小，单次发送的消息大小超过 max.request.size 时，会抛出异常 ，如：org.apache.kafka.common.errors.RecordTooLargeException: The message is 70459102 bytes when serialized which is larger than the maximum request size you have configured with the max.request.size configuration.。 注意：broker 对可接收的消息最大值也有自己的限制 (通过 message.max.bytes 参数设置)，所以两边的配置最好可以匹配，避免生产者发送的消息被 broker 拒绝。 metric.reporters 自定义指标报告器，默认：无。 用作指标报告器的类的列表，需要实现接口：org.apache.kafka.common.metrics.MetricsReporter，该接口允许插入将在创建新度量时得到通知的类。JmxReporter 始终包含在注册 JMX 统计信息中。 interceptor.classes 拦截器，默认：无。 用作拦截器的类的列表，需要实现接口：org.apache.kafka.clients.producer.ProducerInterceptor 。允许将生产者接收到的记录发布到 Kafka 集群之前拦截它们 (可能还会发生突变)。 partitioner.class 分区策略，默认：org.apache.kafka.clients.producer.internals.DefaultPartitioner。 如果自定义分区策略，需要实现接口： org.apache.kafka.clients.producer.Partitioner。 receive.buffer.bytes 默认：32768，即 32 K。 指定了 TCP socket 接收数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 send.buffer.bytes 默认：131072，即 128 K。 指定了 TCP socket 发送数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 transaction.timeout.ms 事务协调器等待生产者更新事务状态的最大毫秒数，默认：60000，即 1 分钟。 如果超过该时间，事务协调器会终止进行中的事务。 如果设置的时间大于 broker 端的 max.transaction.timeout.ms，会抛出 InvalidTransactionTimeout 异常。 transactional.id 用于事务传递的 TransactionalId，默认：空，即不使用事务。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的 TransactionalId 的事务已经完成。如果没有提供 TransactionalId，则生产者被限制为幂等传递。 注意：如果配置了 TransactionalId，则必须启用 enable.idempotence。 Kafka Consumer 核心配置参数bootstrap.servers broke 服务器地址，多个服务器，用逗号隔开。 enable.auto.commit 是否开启自动提交 offset，默认：true。 如果为 true，consumer 的偏移量将在后台定期提交，自动提交频率通过 auto.commit.interval.ms 设置。 auto.commit.interval.ms 自动提交频率，默认：5000。 auto.offset.reset 初始偏移量，默认：latest。 如果 Kafka 中没有初始偏移量，或者服务器上不再存在当前偏移量 (例如，该数据已被删除)，该怎么处理： earliest：自动重置偏移到最早的偏移。 latest：自动将偏移量重置为最新偏移量。 none：如果没有为使用者的组找到以前的偏移量，则向使用者抛出 exception。 anything else：向使用者抛出异常。 client.id 客户端 id，默认：空。 便于跟踪日志。 check.crcs 是否开启数据校验，默认：true。 自动检查消耗的记录的 CRC32。这确保不会发生对消息的在线或磁盘损坏。此检查增加了一些开销，因此在寻求极端性能的情况下可能禁用此检查。 group.id 消费者所属的群组，默认：空。 唯一标识用户群组，每个 partition 只会分配给同一个 group 里面的一个 consumer 来消费。 max.poll.records 拉取的最大记录，默认：500。 单次轮询调用 poll () 方法能返回的记录的最大数量。 max.poll.interval.ms 拉取记录间隔，默认：300000，即 5 分钟。 使用消费者组管理时轮询调用之间的最大延迟。这为使用者在获取更多记录之前空闲的时间设置了上限。如果在此超时过期之前没有调用 poll ()，则认为使用者失败，组将重新平衡，以便将分区重新分配给另一个成员。 request.timeout.ms 请求超时时间，默认：30000 。 配置控制客户机等待请求响应的最长时间。如果在超时之前没有收到响应，客户端将在需要时重新发送请求，或者在重试耗尽时失败请求。 session.timeout.ms consumer session 超时时间，默认：10000。 用于检测 worker 程序失败的超时。worker 定期发送心跳，以向代理表明其活性。如果在此会话超时过期之前代理没有接收到心跳，则代理将从组中删除。 注意：该值必须在 broker 端配置的 group.min.session.timeout 和 group.max.session.timeout.ms 范围之间。 heartbeat.interval.ms 心跳时间，默认：3000。 心跳是在 consumer 与 coordinator 之间进行的。心跳是确定 consumer 存活，加入或者退出 group 的有效手段。 这个值必须设置的小于 session.timeout.ms 的1/3，因为： 当 consumer 由于某种原因不能发 Heartbeat 到 coordinator 时，并且时间超过 session.timeout.ms 时，就会认为该 consumer 已退出，它所订阅的 partition 会分配到同一 group 内的其它的 consumer 上。 connections.max.idle.ms 连接空闲超时时间，默认：540000，即 9 分钟。 在此配置指定的毫秒数之后关闭空闲连接。 key.deserializer key 反序列化器，默认：无。 需要实现接口：org.apache.kafka.common.serialize.Deserializer。Kafka 提供以下几个默认的 key 反序列化器： String：org.apache.kafka.common.serialization.StringDeserializer。 value.deserializer value 反序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Deserializer。Kafka 提供以下几个默认的 value 反序列化器： String：org.apache.kafka.common.serialization.StringDeserializer。 partition.assignment.strategy consumer订阅分区策略，默认：org.apache.kafka.clients.consumer.RangeAssignor。 当使用组管理时，客户端将使用分区分配策略的类名在使用者实例之间分配分区所有权。 max.partition.fetch.bytes 一次 fetch 请求，从一个 partition 中取得的 records 的最大值，默认：1048576，即 1 M。 如果在从 topic 中第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。 broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 message.max.bytes 配置和 topic 端的 max.message.bytes 配置。 fetch.max.bytes 一次 fetch 请求，从一个 broker 中取得的 records 的最大值，默认：52428800，即 50 M。 如果在从 topic中 第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。 broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 message.max.bytes 配置 和 topic 端的 max.message.bytes 配置。 fetch.min.bytes 一次 fetch 请求，从一个 broker 中取得的 records 的最小值，默认：1。 如果 broker 中数据量不够的话会 wait，直到积累的数据大小满足这个条件。默认值设置为1的目的是：使得 consumer 的请求能够尽快的返回。将此设置为大于 1 的值将导致服务器等待更大数量的数据累积，这可以稍微提高服务器吞吐量，但代价是增加一些延迟。 fetch.max.wait.ms 拉取阻塞时间，默认：500。 如果没有足够的数据立即满足 fetch.min.bytes 提供的要求，服务器在响应 fetch 请求之前将阻塞的最长时间。 exclude.internal.topics 公开内部 topic，默认：true。 是否应该将来自内部主题 (如偏移量) 的记录公开给使用者，consumer 共享 offset。如果设置为 true，从内部主题接收记录的唯一方法是订阅它。 isolation.level 隔离级别，默认：read_uncommitted。 控制如何以事务方式读取写入的消息。如果设置为 read_committed，poll () 方法将只返回已提交的事务消息。如果设置为 read_uncommitted，poll () 方法将返回所有消息，甚至是已经中止的事务消息。在任何一种模式下，非事务性消息都将无条件返回。 Kafka Broker 核心配置参数zookeeper.connect zookeeper 地址，多个地址用逗号隔开。 broker.id 服务器的 broke id，默认：-1。 每一个 broker 在集群中的唯一表示，要求是正数。 如果未设置，将生成唯一的代理 id。为了避免 zookeeper 生成的 broke id 和用户配置的 broke id 之间的冲突，生成的代理 id 从 reserve.broker.max.id 开始 id + 1。 advertised.host.name 默认：null。 不赞成使用： 在 server.properties 里还有另一个参数是解决这个问题的， advertised.host.name 参数用来配置返回的 host.name值，把这个参数配置为外网 IP 地址即可。 这个参数默认没有启用，默认是返回的 java.net.InetAddress.getCanonicalHostName() 的值，在我的 mac 上这个值并不等于 hostname 的值而是返回 IP，但在 linux 上这个值就是 hostname 的值。 advertised.listeners hostname 和端口注册到 zookeeper 给生产者和消费者使用的，如果没有设置，将会使用 listeners 的配置，如果 listeners 也没有配置，将使用 java.net.InetAddress.getCanonicalHostName() 来获取这个 hostname 和 port，对于 ipv4，基本就是 localhost 了。 auto.create.topics.enable 是否允许自动创建 topic，默认：true。 如果为 true，第一次发动消息时，允许自动创建 topic。否则，只能通过命令创建 topic。 auto.leader.rebalance.enable 自动 rebalance，默认：true。 支持自动 leader balance。如果需要，后台线程定期检查并触发 leader balance。 background.threads 默认：10。 一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改。 compression.type 压缩类型，默认：producer。 对发送的消息采取的压缩编码方式 (‘gzip’，’snappy’，’lz4’)。 ‘uncompressed’：不压缩， ‘producer’：保持 producer 本身设置的压缩编码。 delete.topic.enable 是否允许删除 topic，默认：true。 如果关闭此配置，则通过管理工具删除主题将无效。 leader.imbalance.check.interval.seconds rebalance 检测频率，默认：300。 控制器触发分区 rebalance 检查的频率。 leader.imbalance.per.broker.percentage 触发 rebalance 得比率，默认：10，即 10%。 每个 broke 允许的 leader 不平衡比率。如果控制器超过每个 broke 的这个值，控制器将触发一个 leader balance。该值以百分比指定。 log.dir 保存日志数据的目录，默认：/tmp/kafka-logs。 log.dirs 保存日志数据的目录，默认：null。 可以指定多个存储路径，以逗号分隔。如果未设置，使用 log.dir 中设置的值。 log.flush.interval.messages 默认：9223372036854775807。 在将消息刷新到磁盘之前，日志分区上累积的消息数量。 log 文件 ”sync” 到磁盘之前累积的消息条数。因为磁盘 IO 操作是一个慢操作，但又是一个”数据可靠性”的必要手段。所以此参数的设置，需要在”数据可靠性”与”性能”之间做必要的权衡。 如果此值过大，将会导致每次 ”fsync” 的时间较长 (IO 阻塞)；如果此值过小，将会导致 ”fsync” 的次数较多，这也意味着整体的 client 请求有一定的延迟。 物理 server 故障，将会导致没有 fsync 的消息丢失。 log.flush.interval.ms 默认：null。 任何 topic 中的消息在刷新到磁盘之前保存在内存中的最长时间。如果没有设置，则使用 log.flush.scheduler.interval.ms 中的值。 log.flush.scheduler.interval.ms 日志刷新器检查是否需要将任何日志刷新到磁盘的频率，默认：9223372036854775807。 log.flush.offset.checkpoint.interval.ms 作为日志恢复点的上次刷新的持久记录的更新频率，默认：60000。 log.retention.bytes 删除前日志的最大大小，默认：-1。 topic 每个分区的最大文件大小，一个 topic 的大小限制 = 分区数 * log.retention.bytes。 log.retention.hours 日志文件最大保存时间 (小时)，默认：168，即 7 天。 删除日志文件之前保存它的小时数。 log.retention.minutes 日志文件最大保存时间 (分钟)，默认：null。 在删除日志文件之前保存它的分钟数，如果没有设置，则使用 log.retention.hours 中的值。 log.retention.ms 日志文件最大保存时间 (毫秒)，默认：null。 在删除日志文件之前保存它的毫秒数，如果没有设置，则使用 log.retention.minutes 中的值。如果设置为 -1，则没有时间限制。 log.roll.hours 新 segment 产生时间，默认：168，即 7 天。 即使文件没有到达 log.segment.bytes 设置的大小，只要文件创建时间到达此属性，也会强制创建新 segment。 log.roll.ms 新 segment 产生时间，默认：null。 如果未设置，则使用 log.roll.hours 中的值。 log.segment.bytes 单个 segment 文件的最大值，默认：1073741824，即 1 G。 log.segment.delete.delay.ms segment 删除前等待时间， 默认：60000，即 1 分钟。 message.max.bytes 最大 batch size，默认：1048588，即 1.000011 M。 Kafka 允许的最大 record batch size (如果启用了压缩，则是压缩后的大小)。如果增加了这个值，并且是 0.10.2 版本之前的 consumer，那么也必须增加 consumer 的 fetch 大小，以便他们能够获取这么大的 record batch。在最新的消息格式版本中，记录总是按批进行分组，以提高效率。在以前的消息格式版本中，未压缩记录没有分组成批，这种限制只适用于单个 record。针对每个 topic，可以使用 max.message.bytes 设置。 min.insync.replicas insync中最小副本值，默认：1。 当生产者将 acks 设置为 “all” (或 “-1”)时，min.insync.replicas 指定了必须确认写操作成功的最小副本数量。如果不能满足这个最小值，则生产者将抛出一个异常 (要么是 NotEnoughReplicas，要么是 NotEnoughReplicasAfterAppend)。 当一起使用时，min.insync.replicas 和 ack 允许你执行更大的持久性保证。一个典型的场景是创建一个复制因子为 3 的主题，设置 min.insync.replicas 为 2，生产者设置 acks 为 “all”，这将确保如果大多数副本没有收到写操作，则生产者会抛出异常。 num.io.threads 服务器用于处理请求的线程数，其中可能包括磁盘 I/O，默认：8。 num.network.threads 服务器用于接收来自网络的请求和向网络发送响应的线程数，默认：3。 num.recovery.threads.per.data.dir 每个数据目录在启动时用于日志恢复和在关闭时用于刷新的线程数，默认：1。 num.replica.alter.log.dirs.threads 可以在日志目录 (可能包括磁盘 I/O) 之间移动副本的线程数，默认：null。 num.replica.fetchers 从 leader 复制数据到 follower 的线程数，默认：1。 offset.metadata.max.bytes 与 offset 提交关联的 metadata 的最大大小，默认：4096。 offsets.commit.timeout.ms offset 提交将被延迟，直到偏移量主题的所有副本收到提交或达到此超时。这类似于生产者请求超时。默认：5000。 offsets.topic.num.partitions 偏移量提交主题的分区数量 (部署后不应再更改)，默认：50。 offsets.topic.replication.factor 副本大小，默认：3。 offsets.topic.segment.bytes 默认104857600，即 100 M。 segment 映射文件 (index) 文件大小，应该保持相对较小以便加快日志压缩和缓存负载。 queued.max.requests 阻塞网络线程之前，允许排队的请求数，默认：500。 replica.fetch.min.bytes 每个 fetch 响应所需的最小字节，默认：1。 如果字节不够，则等待 replicaMaxWaitTimeMs。 replica.lag.time.max.ms 默认：30000。 如果 follower 没有发送任何获取请求，或者至少在这段时间没有消耗到 leader 日志的结束偏移量，那么 leader 将从 isr 中删除 follower。 transaction.max.timeout.ms 默认：900000，即15分钟。 事务执行最长时间，超时则抛出异常。 unclean.leader.election.enable 默认：false。 指示是否在最后不得已的情况下启用 ISR 集中以外的副本作为 leader，即使这样做可能导致数据丢失。 zookeeper.connection.timeout.ms 默认：null。 客户端等待与 zookeeper 建立连接的最长时间。如果未设置，则使用 zookeeper.session.timeout.ms 中的值。 zookeeper.max.in.flight.requests 默认：10。 阻塞之前 consumer 将发送给 zookeeper 的未确认请求的最大数量。 group.max.session.timeout.ms 默认：1800000，即 30 分钟。 注册使用者允许的最大会话超时。超时时间越长，消费者在心跳之间处理消息的时间就越多，而检测故障的时间就越长。 group.min.session.timeout.ms 默认：6000。 注册使用者允许的最小会话超时。更短的超时导致更快的故障检测，但代价是更频繁的用户心跳，这可能会耗尽 broker 资源。 num.partitions 每个主题的默认日志分区数量，默认：1。 本文参考https://www.cnblogs.com/wangzhuxing/p/10111831.html#_label0_11 https://atbug.com/kafka-producer-config/ https://blog.csdn.net/jiecxy/article/details/53389892 本文只整理了部分有关 Kafka 的配置，仅作参考，更多的关于 broker，topic，producer 和 consumer 的配置，请参考 Kafka 官网。 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaConsumer 消费消息的基本流程","slug":"kafka-consumer","date":"2020-10-29T07:10:10.000Z","updated":"2021-01-05T07:33:06.237Z","comments":true,"path":"2020/10/29/kafka-consumer/","link":"","permalink":"http://example.com/2020/10/29/kafka-consumer/","excerpt":"","text":"如何消费数据在上一篇文章中，介绍了 KafkaProducer 如何发送数据到 Kafka，既然有数据发送，那么肯定就有数据消费，KafkaConsumer 也是 Kafka 整个体系中不可缺少的一环。 下面是一段创建 KafkaConsumer 的代码： 1234567891011121314151617181920212223242526272829public class KafkaConsumerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // 必须设置的属性 props.put(&quot;bootstrap.servers&quot;, &quot;192.168.239.131:9092&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;group.id&quot;, &quot;group1&quot;); // 可选设置的属性 props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;auto.offset.reset&quot;, &quot;earliest &quot;); props.put(&quot;client.id&quot;, &quot;test_client_id&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // 订阅主题 consumer.subscribe(Collections.singletonList(&quot;test&quot;)); while (true) &#123; // 拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); records.forEach(record -&gt; System.out.printf(&quot;topic = %s, partition = %d, offset = %d, key = %s, value = %s%n&quot;, record.topic(), record.partition(), record.offset(), record.key(), record.value())); &#125; &#125;&#125; 必须设置的属性创建 KafkaConsumer 时，必须设置的属性有 4 个： bootstrap.servers：连接 Kafka 集群的地址，多个地址以逗号分隔。 key.deserializer：消息中 key 反序列化类，需要和 KafkaProducer 中 key 序列化类相对应。 value.deserializer：消息中 value 的反序列化类，需要和 KafkaProducer 中 value 序列化类相对应。 group.id：消费者所属消费者组的唯一标识。 这里着重说一下 group.id 这个属性，KafkaConsumer 和 KafkaProducer 不一样，KafkaConsumer 中有一个 consumer group (消费者组)，由它来决定同一个 consumer group 中的消费者具体拉取哪个 partition 的数据，所以这里必须指定 group.id 属性。 订阅和取消主题 使用 subscribe () 方式订阅主题 1234// 订阅指定列表的topicpublic void subscribe(Collection&lt;String&gt; topics) &#123; subscribe(topics, new NoOpConsumerRebalanceListener());&#125; 1234// 订阅指定列表的topic，同时指定一个监听器public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; ...&#125; 1234// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行public void subscribe(Pattern pattern) &#123; subscribe(pattern, new NoOpConsumerRebalanceListener());&#125; 1234// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行，同时指定一个监听器public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; ...&#125; 使用 assign () 方式订阅主题和分区 1234// 手动将分区列表分配给consumerpublic void assign(Collection&lt;TopicPartition&gt; partitions) &#123; ...&#125; 使用示例 (仅作参考，assign() 方式的用法，应在使用时再做查询)： 123456List&lt;PartitionInfo&gt; partitionInfoList = kafkaConsumer.partitionsFor(&quot;test&quot;);if (partitionInfoList != null) &#123; for (PartitionInfo partitionInfo : partitionInfoList) &#123; kafkaConsumer.assign(Collections.singletonList(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()))); &#125;&#125; 取消主题的三种方式 123kafkaConsumer.unsubscribe();kafkaConsumer.subscribe(new ArrayList&lt;&gt;());kafkaConsumer.assign(new ArrayList&lt;TopicPartition&gt;()); 上面的三行代码作用相同，都是取消订阅，其中 unsubscribe () 方法即可以取消通过 subscribe () 方式实现的订阅，也可以取消通过 assign () 方式实现的订阅。 拉取数据KafkaConsumer 采用的是主动拉取 broker 数据进行消费的。 一般消息中间件存在推送 (push，server 推送数据给 consumer) 和拉取 (poll，consumer 主动去 server 拉取数据) 两种方式，这两种方式各有优劣。 如果是选择推送的方式，最大的阻碍就是 server 不清楚 consumer 的消费速度，如果 consumer 中执行的操作是比较耗时的，那么 consumer 可能会不堪重负，甚至会导致系统挂掉。 而采用拉取的方式则可以解决这种情况，consumer 根据自己的状态来拉取数据，可以对服务器的数据进行延迟处理。但是这种方式也有一个劣势就是 server 没有数据的时候可能会一直轮询，不过还好 KafkaConsumer 的 poll () 方法有参数允许 consumer 请求在”长轮询”中阻塞，以等待数据到达 (并且可选地等待直到给定数量的字节可用以确保传输大小)。 如何更好的消费数据文章开头处的代码展示了我们是如何消费数据的，但是代码未免过于简单，我们测试的时候这样写没有问题，但是实际开发过程中我们并不会这样写，我们会选择更加高效的方式，这里提供两种方式供大家参考。 一个 consumer group，多个 consumer，数量小于等于 partition 的数量 一个 consumer，多线程处理事件 第一种方式每个 consumer 都要维护一个独立的 TCP 连接，如果 partition 数和创建 consumer 线程的数量过多，会造成不小的系统开销。但是如果处理消息足够快速，消费性能也会提升，如果慢的话就会导致消费性能降低。 第二种方式是采用一个 consumer，多个消息处理线程来处理消息，其实在生产中，瓶颈一般是集中在消息处理上 (因为可能会插入数据到数据库，或者请求第三方 API)，所以我们采用多个线程来处理这些消息。 当然可以结合第一和第二两种方式，采用多 consumer + 多个消息处理线程来消费 Kafka 中的数据，核心代码如下： 1234567891011121314151617181920212223for (int i = 0; i &lt; consumerNum; i++) &#123; // 根据属性创建Consumer，并添加到consumer列表中 final Consumer&lt;String, byte[]&gt; consumer = consumerFactory.getConsumer(getServers(), groupId); consumerList.add(consumer); // 订阅主题 consumer.subscribe(Arrays.asList(this.getTopic())); // consumer.poll()拉取数据 BufferedConsumerRecords bufferedConsumerRecords = new BufferedConsumerRecords(consumer); getExecutor().scheduleWithFixedDelay(() -&gt; &#123; long startTime = System.currentTimeMillis(); // 进行消息处理 consumeEvents(bufferedConsumerRecords); long sleepTime = intervalMillis - (System.currentTimeMillis() - startTime); if (sleepTime &gt; 0) &#123; Thread.sleep(sleepTime); &#125; &#125;, 0, 1000, TimeUnit.MILLISECONDS);&#125; 不过这种方式不能顺序处理数据，如果你的业务是顺序处理，那么第一种方式可能更适合你。所以实际生产中请根据业务选择最适合自己的方式。 消费数据时应该考虑的问题什么是 offset？在 Kafka 中无论是 KafkarPoducer 往 topic 中写数据，还是 KafkaConsumer 从 topic 中读数据，都避免不了和 offset 打交道，关于 offset 主要有以下几个概念： Last Committed Offset：consumer group 最新一次 commit 的 offset，表示这个 consumer group 已经把 Last Committed Offset 之前的数据都消费成功了。 Current Position：consumer group 当前消费数据的 offset，也就是说，Last Committed Offset 到 Current Position 之间的数据已经拉取成功，可能正在处理，但是还未 commit。 High Watermark：HW，已经成功备份到其他 replica 中的最新一条数据的 offset，也就是说，High Watermark 与 Log End Offset 之间的数据已经写入到该 partition 的 leader 中，但是还未完全备份到其他的 replica 中，consumer 也无法消费这部分消息。 Log End Offset：LEO，记录底层日志 (log) 中的下一条消息的 offset。对 KafkaProducer 来说，就是即将插入下一条消息的 offset。 每个 Kafka 副本对象都有两个重要的属性：HW 和 LEO。注意是所有的副本，而不只是 leader 副本。关于这两者更详细解释，参考：[Kafka 的 High Watermark 与 leader epoch 的讨论 对于消费者而言，我们更多时候关注的是消费完成之后如何和服务器进行消费确认，告诉服务器这部分数据我已经消费过了。 这里就涉及到了 2 个 offset，一个是 Current Position，一个是处理完毕向服务器确认的 Last Committed Offset。显然，异步模式下 Last Committed Offset 是落后于 Current Position 的。如果 consumer 挂掉了，那么下一次消费数据又只会从 Last Committed Offset 的位置拉取数据，就会导致数据被重复消费。 如何选择 offset 的提交策略？Kafka 提供了三种提交 offset 的方式。 1. 自动提交 1234// 自动提交，默认trueprops.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// 设置自动每1s提交一次props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 2.手动同步提交 1kafkaConsumer.commitSync(); 3.手动异步提交 1kafkaConsumer.commitAsync(); 上面说了，既然异步提交 offset 可能会重复消费，那么我使用同步提交是否就可以解决数据重复消费的问题呢？我只能说 too young, too sample。且看如下代码： 1234567while (true) &#123; ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(100)); records.forEach(record -&gt; &#123; insertIntoDB(record); kafkaConsumer.commitSync(); &#125;);&#125; 很明显不行，因为 insertIntoDB () 和 kafkaConsumer.commitSync () 两个方法做不到原子操作，如果 insertIntoDB () 成功了，但是提交 offset 的时候 KafkaConsumer 挂掉了，然后服务器重启，仍然会导致重复消费问题。 是否需要做到不重复消费？只要保证处理消息和提交 offset 的操作是原子操作，就可以做到不重复消费。我们可以自己管理 committed offset，而不让 Kafka 来进行管理。 比如如下使用方式： 1.如果消费的数据刚好需要存储在数据库，那么可以把 offset 也存在数据库，就可以在一个事物中提交这两个结果，保证原子操作。 2.借助搜索引擎，把 offset 和数据一起放到索引里面，比如 Elasticsearch。 每条记录都有自己的 offset，所以如果要管理自己的 offset 还得要做下面事情： 1.设置 enable.auto.commit 为 false； 2.使用每个 ConsumerRecord 提供的 offset 来保存消费的位置； 3.在重新启动时使用 seek (TopicPartition partition, long offset) 恢复上次消费的位置。 通过上面的方式就可以在消费端实现 ”Exactly Once” 的语义，即保证只消费一次。但是是否真的需要保证不重复消费呢？这个得看具体业务，如果重复消费数据对整体有什么影响，然后再来决定是否需要做到不重复消费。 再均衡 (reblance) 时怎么办？再均衡是指分区的所属权从一个消费者转移到另一个消费者的行为，再均衡期间，消费者组内的消费者无法读取消息。为了更精确的控制消息的消费，我们可以在订阅主题的时候，通过指定监听器的方式来设定发生再均衡动作前后的一些准备或者收尾的动作。 1234567891011kafkaConsumer.subscribe(Collections.singletonList(&quot;test&quot;), new ConsumerRebalanceListener() &#123; @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; // 再均衡之前和消费者停止读取消息之后被调用 &#125; @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; // 重新分配分区之后和消费者开始消费之前被调用 &#125;&#125;); 具体如何操作，得根据具体的业务逻辑来实现，如果消息比较重要，你可以在再均衡的时候处理 offset，如果不够重要，你可以什么都不做。 无法消费的数据怎么办？可能由于你的业务逻辑有些数据没法消费，这个时候怎么办？同样的还是的看你认为这个数据有多重要或者多不重要，如果重要可以记录日志，把它存入文件或者数据库，以便于稍候进行重试或者定向分析。如果不重要就当做什么事情都没有发生好了。 实际开发中我的处理方式我开发的项目中，用到 Kafka 的其中一个地方是消息通知 (谁给你发了消息，点赞，评论等)，大概的流程就是用户在 client 端做了某些操作，就会发送数据到 Kafka，然后把这些数据进行一定的处理之后插入到 HBase 中。 其中采用了 N consumer thread + N Event Handler 的方式来消费数据，并采用自动提交 offset。对于无法消费的数据往往只是简单处理下，打印下日志以及消息体 (无法消费的情况非常非常少)。 得益于 HBase 的多 version 控制，即使是重复消费了数据也无关紧要。这样做没有去避免重复消费的问题主要是基于以下几点考虑： 1.重复消费的概率较低，服务器整体性能稳定。 2.即便是重复消费了数据，入库了 HBase，获取数据也是只有一条，不影响结果的正确性。 3.有更高的吞吐量。 4.编程简单，不用单独去处理以及保存 offset。 本文参考http://generalthink.github.io/2019/05/06/kafka-consumer-use/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaProducer 部分源码解析","slug":"kafka-producer","date":"2020-10-26T06:15:52.000Z","updated":"2021-01-05T07:33:19.544Z","comments":true,"path":"2020/10/26/kafka-producer/","link":"","permalink":"http://example.com/2020/10/26/kafka-producer/","excerpt":"","text":"先来看一段创建 KafkaProducer 的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class KafkaProducerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // bootstrap.servers 必须设置 props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.239.131:9092&quot;); // key.serializer 必须设置 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // value.serializer 必须设置 props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // client.id props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;client-0&quot;); // retries props.put(ProducerConfig.RETRIES_CONFIG, 3); // acks props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;); // max.in.flight.requests.per.connection props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1); // linger.ms props.put(ProducerConfig.LINGER_MS_CONFIG, 100); // batch.size props.put(ProducerConfig.BATCH_SIZE_CONFIG, 10240); // buffer.memory props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10240); KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props); // 指定topic，key，value ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;test1&quot;, &quot;key1&quot;, &quot;value1&quot;); // 异步发送 kafkaProducer.send(record, (recordMetadata, exception) -&gt; &#123; if (exception != null) &#123; // 发送失败的处理逻辑 exception.printStackTrace(); &#125; else &#123; // 发送成功的处理逻辑 System.out.println(recordMetadata.topic()); &#125; &#125;); // 同步发送 // kafkaProducer.send(record).get(); // 关闭Producer kafkaProducer.close(); &#125;&#125; 主要流程图 简要说明： 1.new KafkaProducer () 后，创建一个后台线程 KafkaThread (实际运行线程是 Sender，KafkaThread 是对 Sender 的封装) 扫描 RecordAccumulator 中是否有消息； 2.调用 kafkaProducer.send () 发送消息，实际是将消息保存到 RecordAccumulator 中，实际上就是保存到一个 Map 中 (ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)，这条消息会被记录到同一个记录批次 (相同主题相同分区算同一个批次) 里面，这个批次的所有消息会被发送到相同的主题和分区上； 3.后台的独立线程扫描到 RecordAccumulator 中有消息后，会将消息发送到 Kafka 集群中 (不是一有消息就发送，而是要看消息是否 ready)； 4.如果发送成功 (消息成功写入 Kafka)，就返回一个 RecordMetaData 对象，它包换了主题和分区信息，以及记录在分区里的偏移量等信息； 5.如果写入失败，就会返回一个错误，生产者在收到错误之后会尝试重新发送消息 (如果允许的话，此时会将消息在保存到 RecordAccumulator 中)，达到重试次数之后如果还是失败就返回错误消息。 缓存器的创建123456789101112this.accumulator = new RecordAccumulator(logContext, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), this.compressionType, lingerMs(config), retryBackoffMs, deliveryTimeoutMs, metrics, PRODUCER_METRIC_GROUP_NAME, time, apiVersions, transactionManager, new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME)); 后台线程的创建12345678910111213141516171819202122this.sender = newSender(logContext, kafkaClient, this.metadata);String ioThreadName = NETWORK_THREAD_PREFIX + &quot; | &quot; + clientId;this.ioThread = new KafkaThread(ioThreadName, this.sender, true);this.ioThread.start();KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient( new Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), this.metrics, time, &quot;producer&quot;, channelBuilder, logContext), metadata, clientId, maxInflightRequests, producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG), producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG), producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG), producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG), requestTimeoutMs, ClientDnsLookup.forConfig(producerConfig.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)), time, true, apiVersions, throttleTimeSensor, logContext); 上述代码中，构造了一个 KafkaClient 负责和 broker 通信，同时构造一个 Sender 并启动一个异步线程，这个线程会被命名为：kafka-producer-network-thread | $&#123;clientId&#125;，如果你在创建 producer 的时候指定 client.id 的值为 myclient，那么线程名称就是 kafka-producer-network-thread | myclient。 发送消息 (缓存消息)发送消息有同步发送以及异步发送两种方式，我们一般不使用同步发送，毕竟太过于耗时，使用异步发送的时候可以指定回调函数，当消息发送完成的时候 (成功或者失败) 会通过回调通知生产者。 同步 send： 123public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record) &#123; return send(record, null);&#125; 异步 send： 12345public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; // intercept the record, which can be potentially modified; this method does not throw exceptions ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record); return doSend(interceptedRecord, callback);&#125; 可以看出，同步和异步实际上调用的是同一个方法，同步发送时，设置回调函数为 null。 消息发送之前，会先对 key 和 value 进行序列化： 12345678910111213141516byte[] serializedKey;try &#123; serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert key of class &quot; + record.key().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in key.serializer&quot;, cce);&#125;byte[] serializedValue;try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert value of class &quot; + record.value().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in value.serializer&quot;, cce);&#125; 计算分区： 1int partition = partition(record, serializedKey, serializedValue, cluster); 发送消息，实际上是将消息缓存起来，核心代码如下： 12RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs); RecordAccumulator 的核心数据结构是 ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;，会将相同 topic 相同 partition 的数据放到一个 Deque (双向队列) 中，这也是我们之前提到的同一个记录批次里面的消息会发送到同一个主题和分区的意思。append () 方法的核心源码如下： 123456789101112131415161718192021// 从batchs(ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)中，根据主题分区获取对应的队列，如果没有则new ArrayDeque&lt;&gt;返回Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);// 计算同一个记录批次占用空间大小，batchSize根据batch.size参数决定int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound( maxUsableMagic, compression, key, value, headers));// 为同一个topic，partition分配buffer，如果同一个记录批次的内存不足，那么会阻塞maxTimeToBlock(max.block.ms参数)这么长时间ByteBuffer buffer = free.allocate(size, maxTimeToBlock);synchronized (dq) &#123; // 创建MemoryRecordBuilder，通过buffer初始化appendStream(DataOutputStream)属性 MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic); ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds()); // 将key，value写入到MemoryRecordsBuilder中的appendStream(DataOutputStream)中 FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds())); // 将需要发送的消息放入到队列中 dq.addLast(batch);&#125; 发送消息到 Kafka上面已经将消息存储 RecordAccumulator 中去了，现在看看怎么发送消息。前面提到创建 KafkaProducer 的时候，会启动一个异步线程去从 RecordAccumulator 中取得消息然后发送到 Kafka，发送消息的核心代码在 Sender 中，它实现了 Runnable 接口并在后台一直运行处理发送请求并将消息发送到合适的节点，直到 KafkaProducer 被关闭。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata * requests to renew its view of the cluster and then sends produce requests to the appropriate nodes. */public class Sender implements Runnable &#123; /** * The main run loop for the sender thread */ public void run() &#123; // main loop, runs until close is called while (running) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // okay we stopped accepting requests but there may still be // requests in the transaction manager, accumulator or waiting for acknowledgment, // wait until these are completed. while (!forceClose &amp;&amp; ((this.accumulator.hasUndrained() || this.client.inFlightRequestCount() &gt; 0) || hasPendingTransactionalRequests())) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // Abort the transaction if any commit or abort didn&#x27;t go through the transaction manager&#x27;s queue while (!forceClose &amp;&amp; transactionManager != null &amp;&amp; transactionManager.hasOngoingTransaction()) &#123; if (!transactionManager.isCompleting()) &#123; log.info(&quot;Aborting incomplete transaction due to shutdown&quot;); transactionManager.beginAbort(); &#125; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; if (forceClose) &#123; // We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on // the futures. if (transactionManager != null) &#123; log.debug(&quot;Aborting incomplete transactional requests due to forced shutdown&quot;); transactionManager.close(); &#125; log.debug(&quot;Aborting incomplete batches due to forced shutdown&quot;); this.accumulator.abortIncompleteBatches(); &#125; &#125;&#125; KafkaProducer 的关闭方法有2个：close () 以及 close (Duration timeout)，close (long timeout, TimeUnit timUnit) 已被弃用，其中 timeout 参数的意思是等待生产者完成任何待处理请求的最长时间，第一种方式的 timeout 为 Long.MAX_VALUE 毫秒，如果采用第二种方式关闭，当 timeout = 0 的时候则表示强制关闭，直接关闭 Sender (设置 running = false)。 Send 中，runOnce () 方法，跳过对 transactionManager 的处理，查看发送消息的主要流程： 123456long currentTimeMs = time.milliseconds();// 将记录批次转移到每个节点的生产请求列表中long pollTimeout = sendProducerData(currentTimeMs);// 轮询进行消息发送client.poll(pollTimeout, currentTimeMs); 首先，查看 sendProducerData (currentTimeMs) 方法，它的核心逻辑在 sendProduceRequest (batches, now) 方法中： 123456789101112131415161718192021222324252627282930313233for (ProducerBatch batch : batches) &#123; TopicPartition tp = batch.topicPartition; // 将ProducerBatch中MemoryRecordsBuilder转换为MemoryRecords(发送的数据就在这里面) MemoryRecords records = batch.records(); // down convert if necessary to the minimum magic used. In general, there can be a delay between the time // that the producer starts building the batch and the time that we send the request, and we may have // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use // the new message format, but found that the broker didn&#x27;t support it, so we need to down-convert on the // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may // not all support the same message format version. For example, if a partition migrates from a broker // which is supporting the new magic version to one which doesn&#x27;t, then we will need to convert. if (!records.hasMatchingMagic(minUsedMagic)) records = batch.records().downConvert(minUsedMagic, 0, time).records(); produceRecordsByPartition.put(tp, records); recordsByPartition.put(tp, batch);&#125;ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout, produceRecordsByPartition, transactionalId);// 消息发送完成时的回调(消息发送失败后，在handleProduceResponse中处理)RequestCompletionHandler callback = new RequestCompletionHandler() &#123; public void onComplete(ClientResponse response) &#123; handleProduceResponse(response, recordsByPartition, time.milliseconds()); &#125;&#125;;// 根据参数构造ClientRequest，此时需要发送的消息在requestBuilder中ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, requestTimeoutMs, callback);// 将clientRequest转换成Send对象(Send.java，包含了需要发送数据的buffer)，给KafkaChannel设置该对象，记住这里还没有发送数据client.send(clientRequest, now); 在没有指定 KafkaClient 时，client.send (clientRequest, now) 方法，实际就是 NetworkClient.send (ClientRequest request, long now) 方法，所有的请求 (无论是 producer 发送消息的请求，还是获取 metadata 的请求) 都是通过该方法设置对应的 Send 对象： 1Send send = request.toSend(destination, header); 需要知道的是，上面只是设置了发送消息所需要准备的内容。 接下来，查看 client.poll (pollTimeout, currentTimeMs) 方法，进入到发送消息的主流程，发送消息的核心代码最终可以定位到 Selector 的 pollSelectionKeys (Set&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos) 方法中，代码如下： 12345678910111213141516/* if channel is ready write to any sockets that have space in their buffer and for which we have data */if (channel.ready() &amp;&amp; key.isWritable() &amp;&amp; !channel.maybeBeginClientReauthentication( () -&gt; channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos)) &#123; Send send; try &#123; // 底层实际调用的是java8 GatheringByteChannel的write方法 send = channel.write(); &#125; catch (Exception e) &#123; sendFailed = true; throw e; &#125; if (send != null) &#123; this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); &#125;&#125; 就这样，我们的消息就发送到了 broker 中了，发送流程分析完毕，注意，这个是完美发送的情况。但是总会有发送失败的时候 (消息过大或者没有可用的 leader 等)，那么发送失败后重发又是在哪里完成的呢？还记得上面的回调函数吗，没错，就是在回调函数这里设置的，先来看下回调函数源码： 12345678910111213141516171819202122232425262728293031323334/** * Handle a produce response */private void handleProduceResponse(ClientResponse response, Map&lt;TopicPartition, ProducerBatch&gt; batches, long now) &#123; RequestHeader requestHeader = response.requestHeader(); long receivedTimeMs = response.receivedTimeMs(); int correlationId = requestHeader.correlationId(); if (response.wasDisconnected()) &#123; // 如果是网络断开则构造Errors.NETWORK_EXCEPTION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION), correlationId, now, 0L); &#125; else if (response.versionMismatch() != null) &#123; // 如果是版本不匹配，则构造Errors.UNSUPPORTED_VERSION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.UNSUPPORTED_VERSION), correlationId, now, 0L); &#125; else &#123; // if we have a response, parse it(如果存在response就返回正常的response) if (response.hasResponse()) &#123; ProduceResponse produceResponse = (ProduceResponse) response.responseBody(); for (Map.Entry&lt;TopicPartition, ProduceResponse.PartitionResponse&gt; entry : produceResponse.responses().entrySet()) &#123; TopicPartition tp = entry.getKey(); ProduceResponse.PartitionResponse partResp = entry.getValue(); ProducerBatch batch = batches.get(tp); completeBatch(batch, partResp, correlationId, now, receivedTimeMs + produceResponse.throttleTimeMs()); &#125; this.sensors.recordLatency(response.destination(), response.requestLatencyMs()); &#125; else &#123; // this is the acks = 0 case, just complete all requests(如果acks=0，那么则构造Errors.NONE的响应，因为这种情况只需要发送不需要响应结果) for (ProducerBatch batch : batches.values()) &#123; completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NONE), correlationId, now, 0L); &#125; &#125; &#125;&#125; 在 completeBatch () 方法中我们主要关注失败的逻辑处理，核心源码如下： 12345678910111213141516171819202122232425262728293031/** * Complete or retry the given batch of records. * * @param batch The record batch * @param response The produce response * @param correlationId The correlation id for the request * @param now The current POSIX timestamp in milliseconds */private void completeBatch(ProducerBatch batch, ProduceResponse.PartitionResponse response, long correlationId, long now, long throttleUntilTimeMs) &#123; Errors error = response.error; if (error == Errors.MESSAGE_TOO_LARGE &amp;&amp; batch.recordCount &gt; 1 &amp;&amp; !batch.isDone() &amp;&amp; (batch.magic() &gt;= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) &#123; // If the batch is too large, we split the batch and send the split batches again. We do not decrement // the retry attempts in this case.(如果发送的消息太大，需要重新进行分割发送) this.accumulator.splitAndReenqueue(batch); maybeRemoveAndDeallocateBatch(batch); this.sensors.recordBatchSplit(); &#125; else if (error != Errors.NONE) &#123; // 发生了错误，如果此时可以retry(retry次数未达到限制以及产生的异常是RetriableException) if (canRetry(batch, response, now)) &#123; if (transactionManager == null) &#123; // 把需要重试的消息放入队列中，等待重试，实际就是调用deque.addFirst(batch) reenqueueBatch(batch, now); &#125; ... &#125; ... &#125;&#125; 以上，就是 KafkaProducer 发送消息的流程。 补充：分区算法在发送消息前，调用的计算分区方法如下： 123456789101112/** * computes partition for given record. * if the record has partition returns the value otherwise * calls configured partitioner class to compute the partition. */private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition : partitioner.partition( record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; 如果在创建 ProducerRecord 的时候，指定了 partition，则使用指定的，否则调用配置的 partitioner 类来计算分区。 如果没有配置自定义的分区器，Kafka 默认使用 org.apache.kafka.clients.producer.internals.DefaultPartitioner，源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * The default partitioning strategy: * &lt;ul&gt; * &lt;li&gt;If a partition is specified in the record, use it * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key * &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion */public class DefaultPartitioner implements Partitioner &#123; private final ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = new ConcurrentHashMap&lt;&gt;(); public void configure(Map&lt;String, ?&gt; configs) &#123;&#125; /** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; // 如果key为null，则使用Round Robin算法 int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition(根据key进行散列，使用murmur2算法) return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125; private int nextValue(String topic) &#123; AtomicInteger counter = topicCounterMap.get(topic); if (null == counter) &#123; counter = new AtomicInteger(ThreadLocalRandom.current().nextInt()); AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter); if (currentCounter != null) &#123; counter = currentCounter; &#125; &#125; return counter.getAndIncrement(); &#125; public void close() &#123;&#125;&#125; DefaultPartitioner 中对于分区的算法有两种情况： 1.如果键值为 null，那么记录键随机地发送到主题内各个可用的分区上。分区器使用轮询 (Round Robin) 算法键消息均衡地分布到各个分区上。 2.如果键不为 null，那么 Kafka 会对键进行散列 (使用 Kafka 自己的散列算法，即使升级 java 版本，散列值也不会发生变化) ，然后根据散列值把消息映射到特定的分区上。应该注意的是，同一个键总是被映射到同一个分区上 (如果分区数量发生了变化则不能保证)，映射的时候会使用主题所有的分区，而不仅仅是可用分区，所以如果写入数据分区是不可用的，那么就会发生错误，当然这种情况很少发生。 当然，如果你想要实现自定义分区，那么只需要实现 Partitioner 接口即可： 123456789101112131415161718192021222324/** * 将key的hash值，对分区总数取余，以确定消息发送到哪个分区 */public class KeyPartitioner implements Partitioner &#123; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; Integer numPartitions = cluster.partitionCountForTopic(topic); if (keyBytes == null) &#123; throw new InvalidRecordException(&quot;key can not be null&quot;); &#125; return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 然后，使用 partitioner.class 参数，指定你自定义的分区器的路径： 1props.put(&quot;partitioner.class&quot;, &quot;cn.xisun.partitioner.KeyPartitioner&quot;); 本文参考https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"什么是 Kafka","slug":"kafka-introduce","date":"2020-10-23T07:58:39.000Z","updated":"2021-01-05T07:33:11.926Z","comments":true,"path":"2020/10/23/kafka-introduce/","link":"","permalink":"http://example.com/2020/10/23/kafka-introduce/","excerpt":"","text":"分布式分布式系统由多个运行的计算机系统组成，所有这些计算机在一个集群中一起工作，对终端用户来讲只是一个单一节点。 Kafka 也是分布式的，因为它在不同的节点 (又被称为 broker) 上存储，接受以及发送消息，这样做的好处是具有很高的可扩展性和容错性。 水平可扩展性在这之前，先看看什么是垂直可扩展，比如你有一个传统的数据库服务器，它开始过度负载，解决这个问题的办法就是给服务器加配置 (cpu，内存，SSD)，这就叫做垂直扩展。但是这种方式存在两个巨大的劣势： 1.硬件存在限制，不可能无限的添加机器配置。 2.它需要停机时间，通常这是很多公司无法容忍的。 水平可扩展就是通过添加更多的机器来解决同样的问题，添加新机器不需要停机，而且集群中也不会对机器的数量有任何的限制。但问题在于并非所有系统都支持水平可伸缩性，因为它们不是设计用于集群中 (在集群中工作会更加复杂)。 容错性非分布式系统中容易最致命的问题就是单点失败，如果你唯一的服务器挂掉了，那么我相信你会很崩溃。 而分布式系统的设计方式就是可以以配置的方式来容许失败。比如在 5 个节点的 Kafka 集群中，即使其中两个节点挂掉了，你仍然可以继续工作。 需要注意的是，容错与性能直接相关，你的系统容错程度越高，性能就越差。 提交日志 (commit log)提交日志 (也被称为预写日志或者事物日志) 是仅支持附加的持久有序数据结构，你无法修改或者删除记录，它从左往右读并且保证日志的顺序。 是不是觉得 Kafka 的数据结构如此简单? 是的，从很多方面来讲，这个数据结构就是 Kafka 的核心。这个数据结构的记录是有序的，而有序的数据可以确保我们的处理流程。这两个在分布式系统中都是极其重要的问题。 Kafka 实际上将所有消息存储到磁盘并在数据结构中对它们进行排序，以便利用顺序磁盘读取。 1.读取和写入都是常量时间 O(1) (当确定了 record id)，与磁盘上其他结构的 O(log N)操作相比是一个巨大的优势，因为每个磁盘搜索都很耗时。 2.读取和写入不会相互影响，写不会锁住读，反之亦然。 这两点有着巨大的优势， 因为数据大小与性能完全分离。无论你的服务器上有 100 KB 还是 100 TB 的数据，Kafka 都具有相同的性能。 如何工作生产者消费者模式：生产者 (producer) 发送消息 (record) 到 Kafka 服务器 (broker)，这些消息存储在主题 (topic) 中，然后消费者 (consumer) 订阅该主题，接受新消息后并进行处理。 随着消息的越来越多，topic 也会越来越大，为了获得更好的性能和可伸缩性，可以在 topic 下建立多个更小的分区 (partition)，在发送消息时，可以根据实际情况，对消息进行分类，同一类的消息发送到同一个 partition (比如存储不同用户发送的消息，可以根据用户名的首字母进行分区匹配)。Kafka 保证 partition 内的所有消息都按照它们的顺序排序，区分特定消息的方式是通过其偏移量 (offset)，你可以将其视为普通数组索引，即为分区中的每个新消息递增的序列号。 Kafka 遵守着愚蠢的 broker 和聪明的 consumer 的准则。这意味着 Kafka 不会跟踪消费者读取了哪些记录并删除它们，而是会将它们存储一定的时间 (比如 1 天，以 log.retention 开头的来决定日志保留时间)，直到达到某个阈值。消费者自己轮询 Kafka 的新消息并且告诉它自己想要读取哪些记录，这允许它们按照自己的意愿递增/递减它们所处的偏移量，从而能够重放和重新处理事件。 需要注意的是消费者是属于消费者组的 (在创建 consumer 时，必须指定其所属的消费者组的 group.id)，消费者组有一个或多个消费者。为了避免两个进程读取同样的消息两次，每个 partition 只能被一个消费者组中的一个消费者访问。 持久化到硬盘正如之前提到的，Kafka 实际上是将所有记录存储到硬盘而不在 RAM 中保存任何内容，这背后有很多优化使得这个方案可行。 1.Kafka 有一个将消息分组的协议，这允许网络请求将消息组合在一起并减少网络开销，服务器反过来一次性保留大量消息，消费者一次获取大量线性块。 2.磁盘上线性读写非常快，现代磁盘非常慢的原因是由于大量磁盘寻址，但是在大量的线性操作中不是问题。 3.操作系统对线性操作进行了大量优化，通过预读 (预取大块多次) 和后写 (将小型逻辑写入组成大型物理写入) 技术。 4.操作系统将磁盘文件缓存在空闲 RAM 中。这称为 page cache，而 Kafka 的读写都大量使用了 page cache： ​ ① 写消息的时候消息先从 java 到 page cache，然后异步线程刷盘，消息从 page cache 刷入磁盘； ​ ② 读消息的时候先从 page cache 找，有就直接转入 socket，没有就先从磁盘 load 到 page cache，然后直接从 socket 发出去。 5.由于 Kafka 在整个流程 (producer → broker → consumer) 中以未经修改的标准化二进制格式存储消息，因此它可以使用零拷贝优化。那时操作系统将数据从 page cache 直接复制到 socket，有效地完全绕过了 Kafka broker。 所有这些优化都使 Kafka 能够以接近网络的速度传递消息。 数据分发和复制下面来谈谈 Kafka 如何实现容错以及它如何在节点之间分配数据。 为了使得一个 broker 挂掉的时候，数据还能得以保留，分区 (partition) 数据在多个 broker 中复制。 在任何时候，一个 broker 拥有一个 partition，应用程序读取/写入都要通过这个节点，这个节点叫做 partition leader。它将收到的数据复制到 N 个其他 broker，这些接收数据的 broker 叫做 follower，follower 也存储数据，一旦 leader 节点死掉的时候，它们就准备竞争上岗成为 leader。 这可以保证你成功发布的消息不会丢失，通过选择更改副本因子，你可以根据数据的重要性来交换性能以获得更强的持久性保证。 这样如果 leader 挂掉了，那么其中一个 follower 就会接替它称为 leader。包括 leader 在内的总副本数就是副本因子 (创建 topic 时，使用 --replication-factor 参数指定)，上图有 1 个 leader，2 个 follower，所以副本因子就是 3。 但是你可能会问：producer 或者 consumer 怎么知道 partition leader 是谁？ 对生产者/消费者对分区的写/读请求，它们需要知道分区的 leader 是哪一个，对吧？这个信息肯定是可以获取到的，Kafka 使用 ZooKeeper 来存储这些元数据。 什么是 ZooKeeperZooKeeper 是一个分布式键值存储。它针对读取进行了高度优化，但写入速度较慢。它最常用于存储元数据和处理集群的机制 (心跳，分发更新/配置等)。 它允许服务的客户 (Kafka broker) 订阅并在发生变更后发送给他们，这就是 Kafka 如何知道何时切换分区领导者。ZooKeeper 本身维护了一个集群，所以它就有很高的容错性，当然它也应该具有，毕竟 Kafka 很大程度上是依赖于它的。 ZooKeeper 用于存储所有的元数据信息，包括但不限于如下几项： 消费者组每个分区的偏移量 (现在客户端在单独的 Kafka topic 上存储偏移量) ACL —— 权限控制 生产者/消费者的流量控制——每秒生产/消费的数据大小。参考：Kafka - 流量控制 Quota 功能 partition leader 以及它们的健康信息 那么 producer/consumer 是如何知道谁是 partition leader 的呢？ 生产者和消费者以前常常直接连接 ZooKeeper 来获取这些信息，但是 Kafka 从 0.8 和 0.9 版本开始移除了这种强耦合关系。客户端直接从 Kafka broker 获取这些元数据，而让 Kafka broker 从 ZooKeeper 那里获取这些元数据。 更多 ZooKeeper 的讲解参考：漫画：什么是 ZooKeeper？ 流式处理 (Streaming)在 Kafka 中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理并生成数据流以输出到其他主题 (或者外部服务，数据库，容器等等)。 什么是数据流呢？首先，数据流是无边界数据集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的，是因为随着时间推移，新的记录会不断加入进来。比如信用卡交易，股票交易等事件都可以用来表示数据流。 我们可以使用 producer/consumer 的 API 直接进行简单处理，但是对于更加复杂的转换，比如将流连接到一起，Kafka 提供了集成 Stream API 库。 这个 API 是在你自己的代码中使用的，它并不是运行在 broker 上，它的工作原理和 consumer API 类似，可帮助你在多个应用程序 (类似于消费者组) 上扩展流处理工作。 无状态处理流的无状态处理是确定性处理，其不依赖于任何外部条件，对于任何给定的数据，将始终生成与其他任何内容无关的相同输出。举个例子，我们要做一个简单的数据转换—“zhangsan” → “Hello, zhangsan” 流-表二义性重要的是要认识到流和表实质上是一样的，流可以被解释称为表，表也可以被解释称为流。 流作为表流可以解释为数据的一系列更新，聚合后的结果就是表的最终结果，这项技术被称为事件溯源 (Event Sourcing)。 如果你了解数据库备份同步，你就会知道它们的技术实现被称为流式复制—将对表的每个更改都发送报副本服务器。比如 redis 中的 AOF 以及 Mysql 中的 binlog。 Kafka 流可以用相同的方式解释 - 当累积形成最终状态时的事件。此类流聚合保存在本地 RocksDB 中 (默认情况下)，被称为 KTable。 表作为流可以将表视为流中每个键的最新值的快照。与流记录可以生成表一样，表更新可以生成更改日志流。 有状态处理我们在 java 中常用的一些操作比如 map() 或者 filter() 是没有状态的，它不会要求你保留任何原始数据。但是现实中，大多数的操作都是有状态的 (比如 count())，因为这需要你存储当前累计的状态。 在流处理器上维护状态的问题是流处理器可能会失败！你需要在哪里保持这种状态才能容错？ 一种简单的方法是简单地将所有状态存储在远程数据库中，并通过网络连接到该存储，这样做的问题是大量的网络带宽会使得你的应用程序变慢。一个更微妙但重要的问题是你的流处理作业的正常运行时间将与远程数据库紧密耦合，并且作业将不是自包含的 (其他 team 更改数据库可能会破坏你的处理)。 那么什么是更好的办法呢？ 回想一下表和流的二元性。这允许我们将流转换为与我们的处理位于同一位置的表。它还为我们提供了一种处理容错的机制—通过将流存储在 Kafka broker 中。 流处理器可以将其状态保持在本地表 (例如 RocksDB) 中，该表将从输入流 (可能在某些任意转换之后) 更新。当进程失败时，它可以通过重放流来恢复其数据。 你甚至可以将远程数据库作为流的生产者，有效地广播用于在本地重建表的更改日志。 KSQL通常，我们不得不使用 JVM 语言编写流处理，因为这是唯一的官方 Kafka Streams API 客户端。2018 年 4 月，KSQL 作为一项新特性被发布，它允许你使用熟悉的类似 SQL 的语言编写简单的 stream jobs。你安装了 KSQL 服务器并通过 CLI 以交互方式查询以及管理。它使用相同的抽象 (KStream 和 KTable)，保证了 Streams API 的相同优点 (可伸缩性，容错性)，并大大简化了流的工作。 这听起来可能不是很多，但在实践中对于测试内容更有用，甚至允许开发之外的人 (例如产品所有者) 使用流处理，可以看看 Confluent 提供的这篇关于 ksql 的使用。 什么时候使用 kafka正如我们已经介绍的那样，Kafka 允许你通过集中式介质获取大量消息并存储它们，而不必担心性能或数据丢失等问题。 这意味着它非常适合用作系统架构的核心，充当连接不同应用程序的集中式媒体。Kafka 可以成为事件驱动架构的中心部分，使你可以真正地将应用程序彼此分离。 Kafka 允许你轻松地分离不同 (微) 服务之间的通信。使用 Streams API，现在可以比以往更轻松地编写业务逻辑，从而丰富 Kafka 主题数据以供服务使用。可能性很大，我恳请你探讨公司如何使用 Kafka。 总结Apache Kafka 是一个分布式流媒体平台，每天可处理数万亿个事件。Kafka 提供低延迟，高吞吐量，容错的发布和订阅管道，并能够处理事件流。我们回顾了它的基本语义 (producer，broker，consumer，topic)，了解了它的一些优化 (page cache)，通过复制数据了解了它的容错能力，并介绍了它不断增长的强大流媒体功能。Kafka 已经在全球数千家公司中大量采用，其中包括财富 500 强企业中的三分之一。随着 Kafka 的积极开发和最近发布的第一个主要版本 1.0 (2017 年 11 月 1 日)，有预测这个流媒体平台将会与关系数据库一样，是数据平台的重要核心。我希望这篇介绍能帮助你熟悉 Apache Kafka。 本文参考http://generalthink.github.io/2019/02/27/introduction-of-kafka/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"使用 hexo 搭建 github 博客","slug":"hexo-blog","date":"2020-10-23T03:33:51.000Z","updated":"2021-01-27T06:29:29.019Z","comments":true,"path":"2020/10/23/hexo-blog/","link":"","permalink":"http://example.com/2020/10/23/hexo-blog/","excerpt":"","text":"使用工具版本默认已经安装 node.js 和 git。 123git version: git version 2.27.0.windows.1npm version: 6.14.7hexo version: 4.2.0 git 客户端与 github 建立 SSH 连接1Please make sure you have the correct access rights and the repository exists. 当 git 客户端出现以上提示时，说明 SSH 连接过期，需要重新建立连接。参考如下方式： 先查看下 name 和 email 123456# 查看user的name和email$ git config user.name$ git config user.email# 如果没设置，按如下命令设置$ git config --global user.name &#123;$yourname&#125;$ git config --global user.email &#123;$youremail&#125; 删除 .ssh 文件夹下的 known_hosts，路径为：C:\\Users\\&#123;$userrname&#125;\\.ssh git bash 输入命令 1$ ssh-keygen -t rsa -C &#123;$youremail&#125; 一直按回车，等结束后，.ssh 文件夹下会生成两个文件：id_rsa 和 id_rsa.pub，将 id_rsa.pub 的内容全部复制。 登录个人 github 账户，进入 Settings → SSH and GPG keys，点击 New SSH key，将复制的内容粘贴到 Key 里，点击 Add SSH key。 git bash 输入命令 1$ ssh -T git@github.com 在弹出的确定对话框输入：yes。 hexo 安装在 git bash 中依次输入以下命令： 123456$ npm install hexo-cli -g$ cd f: # 可以是任何路径$ hexo init blog$ cd blog # 进入blog目录$ npm install$ npm install hexo-deployer-git --save 命令执行完成后，会在 F:\\ 目录下，多一个 blog 文件夹。 修改 _config.yml 文件修改 blog 根目录下的 _config.yml 文件，将 deploy 节点修改为如下内容： 1234deploy: type: git repo: git@github.com:&#123;$yourname&#125;/&#123;$yourname&#125;.github.io.git branch: master 说明：_config.yml 文件的配置均为 [key: value] 形式，value 前面必须要有一个空格。 然后在 git bash 中输入以下命令，发布博客： 1$ hexo deploy 访问自己的博客博客地址：https://&#123;$yourname&#125;.github.io/ 写一个自己的博客hexo 的项目结构是在网站根目录的 source\\_posts 目录下存放你的博客文档，以 .md 文档格式存储，默认已存在一个 hello-world.md 文章。 新建文章 1$ hexo new &lt;title&gt; 会在 blog 的 source\\_posts 目录下，新建一个名叫 &lt;title&gt;.md 文章，如： 12INFO Validating configINFO Created: F:\\blog\\source\\_posts\\tesss.md 之后，在文章中添加自己的内容即可，建议使用 Typora 编辑，其语法参考：如何使用 markdown？ 发布文章 1234$ hexo clean # 清楚缓存$ hexo generate # 生成静态页面$ hexo server # 本地发布，浏览器输入localhost:4000即可访问博客$ hexo deploy # 将public中的静态页面复制到.deploy_git文件夹中，并提交到github 至此，你的第一个自己的博客发布完成。 说明：以上 hexo 的命令，都要在 F:\\blog 目录下执行。 修改博客的 themes如果想修改自己博客的 themes，可以下载好想要的，然后拷贝到 blog 的 themes 目录下，然后修改 _config.yml 文件，将 theme 节点的值，修改为你下载好的 themes 的名称，如： 1theme: next 之后，再按照你下载的 themes 的使用说明，做相应修改即可。 参考：NexT 的使用 NexT 中 tags 的使用 修改 NexT 目录下的 _config.yml 文件，取消 menu 菜单下 tags 字段的注释 123menu: home: / || fa fa-home tags: /tags/ || fa fa-tags 在 blog 根目录的 source 目录下，新建 tags 目录 1$ hexo new page &quot;tags&quot; 修改 tags 目录下的 index.md 文件 1234title: tagsdate: 2020-10-27 16:35:56type: tagslayout: &quot;tags&quot; NexT 中添加字数统计、阅读时长 安装 hexo-symbols-count-time 插件 1$ npm install hexo-symbols-count-time 或者 1$ yarn add hexo-symbols-count-time hexo 配置，根目录下的 _config.yaml 文件，添加 symbols_count_time 节点 123456# Post wordcount display settingssymbols_count_time: symbols: true # 文章字数 time: true # 阅读时长 total_symbols: true # 所有文章总字数 total_time: true # 所有文章阅读中时长 NexT 配置，themes 目录下的 _config.yml 文件，symbols_count_time 节点 123456# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: true # 是否换行显示 字数统计 及 阅读时长 item_text_post: true # 文章 字数统计 阅读时长 使用图标 还是 文本表示 item_text_total: false # 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示 Next 中添加访客统计、访问次数统计、文章阅读次数统计 打开 next 主题配置文件 \\themes\\next\\_config.yml，搜索 busuanzi_count，把 enable 设置为 true。 12345678910# Show Views / Visitors of the website / page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzibusuanzi_count: enable: true total_visitors: true total_visitors_icon: fa fa-user total_views: true total_views_icon: fa fa-eye post_views: true post_views_icon: fa fa-eye 同样是在 next 主题配置文件 \\themes\\next\\_config.yml 下，搜索 footer，在它底下添加 counter，设值为 true。 12345678910111213141516171819202122232425262728293031footer: # Specify the date when the site was setup. If not defined, current year will be used. #since: 2015 # Icon between year and copyright info. icon: # Icon name in Font Awesome. See: https://fontawesome.com/icons name: fa fa-heart # If you want to animate the icon, set it to true. animated: false # Change the color of icon, using Hex Code. color: &quot;#ff0000&quot; # If not defined, `author` from Hexo `_config.yml` will be used. copyright: # Powered by Hexo &amp; NexT powered: true # Beian ICP and gongan information for Chinese users. See: https://beian.miit.gov.cn, http://www.beian.gov.cn beian: enable: false icp: # The digit in the num of gongan beian. gongan_id: # The full num of gongan beian. gongan_num: # The icon for gongan beian. See: http://www.beian.gov.cn/portal/download gongan_icon_url: counter: true 来到 themes\\next\\layout\\_partials，找到 footer.swig 文件，打开编辑，在底下添加代码。 123&#123;% if theme.footer.counter %&#125; &lt;script async src=&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 站点访客数、访问次数显示在网址底部，文章阅读次数在文章开头。 在博客中添加图片md 文件中插入图片的语法为：![]()。 其中，方括号是图片描述，圆括号是图片路径。一般来说有三种图片路径，分别是相对路径，绝对路径和网络路径。 相对而言，使用相对路径会更加方便，设置如下： 安装 hexo-renderer-marked 插件 1$ npm install hexo-renderer-marked 修改根目录下的 _config.yaml 配置 将： 1post_asset_folder: false 修改为： 1234post_asset_folder: truemarked: prependRoot: true postAsset: true 设置 Typora 点击文件 → 偏好设置，设置如下： 这样，在粘贴图片到文件中时，会自动将图片复制到 source\\_posts 目录下，与 .md 文件同名的目录中。 之后，在将博客发布时，会自动上传到文章所生成的静态网页同级目录之下。","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}],"categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"flink","slug":"flink","permalink":"http://example.com/tags/flink/"},{"name":"tool","slug":"tool","permalink":"http://example.com/tags/tool/"},{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://example.com/tags/hadoop/"},{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"},{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"Poetry and Prose","slug":"Poetry-and-Prose","permalink":"http://example.com/tags/Poetry-and-Prose/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}