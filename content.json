{"meta":{"title":"XiSun的博客","subtitle":"Learning is endless","description":"心如止水者，虽世间繁华之红尘纷扰，已然空无一物","author":"XiSun","url":"http://example.com","root":"/"},"pages":[{"title":"tags","date":"2020-10-27T08:35:56.000Z","updated":"2020-10-27T08:40:16.641Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"KafkaConsumer 源码之 consumer 的 offset commit 机制和 partition 分配机制","slug":"kafka-consumer-commitandpartition","date":"2020-11-24T07:17:06.000Z","updated":"2020-11-25T08:30:50.081Z","comments":true,"path":"2020/11/24/kafka-consumer-commitandpartition/","link":"","permalink":"http://example.com/2020/11/24/kafka-consumer-commitandpartition/","excerpt":"","text":"紧接着上篇文章，这篇文章讲述 consumer 提供的 offset commit 机制和 partition 分配机制，具体如何使用是需要用户结合具体的场景进行选择，本文讲述一下其底层实现。 自动 offset commit 机制1234// 自动提交，默认trueprops.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// 设置自动每1s提交一次，默认为5sprops.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 通过上面设置，启动自动提交 offset 以及设置自动提交间隔时间。 手动 offset commit 机制先看下两种不同的手动 offset commit 机制，一种是同步 commit，一种是异步 commit，既然其作用都是 offset commit，应该不难猜到它们底层使用接口都是一样的，其调用流程如下图所示： 同步 commit1234567// 对poll()中返回的所有topics和partition列表进行commit// 这个方法只能将offset提交Kafka中，Kafka将会在每次rebalance之后的第一次拉取或启动时使用同步commit// 这是同步commit，它将会阻塞进程，直到commit成功或者遇到一些错误public void commitSync() &#123;&#125;// 只对指定的topic-partition列表进行commitpublic void commitSync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) &#123;&#125; 其实，从上图中，就已经可以看出，同步 commit 的实现方式，client.poll () 方法会阻塞直到这个 request 完成或超时才会返回。 异步 commit12345public void commitAsync() &#123;&#125;public void commitAsync(OffsetCommitCallback callback) &#123;&#125;public void commitAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback) &#123;&#125; 对于异步的 commit，最后调用的都是 doCommitOffsetsAsync () 方法，其具体实现如下： 12345678910111213141516171819202122232425262728private void doCommitOffsetsAsync(final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, final OffsetCommitCallback callback) &#123; // 发送offset-commit请求 RequestFuture&lt;Void&gt; future = sendOffsetCommitRequest(offsets); final OffsetCommitCallback cb = callback == null ? defaultOffsetCommitCallback : callback; future.addListener(new RequestFutureListener&lt;Void&gt;() &#123; @Override public void onSuccess(Void value) &#123; if (interceptors != null) interceptors.onCommit(offsets); // 添加成功的请求，以唤醒相应的回调函数 completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, null)); &#125; @Override public void onFailure(RuntimeException e) &#123; Exception commitException = e; if (e instanceof RetriableException) &#123; commitException = new RetriableCommitFailedException(e); &#125; // 添加失败的请求，以唤醒相应的回调函数 completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, commitException)); if (commitException instanceof FencedInstanceIdException) &#123; asyncCommitFenced.set(true); &#125; &#125; &#125;);&#125; 在异步 commit 中，可以添加相应的回调函数，如果 request 处理成功或处理失败，ConsumerCoordinator 会通过 invokeCompletedOffsetCommitCallbacks () 方法唤醒相应的回调函数。 上面简单的介绍了同步 commit 和异步 commit，更详细的分析参考：Kafka consumer 的 offset 的提交方式。 注意：手动 commit 时，提交的是下一次要读取的 offset。举例如下： 1234567891011121314151617181920&gt;try &#123; while(running) &#123; // 取得消息 ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE); // 根据分区来遍历数据 for (TopicPartition partition : records.partitions()) &#123; List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition); // 数据处理 for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123; System.out.println(record.offset() + &quot;: &quot; + record.value()); &#125; // 取得当前读取到的最后一条记录的offset long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); // 提交offset，记得要 + 1 consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1))); &#125; &#125;&gt;&#125; finally &#123; consumer.close();&gt;&#125; commit offset 请求的处理当 Kafka Server 端接收到来自 client 端的 offset commit 请求时，对于提交的 offset，GroupCoordinator 会记录在 GroupMetadata 对象中，至于其实现的逻辑细节，此处不再赘述。 partition 分配机制consumer 提供了三种不同的 partition 分配策略，可以通过 partition.assignment.strategy 参数进行配置，默认情况下使用的是 org.apache.kafka.clients.consumer.RangeAssignor，Kafka 中提供了另外两种 partition 的分配策略 org.apache.kafka.clients.consumer.RoundRobinAssignor 和 org.apache.kafka.clients.consumer.StickyAssignor，它们关系如下图所示： 通过上图可以看出，用户可以自定义相应的 partition 分配机制，只需要继承这个 AbstractPartitionAssignor 抽象类即可。 partition 分配策略，其实也就是 reblance 策略。 AbstractPartitionAssignorAbstractPartitionAssignor 有一个抽象方法，如下所示： 12345678910/** * Perform the group assignment given the partition counts and member subscriptions * @param partitionsPerTopic The number of partitions for each subscribed topic. Topics not in metadata will be excluded * from this map. * @param subscriptions Map from the memberId to their respective topic subscription * @return Map from each member to the list of partitions assigned to them. */// 根据partitionsPerTopic和subscriptions进行分配，具体的实现会在子类中实现(不同的子类，其实现方法不相同)public abstract Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions); assign () 这个方法，有两个参数： partitionsPerTopic：所订阅的每个 topic 与其 partition 数的对应关系，metadata 没有的 topic 将会被移除； subscriptions：每个 consumerId 与其所订阅的 topic 列表的关系。 继承 AbstractPartitionAssignor 的子类，通过实现 assign () 方法，来进行相应的 partition 分配。 RangeAssignor 分配模式assign () 方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; // 1.参数含义:(topic, List&lt;consumerId&gt;)，获取每个topic被多少个consumer订阅了 Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions); // 2.存储最终的分配方案 Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) assignment.put(memberId, new ArrayList&lt;&gt;()); for (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123; String topic = topicEntry.getKey(); List&lt;String&gt; consumersForTopic = topicEntry.getValue(); // 3.每个topic的partition数量 Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic == null) continue; Collections.sort(consumersForTopic); // 4.取商，表示平均每个consumer会分配到多少个partition int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size(); // 5.取余，表示平均分配后还剩下多少个partition未被分配 int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size(); List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic); // 6.这里是关键点，分配原则是将未能被平均分配的partition分配到前consumersWithExtraPartition个consumer for (int i = 0, n = consumersForTopic.size(); i &lt; n; i++) &#123; // 假设partition有7个，consumer有5个，则numPartitionsPerConsumer=1，consumersWithExtraPartition=2 // i=0, start: 0, length: 2, topic-partition: p0, p1 // i=1, start: 2, length: 2, topic-partition: p2, p3 // i=2, start: 4, length: 1, topic-partition: p4 // i=3, start: 5, length: 1, topic-partition: p5 // i=4, start: 6, length: 1, topic-partition: p6 int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition); int length = numPartitionsPerConsumer + (i + 1 &gt; consumersWithExtraPartition ? 0 : 1); assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length)); &#125; &#125; return assignment;&#125; 假设 topic 的 partition 数为 numPartitionsForTopic，group 中订阅这个 topic 的 member 数为 consumersForTopic.size()，首先需要算出两个值： numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size()：表示平均每个 consumer 会分配到几个 partition； consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size()：表示平均分配后还剩下多少个 partition 未分配。 分配的规则是：对于剩下的那些 partition 分配到前 consumersWithExtraPartition 个 consumer 上，也就是前 consumersWithExtraPartition 个 consumer 获得 topic-partition 列表会比后面多一个。 在上述的程序中，举了一个例子，假设有一个 topic 有 7 个 partition，group 有5个 consumer，这个5个 consumer 都订阅这个 topic，那么 range 的分配方式如下： 消费者 分配方案 consumer 0 start: 0, length: 2, topic-partition: p0, p1 consumer 1 start: 2, length: 2, topic-partition: p2, p3 consumer 2 start: 4, length: 1, topic-partition: p4 consumer 3 start: 5, length: 1, topic-partition: p5 consumer 4 start: 6, length: 1, topic-partition: p6 而如果 group 中有 consumer 没有订阅这个 topic，那么这个 consumer 将不会参与分配。下面再举个例子，假设有 2 个 topic，一个有 5 个 partition，另一个有 7 个 partition，group 中有 5 个 consumer，但是只有前 3 个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下： consumer 订阅 topic1 的列表 订阅 topic2 的列表 consumer 0 t1 p0, t1 p1 t2 p0, t2 p1 consumer 1 t1 p2, t1 p3 t2 p2, t2 p3 consumer 2 t1 p4 t2 p4 consumer 3 t2 p5 consumer 4 t2 p6 RoundRobinAssignorassign () 方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536@Overridepublic Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = new HashMap&lt;&gt;(); for (String memberId : subscriptions.keySet()) assignment.put(memberId, new ArrayList&lt;&gt;()); // 环状链表，存储所有的consumer，一次迭代完之后又会回到原点 CircularIterator&lt;String&gt; assigner = new CircularIterator&lt;&gt;(Utils.sorted(subscriptions.keySet())); // 获取所有订阅的topic的partition总数 for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) &#123; final String topic = partition.topic(); while (!subscriptions.get(assigner.peek()).topics().contains(topic)) assigner.next(); assignment.get(assigner.next()).add(partition); &#125; return assignment;&#125;public List&lt;TopicPartition&gt; allPartitionsSorted(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; // 所有的topics(有序) SortedSet&lt;String&gt; topics = new TreeSet&lt;&gt;(); for (Subscription subscription : subscriptions.values()) topics.addAll(subscription.topics()); // 订阅的Topic的所有的TopicPartition集合 List&lt;TopicPartition&gt; allPartitions = new ArrayList&lt;&gt;(); for (String topic : topics) &#123; Integer numPartitionsForTopic = partitionsPerTopic.get(topic); if (numPartitionsForTopic != null) // topic的所有partition都添加进去 allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic)); &#125; return allPartitions;&#125; Round Robin 的实现原则，简单来说就是：列出所有 topic-partition 和列出所有的 consumer member，然后开始分配，一轮之后继续下一轮，假设有一个 topic，它有7个 partition，group 中有 3 个 consumer 都订阅了这个 topic，那么其分配方式为： 消费者 分配列表 consumer 0 p0, p3, p6 consumer 1 p1, p4 consumer 2 p2, p5 对于多个 topic 的订阅，假设有 2 个 topic，一个有 5 个 partition，一个有 7 个 partition，group 中有 5 个 consumer，但是只有前 3 个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下： 消费者 订阅 topic1 的列表 订阅的 topic2 的列表 consumer 0 t1 p0, t1 p3 t2 p0, t2 p5 consumer 1 t1 p1, t1 p4 t2 p1, t2 p6 consumer 2 t1 p2 t2 p2 consumer 3 t2 p3 consumer 4 t2 p4 StickyAssignorassign () 方法的实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions) &#123; Map&lt;String, List&lt;TopicPartition&gt;&gt; currentAssignment = new HashMap&lt;&gt;(); Map&lt;TopicPartition, ConsumerGenerationPair&gt; prevAssignment = new HashMap&lt;&gt;(); partitionMovements = new PartitionMovements(); prepopulateCurrentAssignments(subscriptions, currentAssignment, prevAssignment); boolean isFreshAssignment = currentAssignment.isEmpty(); // a mapping of all topic partitions to all consumers that can be assigned to them final Map&lt;TopicPartition, List&lt;String&gt;&gt; partition2AllPotentialConsumers = new HashMap&lt;&gt;(); // a mapping of all consumers to all potential topic partitions that can be assigned to them final Map&lt;String, List&lt;TopicPartition&gt;&gt; consumer2AllPotentialPartitions = new HashMap&lt;&gt;(); // initialize partition2AllPotentialConsumers and consumer2AllPotentialPartitions in the following two for loops for (Entry&lt;String, Integer&gt; entry: partitionsPerTopic.entrySet()) &#123; for (int i = 0; i &lt; entry.getValue(); ++i) partition2AllPotentialConsumers.put(new TopicPartition(entry.getKey(), i), new ArrayList&lt;&gt;()); &#125; for (Entry&lt;String, Subscription&gt; entry: subscriptions.entrySet()) &#123; String consumer = entry.getKey(); consumer2AllPotentialPartitions.put(consumer, new ArrayList&lt;&gt;()); entry.getValue().topics().stream().filter(topic -&gt; partitionsPerTopic.get(topic) != null).forEach(topic -&gt; &#123; for (int i = 0; i &lt; partitionsPerTopic.get(topic); ++i) &#123; TopicPartition topicPartition = new TopicPartition(topic, i); consumer2AllPotentialPartitions.get(consumer).add(topicPartition); partition2AllPotentialConsumers.get(topicPartition).add(consumer); &#125; &#125;); // add this consumer to currentAssignment (with an empty topic partition assignment) if it does not already exist if (!currentAssignment.containsKey(consumer)) currentAssignment.put(consumer, new ArrayList&lt;&gt;()); &#125; // a mapping of partition to current consumer Map&lt;TopicPartition, String&gt; currentPartitionConsumer = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry: currentAssignment.entrySet()) for (TopicPartition topicPartition: entry.getValue()) currentPartitionConsumer.put(topicPartition, entry.getKey()); List&lt;TopicPartition&gt; sortedPartitions = sortPartitions( currentAssignment, prevAssignment.keySet(), isFreshAssignment, partition2AllPotentialConsumers, consumer2AllPotentialPartitions); // all partitions that need to be assigned (initially set to all partitions but adjusted in the following loop) List&lt;TopicPartition&gt; unassignedPartitions = new ArrayList&lt;&gt;(sortedPartitions); for (Iterator&lt;Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt;&gt; it = currentAssignment.entrySet().iterator(); it.hasNext();) &#123; Map.Entry&lt;String, List&lt;TopicPartition&gt;&gt; entry = it.next(); if (!subscriptions.containsKey(entry.getKey())) &#123; // if a consumer that existed before (and had some partition assignments) is now removed, remove it from currentAssignment for (TopicPartition topicPartition: entry.getValue()) currentPartitionConsumer.remove(topicPartition); it.remove(); &#125; else &#123; // otherwise (the consumer still exists) for (Iterator&lt;TopicPartition&gt; partitionIter = entry.getValue().iterator(); partitionIter.hasNext();) &#123; TopicPartition partition = partitionIter.next(); if (!partition2AllPotentialConsumers.containsKey(partition)) &#123; // if this topic partition of this consumer no longer exists remove it from currentAssignment of the consumer partitionIter.remove(); currentPartitionConsumer.remove(partition); &#125; else if (!subscriptions.get(entry.getKey()).topics().contains(partition.topic())) &#123; // if this partition cannot remain assigned to its current consumer because the consumer // is no longer subscribed to its topic remove it from currentAssignment of the consumer partitionIter.remove(); &#125; else // otherwise, remove the topic partition from those that need to be assigned only if // its current consumer is still subscribed to its topic (because it is already assigned // and we would want to preserve that assignment as much as possible) unassignedPartitions.remove(partition); &#125; &#125; &#125; // at this point we have preserved all valid topic partition to consumer assignments and removed // all invalid topic partitions and invalid consumers. Now we need to assign unassignedPartitions // to consumers so that the topic partition assignments are as balanced as possible. // an ascending sorted set of consumers based on how many topic partitions are already assigned to them TreeSet&lt;String&gt; sortedCurrentSubscriptions = new TreeSet&lt;&gt;(new SubscriptionComparator(currentAssignment)); sortedCurrentSubscriptions.addAll(currentAssignment.keySet()); balance(currentAssignment, prevAssignment, sortedPartitions, unassignedPartitions, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer); return currentAssignment;&#125; sticky 分区策略是从 0.11 版本才开始引入的，它主要有两个目的： 分区的分配要尽可能均匀 分区的分配要尽可能与上次分配的保持相同 当两者冲突的时候，第一个目标优先于第二个目标。 sticky 的分区方式作用发生分区重分配的时候，尽可能地让前后两次分配相同，进而减少系统资源的损耗及其他异常情况的发生。因为 sticky 分区策略的代码，要比 range 和 roundrobin 复杂很多，此处不做具体的细节分析，只简单举例如下： 假设有 3 个 topic，一个有 2 个 partition，一个有 3 个 partition，另外一个有 4 个 partition，group 中有 3 个 consumer，第一个 consumer 订阅了第一个 topic，第二个 consumer 订阅了前两个 topic，第三个 consumer 订阅了三个 topic，那么它们的分配方案如下： 消费者 订阅 topic1 的列表 订阅 topic2 的列表 订阅 topic3 的列表 consumer1 t1 p0 consumer2 t1 p1, t2 p1 t2 p0, t2 p3 consumer3 t3 p0, t3 p1, t3 p2, t3 p3 上面三个分区策略有着不同的分配方式，在实际使用过程中，需要根据自己的需求选择合适的策略，但是如果你只有一个 consumer，那么选择哪个方式都是一样的，但是如果是多个 consumer 不在同一台设备上进行消费，那么 sticky 方式应该更加合适。 自定义分区策略如之前所说，只需要继承 AbstractPartitionAssignor 并复写其中方法即可 (当然也可以直接实现 PartitionAssignor 接口) 自定义分区策略，其中有两个方法需要复写： 1234public String name();public abstract Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic, Map&lt;String, Subscription&gt; subscriptions); 其中 assign () 方法表示的是分区分配方案的实现，而 name () 方法则表示了这个分配策略的唯一名称，比如之前提到的 range，roundrobin 和 sticky, 这个名字会在和 GroupCoordinator 的通信中返回，通过它 consumer leader 来确定整个 group 的分区方案 (分区策略是由 group 中的 consumer 共同投票决定的，谁使用的多，就使用哪个策略)。 本文参考http://generalthink.github.io/2019/06/06/kafka-consumer-partition-assign/ https://matt33.com/2017/11/19/consumer-two-summary/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com 至此，关于 Kafka 的学习暂时告一段落，未来有需要时，会继续学习。更多关于 Kafka 原理等知识的介绍，参考： http://generalthink.github.io/tags/Kafka/ https://matt33.com/tags/kafka/","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaConsumer 源码之 consumer 的两种订阅模式","slug":"kafka-consumer-subscribeandassign","date":"2020-11-24T02:18:47.000Z","updated":"2020-11-24T07:01:05.486Z","comments":true,"path":"2020/11/24/kafka-consumer-subscribeandassign/","link":"","permalink":"http://example.com/2020/11/24/kafka-consumer-subscribeandassign/","excerpt":"","text":"在前面的文章中，有简单的介绍了 KafkaConsumer 的两种订阅模式，本篇文章对此进行扩展说明一下。 KafkaConsumer 的两种订阅模式， subscribe () 模式和 assign () 模式，前者是 topic 粒度 (使用 group 管理)，后者是 topic-partition 粒度 (用户自己去管理)。 订阅模式KafkaConsumer 为订阅模式提供了 4 种 API，如下： 12345678910111213// 订阅指定的topic列表，并且会自动进行动态partition订阅// 当发生以下情况时，会进行rebalance:1.订阅的topic列表改变；2.topic被创建或删除；3.consumer线程die；4.加一个新的consumer线程// 当发生rebalance时，会唤醒ConsumerRebalanceListener线程public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123;&#125;// 同上，但是这里没有设置listenerpublic void subscribe(Collection&lt;String&gt; topics) &#123;&#125;// 订阅那些满足一定规则(pattern)的topicpublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123;&#125;// 同上，但是这里没有设置listenerpublic void subscribe(Pattern pattern) &#123;&#125; 以上 4 种 API 都是按照 topic 级别去订阅，可以动态地获取其分配的 topic-partition，这是使用 Group 动态管理，它不能与手动 partition 管理一起使用。当监控到发生下面的事件时，Group 将会触发 rebalance 操作： 订阅的 topic 列表变化； topic 被创建或删除； consumer group 的某个 consumer 实例挂掉； 一个新的 consumer 实例通过 join 方法加入到一个 group 中。 在这种模式下，当 KafkaConsumer 调用 poll () 方法时，第一步会首先加入到一个 group 中，并获取其分配的 topic-partition 列表，具体细节在前面的文章中已经分析过了。 这里介绍一下当调用 subscribe () 方法之后，consumer 所做的事情，分两种情况介绍，一种按 topic 列表订阅，一种是按 pattern 模式订阅： topic 列表订阅 topic 列表订阅，最终调用如下方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Subscribe to the given list of topics to get dynamically * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * As part of group management, the consumer will keep track of the list of consumers that belong to a particular * group and will trigger a rebalance operation if any one of the following events are triggered: * &lt;ul&gt; * &lt;li&gt;Number of partitions change for any of the subscribed topics * &lt;li&gt;A subscribed topic is created or deleted * &lt;li&gt;An existing member of the consumer group is shutdown or fails * &lt;li&gt;A new member is added to the consumer group * &lt;/ul&gt; * &lt;p&gt; * When any of these events are triggered, the provided listener will be invoked first to indicate that * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received. * Note that rebalances will only occur during an active call to &#123;@link #poll(Duration)&#125;, so callbacks will * also only be invoked during that time. * * The provided listener will immediately override any listener set in a previous call to subscribe. * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics * subscribed in this call. See &#123;@link ConsumerRebalanceListener&#125; for more details. * * @param topics The list of topics to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; acquireAndEnsureOpen(); try &#123; maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot be null&quot;); if (topics.isEmpty()) &#123; // treat subscribing to empty topic list as the same as unsubscribing this.unsubscribe(); &#125; else &#123; for (String topic : topics) &#123; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;); &#125; throwIfNoAssignorsConfigured(); fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(&quot;Subscribed to topic(s): &#123;&#125;&quot;, Utils.join(topics, &quot;, &quot;)); // 核心步骤在此处执行 if (this.subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; 将 SubscriptionType 类型设置为 AUTO_TOPICS，并更新 SubscriptionState 中记录的 subscription 属性 (记录的是订阅的 topic 列表)； 12345678910111213141516171819public synchronized boolean subscribe(Set&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_TOPICS); return changeSubscription(topics);&#125;private boolean changeSubscription(Set&lt;String&gt; topicsToSubscribe) &#123; if (subscription.equals(topicsToSubscribe)) return false; subscription = topicsToSubscribe; if (subscriptionType != SubscriptionType.USER_ASSIGNED) &#123; groupSubscription = new HashSet&lt;&gt;(groupSubscription); groupSubscription.addAll(topicsToSubscribe); &#125; else &#123; groupSubscription = new HashSet&lt;&gt;(topicsToSubscribe); &#125; return true;&#125; 请求更新 metadata。 1234567891011121314public synchronized void requestUpdateForNewTopics() &#123; // Override the timestamp of last refresh to let immediate update. this.lastRefreshMs = 0; this.requestVersion++; requestUpdate();&#125;/** * Request an update of the current cluster metadata info, return the current updateVersion before the update */public synchronized int requestUpdate() &#123; this.needUpdate = true; return this.updateVersion;&#125; pattern 模式订阅 pattern 模式订阅，最终调用如下方法： 123456789101112131415161718192021222324252627282930313233343536/** * Subscribe to all topics matching specified pattern to get dynamically assigned partitions. * The pattern matching will be done periodically against all topics existing at the time of check. * This can be controlled through the &#123;@code metadata.max.age.ms&#125; configuration: by lowering * the max metadata age, the consumer will refresh metadata more often and check for matching topics. * &lt;p&gt; * See &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125; for details on the * use of the &#123;@link ConsumerRebalanceListener&#125;. Generally rebalances are triggered when there * is a change to the topics matching the provided pattern and when consumer group membership changes. * Group rebalances only take place during an active call to &#123;@link #poll(Duration)&#125;. * * @param pattern Pattern to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If pattern or listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with topics, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; maybeThrowInvalidGroupIdException(); if (pattern == null) throw new IllegalArgumentException(&quot;Topic pattern to subscribe to cannot be null&quot;); acquireAndEnsureOpen(); try &#123; throwIfNoAssignorsConfigured(); log.info(&quot;Subscribed to pattern: &#x27;&#123;&#125;&#x27;&quot;, pattern); this.subscriptions.subscribe(pattern, listener); this.coordinator.updatePatternSubscription(metadata.fetch()); this.metadata.requestUpdateForNewTopics(); &#125; finally &#123; release(); &#125;&#125; 将 SubscriptionType 类型设置为 AUTO_PATTERN，并更新 SubscriptionState 中记录的 subscribedPattern 属性，设置为 pattern； 12345public synchronized void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_PATTERN); this.subscribedPattern = pattern;&#125; 调用 coordinator 的 updatePatternSubscription () 方法，遍历所有 topic 的 metadata，找到所有满足 pattern 的 topic 列表，更新到 SubscriptionState 的 subscriptions 属性，并请求更新 Metadata； 1234567891011121314151617181920212223242526272829public void updatePatternSubscription(Cluster cluster) &#123; final Set&lt;String&gt; topicsToSubscribe = cluster.topics().stream() .filter(subscriptions::matchesSubscribedPattern) .collect(Collectors.toSet()); if (subscriptions.subscribeFromPattern(topicsToSubscribe)) metadata.requestUpdateForNewTopics();&#125;public synchronized boolean subscribeFromPattern(Set&lt;String&gt; topics) &#123; if (subscriptionType != SubscriptionType.AUTO_PATTERN) throw new IllegalArgumentException(&quot;Attempt to subscribe from pattern while subscription type set to &quot; + subscriptionType); return changeSubscription(topics);&#125;private boolean changeSubscription(Set&lt;String&gt; topicsToSubscribe) &#123; if (subscription.equals(topicsToSubscribe)) return false; subscription = topicsToSubscribe; if (subscriptionType != SubscriptionType.USER_ASSIGNED) &#123; groupSubscription = new HashSet&lt;&gt;(groupSubscription); groupSubscription.addAll(topicsToSubscribe); &#125; else &#123; groupSubscription = new HashSet&lt;&gt;(topicsToSubscribe); &#125; return true;&#125; 1234567891011121314public synchronized void requestUpdateForNewTopics() &#123; // Override the timestamp of last refresh to let immediate update. this.lastRefreshMs = 0; this.requestVersion++; requestUpdate();&#125;/** * Request an update of the current cluster metadata info, return the current updateVersion before the update */public synchronized int requestUpdate() &#123; this.needUpdate = true; return this.updateVersion;&#125; 其他部分，两者基本一样，只是 pattern 模型在每次更新 topic-metadata 时，获取全局的 topic 列表，如果发现有新加入的符合条件的 topic，就立马去订阅，其他的地方，包括 group 管理、topic-partition 的分配都是一样的。 分配模式当调用 assign () 方法手动分配 topic-partition 列表时，不会使用 consumer 的 Group 管理机制，也即是当 consumer group member 变化或 topic 的 metadata 信息变化时，不会触发 rebalance 操作。比如：当 topic 的 partition 增加时，这里无法感知，需要用户进行相应的处理，Apache Flink 就是使用的这种方式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Manually assign a list of partitions to this consumer. This interface does not allow for incremental assignment * and will replace the previous assignment (if there is one). * &lt;p&gt; * If the given list of topic partitions is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * &lt;p&gt; * Manual topic assignment through this method does not use the consumer&#x27;s group management * functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic * metadata change. Note that it is not possible to use both manual partition assignment with &#123;@link #assign(Collection)&#125; * and group assignment with &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;. * &lt;p&gt; * If auto-commit is enabled, an async commit (based on the old assignment) will be triggered before the new * assignment replaces the old one. * * @param partitions The list of partitions to assign this consumer * @throws IllegalArgumentException If partitions is null or contains null or empty topics * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with topics or pattern * (without a subsequent call to &#123;@link #unsubscribe()&#125;) */@Overridepublic void assign(Collection&lt;TopicPartition&gt; partitions) &#123; acquireAndEnsureOpen(); try &#123; if (partitions == null) &#123; throw new IllegalArgumentException(&quot;Topic partition collection to assign to cannot be null&quot;); &#125; else if (partitions.isEmpty()) &#123; this.unsubscribe(); &#125; else &#123; for (TopicPartition tp : partitions) &#123; String topic = (tp != null) ? tp.topic() : null; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic partitions to assign to cannot have null or empty topic&quot;); &#125; fetcher.clearBufferedDataForUnassignedPartitions(partitions); // make sure the offsets of topic partitions the consumer is unsubscribing from // are committed since there will be no following rebalance if (coordinator != null) this.coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds()); log.info(&quot;Subscribed to partition(s): &#123;&#125;&quot;, Utils.join(partitions, &quot;, &quot;)); if (this.subscriptions.assignFromUser(new HashSet&lt;&gt;(partitions))) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; assign () 方法是手动向 consumer 分配一些 topic-partition 列表，并且这个接口不允许增加分配的 topic-partition 列表，将会覆盖之前分配的 topic-partition 列表，如果给定的 topic-partition 列表为空，它的作用将会与 unsubscribe () 方法一样。 这种手动 topic 分配也不会使用 consumer 的 group 管理，当 group 的 member 变化或 topic 的 metadata 变化时，也不会触发 rebalance 操作。 这里所说的 consumer 的 group 管理，就是前面所说的 consumer 如何加入 group 的管理过程。如果使用的是 assign 模式，也即是非 AUTO_TOPICS 或 AUTO_PATTERN 模式时，consumer 实例在调用 poll () 方法时，不会向 GroupCoordinator 发送 join-group、sync-group、heartbeat 请求，也就是说 GroupCoordinator 拿不到这个 consumer 实例的相关信息，也不会去维护这个 member 是否存活，这种情况下就需要用户自己管理自己的处理程序。但是这种模式可以进行 offset commit，这将在下一篇文章进行分析。 小结根据上面的讲述，这里做一下小结，两种模式对比如下图所示： 简单说明如下： 模式 不同之处 相同之处 subscribe () 使用 Kafka group 管理，自动进行 rebalance 操作 可以在 Kafka 保存 offset assign () 用户自己进行相关的处理 也可以进行 offset commit，但是尽量保证 group.id 唯一性，如果使用一个与上面模式一样的 group，offset commit 请求将会被拒绝 本文参考https://matt33.com/2017/11/18/consumer-subscribe/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"如何使用 maven","slug":"maven","date":"2020-11-18T07:18:48.000Z","updated":"2020-11-19T07:39:26.956Z","comments":true,"path":"2020/11/18/maven/","link":"","permalink":"http://example.com/2020/11/18/maven/","excerpt":"","text":"maven 的功能 依赖的管理：仅仅通过 jar 包的几个属性，就能确定唯一的 jar 包，在指定的文件 pom.xml 中，只要写入这些依赖属性，就会自动下载并管理 jar 包。 项目的构建：内置很多的插件与生命周期，支持多种任务，比如校验、编译、测试、打包、部署、发布… 项目的知识管理：管理项目相关的其他内容，比如开发者信息，版本等等 。 安装与配置 下载，地址：http://maven.apache.org/download.cgi 注意：安装 maven 之前，必须先确保你的机器中已经安装了 jdk，如果是 maven 3 则必须 jdk 1.7 以上。 解压，添加环境变量 MAVEN_HOME，值为解压后的 maven 路径： 在 Path 环境变量的变量值末尾添加 %MAVEN_HOME%\\bin; 在 cmd 窗口输入 mvn –version，显示 maven 版本信息，说明安装配置成功： 本地仓库配置如果不配置，默认会在如下位置存放从远程下载到的包：C:\\Users\\XiSun\\.m2\\repository。 打开 maven 安装目录，conf 目录下的 settings.xml 文件，配置如下： 1&lt;localRepository&gt;D:\\Program Files\\Maven\\apache-maven-3.6.3-maven-repository&lt;/localRepository&gt; 中央仓库配置当构建一个 maven 项目时，首先检查 pom.xml 文件以确定依赖包的下载位置，执行顺序如下： 从本地资源库中查找并获得依赖包，如果没有，执行第2步； 从 maven 默认中央仓库中查找并获得依赖包 (http://repo1.maven.org/maven2/)，如果没有，执行第3步； 如果在 pom.xml 中定义了自定义的远程仓库，那么也会在这里的仓库中进行查找并获得依赖包，如果都没有找到，那么 maven 就会抛出异常。 将本地 jar 包放入 maven 的本地仓库中本地 jar 包：有时候，项目中用到的 jar 包在中央仓库中没有，这时候就需要自己从网上下载下来相应的 jar 包，然后再放入 maven 的本地仓库中。 打开 cmd 窗口，切换到 jar 包所在目录，输入 mvn 命令，命令格式如下： mvn install:install-file -DgroupId=net.sf (自定义，需要与 pom 文件中的 groupId 一致) -DartifactId=classifier4j (自定义，需要与 pom 文件中的 artifaceId 一致) -Dversion=0.6 (自定义，需要与 pom 文件中的 version 一致) -Dpackaging=jar -Dfile=Classifier4J-0.6.jar (本地 jar 包) 注意：-DgroupId、-DartifactId、-Dversion、-Dpackaging、-Dfile 前面均有一个空格。 使用示例如下： 之后，在 maven 的本地仓库，根据 groupId —— artifactId —— version，即可找到打包进来的本地 jar 包，也可以在项目中的 pom 文件引入： 12345&lt;dependency&gt; &lt;groupId&gt;net.sf&lt;/groupId&gt; &lt;artifactId&gt;classifier4j&lt;/artifactId&gt; &lt;version&gt;0.6&lt;/version&gt;&lt;/dependency&gt; 未完，待续…","categories":[],"tags":[{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"}]},{"title":"KafkaConsumer 源码之 consumer 如何拉取 offset 和数据","slug":"kafka-consumer-offsetandfetcher","date":"2020-11-10T03:32:06.000Z","updated":"2020-11-24T02:19:48.579Z","comments":true,"path":"2020/11/10/kafka-consumer-offsetandfetcher/","link":"","permalink":"http://example.com/2020/11/10/kafka-consumer-offsetandfetcher/","excerpt":"","text":"上一篇文章讲了 consumer 如何加入 consumer group，现在加入 group 成功之后，就要准备开始消费。 kafkaConsumer.poll () 方法的核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private ConsumerRecords&lt;K, V&gt; poll(final Timer timer, final boolean includeMetadataInTimeout) &#123; // Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭 acquireAndEnsureOpen(); try &#123; if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123; throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;); &#125; // poll for new data until the timeout expires do &#123; client.maybeTriggerWakeup(); if (includeMetadataInTimeout) &#123; // Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息 if (!updateAssignmentMetadataIfNeeded(timer)) &#123; return ConsumerRecords.empty(); &#125; &#125; else &#123; while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123; log.warn(&quot;Still waiting for metadata&quot;); &#125; &#125; // Step3:拉取数据，核心步骤 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer); if (!records.isEmpty()) &#123; // before returning the fetched records, we can send off the next round of fetches // and avoid block waiting for their responses to enable pipelining while the user // is handling the fetched records. // // NOTE: since the consumed position has already been updated, we must not allow // wakeups or any other errors to be triggered prior to returning the fetched records. // 在返回数据之前，发送下次的fetch请求，避免用户在下次获取数据时线程block if (fetcher.sendFetches() &gt; 0 || client.hasPendingRequests()) &#123; client.pollNoWakeup(); &#125; return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records)); &#125; &#125; while (timer.notExpired()); return ConsumerRecords.empty(); &#125; finally &#123; release(); &#125;&#125; 紧跟上一篇文章，我们继续分析 consumer 加入 group 后的行为： 123456789101112/** * Visible for testing */boolean updateAssignmentMetadataIfNeeded(final Timer timer) &#123; // 1.上一篇主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group) if (coordinator != null &amp;&amp; !coordinator.poll(timer)) &#123; return false; &#125; // 2.本篇文章从updateFetchPositions(timer)方法开始继续分析(主要功能是:consumer获得partition的offset) return updateFetchPositions(timer);&#125; KafkaConsumer 的消费策略首先，我们应该知道，KafkaConsumer 关于如何消费的 2 种策略： 手动指定：调用 consumer.seek(TopicPartition, offset)，然后开始 poll ()。 自动指定：poll () 之前给集群发送请求，让集群告知客户端，当前该 TopicPartition 的 offset 是多少，这也是我们此次分析的重点。 在讲如何拉取 offset 之前，先认识下下面这个类 (SubscriptionState 的内部类)： 12345678910111213private static class TopicPartitionState &#123; private FetchState fetchState; private FetchPosition position; // last consumed position private Long highWatermark; // the high watermark from last fetch private Long logStartOffset; // the log start offset private Long lastStableOffset; private boolean paused; // whether this partition has been paused by the user private OffsetResetStrategy resetStrategy; // the strategy to use if the offset needs resetting private Long nextRetryTimeMs; private Integer preferredReadReplica; private Long preferredReadReplicaExpireTimeMs; ...&#125; consumer 实例订阅的每个 topic-partition 都会有一个对应的 TopicPartitionState 对象，在这个对象中会记录上面内容，最需要关注的就是 position 这个属性，它表示上一次消费的位置。通过 consumer.seek () 方式指定消费 offset 的时候，其实设置的就是这个 position 值。 updateFetchPositions - 拉取 offset在 consumer 成功加入 group 并开始消费之前，我们还需要知道 consumer 是从 offset 为多少的位置开始消费。consumer 加入 group 之后，就得去获取 offset 了，下面的方法，就是开始更新 position (offset)： 1234567891011121314151617181920212223242526272829303132333435363738/** * Set the fetch position to the committed position (if there is one) * or reset it using the offset reset policy the user has configured. * * @throws org.apache.kafka.common.errors.AuthenticationException if authentication fails. See the exception for more details * @throws NoOffsetForPartitionException If no offset is stored for a given partition and no offset reset policy is * defined * @return true iff the operation completed without timing out */private boolean updateFetchPositions(final Timer timer) &#123; // If any partitions have been truncated due to a leader change, we need to validate the offsets fetcher.validateOffsetsIfNeeded(); // Step1:查看TopicPartitionState的position是否为空，第一次消费肯定为空 cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions(); if (cachedSubscriptionHashAllFetchPositions) return true; // If there are any partitions which do not have a valid position and are not // awaiting reset, then we need to fetch committed offsets. We will only do a // coordinator lookup if there are partitions which have missing positions, so // a consumer with manually assigned partitions can avoid a coordinator dependence // by always ensuring that assigned partitions have an initial position. // Step2:如果没有有效的offset，那么需要从GroupCoordinator中获取 if (coordinator != null &amp;&amp; !coordinator.refreshCommittedOffsetsIfNeeded(timer)) return false; // If there are partitions still needing a position and a reset policy is defined, // request reset using the default policy. If no reset strategy is defined and there // are partitions with a missing position, then we will raise an exception. // Step3:如果还存在partition不知道position，并且设置了offsetreset策略，那么就等待重置，不然就抛出异常 subscriptions.resetMissingPositions(); // Finally send an asynchronous request to lookup and update the positions of any // partitions which are awaiting reset. // Step4:向PartitionLeader(GroupCoordinator所在机器)发送ListOffsetRequest重置position fetcher.resetOffsetsIfNeeded(); return true;&#125; 上面的代码主要分为 4 个步骤，具体如下： 首先，查看当前 TopicPartition 的 position 是否为空，如果不为空，表示知道下次 fetch position (即拉取数据时从哪个位置开始拉取)，但如果是第一次消费，这个 TopicPartitionState.position 肯定为空。 然后，通过 GroupCoordinator 为缺少 fetch position 的 partition 拉取 position (即 last committed offset)。 继而，仍不知道 partition 的 position (_consumer_offsets 中未保存位移信息)，且设置了 offsetreset 策略，那么就等待重置，如果没有设置重置策略，就抛出 NoOffsetForPartitionException 异常。 最后，为那些需要重置 fetch position 的 partition 发送 ListOffsetRequest 重置 position (consumer.beginningOffsets ()，consumer.endOffsets ()，consumer.offsetsForTimes ()，consumer.seek () 都会发送 ListOffRequest 请求)。 上面说的几个方法相当于都是用户自己自定义消费的 offset，所以可能出现越界 (消费位置无法在实际分区中查到) 的情况，所以也是会发送 ListOffsetRequest 请求的，即触发 auto.offset.reset 参数的执行。比如现在某个 partition 的可拉取 offset 最大值为 100，如果你指定消费 offset=200 的位置，那肯定拉取不到，此时就会根据 auto.offset.reset 策略将拉取位置重置为 100 (默认的 auto.offset.reset 为 latest)。 refreshCommittedOffsetsIfNeeded我们先看下 Setp 2 中 GroupCoordinator 是如何 fetch position 的： 1234567891011121314151617181920212223242526272829/** * Refresh the committed offsets for provided partitions. * * @param timer Timer bounding how long this method can block * @return true iff the operation completed within the timeout */public boolean refreshCommittedOffsetsIfNeeded(Timer timer) &#123; final Set&lt;TopicPartition&gt; missingFetchPositions = subscriptions.missingFetchPositions(); // 1.发送获取offset的请求，核心步骤 final Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = fetchCommittedOffsets(missingFetchPositions, timer); if (offsets == null) return false; for (final Map.Entry&lt;TopicPartition, OffsetAndMetadata&gt; entry : offsets.entrySet()) &#123; final TopicPartition tp = entry.getKey(); // 2.获取response中的offset final OffsetAndMetadata offsetAndMetadata = entry.getValue(); final ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.leaderAndEpoch(tp); final SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(), leaderAndEpoch); log.info(&quot;Setting offset for partition &#123;&#125; to the committed offset &#123;&#125;&quot;, tp, position); entry.getValue().leaderEpoch().ifPresent(epoch -&gt; this.metadata.updateLastSeenEpochIfNewer(entry.getKey(), epoch)); // 3.实际就是设置SubscriptionState的position值 this.subscriptions.seekUnvalidated(tp, position); &#125; return true;&#125; fetchCommittedOffsets () 方法的核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Fetch the current committed offsets from the coordinator for a set of partitions. * * @param partitions The partitions to fetch offsets for * @return A map from partition to the committed offset or null if the operation timed out */public Map&lt;TopicPartition, OffsetAndMetadata&gt; fetchCommittedOffsets(final Set&lt;TopicPartition&gt; partitions, final Timer timer) &#123; if (partitions.isEmpty()) return Collections.emptyMap(); final Generation generation = generation(); if (pendingCommittedOffsetRequest != null &amp;&amp; !pendingCommittedOffsetRequest.sameRequest(partitions, generation)) &#123; // if we were waiting for a different request, then just clear it. pendingCommittedOffsetRequest = null; &#125; do &#123; if (!ensureCoordinatorReady(timer)) return null; // contact coordinator to fetch committed offsets final RequestFuture&lt;Map&lt;TopicPartition, OffsetAndMetadata&gt;&gt; future; if (pendingCommittedOffsetRequest != null) &#123; future = pendingCommittedOffsetRequest.response; &#125; else &#123; // 1.封装FetchRequest请求 future = sendOffsetFetchRequest(partitions); pendingCommittedOffsetRequest = new PendingCommittedOffsetRequest(partitions, generation, future); &#125; // 2.通过KafkaClient发送请求 client.poll(future, timer); if (future.isDone()) &#123; pendingCommittedOffsetRequest = null; if (future.succeeded()) &#123; // 3.请求成功，获取请求的响应数据 return future.value(); &#125; else if (!future.isRetriable()) &#123; throw future.exception(); &#125; else &#123; timer.sleep(retryBackoffMs); &#125; &#125; else &#123; return null; &#125; &#125; while (timer.notExpired()); return null;&#125; 上面的步骤和我们之前提到的发送其他请求毫无区别，基本就是这三个套路。 在获取到响应之后，会通过 subscriptions.seekUnvalidated () 方法为每个 TopicPartition 设置 position 值后，就知道从哪里开始消费订阅 topic 下的 partition 了。 resetMissingPositions在 Step 3 中，什么时候发起 FetchRequest 拿不到 position 呢？ 我们知道消费位移 (consume offset) 是保存在 _consumer_offsets 这个 topic 里面的，当我们进行消费的时候需要知道上次消费到了什么位置。那么就会发起请求去看上次消费到了 topic 的 partition 的哪个位置，但是这个消费位移是有保存时长的，默认为 7 天 (broker 端通过 offsets.retention.minutes 设置)。 当隔了一段时间再进行消费，如果这个间隔时间超过了参数的配置值，那么原先的位移信息就会丢失，最后只能通过客户端参数 auto.offset.reset 来确定开始消费的位置。 如果我们第一次消费 topic，那么在 _consumer_offsets 中也是找不到消费位移的，所以就会执行第四个步骤，发起 ListOffsetRequest 请求根据配置的 reset 策略 (即 auto.offset.reset) 来决定开始消费的位置。 resetOffsetsIfNeeded在 Step 4 中，发起 ListOffsetRequest 请求和处理 response 的核心代码如下： 123456789101112131415161718192021222324252627/** * Reset offsets for all assigned partitions that require it. * * @throws org.apache.kafka.clients.consumer.NoOffsetForPartitionException If no offset reset strategy is defined * and one or more partitions aren&#x27;t awaiting a seekToBeginning() or seekToEnd(). */public void resetOffsetsIfNeeded() &#123; // Raise exception from previous offset fetch if there is one RuntimeException exception = cachedListOffsetsException.getAndSet(null); if (exception != null) throw exception; // 1.需要执行reset策略的partition Set&lt;TopicPartition&gt; partitions = subscriptions.partitionsNeedingReset(time.milliseconds()); if (partitions.isEmpty()) return; final Map&lt;TopicPartition, Long&gt; offsetResetTimestamps = new HashMap&lt;&gt;(); for (final TopicPartition partition : partitions) &#123; Long timestamp = offsetResetStrategyTimestamp(partition); if (timestamp != null) offsetResetTimestamps.put(partition, timestamp); &#125; // 2.执行reset策略 resetOffsetsAsync(offsetResetTimestamps);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940private void resetOffsetsAsync(Map&lt;TopicPartition, Long&gt; partitionResetTimestamps) &#123; Map&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; timestampsToSearchByNode = groupListOffsetRequests(partitionResetTimestamps, new HashSet&lt;&gt;()); for (Map.Entry&lt;Node, Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt;&gt; entry : timestampsToSearchByNode.entrySet()) &#123; Node node = entry.getKey(); final Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; resetTimestamps = entry.getValue(); subscriptions.setNextAllowedRetry(resetTimestamps.keySet(), time.milliseconds() + requestTimeoutMs); // 1.发送ListOffsetRequest请求 RequestFuture&lt;ListOffsetResult&gt; future = sendListOffsetRequest(node, resetTimestamps, false); // 2.为ListOffsetRequest请求添加监听器 future.addListener(new RequestFutureListener&lt;ListOffsetResult&gt;() &#123; @Override public void onSuccess(ListOffsetResult result) &#123; if (!result.partitionsToRetry.isEmpty()) &#123; subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs); metadata.requestUpdate(); &#125; for (Map.Entry&lt;TopicPartition, ListOffsetData&gt; fetchedOffset : result.fetchedOffsets.entrySet()) &#123; TopicPartition partition = fetchedOffset.getKey(); ListOffsetData offsetData = fetchedOffset.getValue(); ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition); // 3.发送ListOffsetRequest请求成功，对结果reset，如果reset策略设置的是latest，那么requestedReset.timestamp = -1，如果是earliest，requestedReset.timestamp = -2 resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData); &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; subscriptions.requestFailed(resetTimestamps.keySet(), time.milliseconds() + retryBackoffMs); metadata.requestUpdate(); if (!(e instanceof RetriableException) &amp;&amp; !cachedListOffsetsException.compareAndSet(null, e)) log.error(&quot;Discarding error in ListOffsetResponse because another error is pending&quot;, e); &#125; &#125;); &#125;&#125; sendListOffsetRequest () 方法的核心代码如下： 1234567891011121314151617181920212223242526/** * Send the ListOffsetRequest to a specific broker for the partitions and target timestamps. * * @param node The node to send the ListOffsetRequest to. * @param timestampsToSearch The mapping from partitions to the target timestamps. * @param requireTimestamp True if we require a timestamp in the response. * @return A response which can be polled to obtain the corresponding timestamps and offsets. */private RequestFuture&lt;ListOffsetResult&gt; sendListOffsetRequest(final Node node, final Map&lt;TopicPartition, ListOffsetRequest.PartitionData&gt; timestampsToSearch, boolean requireTimestamp) &#123; ListOffsetRequest.Builder builder = ListOffsetRequest.Builder .forConsumer(requireTimestamp, isolationLevel) .setTargetTimes(timestampsToSearch); log.debug(&quot;Sending ListOffsetRequest &#123;&#125; to broker &#123;&#125;&quot;, builder, node); return client.send(node, builder) .compose(new RequestFutureAdapter&lt;ClientResponse, ListOffsetResult&gt;() &#123; @Override public void onSuccess(ClientResponse response, RequestFuture&lt;ListOffsetResult&gt; future) &#123; ListOffsetResponse lor = (ListOffsetResponse) response.responseBody(); log.trace(&quot;Received ListOffsetResponse &#123;&#125; from broker &#123;&#125;&quot;, lor, node); handleListOffsetResponse(timestampsToSearch, lor, future); &#125; &#125;);&#125; resetOffsetIfNeeded () 方法的核心代码如下： 1234567private void resetOffsetIfNeeded(TopicPartition partition, OffsetResetStrategy requestedResetStrategy, ListOffsetData offsetData) &#123; SubscriptionState.FetchPosition position = new SubscriptionState.FetchPosition( offsetData.offset, offsetData.leaderEpoch, metadata.leaderAndEpoch(partition)); offsetData.leaderEpoch.ifPresent(epoch -&gt; metadata.updateLastSeenEpochIfNewer(partition, epoch)); // reset对应的TopicPartition fetch的position subscriptions.maybeSeekUnvalidated(partition, position.offset, requestedResetStrategy);&#125; 这里解释下 auto.offset.reset 的两个值 (latest 和 earliest) 的区别： 假设我们现在要消费 MyConsumerTopic 的数据，它有 3 个分区，生产者往这个 topic 发送了 10 条数据，然后分区数据按照 MyConsumerTopic-0 (3 条数据)，MyConsumerTopic-1 (3 条数据)，MyConsumerTopic-2 (4 条数据) 这样分配。 当设置为 latest 的时候，返回的 offset 具体到每个 partition 就是 HW 值 (partition 0 是 3，partition 1 是 3，partition 2 是 4)。 当设置为 earliest 的时候，就会从起始处 (即 LogStartOffset，注意不是 LSO) 开始消费，这里就是从 0 开始。 Log Start Offset：表示 partition 的起始位置，初始值为 0，由于消息的增加以及日志清除策略影响，这个值会阶段性增大。尤其注意这个不能缩写为 LSO，LSO 代表的是 LastStableOffset，和事务有关。 Consumer Offset：消费位移，表示 partition 的某个消费者消费到的位移位置。 High Watermark：简称 HW，代表消费端能看到的 partition 的最高日志位移，HW 大于等于 ConsumerOffset 的值。 Log End Offset：简称 LEO，代表 partition 的最高日志位移，对消费者不可见，HW 到 LEO 这之间的数据未被 follwer 完全同步。 至此，我们成功的知道 consumer 消费的 partition 的 offset 位置在哪里，下面就开始拉取 partition 里的数据。 pollForFetches - 拉取数据现在万事俱备只欠东风了，consumer 成功加入 group，也确定了需要拉取的 topic partition 的 offset，那么现在就应该去拉取数据了，其核心源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) &#123; long pollTimeout = coordinator == null ? timer.remainingMs() : Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs()); // if data is available already, return it immediately // 1.获取fetcher已经拉取到的数据 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords(); if (!records.isEmpty()) &#123; return records; &#125; // 到此，说明上次fetch到的数据已经全部拉取了，需要再次发送fetch请求，从broker拉取新的数据 // send any new fetches (won&#x27;t resend pending fetches) // 2.发送fetch请求，会从多个topic-partition拉取数据(只要对应的topic-partition没有未完成的请求) fetcher.sendFetches(); // We do not want to be stuck blocking in poll if we are missing some positions // since the offset lookup may be backing off after a failure // NOTE: the use of cachedSubscriptionHashAllFetchPositions means we MUST call // updateAssignmentMetadataIfNeeded before this method. if (!cachedSubscriptionHashAllFetchPositions &amp;&amp; pollTimeout &gt; retryBackoffMs) &#123; pollTimeout = retryBackoffMs; &#125; Timer pollTimer = time.timer(pollTimeout); // 3.真正开始发送，底层同样使用NIO client.poll(pollTimer, () -&gt; &#123; // since a fetch might be completed by the background thread, we need this poll condition // to ensure that we do not block unnecessarily in poll() return !fetcher.hasCompletedFetches(); &#125;); timer.update(pollTimer.currentTimeMs()); // after the long poll, we should check whether the group needs to rebalance // prior to returning data so that the group can stabilize faster // 4.如果group需要rebalance，直接返回空数据，这样更快地让group进入稳定状态 if (coordinator != null &amp;&amp; coordinator.rejoinNeededOrPending()) &#123; return Collections.emptyMap(); &#125; // 5.返回拉取到的新数据 return fetcher.fetchedRecords();&#125; fetcher.sendFetches这里需要注意的是 fetcher.sendFetches () 方法，在发送请求的同时会注册回调函数，当有 response 的时候，会解析 response，将返回的数据放到 Fetcher 的成员变量中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/** * Set-up a fetch request for any node that we have assigned partitions for which doesn&#x27;t already have * an in-flight fetch or pending fetch data. * @return number of fetches sent */public synchronized int sendFetches() &#123; // Update metrics in case there was an assignment change sensors.maybeUpdateAssignment(subscriptions); // 1.创建FetchRequest Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; fetchRequestMap = prepareFetchRequests(); for (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData&gt; entry : fetchRequestMap.entrySet()) &#123; final Node fetchTarget = entry.getKey(); final FetchSessionHandler.FetchRequestData data = entry.getValue(); final FetchRequest.Builder request = FetchRequest.Builder .forConsumer(this.maxWaitMs, this.minBytes, data.toSend()) .isolationLevel(isolationLevel) .setMaxBytes(this.maxBytes) .metadata(data.metadata()) .toForget(data.toForget()) .rackId(clientRackId); if (log.isDebugEnabled()) &#123; log.debug(&quot;Sending &#123;&#125; &#123;&#125; to broker &#123;&#125;&quot;, isolationLevel, data.toString(), fetchTarget); &#125; // 2.发送FetchRequest RequestFuture&lt;ClientResponse&gt; future = client.send(fetchTarget, request); // We add the node to the set of nodes with pending fetch requests before adding the // listener because the future may have been fulfilled on another thread (e.g. during a // disconnection being handled by the heartbeat thread) which will mean the listener // will be invoked synchronously. this.nodesWithPendingFetchRequests.add(entry.getKey().id()); future.addListener(new RequestFutureListener&lt;ClientResponse&gt;() &#123; @Override public void onSuccess(ClientResponse resp) &#123; synchronized (Fetcher.this) &#123; try &#123; @SuppressWarnings(&quot;unchecked&quot;) FetchResponse&lt;Records&gt; response = (FetchResponse&lt;Records&gt;) resp.responseBody(); FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler == null) &#123; log.error(&quot;Unable to find FetchSessionHandler for node &#123;&#125;. Ignoring fetch response.&quot;, fetchTarget.id()); return; &#125; if (!handler.handleResponse(response)) &#123; return; &#125; Set&lt;TopicPartition&gt; partitions = new HashSet&lt;&gt;(response.responseData().keySet()); FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions); for (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&lt;Records&gt;&gt; entry : response.responseData().entrySet()) &#123; TopicPartition partition = entry.getKey(); FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition); if (requestData == null) &#123; String message; if (data.metadata().isFull()) &#123; message = MessageFormatter.arrayFormat( &quot;Response for missing full request partition: partition=&#123;&#125;; metadata=&#123;&#125;&quot;, new Object[]&#123;partition, data.metadata()&#125;).getMessage(); &#125; else &#123; message = MessageFormatter.arrayFormat( &quot;Response for missing session request partition: partition=&#123;&#125;; metadata=&#123;&#125;; toSend=&#123;&#125;; toForget=&#123;&#125;&quot;, new Object[]&#123;partition, data.metadata(), data.toSend(), data.toForget()&#125;).getMessage(); &#125; // Received fetch response for missing session partition throw new IllegalStateException(message); &#125; else &#123; long fetchOffset = requestData.fetchOffset; FetchResponse.PartitionData&lt;Records&gt; fetchData = entry.getValue(); log.debug(&quot;Fetch &#123;&#125; at offset &#123;&#125; for partition &#123;&#125; returned fetch data &#123;&#125;&quot;, isolationLevel, fetchOffset, partition, fetchData); // 3.发送FetchRequest请求成功，将返回的数据放到ConcurrentLinkedQueue&lt;CompletedFetch&gt;中 completedFetches.add(new CompletedFetch(partition, fetchOffset, fetchData, metricAggregator, resp.requestHeader().apiVersion())); &#125; &#125; sensors.fetchLatency.record(resp.requestLatencyMs()); &#125; finally &#123; nodesWithPendingFetchRequests.remove(fetchTarget.id()); &#125; &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; synchronized (Fetcher.this) &#123; try &#123; FetchSessionHandler handler = sessionHandler(fetchTarget.id()); if (handler != null) &#123; handler.handleError(e); &#125; &#125; finally &#123; nodesWithPendingFetchRequests.remove(fetchTarget.id()); &#125; &#125; &#125; &#125;); &#125; return fetchRequestMap.size();&#125; 该方法主要分为以下两步： prepareFetchRequests ()：为订阅的所有 topic-partition list 创建 fetch 请求 (只要该 topic-partition 没有还在处理的请求)，创建的 fetch 请求依然是按照 node 级别创建的； client.send ()：发送 fetch 请求，并设置相应的 Listener，请求处理成功的话，就加入到 completedFetches 中，在加入这个 completedFetches 队列时，是按照 topic-partition 级别去加入，这样也就方便了后续的处理。 从这里可以看出，在每次发送 fetch 请求时，都会向所有可发送的 topic-partition 发送 fetch 请求，调用一次 fetcher.sendFetches，拉取到的数据，可能需要多次 pollForFetches 循环才能处理完，因为 Fetcher 线程是在后台运行，这也保证了尽可能少地阻塞用户的处理线程，因为如果 Fetcher 中没有可处理的数据，用户的线程是会阻塞在 poll 方法中的。 fetcher.fetchedRecords这个方法的作用就是获取已经从 server 拉取到的 Records，其核心源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117/** * Return the fetched records, empty the record buffer and update the consumed position. * * NOTE: returning empty records guarantees the consumed position are NOT updated. * * @return The fetched records per partition * @throws OffsetOutOfRangeException If there is OffsetOutOfRange error in fetchResponse and * the defaultResetPolicy is NONE * @throws TopicAuthorizationException If there is TopicAuthorization error in fetchResponse. */public Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords() &#123; Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetched = new HashMap&lt;&gt;(); // 在max.poll.records中设置单词最大的拉取条数，默认500条 int recordsRemaining = maxPollRecords; try &#123; while (recordsRemaining &gt; 0) &#123; if (nextInLineRecords == null || nextInLineRecords.isFetched) &#123;// nextInLineRecords为空时 // Step1:当一个nextInLineRecords处理完，就从completedFetches处理下一个完成的Fetch请求 CompletedFetch completedFetch = completedFetches.peek(); if (completedFetch == null) break; try &#123; // Step2:获取下一个要处理的nextInLineRecords nextInLineRecords = parseCompletedFetch(completedFetch); &#125; catch (Exception e) &#123; // Remove a completedFetch upon a parse with exception if (1) it contains no records, and // (2) there are no fetched records with actual content preceding this exception. // The first condition ensures that the completedFetches is not stuck with the same completedFetch // in cases such as the TopicAuthorizationException, and the second condition ensures that no // potential data loss due to an exception in a following record. FetchResponse.PartitionData partition = completedFetch.partitionData; if (fetched.isEmpty() &amp;&amp; (partition.records == null || partition.records.sizeInBytes() == 0)) &#123; completedFetches.poll(); &#125; throw e; &#125; completedFetches.poll(); &#125; else &#123; // Step3:拉取records，更新position List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = fetchRecords(nextInLineRecords, recordsRemaining); TopicPartition partition = nextInLineRecords.partition; if (!records.isEmpty()) &#123; List&lt;ConsumerRecord&lt;K, V&gt;&gt; currentRecords = fetched.get(partition); if (currentRecords == null) &#123;// 正常情况下，一个node只会发送一个request，一般只会有一个 fetched.put(partition, records); &#125; else &#123; // this case shouldn&#x27;t usually happen because we only send one fetch at a time per partition, // but it might conceivably happen in some rare cases (such as partition leader changes). // we have to copy to a new list because the old one may be immutable List&lt;ConsumerRecord&lt;K, V&gt;&gt; newRecords = new ArrayList&lt;&gt;(records.size() + currentRecords.size()); newRecords.addAll(currentRecords); newRecords.addAll(records); fetched.put(partition, newRecords); &#125; recordsRemaining -= records.size(); &#125; &#125; &#125; &#125; catch (KafkaException e) &#123; if (fetched.isEmpty()) throw e; &#125; // Step4:返回相应的Records数据 return fetched;&#125;private List&lt;ConsumerRecord&lt;K, V&gt;&gt; fetchRecords(PartitionRecords partitionRecords, int maxRecords) &#123; if (!subscriptions.isAssigned(partitionRecords.partition)) &#123; // this can happen when a rebalance happened before fetched records are returned to the consumer&#x27;s poll call log.debug(&quot;Not returning fetched records for partition &#123;&#125; since it is no longer assigned&quot;, partitionRecords.partition); &#125; else if (!subscriptions.isFetchable(partitionRecords.partition)) &#123; // this can happen when a partition is paused before fetched records are returned to the consumer&#x27;s // poll call or if the offset is being reset // 这个topic-partition不能被消费了，比如调用了pause log.debug(&quot;Not returning fetched records for assigned partition &#123;&#125; since it is no longer fetchable&quot;, partitionRecords.partition); &#125; else &#123; SubscriptionState.FetchPosition position = subscriptions.position(partitionRecords.partition); if (partitionRecords.nextFetchOffset == position.offset) &#123;// offset对的上，也就是拉取是按顺序拉的 // 获取该topic-partition对应的records，并更新partitionRecords的fetchOffset(用于判断是否顺序) List&lt;ConsumerRecord&lt;K, V&gt;&gt; partRecords = partitionRecords.fetchRecords(maxRecords); if (partitionRecords.nextFetchOffset &gt; position.offset) &#123; SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition( partitionRecords.nextFetchOffset, partitionRecords.lastEpoch, position.currentLeader); log.trace(&quot;Returning fetched records at offset &#123;&#125; for assigned partition &#123;&#125; and update &quot; + &quot;position to &#123;&#125;&quot;, position, partitionRecords.partition, nextPosition); // 更新消费到的offset(the fetch position) subscriptions.position(partitionRecords.partition, nextPosition); &#125; // 获取Lag(即position与hw之间差值)，hw为null时，才返回null Long partitionLag = subscriptions.partitionLag(partitionRecords.partition, isolationLevel); if (partitionLag != null) this.sensors.recordPartitionLag(partitionRecords.partition, partitionLag); Long lead = subscriptions.partitionLead(partitionRecords.partition); if (lead != null) &#123; this.sensors.recordPartitionLead(partitionRecords.partition, lead); &#125; return partRecords; &#125; else &#123; // these records aren&#x27;t next in line based on the last consumed position, ignore them // they must be from an obsolete request log.debug(&quot;Ignoring fetched records for &#123;&#125; at offset &#123;&#125; since the current position is &#123;&#125;&quot;, partitionRecords.partition, partitionRecords.nextFetchOffset, position); &#125; &#125; partitionRecords.drain(); return emptyList();&#125; consumer 的 Fetcher 处理从 server 获取的 fetch response 大致分为以下几个过程： 通过 completedFetches.peek() 获取已经成功的 fetch response (在 fetcher.sendFetches () 方法中会把发送FetchRequest请求成功后的结果放在这个集合中，是拆分为 topic-partition 的粒度放进去的)； parseCompletedFetch() 处理上面获取的 completedFetch，构造成 PartitionRecords 类型； 通过 fetchRecords() 方法处理 PartitionRecords 对象，在这个里面会去验证 fetchOffset 是否能对得上，只有 fetchOffset 是一致的情况下才会去处理相应的数据，并更新 the fetch offset 的信息，如果 fetchOffset 不一致，这里就不会处理，the fetch offset 就不会更新，下次 fetch 请求时是会接着 the fetch offset 的位置去请求相应的数据； 返回相应的 Records 数据。 至此，KafkaConsumer 如何拉取消息的整体流程也分析完毕。 本文参考http://generalthink.github.io/2019/05/31/kafka-consumer-offset/ https://matt33.com/2017/11/11/consumer-pollonce/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 &#119;&#x64;&#115;&#x68;&#102;&#117;&#x74;&#64;&#49;&#x36;&#x33;&#46;&#99;&#x6f;&#109;","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"欧阳修","slug":"ouyangxiu","date":"2020-11-10T00:51:06.000Z","updated":"2020-11-23T08:48:11.442Z","comments":true,"path":"2020/11/10/ouyangxiu/","link":"","permalink":"http://example.com/2020/11/10/ouyangxiu/","excerpt":"","text":"伐树记署之东园，久茀不治。修至始辟之，粪瘠溉枯，为蔬圃十数畦，又植花果桐竹凡百本。 春阳既浮，萌者将动。园之守启曰：“园有樗焉，其根壮而叶大。根壮则梗地脉，耗阳气，而新植者不得滋；叶大则阴翳蒙碍，而新植者不得畅以茂。又其材拳曲臃肿，疏轻而不坚，不足养，是宜伐。”因尽薪之。明日，圃之守又曰：“圃之南有杏焉，凡其根庇之广可六七尺，其下之地最壤腴，以杏故，特不得蔬，是亦宜薪。”修曰：“噫！今杏方春且华，将待其实，若独不能损数畦之广为杏地邪？“因勿伐。 既而悟且叹曰：“吁！庄周之说曰：樗、栎以不材终其天年，桂、漆以有用而见伤夭。今樗诚不材矣，然一旦悉翦弃；杏之体最坚密，美泽可用，反见存。岂才不才各遭其时之可否邪？” 他日，客有过修者。仆夫曳薪过堂下，因指而语客以所疑。客曰： “是何怪邪？夫以无用处无用，庄周之贵也。以无用而贼有用，乌能免哉！彼杏之有华实也，以有生之具而庇其根，幸矣。若桂、漆之不能逃乎斤斧者，盖有利之者在死，势不得以生也，与乎杏实异矣。今樗之臃肿不材，而以壮大害物，其见伐，诚宜尔。与夫‘才者死、不才者生’之说，又异矣。凡物幸之与不幸，视其处之而已。”客既去，修善其言而记之。 非非堂记权衡之平物，动则轻重差，其于静也，锱铢不失。水之鉴物，动则不能有睹，其于静也，毫发可辨。在乎人，耳司听，目司视，动则乱于聪明，其于静也，闻见必审。处身者不为外物眩晃而动，则其心静，心静则智识明，是是非非，无所施而不中。夫是是近乎谄，非非近乎讪，不幸而过，宁讪无谄。是者，君子之常，是之何加？一以视之，未若非非之为正也。 予居洛之明年，既新厅事，有文纪于壁末。营其西偏作堂，户北向，植丛竹，辟户于其南，纳日月之光。设一几一榻，架书数百卷，朝夕居其中。以其静也，闭目澄心，览今照古，思虑无所不至焉。故其堂以非非为名云。 浪淘沙 · 把酒祝东风把酒祝东风，且共从容。垂杨紫陌洛城东。总是当时携手处，游遍芳丛。 聚散苦匆匆，此恨无穷。今年花胜去年红。可惜明年花更好，知与谁同？ 玉楼春 · 尊前拟把归期说尊前拟把归期说，欲语春容先惨咽。人生自是有情痴，此恨不关风与月。 离歌且莫翻新阕，一曲能教肠寸结。直须看尽洛城花，始共春风容易别。 生查子 · 元夕去年元夜时，花市灯如昼。 月上柳梢头，人约黄昏后。 今年元夜时，月与灯依旧。 不见去年人，泪满春衫袖。 读李翱文予始读翱《复性书》三篇，曰：此《中庸》之义疏尔。智者诚其性，当读《中庸》；愚者虽读此不晓也，不作可焉。又读《与韩侍郎荐贤书》，以谓翱特穷时愤世无荐己者，故丁宁如此；使其得志，亦未必。然以韩为秦汉间好侠行义之一豪俊，亦善论人者也。最后读《幽怀赋》，然后置书而叹，叹已复读，不自休。恨翱不生于今，不得与之交；又恨予不得生翱时，与翱上下其论也。 凡昔翱一时人，有道而能文者，莫若韩愈。愈尝有赋矣，不过羡二鸟之光荣，叹一饱之无时尔。此其心使光荣而饱，则不复云矣。若翱独不然，其赋曰：“众嚣嚣而杂处兮，咸叹老而嗟卑；视予心之不然兮，虑行道之犹非。”又怪神尧以一旅取天下，后世子孙不能以天下取河北，以为忧。呜呼！使当时君子皆易其叹老嗟卑之心为翱所忧之心，则唐之天下岂有乱与亡哉！ 然翱幸不生今时，见今之事，则其忧又甚矣。奈何今之人不忧也？余行天下，见人多矣，脱有一人能如翱忧者，又皆贱远，与翱无异；其余光荣而饱者，一闻忧世之言，不以为狂人，则以为病痴子，不怒则笑之矣。呜呼，在位而不肯自忧，又禁他人使皆不得忧，可叹也夫! 景祐三年十月十七日，欧阳修书。 答吴充秀才书修顿首白，先辈吴君足下。前辱示书及文三篇，发而读之，浩乎若千万言之多，及少定而视焉，才数百言尔。非夫辞丰意雄，沛然有不可御之势，何以至此！然犹自患伥伥莫有开之使前者，此好学之谦言也。 修材不足用于时，仕不足荣于世，其毁誉不足轻重，气力不足动人。世之欲假誉以为重，借力而后进者，奚取于修焉？先辈学精文雄，其施于时，又非待修誉而为重，力而后进者也。然而惠然见临，若有所责，得非急于谋道，不择其人而问焉者欤？ 夫学者未始不为道，而至者鲜焉；非道之于人远也，学者有所溺焉尔。盖文之为言，难工而可喜，易悦而自足。世之学者往往溺之，一有工焉，则曰：“吾学足矣。“甚者至弃百事不关于心，曰：“吾文士也，职于文而已。”此其所以至之鲜也。 昔孔子老而归鲁，六经之作，数年之顷尔。然读《易》者如无《春秋》，读《书》者如无《诗》，何其用功少而至于至也！圣人之文虽不可及，然大抵道胜者，文不难而自至也。故孟子皇皇不暇著书，荀卿盖亦晚而有作。若子云、仲淹，方勉焉以模言语，此道未足而强言者也。后之惑者，徒见前世之文传，以为学者文而已，故愈力愈勤而愈不至。此足下所谓”终日不出于轩序，不能纵横高下皆如意“者，道未足也。若道之充焉，虽行乎天地，入于渊泉，无不之也。 先辈之文浩乎沛然，可谓善矣。而又志于为道，犹自以为未广。若不止焉，孟、荀可至而不难也。修，学道而不至者，然幸不甘于所悦而溺于所止。因吾子之能不自止，又以励修之少进焉。幸甚！幸甚！修白。 答祖择之书修启。秀才人至，蒙示书一通，并诗赋杂文两策，谕之曰：“一览以为如何？”某既陋，不足以辱好学者之问；又其少贱而长穷，其素所为未有足称以取信于人。亦尝有人问者，以不足问之愚，而未尝答人之问。足下卒然及之，是以愧惧不知所言。虽然，不远数百里走使者以及门，意厚礼勤，何敢不报。 某闻古之学者必严其师，师严然后道尊，道尊然后笃敬，笃敬然后能自守，能自守然后果于用，果于用然后不畏而不迁。三代之衰，学校废。至两汉，师道尚存，故其学者各守其经以自用。是以汉之政理文章与其当时之事，后世莫及者，其所从来深矣。后世师，法渐坏，而今世无师，则学者不尊严，故自轻其道。轻之则不能至，不至则不能笃信，信不笃则不知所守，守不固则有所畏而物可移。是故学者惟俯仰徇时，以希禄利为急，至于忘本趋末，流而不返。夫以不信不固之心，守不至之学，虽欲果于自用，而莫知其所以用之之道，又况有禄利之诱、刑祸之惧以迁之哉！此足下所谓志古知道之士世所鲜，而未有合者，由此也。 足下所为文，用意甚高，卓然有不顾世俗之心，直欲自到于古人。今世之人用心如足下者有几？是则乡曲之中能为足下之师者谓谁，交游之间能发足下之议论者谓谁？学不师则守不一，议论不博则无所发明而究其深。足下之言高趣远，甚善，然所守未一而议论未精，此其病也。窃惟足下之交游能为足下称才誉美者不少，今皆舍之，远而见及，乃知足下是欲求其不至。此古君子之用心也，是以言之不敢隐。 夫世无师矣，学者当师经，师经必先求其意，意得则心定，心定则道纯，道纯则充于中者实，中充实则发为文者辉光，施于世者果致。三代、两汉之学，不过此也。足下患世未有合者，而不弃其愚，将某以为合，故敢道此。未知足下之意合否？ 与荆南乐秀才书修顿首白秀才足下。前者舟行往来，屡辱见过。又辱以所业一编，先之启事，及门而贽。田秀才西来，辱书；其后予家奴自府还县，比又辱书。仆有罪之人，人所共弃，而足下见礼如此，何以当之？当之未暇答，宜遂绝，而再辱书；再而未答，益宜绝，而又辱之。何其勤之甚也！如修者，天下穷贱之人尔，安能使足下之切切如是邪？盖足下力学好问，急于自为谋而然也。然蒙索仆所为文字者，此似有所过听也。 仆少从进士举于有司，学为诗赋，以备程试，凡三举而得第。与士君子相识者多，故往往能道仆名字，而又以游从相爱之私，或过称其文字。故使足下闻仆虚名，而欲见其所为者，由此也。 仆少孤贫，贪禄仕以养亲，不暇就师穷经，以学圣人之遗业。而涉猎书史，姑随世俗作所谓时文者，皆穿蠹经传，移此俪彼，以为浮薄，惟恐不悦于时人，非有卓然自立之言如古人者。然有司过采，屡以先多士。及得第已来，自以前所为不足以称有司之举而当长者之知，始大改其为，庶几有立。然言出而罪至，学成而身辱，为彼则获誉，为此则受祸，此明效也。 夫时文虽曰浮巧，然其为功，亦不易也。仆天姿不好而强为之，故比时人之为者尤不工，然已足以取禄仕而窃名誉者，顺时故也。先辈少年志盛，方欲取荣誉于世，则莫若顺时。天圣中，天子下诏书，敕学者去浮华，其后风俗大变。今时之士大夫所为，彬彬有两汉之风矣。先辈往学之，非徒足以顺时取誉而已，如其至之，是直齐肩于两汉之士也。若仆者，其前所为既不足学，其后所为慎不可学，是以徘徊不敢出其所为者，为此也。 在《易》之《困》曰：“有言不信。”谓夫人方困时，其言不为人所信也。今可谓困矣，安足为足下所取信哉？辱书既多且切，不敢不答。幸察。 答李诩第一书修白。人至，辱书及《性诠》三篇，曰以质其果是。夫自信笃者，无所待于人；有质于人者，自疑者也。今吾子自谓“夫子与孟、荀、扬、韩复生，不能夺吾言”，其可谓自信不疑者矣。而返以质于修。使修有过于夫子者，乃可为吾子辩，况修未及孟、荀、扬、韩之一二也。修非知道者，好学而未至者也。世无师久矣，尚赖朋友切磋之益，苟不自满而中止，庶几终身而有成。固常乐与学者论议往来，非敢以益于人，盖求益于人者也。况如吾子之文章论议，岂易得哉？固乐为吾子辩也。苟尚有所疑，敢不尽其所学以告，既吾子自信如是，虽夫子不能夺，使修何所说焉？人还索书，未知所答，惭惕惭惕。修再拜。 答李诩第二书修白。前辱示书及《性诠》三篇，见吾子好学善辩，而文能尽其意之详。令世之言性者多矣，有所不及也，故思与吾子卒其说。 修患世之学者多言性，故常为说曰“夫性，非学者之所急，而圣人之所罕言也。《易》六十四卦不言性，其言者动静得失吉凶之常理也；《春秋》二百四十二年不言性，其言者善恶是非之实录也；《诗》三百五篇不言性，其言者政教兴衰之美刺也；《书》五十九篇不言性，其言者尧、舜、三代之治乱也；《礼》、《乐》之书虽不完，而杂出于诸儒之记，然其大要，治国修身之法也。六经之所载，皆人事之切于世者，是以言之甚详。至于性也，百不一二言之，或因言而及焉，非为性而言也，故虽言而不究。 予之所谓不言者，非谓绝而无言，盖其言者鲜，而又不主于性而言也。《论语》所载七十二子之问于孔子者，问孝、问忠、问仁义、问礼乐、问修身、问为政、问朋友、问鬼神者有矣，未尝有问性者。孔子之告其弟子者，凡数千言，其及于性者一言而已。予故曰：非学者之所急，而圣人之罕言也。 《书》曰“习与性成”，《语》曰“性相近，习相远”者，戒人慎所习而言也。《中庸》曰“天命之谓性，率性之谓道”者，明性无常，必有以率之也。《乐记》亦曰“感物而动，性之欲”者，明物之感人无不至也。然终不言性果善果恶，但戒人慎所习与所感，而勤其所以率之者尔。予故曰“因言以及之，而不究也。 修少好学，知学之难。凡所谓六经之所载，七十二子之所问者，学之终身，有不能达者矣；于其所达，行之终身，有不能至者矣。以予之汲汲于此而不暇乎其他，因以知七十二子亦以是汲汲而不暇也，又以知圣人所以教人垂世，亦皇皇而不暇也。今之学者于古圣贤所皇皇汲汲者，学之行之，或未至其一二，而好为性说，以穷圣贤之所罕言而不究者，执后儒之偏说，事无用之空言，此予之所不暇也。 或有问曰：性果不足学乎？予曰：性者，与身俱生而人之所皆有也。为君子者，修身治人而已，性之善恶不必究也。使性果善邪，身不可以不修，人不可以不治；使性果恶邪，身不可以不修，人不可以不治。不修其身，虽君子而为小人，《书》曰“惟圣罔念作狂”是也；能修其身，虽小人而为君子，《书》曰“惟狂克念作圣”是也。治道备，人斯为善矣，《书》曰“黎民于变时雍”是也；治道失，人斯为恶矣，《书》曰“殷顽民”，又曰“旧染污俗”是也。故为君子者，以修身治人为急，而不穷性以为言。夫七十二子之不问，六经之不主言，或虽言而不究，岂略之哉，盖有意也。 或又问曰：然则三子言性，过欤？曰：不过也。其不同何也？曰：始异而终同也。使孟子曰人性善矣，遂怠而不教，则是过也；使荀子曰人性恶矣，遂弃而不教，则是过也；使扬子曰人性混矣，遂肆而不教，则是过也。然三子者，或身奔走诸侯以行其道，或著书累千万言以告于后世，未尝不区区以仁义礼乐为急。盖其意以谓善者一日不教，则失而入于恶；恶者勤而教之，则可使至于善；混者驱而率之，则可使去恶而就善也。其说与《书》之“习与性成”，《语》之“性近习远”，《中庸》之“有以率之”，《乐记》之“慎物所感”皆合。夫三子者，推其言则殊，察其用心则一，故予以为推其言不过始异而终同也。凡论三子者，以予言而一之，则譊譊者可以息矣。 予之所说如此，吾子其择焉。 醉翁亭记环滁皆山也。其西南诸峰，林壑尤美，望之蔚然而深秀者，琅琊也。山行六七里，渐闻水声潺潺，而泻出于两峰之间者，酿泉也。峰回路转，有亭翼然临于泉上者，醉翁亭也。作亭者谁？山之僧智仙也。名之者谁？太守自谓也。太守与客来饮于此，饮少辄醉，而年又最高，故自号曰醉翁也。醉翁之意不在酒，在乎山水之间也。山水之乐，得之心而寓之酒也。 若夫日出而林霏开，云归而岩穴暝，晦明变化者，山间之朝暮也。野芳发而幽香，佳木秀而繁阴，风霜高洁，水落而石出者，山间之四时也。朝而往，暮而归，四时之景不同，而乐亦无穷也。 至于负者歌于途，行者休于树，前者呼，后者应，伛偻提携，往来而不绝者，滁人游也。临溪而渔，溪深而鱼肥。酿泉为酒，泉香而酒洌；山肴野蔌，杂然而前陈者，太守宴也。宴酣之乐，非丝非竹，射者中，弈者胜，觥筹交错，起坐而喧哗者，众宾欢也。苍颜白发，颓然乎其间者，太守醉也。 已而夕阳在山，人影散乱，太守归而宾客从也。树林阴翳，鸣声上下，游人去而禽鸟乐也。然而禽鸟知山林之乐，而不知人之乐；人知从太守游而乐，而不知太守之乐其乐也。醉能同其乐，醒能述以文者，太守也。太守谓谁？庐陵欧阳修也。 朝中措 · 送刘仲原甫出守维扬平山阑槛倚晴空，山色有无中。手种堂前垂柳，别来几度春风？ 文章太守，挥毫万字，一饮千钟。行乐直须年少，尊前看取衰翁。 夜行船 · 忆昔西都欢纵忆昔西都欢纵。自别后、有谁能共。伊川山水洛川花，细寻思、旧游如梦。 今日相逢情愈重。愁闻唱、画楼钟动。白发天涯逢此景，倒金尊、殢谁相送。 伶官传序呜呼！盛衰之理，虽曰天命，岂非人事哉！原庄宗之所以得天下，与其所以失之者，可以知之矣。 世言晋王之将终也，以三矢赐庄宗而告之曰：“梁，吾仇也；燕王，吾所立，契丹，与吾约为兄弟，而皆背晋以归梁。此三者，吾遗恨也。与尔三矢，尔其无忘乃父之志！”庄宗受而藏之于庙。其后用兵，则遣从事以一少牢告庙，请其矢，盛以锦囊，负而前驱，及凯旋而纳之。 方其系燕父子以组，函梁君臣之首，入于太庙，还矢先王，而告以成功，其意气之盛，可谓壮哉！及仇雠已灭，天下已定，一夫夜呼，乱者四应，仓皇东出，未及见贼而士卒离散，君臣相顾，不知所归。至于誓天断发，泣下沾襟，何其衰也！岂得之难而失之易欤？抑本其成败之迹，而皆自于人欤？ 《书》曰：“满招损，谦得益。”忧劳可以兴国，逸豫可以亡身，自然之理也。故方其盛也，举天下之豪杰莫能与之争；及其衰也，数十伶人困之，而身死国灭，为天下笑。夫祸患常积于忽微，而智勇多困于所溺，岂独伶人也哉！作《伶官传》。","categories":[],"tags":[{"name":"Poetry and Prose","slug":"Poetry-and-Prose","permalink":"http://example.com/tags/Poetry-and-Prose/"}]},{"title":"KafkaConsumer 源码之 consumer 如何加入 consumer group","slug":"kafka-consumer-group","date":"2020-11-04T01:35:04.000Z","updated":"2020-11-25T07:56:47.479Z","comments":true,"path":"2020/11/04/kafka-consumer-group/","link":"","permalink":"http://example.com/2020/11/04/kafka-consumer-group/","excerpt":"","text":"Kafka 的 consumer 比 producer 要复杂许多，producer 没有 group 的概念，也不需要关注 offset，而 consumer 不一样，它有组织 (consumer group)，有纪律 (offset)。这些对 consumer 的要求就会很高，这篇文章就主要介绍 consumer 是如何加入 consumer group 的。 在这之前，我们需要先了解一下什么是 GroupCoordinator。简单地说，GroupCoordinator 是运行在服务器上的一个服务，Kafka 集群上的每一个 broker 节点启动的时候，都会启动一个 GroupCoordinator 服务，其功能是负责进行 consumer 的 group 成员与 offset 管理 (但每个 GroupCoordinator 只是管理一部分的 consumer group member 和 offset 信息)。 consumer group 对应的 GroupCoordinator 节点的确定，会通过如下方式： 将 consumer group 的 group.id 进行 hash，把得到的值的绝对值，对 _consumer_offsets 的 partition 总数取余，然后得到其对应的 partition 值，该 partition 的 leader 所在的 broker 即为该 consumer group 所对应的 GroupCoordinator 节点，GroupCoordinator 会存储与该 consumer group 相关的所有的 Meta 信息。 1._consumer_offsets 这个 topic 是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有 50 个 partition，每个 partition 默认有三个副本。 2.partition 计算方式：abs(GroupId.hashCode()) % NumPartitions，其中，NumPartitions 是 _consumer_offsets 的 partition 数，默认是 50 个。 3.比如，现在通过计算 abs(GroupId.hashCode()) % NumPartitions 的值为 35，然后就找第 35 个 partition 的 leader 在哪个 broker 上 (假设在 192.168.1.12)，那么 GroupCoordinator 节点就在这个 broker 上。 同时，这个 consumer group 所提交的消费 offset 信息也会发送给这个 partition 的 leader 所对应的 broker 节点，因此，这个节点不仅是 GroupCoordinator，而且还保存分区分配方案和组内消费者 offset 信息。 更多关于 GroupCoordinator 的解析，参考：Kafka 源码解析之 GroupCoordinator 详解。 KafkaConsumer 消费消息的主体流程接下来，我们回顾下 KafkaConsumer 消费消息的主体流程： 12345678// 创建消费者KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props);// 订阅主题kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;))// 从服务器拉取数据ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(100)); 创建 KafkaConsumer创建 KafkaConsumer 的时候，会创建一个 ConsumerCoordinator 服务，由它来负责和 GroupCoordinator 通信： 1234567891011121314151617181920// no coordinator will be constructed for the default (null) group idthis.coordinator = groupId == null ? null : new ConsumerCoordinator(logContext, this.client, groupId, this.groupInstanceId, maxPollIntervalMs, sessionTimeoutMs, new Heartbeat(time, sessionTimeoutMs, heartbeatIntervalMs, maxPollIntervalMs, retryBackoffMs), assignors, this.metadata, this.subscriptions, metrics, metricGrpPrefix, this.time, retryBackoffMs, enableAutoCommit, config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG), this.interceptors, config.getBoolean(ConsumerConfig.LEAVE_GROUP_ON_CLOSE_CONFIG)); 订阅 topicKafkaConsumer 订阅 topic 的方式有好几种，这在前面的文章有提到过。订阅的时候，会根据订阅的方式，设置其对应的订阅类型，默认存在四种订阅类型： 12345678910private enum SubscriptionType &#123; // 默认 NONE, // subscribe方式订阅 AUTO_TOPICS, // subscribe方式订阅 AUTO_PATTERN, // assign方式订阅 USER_ASSIGNED&#125; 比如，采用 kafkaConsumer.subscribe(Collections.singletonList(&quot;consumerCodeTopic&quot;)) 方式订阅 topic 时，会将订阅类型设置为 SubscriptionType.AUTO_TOPICS，其核心代码如下： 12345678910111213141516171819202122232425/** * Subscribe to the given list of topics to get dynamically assigned partitions. * &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; It is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * This is a short-hand for &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;, which * uses a no-op listener. If you need the ability to seek to particular offsets, you should prefer * &#123;@link #subscribe(Collection, ConsumerRebalanceListener)&#125;, since group rebalances will cause partition offsets * to be reset. You should also provide your own listener if you are doing your own offset * management since the listener gives you an opportunity to commit offsets before a rebalance finishes. * * @param topics The list of topics to subscribe to * @throws IllegalArgumentException If topics is null or contains null or empty elements * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics) &#123; subscribe(topics, new NoOpConsumerRebalanceListener());&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Subscribe to the given list of topics to get dynamically * assigned partitions. &lt;b&gt;Topic subscriptions are not incremental. This list will replace the current * assignment (if there is one).&lt;/b&gt; Note that it is not possible to combine topic subscription with group management * with manual partition assignment through &#123;@link #assign(Collection)&#125;. * * If the given list of topics is empty, it is treated the same as &#123;@link #unsubscribe()&#125;. * * &lt;p&gt; * As part of group management, the consumer will keep track of the list of consumers that belong to a particular * group and will trigger a rebalance operation if any one of the following events are triggered: * &lt;ul&gt; * &lt;li&gt;Number of partitions change for any of the subscribed topics * &lt;li&gt;A subscribed topic is created or deleted * &lt;li&gt;An existing member of the consumer group is shutdown or fails * &lt;li&gt;A new member is added to the consumer group * &lt;/ul&gt; * &lt;p&gt; * When any of these events are triggered, the provided listener will be invoked first to indicate that * the consumer&#x27;s assignment has been revoked, and then again when the new assignment has been received. * Note that rebalances will only occur during an active call to &#123;@link #poll(Duration)&#125;, so callbacks will * also only be invoked during that time. * * The provided listener will immediately override any listener set in a previous call to subscribe. * It is guaranteed, however, that the partitions revoked/assigned through this interface are from topics * subscribed in this call. See &#123;@link ConsumerRebalanceListener&#125; for more details. * * @param topics The list of topics to subscribe to * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the * subscribed topics * @throws IllegalArgumentException If topics is null or contains null or empty elements, or if listener is null * @throws IllegalStateException If &#123;@code subscribe()&#125; is called previously with pattern, or assign is called * previously (without a subsequent call to &#123;@link #unsubscribe()&#125;), or if not * configured at-least one partition assignment strategy */@Overridepublic void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; acquireAndEnsureOpen(); try &#123; maybeThrowInvalidGroupIdException(); if (topics == null) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot be null&quot;); if (topics.isEmpty()) &#123; // treat subscribing to empty topic list as the same as unsubscribing this.unsubscribe(); &#125; else &#123; for (String topic : topics) &#123; if (topic == null || topic.trim().isEmpty()) throw new IllegalArgumentException(&quot;Topic collection to subscribe to cannot contain null or empty topic&quot;); &#125; throwIfNoAssignorsConfigured(); fetcher.clearBufferedDataForUnassignedTopics(topics); log.info(&quot;Subscribed to topic(s): &#123;&#125;&quot;, Utils.join(topics, &quot;, &quot;)); if (this.subscriptions.subscribe(new HashSet&lt;&gt;(topics), listener)) metadata.requestUpdateForNewTopics(); &#125; &#125; finally &#123; release(); &#125;&#125; 12345public synchronized boolean subscribe(Set&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; registerRebalanceListener(listener); setSubscriptionType(SubscriptionType.AUTO_TOPICS); return changeSubscription(topics);&#125; 123456789101112/** * This method sets the subscription type if it is not already set (i.e. when it is NONE), * or verifies that the subscription type is equal to the give type when it is set (i.e. * when it is not NONE) * @param type The given subscription type */private void setSubscriptionType(SubscriptionType type) &#123; if (this.subscriptionType == SubscriptionType.NONE) this.subscriptionType = type; else if (this.subscriptionType != type) throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);&#125; 从服务器拉取数据订阅完成后，就可以从服务器拉取数据了，应该注意的是，KafkaConsumer 没有后台线程默默的拉取数据，它的所有行为都集中在 poll () 方法中，KafkaConsumer 是线程不安全的，同时只能允许一个线程运行。 kafkaConsumer.poll () 方法的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445private ConsumerRecords&lt;K, V&gt; poll(final Timer timer, final boolean includeMetadataInTimeout) &#123; // Step1:确认KafkaConsumer实例是单线程运行，以及没有被关闭 acquireAndEnsureOpen(); try &#123; if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123; throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;); &#125; // poll for new data until the timeout expires do &#123; client.maybeTriggerWakeup(); if (includeMetadataInTimeout) &#123; // Step2:更新metadata信息，获取GroupCoordinator的ip以及接口，并连接、 join-group、sync-group，期间group会进行rebalance。在此步骤，consumer会先加入group，然后获取需要消费的topic partition的offset信息 if (!updateAssignmentMetadataIfNeeded(timer)) &#123; return ConsumerRecords.empty(); &#125; &#125; else &#123; while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123; log.warn(&quot;Still waiting for metadata&quot;); &#125; &#125; // Step3:拉取数据 final Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer); if (!records.isEmpty()) &#123; // before returning the fetched records, we can send off the next round of fetches // and avoid block waiting for their responses to enable pipelining while the user // is handling the fetched records. // // NOTE: since the consumed position has already been updated, we must not allow // wakeups or any other errors to be triggered prior to returning the fetched records. if (fetcher.sendFetches() &gt; 0 || client.hasPendingRequests()) &#123; client.pollNoWakeup(); &#125; return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records)); &#125; &#125; while (timer.notExpired()); return ConsumerRecords.empty(); &#125; finally &#123; release(); &#125;&#125; 可以看出，在 Step 1 阶段， poll () 方法会先进行判定，如果有多个线程同时使用一个 KafkaConsumer 则会抛出异常： 1234567891011/** * Acquire the light lock and ensure that the consumer hasn&#x27;t been closed. * @throws IllegalStateException If the consumer has been closed */private void acquireAndEnsureOpen() &#123; acquire(); if (this.closed) &#123; release(); throw new IllegalStateException(&quot;This consumer has already been closed.&quot;); &#125;&#125; 123456789101112/** * Acquire the light lock protecting this consumer from multi-threaded access. Instead of blocking * when the lock is not available, however, we just throw an exception (since multi-threaded usage is not * supported). * @throws ConcurrentModificationException if another thread already has the lock */private void acquire() &#123; long threadId = Thread.currentThread().getId(); if (threadId != currentThread.get() &amp;&amp; !currentThread.compareAndSet(NO_CURRENT_THREAD, threadId)) throw new ConcurrentModificationException(&quot;KafkaConsumer is not safe for multi-threaded access&quot;); refcount.incrementAndGet();&#125; KafkaConsumer 如何加入 consumer group一个 KafkaConsumer 实例消费数据的前提是能够加入一个 consumer group 成功，并获取其要订阅的 tp（topic-partition）列表，因此首先要做的就是和 GroupCoordinator 建立连接，加入组织。 consumer 加入 group 的过程，也就是 reblance 的过程。如果出现了频繁 reblance 的问题，可能和 max.poll.interval.ms 和 max.poll.records 两个参数有关。 因此，我们先把目光集中在 ConsumerCoordinator 上，这个过程主要发生在 Step 2 阶段： 123456789101112/** * Visible for testing */boolean updateAssignmentMetadataIfNeeded(final Timer timer) &#123; // 1.本篇文章的内容主要集中在coordinator.poll(timer)方法源码分析(主要功能是:consumer加入group) if (coordinator != null &amp;&amp; !coordinator.poll(timer)) &#123; return false; &#125; // 2.updateFetchPositions(timer)方法留待下一篇文章分析(主要功能是:consumer获得partition的offset) return updateFetchPositions(timer);&#125; 关于对 ConsumerCoordinator 的处理都集中在 coordinator.poll () 方法中。其主要逻辑如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * Poll for coordinator events. This ensures that the coordinator is known and that the consumer * has joined the group (if it is using group management). This also handles periodic offset commits * if they are enabled. * (确保group的coordinator是已知的，并且这个consumer是已经加入到了group中，也用于offset周期性的commit) * &lt;p&gt; * Returns early if the timeout expires * * @param timer Timer bounding how long this method can block * @return true iff the operation succeeded */public boolean poll(Timer timer) &#123; maybeUpdateSubscriptionMetadata(); invokeCompletedOffsetCommitCallbacks(); // 如果是subscribe方式订阅的topic if (subscriptions.partitionsAutoAssigned()) &#123; // Always update the heartbeat last poll time so that the heartbeat thread does not leave the // group proactively due to application inactivity even if (say) the coordinator cannot be found. // 1.检查心跳线程运行是否正常，如果心跳线程失败则抛出异常，反之则更新poll调用的时间 pollHeartbeat(timer.currentTimeMs()); // 2.如果coordinator未知，则初始化ConsumeCoordinator if (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) &#123; return false; &#125; // 判断是否需要重新加入group，如果订阅的partition变化或者分配的partition变化，都可能需要重新加入group if (rejoinNeededOrPending()) &#123; // due to a race condition between the initial metadata fetch and the initial rebalance, // we need to ensure that the metadata is fresh before joining initially. This ensures // that we have matched the pattern against the cluster&#x27;s topics at least once before joining. if (subscriptions.hasPatternSubscription()) &#123; // For consumer group that uses pattern-based subscription, after a topic is created, // any consumer that discovers the topic after metadata refresh can trigger rebalance // across the entire consumer group. Multiple rebalances can be triggered after one topic // creation if consumers refresh metadata at vastly different times. We can significantly // reduce the number of rebalances caused by single topic creation by asking consumer to // refresh metadata before re-joining the group as long as the refresh backoff time has // passed. if (this.metadata.timeToAllowUpdate(timer.currentTimeMs()) == 0) &#123; this.metadata.requestUpdate(); &#125; if (!client.ensureFreshMetadata(timer)) &#123; return false; &#125; maybeUpdateSubscriptionMetadata(); &#125; // 3.确保group是active的，重新加入group，分配订阅的partition if (!ensureActiveGroup(timer)) &#123; return false; &#125; &#125; &#125; else &#123; // For manually assigned partitions, if there are no ready nodes, await metadata. // If connections to all nodes fail, wakeups triggered while attempting to send fetch // requests result in polls returning immediately, causing a tight loop of polls. Without // the wakeup, poll() with no channels would block for the timeout, delaying re-connection. // awaitMetadataUpdate() initiates new connections with configured backoff and avoids the busy loop. // When group management is used, metadata wait is already performed for this scenario as // coordinator is unknown, hence this check is not required. if (metadata.updateRequested() &amp;&amp; !client.hasReadyNodes(timer.currentTimeMs())) &#123; client.awaitMetadataUpdate(timer); &#125; &#125; // 4.如果设置的是自动commit,如果定时达到则自动commit maybeAutoCommitOffsetsAsync(timer.currentTimeMs()); return true;&#125; coordinator.poll () 方法中，具体实现可以分为四个步骤： pollHeartbeat ()：检测心跳线程运行是否正常，需要定时向 GroupCoordinator 发送心跳，如果超时未发送心跳，consumer 会离开 consumer group。 ensureCoordinatorReady ()：当通过 subscribe () 方法订阅 topic 时，如果 coordinator 未知，则初始化 ConsumerCoordinator (在 ensureCoordinatorReady () 中实现，该方法主要的作用是发送 FindCoordinatorRequest 请求，并建立连接)。 ensureActiveGroup ()：判断是否需要重新加入 group，如果订阅的 partition 变化或者分配的 partition 变化时，需要 rejoin，则通过 ensureActiveGroup () 发送 join-group、sync-group 请求，加入 group 并获取其 assign 的 TopicPartition list。 maybeAutoCommitOffsetsAsync ()：如果设置的是自动 commit，并且达到了发送时限则自动 commit offset。 关于 rejoin，下列几种情况会触发再均衡 (reblance) 操作： 订阅的 topic 列表变化 topic 被创建或删除 新的消费者加入消费者组 (第一次进行消费也属于这种情况) 消费者宕机下线 (长时间未发送心跳包) 消费者主动退出消费组，比如调用 unsubscrible () 方法取消对主题的订阅 消费者组对应的 GroupCoorinator 节点发生了变化 消费者组内所订阅的任一主题或者主题的分区数量发生了变化 取消 topic 订阅，consumer 心跳线程超时以及在 Server 端给定的时间内未收到心跳请求，这三个都是触发的 LEAVE_GROUP 请求。 下面重点介绍下第二步中的 ensureCoordinatorReady () 方法和第三步中的 ensureActiveGroup () 方法。 ensureCoordinatorReadyensureCoordinatorReady ()这个方法主要作用：选择一个连接数最少的 broker (还未响应请求最少的 broker)，发送 FindCoordinator 请求，找到 GroupCoordinator 后，建立对应的 TCP 连接。 方法调用流程是 ensureCoordinatorReady () → lookupCoordinator () → sendFindCoordinatorRequest ()。 如果 client 收到 server response，那么就与 GroupCoordinator 建立连接。 1234567891011121314151617181920212223242526272829303132333435363738/** * Visible for testing. * * Ensure that the coordinator is ready to receive requests. * * @param timer Timer bounding how long this method can block * @return true If coordinator discovery and initial connection succeeded, false otherwise */protected synchronized boolean ensureCoordinatorReady(final Timer timer) &#123; if (!coordinatorUnknown()) return true; do &#123; // 找到GroupCoordinator，并建立连接 final RequestFuture&lt;Void&gt; future = lookupCoordinator(); client.poll(future, timer); if (!future.isDone()) &#123; // ran out of time break; &#125; if (future.failed()) &#123; if (future.isRetriable()) &#123; log.debug(&quot;Coordinator discovery failed, refreshing metadata&quot;); client.awaitMetadataUpdate(timer); &#125; else throw future.exception(); &#125; else if (coordinator != null &amp;&amp; client.isUnavailable(coordinator)) &#123; // we found the coordinator, but the connection has failed, so mark // it dead and backoff before retrying discovery markCoordinatorUnknown(); timer.sleep(retryBackoffMs); &#125; &#125; while (coordinatorUnknown() &amp;&amp; timer.notExpired()); return !coordinatorUnknown();&#125; 12345678910111213protected synchronized RequestFuture&lt;Void&gt; lookupCoordinator() &#123; if (findCoordinatorFuture == null) &#123; // find a node to ask about the coordinator(找一个最少连接的broker，此处对应的应该就是文章开头处确定GroupCoordinator节点的发发) Node node = this.client.leastLoadedNode(); if (node == null) &#123; log.debug(&quot;No broker available to send FindCoordinator request&quot;); return RequestFuture.noBrokersAvailable(); &#125; else // 对找到的broker发送FindCoordinator请求，并对response进行处理 findCoordinatorFuture = sendFindCoordinatorRequest(node); &#125; return findCoordinatorFuture;&#125; 12345678910111213141516171819/** * Discover the current coordinator for the group. Sends a GroupMetadata request to * one of the brokers. The returned future should be polled to get the result of the request. * @return A request future which indicates the completion of the metadata request */private RequestFuture&lt;Void&gt; sendFindCoordinatorRequest(Node node) &#123; // initiate the group metadata request log.debug(&quot;Sending FindCoordinator request to broker &#123;&#125;&quot;, node); FindCoordinatorRequest.Builder requestBuilder = new FindCoordinatorRequest.Builder( new FindCoordinatorRequestData() .setKeyType(CoordinatorType.GROUP.id()) .setKey(this.groupId)); // 发送请求，并将response转换为RequestFuture // compose的作用是将FindCoordinatorResponseHandler类转换为RequestFuture // 实际上就是为返回的Future类重置onSuccess()和onFailure()方法 return client.send(node, requestBuilder) .compose(new FindCoordinatorResponseHandler());&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142// 根据response返回的ip以及端口信息，和该broke上开启的GroupCoordinator建立连接private class FindCoordinatorResponseHandler extends RequestFutureAdapter&lt;ClientResponse, Void&gt; &#123; @Override public void onSuccess(ClientResponse resp, RequestFuture&lt;Void&gt; future) &#123; log.debug(&quot;Received FindCoordinator response &#123;&#125;&quot;, resp); clearFindCoordinatorFuture(); FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) resp.responseBody(); Errors error = findCoordinatorResponse.error(); if (error == Errors.NONE) &#123; // 如果正确获取broker上的GroupCoordinator，建立连接，并更新心跳时间 synchronized (AbstractCoordinator.this) &#123; // use MAX_VALUE - node.id as the coordinator id to allow separate connections // for the coordinator in the underlying network client layer int coordinatorConnectionId = Integer.MAX_VALUE - findCoordinatorResponse.data().nodeId(); AbstractCoordinator.this.coordinator = new Node( coordinatorConnectionId, findCoordinatorResponse.data().host(), findCoordinatorResponse.data().port()); log.info(&quot;Discovered group coordinator &#123;&#125;&quot;, coordinator); // 初始化tcp连接 client.tryConnect(coordinator); // 更新心跳时间 heartbeat.resetSessionTimeout(); &#125; future.complete(null); &#125; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else &#123; log.debug(&quot;Group coordinator lookup failed: &#123;&#125;&quot;, findCoordinatorResponse.data().errorMessage()); future.raise(error); &#125; &#125; @Override public void onFailure(RuntimeException e, RequestFuture&lt;Void&gt; future) &#123; clearFindCoordinatorFuture(); super.onFailure(e, future); &#125;&#125; 上面代码主要作用就是：往一个负载最小的 broker 节点发起 FindCoordinator 请求，Kafka 在走到这个请求后会根据 group_id 查找对应的 GroupCoordinator 节点 (文章开头处介绍的方法)，如果找到对应的 GroupCoordinator 则会返回其对应的 node_id，host 和 port 信息，并建立连接。 这里的 GroupCoordinator 节点的确定在文章开头提到过，是通过 group.id 和 partitionCount 来确定的。 ensureActiveGroup现在已经知道了 GroupCoordinator 节点，并建立了连接。ensureActiveGroup () 这个方法的主要作用：向 GroupCoordinator 发送 join-group、sync-group 请求，获取 assign 的 tp list。 调用过程是 ensureActiveGroup () → ensureCoordinatorReady () → startHeartbeatThreadIfNeeded () → joinGroupIfNeeded ()。 joinGroupIfNeeded () 方法中最重要的方法是 initiateJoinGroup ()，它的的调用流程是 disableHeartbeatThread () → sendJoinGroupRequest () → JoinGroupResponseHandler::handle () → onJoinLeader ()，onJoinFollower () → sendSyncGroupRequest ()。 12345678910111213141516171819/** * Ensure the group is active (i.e., joined and synced) * * @param timer Timer bounding how long this method can block * @return true iff the group is active */boolean ensureActiveGroup(final Timer timer) &#123; // always ensure that the coordinator is ready because we may have been disconnected // when sending heartbeats and does not necessarily require us to rejoin the group. // 1.确保GroupCoordinator已经连接 if (!ensureCoordinatorReady(timer)) &#123; return false; &#125; // 2.启动心跳线程，但是并不一定发送心跳，满足条件后才会发送心跳 startHeartbeatThreadIfNeeded(); // 3.发送joinGroup请求，并对返回的信息进行处理，核心步骤 return joinGroupIfNeeded(timer);&#125; 心跳线程就是在这里启动的，但是并不一定马上发送心跳包，会在满足条件之后才会开始发送。后面最主要的逻辑就集中在 joinGroupIfNeeded () 方法，它的核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Joins the group without starting the heartbeat thread. * * Visible for testing. * * @param timer Timer bounding how long this method can block * @return true iff the operation succeeded */boolean joinGroupIfNeeded(final Timer timer) &#123; while (rejoinNeededOrPending()) &#123; if (!ensureCoordinatorReady(timer)) &#123; return false; &#125; // call onJoinPrepare if needed. We set a flag to make sure that we do not call it a second // time if the client is woken up before a pending rebalance completes. This must be called // on each iteration of the loop because an event requiring a rebalance (such as a metadata // refresh which changes the matched subscription set) can occur while another rebalance is // still in progress. // 触发onJoinPrepare，包括offset commit和rebalance listener if (needsJoinPrepare) &#123; // 如果是自动提交，则要开始提交offset以及在join group之前回调reblance listener接口 onJoinPrepare(generation.generationId, generation.memberId); needsJoinPrepare = false; &#125; // 初始化joinGroup请求，并发送joinGroup请求，核心步骤 final RequestFuture&lt;ByteBuffer&gt; future = initiateJoinGroup(); client.poll(future, timer); if (!future.isDone()) &#123; // we ran out of time return false; &#125; // join succeed，这一步时，时间上sync-group已经成功了 if (future.succeeded()) &#123; // Duplicate the buffer in case `onJoinComplete` does not complete and needs to be retried. ByteBuffer memberAssignment = future.value().duplicate(); // 发送完成，consumer加入group成功，触发onJoinComplete()方法 onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment); // We reset the join group future only after the completion callback returns. This ensures // that if the callback is woken up, we will retry it on the next joinGroupIfNeeded. // 重置joinFuture为空 resetJoinGroupFuture(); needsJoinPrepare = true; &#125; else &#123; resetJoinGroupFuture(); final RuntimeException exception = future.exception(); if (exception instanceof UnknownMemberIdException || exception instanceof RebalanceInProgressException || exception instanceof IllegalGenerationException || exception instanceof MemberIdRequiredException) continue; else if (!future.isRetriable()) throw exception; timer.sleep(retryBackoffMs); &#125; &#125; return true;&#125; initiateJoinGroup () 方法的核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private synchronized RequestFuture&lt;ByteBuffer&gt; initiateJoinGroup() &#123; // we store the join future in case we are woken up by the user after beginning the // rebalance in the call to poll below. This ensures that we do not mistakenly attempt // to rejoin before the pending rebalance has completed. if (joinFuture == null) &#123; // fence off the heartbeat thread explicitly so that it cannot interfere with the join group. // Note that this must come after the call to onJoinPrepare since we must be able to continue // sending heartbeats if that callback takes some time. // Step1:rebalance期间，心跳线程停止运行 disableHeartbeatThread(); // 设置当前状态为rebalance state = MemberState.REBALANCING; // Step2:发送joinGroup请求，核心步骤 joinFuture = sendJoinGroupRequest(); // Step3:为joinGroup请求添加监听器，监听joinGroup请求的结果并做相应的处理 joinFuture.addListener(new RequestFutureListener&lt;ByteBuffer&gt;() &#123; @Override public void onSuccess(ByteBuffer value) &#123; // handle join completion in the callback so that the callback will be invoked // even if the consumer is woken up before finishing the rebalance synchronized (AbstractCoordinator.this) &#123; log.info(&quot;Successfully joined group with generation &#123;&#125;&quot;, generation.generationId); // 如果joinGroup成功，设置状态为stable state = MemberState.STABLE; rejoinNeeded = false; if (heartbeatThread != null) // Step4:允许心跳线程继续运行 heartbeatThread.enable(); &#125; &#125; @Override public void onFailure(RuntimeException e) &#123; // we handle failures below after the request finishes. if the join completes // after having been woken up, the exception is ignored and we will rejoin synchronized (AbstractCoordinator.this) &#123; // 如果joinGroup失败，设置状态为unjoined state = MemberState.UNJOINED; &#125; &#125; &#125;); &#125; return joinFuture;&#125; 可以看到在 joinGroup 之前会让心跳线程暂时停下来，此时会将 ConsumerCoordinator 的状态设置为 rebalance 状态，当 joinGroup 成功之后会将状态设置为 stable 状态，同时让之前停下来的心跳线程继续运行。 sendJoinGroupRequest () 方法的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536/** * Join the group and return the assignment for the next generation. This function handles both * JoinGroup and SyncGroup, delegating to &#123;@link #performAssignment(String, String, List)&#125; if * elected leader by the coordinator. * * NOTE: This is visible only for testing * * @return A request future which wraps the assignment returned from the group leader */// 发送joinGroup请求RequestFuture&lt;ByteBuffer&gt; sendJoinGroupRequest() &#123; if (coordinatorUnknown()) return RequestFuture.coordinatorNotAvailable(); // send a join group request to the coordinator log.info(&quot;(Re-)joining group&quot;); JoinGroupRequest.Builder requestBuilder = new JoinGroupRequest.Builder( new JoinGroupRequestData() .setGroupId(groupId) .setSessionTimeoutMs(this.sessionTimeoutMs) .setMemberId(this.generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setProtocolType(protocolType()) .setProtocols(metadata()) .setRebalanceTimeoutMs(this.rebalanceTimeoutMs) ); log.debug(&quot;Sending JoinGroup (&#123;&#125;) to coordinator &#123;&#125;&quot;, requestBuilder, this.coordinator); // Note that we override the request timeout using the rebalance timeout since that is the // maximum time that it may block on the coordinator. We add an extra 5 seconds for small delays. int joinGroupTimeoutMs = Math.max(rebalanceTimeoutMs, rebalanceTimeoutMs + 5000); return client.send(coordinator, requestBuilder, joinGroupTimeoutMs) .compose(new JoinGroupResponseHandler());// Step5:处理joinGroup请求后的response&#125; 在发送 joinGroup 请求之后，会收到来自服务器的响应，然后针对这个响应再做一些重要的事情： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// 处理发送joinGroup请求后的response的handler(同步group信息)private class JoinGroupResponseHandler extends CoordinatorResponseHandler&lt;JoinGroupResponse, ByteBuffer&gt; &#123; @Override public void handle(JoinGroupResponse joinResponse, RequestFuture&lt;ByteBuffer&gt; future) &#123; Errors error = joinResponse.error(); if (error == Errors.NONE) &#123; log.debug(&quot;Received successful JoinGroup response: &#123;&#125;&quot;, joinResponse); sensors.joinLatency.record(response.requestLatencyMs()); synchronized (AbstractCoordinator.this) &#123; if (state != MemberState.REBALANCING) &#123; // if the consumer was woken up before a rebalance completes, we may have already left // the group. In this case, we do not want to continue with the sync group. future.raise(new UnjoinedGroupException()); &#125; else &#123; AbstractCoordinator.this.generation = new Generation(joinResponse.data().generationId(), joinResponse.data().memberId(), joinResponse.data().protocolName()); // Step6:joinGroup成功，下面需要进行sync-group，获取分配的tp列表 if (joinResponse.isLeader()) &#123; // 当前consumer是leader onJoinLeader(joinResponse).chain(future); &#125; else &#123; // 当前consumer是follower onJoinFollower().chain(future); &#125; &#125; &#125; &#125; else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS) &#123; log.debug(&quot;Attempt to join group rejected since coordinator &#123;&#125; is loading the group.&quot;, coordinator()); // backoff and retry future.raise(error); &#125; else if (error == Errors.UNKNOWN_MEMBER_ID) &#123; // reset the member id and retry immediately resetGeneration(); log.debug(&quot;Attempt to join group failed due to unknown member id.&quot;); future.raise(Errors.UNKNOWN_MEMBER_ID); &#125; else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123; // re-discover the coordinator and retry with backoff markCoordinatorUnknown(); log.debug(&quot;Attempt to join group failed due to obsolete coordinator information: &#123;&#125;&quot;, error.message()); future.raise(error); &#125; else if (error == Errors.FENCED_INSTANCE_ID) &#123; log.error(&quot;Received fatal exception: group.instance.id gets fenced&quot;); future.raise(error); &#125; else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL || error == Errors.INVALID_SESSION_TIMEOUT || error == Errors.INVALID_GROUP_ID || error == Errors.GROUP_AUTHORIZATION_FAILED || error == Errors.GROUP_MAX_SIZE_REACHED) &#123; // log the error and re-throw the exception log.error(&quot;Attempt to join group failed due to fatal error: &#123;&#125;&quot;, error.message()); if (error == Errors.GROUP_MAX_SIZE_REACHED) &#123; future.raise(new GroupMaxSizeReachedException(groupId)); &#125; else if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else &#123; future.raise(error); &#125; &#125; else if (error == Errors.UNSUPPORTED_VERSION) &#123; log.error(&quot;Attempt to join group failed due to unsupported version error. Please unset field group.instance.id and retry&quot; + &quot;to see if the problem resolves&quot;); future.raise(error); &#125; else if (error == Errors.MEMBER_ID_REQUIRED) &#123; // Broker requires a concrete member id to be allowed to join the group. Update member id // and send another join group request in next cycle. synchronized (AbstractCoordinator.this) &#123; AbstractCoordinator.this.generation = new Generation(OffsetCommitRequest.DEFAULT_GENERATION_ID, joinResponse.data().memberId(), null); AbstractCoordinator.this.rejoinNeeded = true; AbstractCoordinator.this.state = MemberState.UNJOINED; &#125; future.raise(Errors.MEMBER_ID_REQUIRED); &#125; else &#123; // unexpected error, throw the exception log.error(&quot;Attempt to join group failed due to unexpected error: &#123;&#125;&quot;, error.message()); future.raise(new KafkaException(&quot;Unexpected error in join group response: &quot; + error.message())); &#125; &#125;&#125; 上面代码的主要过程如下： 如果 group 是新的 group.id，那么此时 group 初始化的状态为 Empty； 当 GroupCoordinator 接收到 consumer 的 join-group 请求后，由于此时这个 group 的 member 列表还是空 (group 是新建的，每个 consumer 实例被称为这个 group 的一个 member)，第一个加入的 member 将被选为 leader，也就是说，对于一个新的 consumer group 而言，当第一个 consumer 实例加入后将会被选为 leader。如果后面 leader 挂了，会从其他 member 里面随机选择一个 member 成为新的 leader； 如果 GroupCoordinator 接收到 leader 发送 join-group 请求，将会触发 rebalance，group 的状态变为 PreparingRebalance； 此时，GroupCoordinator 将会等待一定的时间，如果在一定时间内，接收到 join-group 请求的 consumer 将被认为是依然存活的，此时 group 会变为 AwaitSync 状态，并且 GroupCoordinator 会向这个 group 的所有 member 返回其 response； consumer 在接收到 GroupCoordinator 的 response 后，如果这个 consumer 是 group 的 leader，那么这个 consumer 将会负责为整个 group assign partition 订阅安排（默认是按 range 的策略，目前也可选 roundrobin），然后 leader 将分配后的信息以 sendSyncGroupRequest () 请求的方式发给 GroupCoordinator，而作为 follower 的 consumer 实例会发送一个空列表； GroupCoordinator 在接收到 leader 发来的请求后，会将 assign 的结果返回给所有已经发送 sync-group 请求的 consumer 实例，并且 group 的状态将会转变为 Stable，如果后续再收到 sync-group 请求，由于 group 的状态已经是 Stable，将会直接返回其分配结果。 sync-group 发送请求核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 当consumer为follower时，从GroupCoordinator拉取分配结果private RequestFuture&lt;ByteBuffer&gt; onJoinFollower() &#123; // send follower&#x27;s sync group with an empty assignment SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder( new SyncGroupRequestData() .setGroupId(groupId) .setMemberId(generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setGenerationId(generation.generationId) .setAssignments(Collections.emptyList()) ); log.debug(&quot;Sending follower SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;, this.coordinator, requestBuilder); // 发送sync-group请求 return sendSyncGroupRequest(requestBuilder);&#125;// 当consumer客户端为leader时，对group下的所有实例进行分配，将assign的结果发送到GroupCoordinatorprivate RequestFuture&lt;ByteBuffer&gt; onJoinLeader(JoinGroupResponse joinResponse) &#123; try &#123; // perform the leader synchronization and send back the assignment for the group(assign 操作) Map&lt;String, ByteBuffer&gt; groupAssignment = performAssignment(joinResponse.data().leader(), joinResponse.data().protocolName(), joinResponse.data().members()); List&lt;SyncGroupRequestData.SyncGroupRequestAssignment&gt; groupAssignmentList = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, ByteBuffer&gt; assignment : groupAssignment.entrySet()) &#123; groupAssignmentList.add(new SyncGroupRequestData.SyncGroupRequestAssignment() .setMemberId(assignment.getKey()) .setAssignment(Utils.toArray(assignment.getValue())) ); &#125; SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder( new SyncGroupRequestData() .setGroupId(groupId) .setMemberId(generation.memberId) .setGroupInstanceId(this.groupInstanceId.orElse(null)) .setGenerationId(generation.generationId) .setAssignments(groupAssignmentList) ); log.debug(&quot;Sending leader SyncGroup to coordinator &#123;&#125;: &#123;&#125;&quot;, this.coordinator, requestBuilder); // 发送sync-group请求 return sendSyncGroupRequest(requestBuilder); &#125; catch (RuntimeException e) &#123; return RequestFuture.failure(e); &#125;&#125;// 发送SyncGroup请求，获取对partition分配的安排private RequestFuture&lt;ByteBuffer&gt; sendSyncGroupRequest(SyncGroupRequest.Builder requestBuilder) &#123; if (coordinatorUnknown()) return RequestFuture.coordinatorNotAvailable(); return client.send(coordinator, requestBuilder) .compose(new SyncGroupResponseHandler());&#125;private class SyncGroupResponseHandler extends CoordinatorResponseHandler&lt;SyncGroupResponse, ByteBuffer&gt; &#123; @Override public void handle(SyncGroupResponse syncResponse, RequestFuture&lt;ByteBuffer&gt; future) &#123; Errors error = syncResponse.error(); if (error == Errors.NONE) &#123; // sync-group成功 sensors.syncLatency.record(response.requestLatencyMs()); future.complete(ByteBuffer.wrap(syncResponse.data.assignment())); &#125; else &#123; // join的标志位设置为true requestRejoin(); if (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123; future.raise(new GroupAuthorizationException(groupId)); &#125; else if (error == Errors.REBALANCE_IN_PROGRESS) &#123; // group正在进行rebalance，任务失败 log.debug(&quot;SyncGroup failed because the group began another rebalance&quot;); future.raise(error); &#125; else if (error == Errors.FENCED_INSTANCE_ID) &#123; log.error(&quot;Received fatal exception: group.instance.id gets fenced&quot;); future.raise(error); &#125; else if (error == Errors.UNKNOWN_MEMBER_ID || error == Errors.ILLEGAL_GENERATION) &#123; log.debug(&quot;SyncGroup failed: &#123;&#125;&quot;, error.message()); resetGeneration(); future.raise(error); &#125; else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123; log.debug(&quot;SyncGroup failed: &#123;&#125;&quot;, error.message()); markCoordinatorUnknown(); future.raise(error); &#125; else &#123; future.raise(new KafkaException(&quot;Unexpected error from SyncGroup: &quot; + error.message())); &#125; &#125; &#125;&#125; 这个阶段主要是将分区分配方案同步给各个消费者，这个同步仍然是通过 GroupCoordinator 来转发的。 分区策略并非由 leader 消费者来决定，而是各个消费者投票决定的，谁的票多就采用什么分区策略。这里的分区策略是通过 partition.assignment.strategy 参数设置的，可以设置多个。如果选举出了消费者不支持的策略，那么就会抛出异常 IllegalArgumentException: Member does not support protocol。 经过上面的步骤，一个 consumer 实例就已经加入 group 成功了，加入 group 成功后，将会触发 ConsumerCoordinator 的 onJoinComplete () 方法，其作用就是：更新订阅的 tp 列表、更新其对应的 metadata 及触发注册的 listener。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 加入group成功@Overrideprotected void onJoinComplete(int generation, String memberId, String assignmentStrategy, ByteBuffer assignmentBuffer) &#123; // only the leader is responsible for monitoring for metadata changes (i.e. partition changes) if (!isLeader) assignmentSnapshot = null; PartitionAssignor assignor = lookupAssignor(assignmentStrategy); if (assignor == null) throw new IllegalStateException(&quot;Coordinator selected invalid assignment protocol: &quot; + assignmentStrategy); Assignment assignment = ConsumerProtocol.deserializeAssignment(assignmentBuffer); if (!subscriptions.assignFromSubscribed(assignment.partitions())) &#123; handleAssignmentMismatch(assignment); return; &#125; Set&lt;TopicPartition&gt; assignedPartitions = subscriptions.assignedPartitions(); // The leader may have assigned partitions which match our subscription pattern, but which // were not explicitly requested, so we update the joined subscription here. maybeUpdateJoinedSubscription(assignedPartitions); // give the assignor a chance to update internal state based on the received assignment assignor.onAssignment(assignment, generation); // reschedule the auto commit starting from now if (autoCommitEnabled) this.nextAutoCommitTimer.updateAndReset(autoCommitIntervalMs); // execute the user&#x27;s callback after rebalance ConsumerRebalanceListener listener = subscriptions.rebalanceListener(); log.info(&quot;Setting newly assigned partitions: &#123;&#125;&quot;, Utils.join(assignedPartitions, &quot;, &quot;)); try &#123; listener.onPartitionsAssigned(assignedPartitions); &#125; catch (WakeupException | InterruptException e) &#123; throw e; &#125; catch (Exception e) &#123; log.error(&quot;User provided listener &#123;&#125; failed on partition assignment&quot;, listener.getClass().getName(), e); &#125;&#125; 至此，一个 consumer 实例算是真正上意义上加入 group 成功。 然后 consumer 就进入正常工作状态，同时 consumer 也通过向 GroupCoordinator 发送心跳来维持它们与消费者组的从属关系，以及它们对分区的所有权关系。只要以正常的间隔发送心跳，就被认为是活跃的，但是如果 GroupCoordinator 没有响应，那么就会发送 LeaveGroup 请求退出消费者组。 本文参考http://generalthink.github.io/2019/05/15/how-to-join-kafka-consumer-group/ https://matt33.com/2017/10/22/consumer-join-group/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 &#119;&#x64;&#115;&#104;&#102;&#x75;&#x74;&#x40;&#x31;&#x36;&#51;&#46;&#x63;&#x6f;&#x6d;","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"Kafka 的核心配置参数","slug":"kafka-properties","date":"2020-10-30T07:48:10.000Z","updated":"2020-11-09T08:15:53.480Z","comments":true,"path":"2020/10/30/kafka-properties/","link":"","permalink":"http://example.com/2020/10/30/kafka-properties/","excerpt":"","text":"Kafka Producer 核心配置参数bootstrap.servers broke 服务器地址，多个服务器，用逗号隔开。 acks 发送应答，默认：1。 acks 参数指定了生产者希望 leader 返回的用于确认请求完成的确认数量，即必须要有多少个分区副本收到该消息，生产者才会认为消息写入是成功的。 允许以下设置： acks=0：生产者将完全不等待来自服务器的任何确认。记录将立即添加到 socket 缓冲区，并被认为已发送。在这种情况下，不能保证服务器已经收到记录，重试配置将不会生效 (因为客户机通常不会知道任何失败)。响应里来自服务端的 offset 总是-1。同时，由于不需要等待响应，所以可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量。 acks=1：只需要集群的 leader 收到消息，生产者就会收到一个来自服务器的成功响应。leader 会将记录写到本地日志中，但不会等待所有 follower 的完全确认。在这种情况下，如果 follower 复制数据之前，leader 挂掉，数据就会丢失。 acks=all / -1：当所有参与复制的节点全部收到消息的时候，生产者才会收到一个来自服务器的成功响应，最安全不过延迟比较高。如果需要保证消息不丢失, 需要使用该设置，同时需要设置 broke端 unclean.leader.election.enable 为 true，保证当 ISR 列表为空时，选择其他存活的副本作为新的 leader。 batch.size 批量发送大小，默认：16384，即 16 K。 当有多个消息需要被发送到同一个 partition 的时候，生产者会把他们放到同一个批次里面 (Deque)，该参数指定了一个批次可以使用的内存大小，按照字节数计算，当批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定会等到批次被填满才发送，半满甚至只包含一个消息的批次也有可能被发送。 生产者产生的消息缓存到本地，每次批量发送 batch.size 大小到服务器。太小的 batch 会降低吞吐，太大则会浪费内存。 linger.ms 发送延迟时间，默认：0。 指定了生产者在发送批次之前等待更多消息加入批次的时间。生产者会在批次填满或 linger.ms 达到上限时把批次发送出去。把 linger.ms 设置成比0大的数，让生产者在发送批次之前等待一会儿，使更多的消息加入到这个批次，虽然这样会增加延迟，但也会提升吞吐量。 说明：batch.size 和 linger.ms 满足任何一个条件都会发送。 buffer.memory 生产者最大可用缓存，默认：33554432，即 32 M。 生产者可以用来缓冲等待发送到服务器的记录的总内存字节。如果应用程序发送消息的速度超过生产者发送消息到服务器的速度，即超出 max.block.ms，将会抛出异常。 该设置应该大致与生产者将使用的总内存相对应，但不是硬绑定，因为生产者使用的内存并非全部都用于缓冲。一些额外的内存将用于压缩 (如果启用了压缩) 以及维护飞行中的请求。 max.block.ms 阻塞时间，默认：60000，即 1 分钟。 指定了在调用 send () 方法或者 partitionsFor () 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 max.block.ms 时，就会抛出 new TimeoutException(“Failed to allocate memory within the configured max blocking time “ + maxTimeToBlockMs + “ ms.”);。 用户提供的序列化器或分区程序中的阻塞将不计入此超时。 client.id 生产者 ID，默认：空。 请求时传递给服务器的 id 字符串，用来标识消息来源，后台线程会根据它命名。这样做的目的是通过允许在服务器端请求日志中包含逻辑应用程序名称，从而能够跟踪 ip/端口之外的请求源。 compression.type 生产者数据被发送到服务器之前被压缩的压缩类型，默认：none，即不压缩。 指定给定主题的最终压缩类型。此配置接受标准压缩编解码器 (“gzip”、“snappy”、“lz4”、“zstd”)。 “gzip”：压缩效率高，适合高内存、CPU。 “snappy”：适合带宽敏感性，压缩力度大。 retries 失败重试次数，默认：2147483647。 异常是 RetriableException 类型，或者 TransactionManager 允许重试 (transactionManager.canRetry () )。 RetriableException 类型异常如下： retry.backoff.ms 失败请求重试的间隔时间，默认：100。 这避免了在某些失败场景下以紧密循环的方式重复发送请求。 max.in.flight.requests.per.connection 单个连接上发送的未确认请求的最大数量，默认：5。 阻塞前客户端在单个连接上发送的未确认请求的最大数量。即指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。 如果设置为 1，可以保证消息是按照发送的顺序写入服务器的，即便发生了重试。 如果设置大于 1，在 retries 不为0的情况下可能会出现消息发送顺序的错误。例如将两个批发送到同一个分区，第一个批处理失败并重试，但是第二个批处理成功，那么第二个批处理中的记录可能会先出现。 delivery.timeout.ms 传输时间，默认：120000，即 2 分钟。 生产者发送完请求接受服务器 ACK 的时间，该时间允许重试 ，该配置应该大于 request.timeout.ms + linger.ms。 request.timeout.ms 请求超时时间，默认：30000，即30秒。 配置控制客户端等待请求响应的最长时间。 如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽，则该请求将失败。 这应该大于replica.lag.time.max.ms (broker 端配置)，以减少由于不必要的生产者重试引起的消息重复的可能性。 connections.max.idle.ms 连接空闲超时时间，默认：540000，即 9 分钟。 在此配置指定的毫秒数之后关闭空闲连接。 enable.idempotence 开启幂等，默认：false。 如果设置为 true ，将开启 exactly-once 模式，生产者将确保在流中准确地写入每个消息的副本。如果设置为 false，则由于代理失败而导致生产者重试，等等，可能会在流中写入重试消息的副本。 注意，启用幂等需要以下条件 ：max.in.flight.requests.per.connection 小于或等于 5，retries 大于 0， acks 必须为 all 或者 -1。如果用户没有显式地设置这些值，将选择合适的值。如果设置了不兼容的值，就会抛出 ConfigException。 key.serializer key 序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Serializer 。Kafka 提供以下几个默认的 key 序列化器： String：org.apache.kafka.common.serialization.StringSerializer。 value.serializer value 序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Serializer。Kafka 提供以下几个默认的 value 序列化器： byte[]：org.apache.kafka.common.serialization.ByteArraySerializer。 String：org.apache.kafka.common.serialization.StringSerializer。 max.request.size 请求的最大字节大小，默认：1048576，即 1 M。 该参数用于控制生产者发送的请求大小，单次发送的消息大小超过 max.request.size 时，会抛出异常 ，如：org.apache.kafka.common.errors.RecordTooLargeException: The message is 70459102 bytes when serialized which is larger than the maximum request size you have configured with the max.request.size configuration.。 注意：broker 对可接收的消息最大值也有自己的限制 (通过 message.max.bytes 参数设置)，所以两边的配置最好可以匹配，避免生产者发送的消息被 broker 拒绝。 metric.reporters 自定义指标报告器，默认：无。 用作指标报告器的类的列表，需要实现接口：org.apache.kafka.common.metrics.MetricsReporter，该接口允许插入将在创建新度量时得到通知的类。JmxReporter 始终包含在注册 JMX 统计信息中。 interceptor.classes 拦截器，默认：无。 用作拦截器的类的列表，需要实现接口：org.apache.kafka.clients.producer.ProducerInterceptor 。允许将生产者接收到的记录发布到 Kafka 集群之前拦截它们 (可能还会发生突变)。 partitioner.class 分区策略，默认：org.apache.kafka.clients.producer.internals.DefaultPartitioner。 如果自定义分区策略，需要实现接口： org.apache.kafka.clients.producer.Partitioner。 receive.buffer.bytes 默认：32768，即 32 K。 指定了 TCP socket 接收数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 send.buffer.bytes 默认：131072，即 128 K。 指定了 TCP socket 发送数据包的缓冲区大小 (和 broker 通信还是通过 socket )。如果被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 transaction.timeout.ms 事务协调器等待生产者更新事务状态的最大毫秒数，默认：60000，即 1 分钟。 如果超过该时间，事务协调器会终止进行中的事务。 如果设置的时间大于 broker 端的 max.transaction.timeout.ms，会抛出 InvalidTransactionTimeout 异常。 transactional.id 用于事务传递的 TransactionalId，默认：空，即不使用事务。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的 TransactionalId 的事务已经完成。如果没有提供 TransactionalId，则生产者被限制为幂等传递。 注意：如果配置了 TransactionalId，则必须启用 enable.idempotence。 Kafka Consumer 核心配置参数bootstrap.servers broke 服务器地址，多个服务器，用逗号隔开。 enable.auto.commit 是否开启自动提交 offset，默认：true。 如果为 true，consumer 的偏移量将在后台定期提交，自动提交频率通过 auto.commit.interval.ms 设置。 auto.commit.interval.ms 自动提交频率，默认：5000。 auto.offset.reset 初始偏移量，默认：latest。 如果 Kafka 中没有初始偏移量，或者服务器上不再存在当前偏移量 (例如，该数据已被删除)，该怎么处理： earliest：自动重置偏移到最早的偏移。 latest：自动将偏移量重置为最新偏移量。 none：如果没有为使用者的组找到以前的偏移量，则向使用者抛出 exception。 anything else：向使用者抛出异常。 client.id 客户端 id，默认：空。 便于跟踪日志。 check.crcs 是否开启数据校验，默认：true。 自动检查消耗的记录的 CRC32。这确保不会发生对消息的在线或磁盘损坏。此检查增加了一些开销，因此在寻求极端性能的情况下可能禁用此检查。 group.id 消费者所属的群组，默认：空。 唯一标识用户群组，每个 partition 只会分配给同一个 group 里面的一个 consumer 来消费。 max.poll.records 拉取的最大记录，默认：500。 单次轮询调用 poll () 方法能返回的记录的最大数量。 max.poll.interval.ms 拉取记录间隔，默认：300000，即 5 分钟。 使用消费者组管理时轮询调用之间的最大延迟。这为使用者在获取更多记录之前空闲的时间设置了上限。如果在此超时过期之前没有调用 poll ()，则认为使用者失败，组将重新平衡，以便将分区重新分配给另一个成员。 request.timeout.ms 请求超时时间，默认：30000 。 配置控制客户机等待请求响应的最长时间。如果在超时之前没有收到响应，客户端将在需要时重新发送请求，或者在重试耗尽时失败请求。 session.timeout.ms consumer session 超时时间，默认：10000。 用于检测 worker 程序失败的超时。worker 定期发送心跳，以向代理表明其活性。如果在此会话超时过期之前代理没有接收到心跳，则代理将从组中删除。 注意：该值必须在 broker 端配置的 group.min.session.timeout 和 group.max.session.timeout.ms 范围之间。 heartbeat.interval.ms 心跳时间，默认：3000。 心跳是在 consumer 与 coordinator 之间进行的。心跳是确定 consumer 存活，加入或者退出 group 的有效手段。 这个值必须设置的小于 session.timeout.ms 的1/3，因为： 当 consumer 由于某种原因不能发 Heartbeat 到 coordinator 时，并且时间超过 session.timeout.ms 时，就会认为该 consumer 已退出，它所订阅的 partition 会分配到同一 group 内的其它的 consumer 上。 connections.max.idle.ms 连接空闲超时时间，默认：540000，即 9 分钟。 在此配置指定的毫秒数之后关闭空闲连接。 key.deserializer key 反序列化器，默认：无。 需要实现接口：org.apache.kafka.common.serialize.Deserializer。Kafka 提供以下几个默认的 key 反序列化器： String：org.apache.kafka.common.serialization.StringDeserializer。 value.deserializer value 反序列化器，默认：无。 需要实现接口：org.apache.kafka.common. serialize .Deserializer。Kafka 提供以下几个默认的 value 反序列化器： String：org.apache.kafka.common.serialization.StringDeserializer。 partition.assignment.strategy consumer订阅分区策略，默认：org.apache.kafka.clients.consumer.RangeAssignor。 当使用组管理时，客户端将使用分区分配策略的类名在使用者实例之间分配分区所有权。 max.partition.fetch.bytes 一次 fetch 请求，从一个 partition 中取得的 records 的最大值，默认：1048576，即 1 M。 如果在从 topic 中第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。 broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 message.max.bytes 配置和 topic 端的 max.message.bytes 配置。 fetch.max.bytes 一次 fetch 请求，从一个 broker 中取得的 records 的最大值，默认：52428800，即 50 M。 如果在从 topic中 第一个非空的 partition 取消息时，取到的第一个 record 的大小就超过这个配置时，仍然会读取这个 record，也就是说在这种情况下，只会返回这一条 record。 broker、topic 都会对 producer 发给它的 message size 做限制。所以在配置这值时，可以参考 broker 端的 message.max.bytes 配置 和 topic 端的 max.message.bytes 配置。 fetch.min.bytes 一次 fetch 请求，从一个 broker 中取得的 records 的最小值，默认：1。 如果 broker 中数据量不够的话会 wait，直到积累的数据大小满足这个条件。默认值设置为1的目的是：使得 consumer 的请求能够尽快的返回。将此设置为大于 1 的值将导致服务器等待更大数量的数据累积，这可以稍微提高服务器吞吐量，但代价是增加一些延迟。 fetch.max.wait.ms 拉取阻塞时间，默认：500。 如果没有足够的数据立即满足 fetch.min.bytes 提供的要求，服务器在响应 fetch 请求之前将阻塞的最长时间。 exclude.internal.topics 公开内部 topic，默认：true。 是否应该将来自内部主题 (如偏移量) 的记录公开给使用者，consumer 共享 offset。如果设置为 true，从内部主题接收记录的唯一方法是订阅它。 isolation.level 隔离级别，默认：read_uncommitted。 控制如何以事务方式读取写入的消息。如果设置为 read_committed，poll () 方法将只返回已提交的事务消息。如果设置为 read_uncommitted，poll () 方法将返回所有消息，甚至是已经中止的事务消息。在任何一种模式下，非事务性消息都将无条件返回。 Kafka Broker 核心配置参数zookeeper.connect zookeeper 地址，多个地址用逗号隔开。 broker.id 服务器的 broke id，默认：-1。 每一个 broker 在集群中的唯一表示，要求是正数。 如果未设置，将生成唯一的代理 id。为了避免 zookeeper 生成的 broke id 和用户配置的 broke id 之间的冲突，生成的代理 id 从 reserve.broker.max.id 开始 id + 1。 advertised.host.name 默认：null。 不赞成使用： 在 server.properties 里还有另一个参数是解决这个问题的， advertised.host.name 参数用来配置返回的 host.name值，把这个参数配置为外网 IP 地址即可。 这个参数默认没有启用，默认是返回的 java.net.InetAddress.getCanonicalHostName() 的值，在我的 mac 上这个值并不等于 hostname 的值而是返回 IP，但在 linux 上这个值就是 hostname 的值。 advertised.listeners hostname 和端口注册到 zookeeper 给生产者和消费者使用的，如果没有设置，将会使用 listeners 的配置，如果 listeners 也没有配置，将使用 java.net.InetAddress.getCanonicalHostName() 来获取这个 hostname 和 port，对于 ipv4，基本就是 localhost 了。 auto.create.topics.enable 是否允许自动创建 topic，默认：true。 如果为 true，第一次发动消息时，允许自动创建 topic。否则，只能通过命令创建 topic。 auto.leader.rebalance.enable 自动 rebalance，默认：true。 支持自动 leader balance。如果需要，后台线程定期检查并触发 leader balance。 background.threads 默认：10。 一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改。 compression.type 压缩类型，默认：producer。 对发送的消息采取的压缩编码方式 (‘gzip’，’snappy’，’lz4’)。 ‘uncompressed’：不压缩， ‘producer’：保持 producer 本身设置的压缩编码。 delete.topic.enable 是否允许删除 topic，默认：true。 如果关闭此配置，则通过管理工具删除主题将无效。 leader.imbalance.check.interval.seconds rebalance 检测频率，默认：300。 控制器触发分区 rebalance 检查的频率。 leader.imbalance.per.broker.percentage 触发 rebalance 得比率，默认：10，即 10%。 每个 broke 允许的 leader 不平衡比率。如果控制器超过每个 broke 的这个值，控制器将触发一个 leader balance。该值以百分比指定。 log.dir 保存日志数据的目录，默认：/tmp/kafka-logs。 log.dirs 保存日志数据的目录，默认：null。 可以指定多个存储路径，以逗号分隔。如果未设置，使用 log.dir 中设置的值。 log.flush.interval.messages 默认：9223372036854775807。 在将消息刷新到磁盘之前，日志分区上累积的消息数量。 log 文件 ”sync” 到磁盘之前累积的消息条数。因为磁盘 IO 操作是一个慢操作，但又是一个”数据可靠性”的必要手段。所以此参数的设置，需要在”数据可靠性”与”性能”之间做必要的权衡。 如果此值过大，将会导致每次 ”fsync” 的时间较长 (IO 阻塞)；如果此值过小，将会导致 ”fsync” 的次数较多，这也意味着整体的 client 请求有一定的延迟。 物理 server 故障，将会导致没有 fsync 的消息丢失。 log.flush.interval.ms 默认：null。 任何 topic 中的消息在刷新到磁盘之前保存在内存中的最长时间。如果没有设置，则使用 log.flush.scheduler.interval.ms 中的值。 log.flush.scheduler.interval.ms 日志刷新器检查是否需要将任何日志刷新到磁盘的频率，默认：9223372036854775807。 log.flush.offset.checkpoint.interval.ms 作为日志恢复点的上次刷新的持久记录的更新频率，默认：60000。 log.retention.bytes 删除前日志的最大大小，默认：-1。 topic 每个分区的最大文件大小，一个 topic 的大小限制 = 分区数 * log.retention.bytes。 log.retention.hours 日志文件最大保存时间 (小时)，默认：168，即 7 天。 删除日志文件之前保存它的小时数。 log.retention.minutes 日志文件最大保存时间 (分钟)，默认：null。 在删除日志文件之前保存它的分钟数，如果没有设置，则使用 log.retention.hours 中的值。 log.retention.ms 日志文件最大保存时间 (毫秒)，默认：null。 在删除日志文件之前保存它的毫秒数，如果没有设置，则使用 log.retention.minutes 中的值。如果设置为 -1，则没有时间限制。 log.roll.hours 新 segment 产生时间，默认：168，即 7 天。 即使文件没有到达 log.segment.bytes 设置的大小，只要文件创建时间到达此属性，也会强制创建新 segment。 log.roll.ms 新 segment 产生时间，默认：null。 如果未设置，则使用 log.roll.hours 中的值。 log.segment.bytes 单个 segment 文件的最大值，默认：1073741824，即 1 G。 log.segment.delete.delay.ms segment 删除前等待时间， 默认：60000，即 1 分钟。 message.max.bytes 最大 batch size，默认：1048588，即 1.000011 M。 Kafka 允许的最大 record batch size (如果启用了压缩，则是压缩后的大小)。如果增加了这个值，并且是 0.10.2 版本之前的 consumer，那么也必须增加 consumer 的 fetch 大小，以便他们能够获取这么大的 record batch。在最新的消息格式版本中，记录总是按批进行分组，以提高效率。在以前的消息格式版本中，未压缩记录没有分组成批，这种限制只适用于单个 record。针对每个 topic，可以使用 max.message.bytes 设置。 min.insync.replicas insync中最小副本值，默认：1。 当生产者将 acks 设置为 “all” (或 “-1”)时，min.insync.replicas 指定了必须确认写操作成功的最小副本数量。如果不能满足这个最小值，则生产者将抛出一个异常 (要么是 NotEnoughReplicas，要么是 NotEnoughReplicasAfterAppend)。 当一起使用时，min.insync.replicas 和 ack 允许你执行更大的持久性保证。一个典型的场景是创建一个复制因子为 3 的主题，设置 min.insync.replicas 为 2，生产者设置 acks 为 “all”，这将确保如果大多数副本没有收到写操作，则生产者会抛出异常。 num.io.threads 服务器用于处理请求的线程数，其中可能包括磁盘 I/O，默认：8。 num.network.threads 服务器用于接收来自网络的请求和向网络发送响应的线程数，默认：3。 num.recovery.threads.per.data.dir 每个数据目录在启动时用于日志恢复和在关闭时用于刷新的线程数，默认：1。 num.replica.alter.log.dirs.threads 可以在日志目录 (可能包括磁盘 I/O) 之间移动副本的线程数，默认：null。 num.replica.fetchers 从 leader 复制数据到 follower 的线程数，默认：1。 offset.metadata.max.bytes 与 offset 提交关联的 metadata 的最大大小，默认：4096。 offsets.commit.timeout.ms offset 提交将被延迟，直到偏移量主题的所有副本收到提交或达到此超时。这类似于生产者请求超时。默认：5000。 offsets.topic.num.partitions 偏移量提交主题的分区数量 (部署后不应再更改)，默认：50。 offsets.topic.replication.factor 副本大小，默认：3。 offsets.topic.segment.bytes 默认104857600，即 100 M。 segment 映射文件 (index) 文件大小，应该保持相对较小以便加快日志压缩和缓存负载。 queued.max.requests 阻塞网络线程之前，允许排队的请求数，默认：500。 replica.fetch.min.bytes 每个 fetch 响应所需的最小字节，默认：1。 如果字节不够，则等待 replicaMaxWaitTimeMs。 replica.lag.time.max.ms 默认：30000。 如果 follower 没有发送任何获取请求，或者至少在这段时间没有消耗到 leader 日志的结束偏移量，那么 leader 将从 isr 中删除 follower。 transaction.max.timeout.ms 默认：900000，即15分钟。 事务执行最长时间，超时则抛出异常。 unclean.leader.election.enable 默认：false。 指示是否在最后不得已的情况下启用 ISR 集中以外的副本作为 leader，即使这样做可能导致数据丢失。 zookeeper.connection.timeout.ms 默认：null。 客户端等待与 zookeeper 建立连接的最长时间。如果未设置，则使用 zookeeper.session.timeout.ms 中的值。 zookeeper.max.in.flight.requests 默认：10。 阻塞之前 consumer 将发送给 zookeeper 的未确认请求的最大数量。 group.max.session.timeout.ms 默认：1800000，即 30 分钟。 注册使用者允许的最大会话超时。超时时间越长，消费者在心跳之间处理消息的时间就越多，而检测故障的时间就越长。 group.min.session.timeout.ms 默认：6000。 注册使用者允许的最小会话超时。更短的超时导致更快的故障检测，但代价是更频繁的用户心跳，这可能会耗尽 broker 资源。 num.partitions 每个主题的默认日志分区数量，默认：1。 本文参考https://www.cnblogs.com/wangzhuxing/p/10111831.html#_label0_11 https://atbug.com/kafka-producer-config/ https://blog.csdn.net/jiecxy/article/details/53389892 本文只整理了部分有关 Kafka 的配置，仅作参考，更多的关于 broker，topic，producer 和 consumer 的配置，请参考 Kafka 官网。 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 wdshfut@163.com","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaConsumer 消费消息的基本流程","slug":"kafka-consumer","date":"2020-10-29T07:10:10.000Z","updated":"2020-11-11T06:11:00.670Z","comments":true,"path":"2020/10/29/kafka-consumer/","link":"","permalink":"http://example.com/2020/10/29/kafka-consumer/","excerpt":"","text":"如何消费数据在上一篇文章中，介绍了 KafkaProducer 如何发送数据到 Kafka，既然有数据发送，那么肯定就有数据消费，KafkaConsumer 也是 Kafka 整个体系中不可缺少的一环。 下面是一段创建 KafkaConsumer 的代码： 1234567891011121314151617181920212223242526272829public class KafkaConsumerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // 必须设置的属性 props.put(&quot;bootstrap.servers&quot;, &quot;192.168.239.131:9092&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;group.id&quot;, &quot;group1&quot;); // 可选设置的属性 props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;auto.offset.reset&quot;, &quot;earliest &quot;); props.put(&quot;client.id&quot;, &quot;test_client_id&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // 订阅主题 consumer.subscribe(Collections.singletonList(&quot;test&quot;)); while (true) &#123; // 拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100)); records.forEach(record -&gt; System.out.printf(&quot;topic = %s, partition = %d, offset = %d, key = %s, value = %s%n&quot;, record.topic(), record.partition(), record.offset(), record.key(), record.value())); &#125; &#125;&#125; 必须设置的属性创建 KafkaConsumer 时，必须设置的属性有 4 个： bootstrap.servers：连接 Kafka 集群的地址，多个地址以逗号分隔。 key.deserializer：消息中 key 反序列化类，需要和 KafkaProducer 中 key 序列化类相对应。 value.deserializer：消息中 value 的反序列化类，需要和 KafkaProducer 中 value 序列化类相对应。 group.id：消费者所属消费者组的唯一标识。 这里着重说一下 group.id 这个属性，KafkaConsumer 和 KafkaProducer 不一样，KafkaConsumer 中有一个 consumer group (消费者组)，由它来决定同一个 consumer group 中的消费者具体拉取哪个 partition 的数据，所以这里必须指定 group.id 属性。 订阅和取消主题 使用 subscribe () 方式订阅主题 1234// 订阅指定列表的topicpublic void subscribe(Collection&lt;String&gt; topics) &#123; subscribe(topics, new NoOpConsumerRebalanceListener());&#125; 1234// 订阅指定列表的topic，同时指定一个监听器public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123; ...&#125; 1234// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行public void subscribe(Pattern pattern) &#123; subscribe(pattern, new NoOpConsumerRebalanceListener());&#125; 1234// 订阅所有匹配指定模式的topic，模式匹配将定期对检查时存在的所有topic进行，同时指定一个监听器public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123; ...&#125; 使用 assign () 方式订阅主题和分区 1234// 手动将分区列表分配给consumerpublic void assign(Collection&lt;TopicPartition&gt; partitions) &#123; ...&#125; 使用示例 (仅作参考，assign() 方式的用法，应在使用时再做查询)： 123456List&lt;PartitionInfo&gt; partitionInfoList = kafkaConsumer.partitionsFor(&quot;test&quot;);if (partitionInfoList != null) &#123; for (PartitionInfo partitionInfo : partitionInfoList) &#123; kafkaConsumer.assign(Collections.singletonList(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()))); &#125;&#125; 取消主题的三种方式 123kafkaConsumer.unsubscribe();kafkaConsumer.subscribe(new ArrayList&lt;&gt;());kafkaConsumer.assign(new ArrayList&lt;TopicPartition&gt;()); 上面的三行代码作用相同，都是取消订阅，其中 unsubscribe () 方法即可以取消通过 subscribe () 方式实现的订阅，也可以取消通过 assign () 方式实现的订阅。 拉取数据KafkaConsumer 采用的是主动拉取 broker 数据进行消费的。 一般消息中间件存在推送 (push，server 推送数据给 consumer) 和拉取 (poll，consumer 主动去 server 拉取数据) 两种方式，这两种方式各有优劣。 如果是选择推送的方式，最大的阻碍就是 server 不清楚 consumer 的消费速度，如果 consumer 中执行的操作是比较耗时的，那么 consumer 可能会不堪重负，甚至会导致系统挂掉。 而采用拉取的方式则可以解决这种情况，consumer 根据自己的状态来拉取数据，可以对服务器的数据进行延迟处理。但是这种方式也有一个劣势就是 server 没有数据的时候可能会一直轮询，不过还好 KafkaConsumer 的 poll () 方法有参数允许 consumer 请求在”长轮询”中阻塞，以等待数据到达 (并且可选地等待直到给定数量的字节可用以确保传输大小)。 如何更好的消费数据文章开头处的代码展示了我们是如何消费数据的，但是代码未免过于简单，我们测试的时候这样写没有问题，但是实际开发过程中我们并不会这样写，我们会选择更加高效的方式，这里提供两种方式供大家参考。 一个 consumer group，多个 consumer，数量小于等于 partition 的数量 一个 consumer，多线程处理事件 第一种方式每个 consumer 都要维护一个独立的 TCP 连接，如果 partition 数和创建 consumer 线程的数量过多，会造成不小的系统开销。但是如果处理消息足够快速，消费性能也会提升，如果慢的话就会导致消费性能降低。 第二种方式是采用一个 consumer，多个消息处理线程来处理消息，其实在生产中，瓶颈一般是集中在消息处理上 (因为可能会插入数据到数据库，或者请求第三方 API)，所以我们采用多个线程来处理这些消息。 当然可以结合第一和第二两种方式，采用多 consumer + 多个消息处理线程来消费 Kafka 中的数据，核心代码如下： 1234567891011121314151617181920212223for (int i = 0; i &lt; consumerNum; i++) &#123; // 根据属性创建Consumer，并添加到consumer列表中 final Consumer&lt;String, byte[]&gt; consumer = consumerFactory.getConsumer(getServers(), groupId); consumerList.add(consumer); // 订阅主题 consumer.subscribe(Arrays.asList(this.getTopic())); // consumer.poll()拉取数据 BufferedConsumerRecords bufferedConsumerRecords = new BufferedConsumerRecords(consumer); getExecutor().scheduleWithFixedDelay(() -&gt; &#123; long startTime = System.currentTimeMillis(); // 进行消息处理 consumeEvents(bufferedConsumerRecords); long sleepTime = intervalMillis - (System.currentTimeMillis() - startTime); if (sleepTime &gt; 0) &#123; Thread.sleep(sleepTime); &#125; &#125;, 0, 1000, TimeUnit.MILLISECONDS);&#125; 不过这种方式不能顺序处理数据，如果你的业务是顺序处理，那么第一种方式可能更适合你。所以实际生产中请根据业务选择最适合自己的方式。 消费数据时应该考虑的问题什么是 offset？在 Kafka 中无论是 KafkarPoducer 往 topic 中写数据，还是 KafkaConsumer 从 topic 中读数据，都避免不了和 offset 打交道，关于 offset 主要有以下几个概念： Last Committed Offset：consumer group 最新一次 commit 的 offset，表示这个 consumer group 已经把 Last Committed Offset 之前的数据都消费成功了。 Current Position：consumer group 当前消费数据的 offset，也就是说，Last Committed Offset 到 Current Position 之间的数据已经拉取成功，可能正在处理，但是还未 commit。 High Watermark：HW，已经成功备份到其他 replica 中的最新一条数据的 offset，也就是说，High Watermark 与 Log End Offset 之间的数据已经写入到该 partition 的 leader 中，但是还未完全备份到其他的 replica 中，consumer 也无法消费这部分消息。 Log End Offset：LEO，记录底层日志 (log) 中的下一条消息的 offset。对 KafkaProducer 来说，就是即将插入下一条消息的 offset。 每个 Kafka 副本对象都有两个重要的属性：HW 和 LEO。注意是所有的副本，而不只是 leader 副本。关于这两者更详细解释，参考：[Kafka 的 High Watermark 与 leader epoch 的讨论 对于消费者而言，我们更多时候关注的是消费完成之后如何和服务器进行消费确认，告诉服务器这部分数据我已经消费过了。 这里就涉及到了 2 个 offset，一个是 Current Position，一个是处理完毕向服务器确认的 Last Committed Offset。显然，异步模式下 Last Committed Offset 是落后于 Current Position 的。如果 consumer 挂掉了，那么下一次消费数据又只会从 Last Committed Offset 的位置拉取数据，就会导致数据被重复消费。 如何选择 offset 的提交策略？Kafka 提供了三种提交 offset 的方式。 1. 自动提交 1234// 自动提交，默认trueprops.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);// 设置自动每1s提交一次props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); 2.手动同步提交 1kafkaConsumer.commitSync(); 3.手动异步提交 1kafkaConsumer.commitAsync(); 上面说了，既然异步提交 offset 可能会重复消费，那么我使用同步提交是否就可以解决数据重复消费的问题呢？我只能说 too young, too sample。且看如下代码： 1234567while (true) &#123; ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofMillis(100)); records.forEach(record -&gt; &#123; insertIntoDB(record); kafkaConsumer.commitSync(); &#125;);&#125; 很明显不行，因为 insertIntoDB () 和 kafkaConsumer.commitSync () 两个方法做不到原子操作，如果 insertIntoDB () 成功了，但是提交 offset 的时候 KafkaConsumer 挂掉了，然后服务器重启，仍然会导致重复消费问题。 是否需要做到不重复消费？只要保证处理消息和提交 offset 的操作是原子操作，就可以做到不重复消费。我们可以自己管理 committed offset，而不让 Kafka 来进行管理。 比如如下使用方式： 1.如果消费的数据刚好需要存储在数据库，那么可以把 offset 也存在数据库，就可以在一个事物中提交这两个结果，保证原子操作。 2.借助搜索引擎，把 offset 和数据一起放到索引里面，比如 Elasticsearch。 每条记录都有自己的 offset，所以如果要管理自己的 offset 还得要做下面事情： 1.设置 enable.auto.commit 为 false； 2.使用每个 ConsumerRecord 提供的 offset 来保存消费的位置； 3.在重新启动时使用 seek (TopicPartition partition, long offset) 恢复上次消费的位置。 通过上面的方式就可以在消费端实现 ”Exactly Once” 的语义，即保证只消费一次。但是是否真的需要保证不重复消费呢？这个得看具体业务，如果重复消费数据对整体有什么影响，然后再来决定是否需要做到不重复消费。 再均衡 (reblance) 时怎么办？再均衡是指分区的所属权从一个消费者转移到另一个消费者的行为，再均衡期间，消费者组内的消费者无法读取消息。为了更精确的控制消息的消费，我们可以在订阅主题的时候，通过指定监听器的方式来设定发生再均衡动作前后的一些准备或者收尾的动作。 1234567891011kafkaConsumer.subscribe(Collections.singletonList(&quot;test&quot;), new ConsumerRebalanceListener() &#123; @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; // 再均衡之前和消费者停止读取消息之后被调用 &#125; @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; // 重新分配分区之后和消费者开始消费之前被调用 &#125;&#125;); 具体如何操作，得根据具体的业务逻辑来实现，如果消息比较重要，你可以在再均衡的时候处理 offset，如果不够重要，你可以什么都不做。 无法消费的数据怎么办？可能由于你的业务逻辑有些数据没法消费，这个时候怎么办？同样的还是的看你认为这个数据有多重要或者多不重要，如果重要可以记录日志，把它存入文件或者数据库，以便于稍候进行重试或者定向分析。如果不重要就当做什么事情都没有发生好了。 实际开发中我的处理方式我开发的项目中，用到 Kafka 的其中一个地方是消息通知 (谁给你发了消息，点赞，评论等)，大概的流程就是用户在 client 端做了某些操作，就会发送数据到 Kafka，然后把这些数据进行一定的处理之后插入到 HBase 中。 其中采用了 N consumer thread + N Event Handler 的方式来消费数据，并采用自动提交 offset。对于无法消费的数据往往只是简单处理下，打印下日志以及消息体 (无法消费的情况非常非常少)。 得益于 HBase 的多 version 控制，即使是重复消费了数据也无关紧要。这样做没有去避免重复消费的问题主要是基于以下几点考虑： 1.重复消费的概率较低，服务器整体性能稳定。 2.即便是重复消费了数据，入库了 HBase，获取数据也是只有一条，不影响结果的正确性。 3.有更高的吞吐量。 4.编程简单，不用单独去处理以及保存 offset。 本文参考http://generalthink.github.io/2019/05/06/kafka-consumer-use/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 &#119;&#x64;&#x73;&#x68;&#102;&#117;&#116;&#64;&#49;&#54;&#x33;&#x2e;&#99;&#x6f;&#109;","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"KafkaProducer 部分源码解析","slug":"kafka-producer","date":"2020-10-26T06:15:52.000Z","updated":"2020-11-10T03:06:33.975Z","comments":true,"path":"2020/10/26/kafka-producer/","link":"","permalink":"http://example.com/2020/10/26/kafka-producer/","excerpt":"","text":"先来看一段创建 KafkaProducer 的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class KafkaProducerDemo &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // bootstrap.servers 必须设置 props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.239.131:9092&quot;); // key.serializer 必须设置 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // value.serializer 必须设置 props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // client.id props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;client-0&quot;); // retries props.put(ProducerConfig.RETRIES_CONFIG, 3); // acks props.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;); // max.in.flight.requests.per.connection props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1); // linger.ms props.put(ProducerConfig.LINGER_MS_CONFIG, 100); // batch.size props.put(ProducerConfig.BATCH_SIZE_CONFIG, 10240); // buffer.memory props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10240); KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props); // 指定topic，key，value ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;test1&quot;, &quot;key1&quot;, &quot;value1&quot;); // 异步发送 kafkaProducer.send(record, (recordMetadata, exception) -&gt; &#123; if (exception != null) &#123; // 发送失败的处理逻辑 exception.printStackTrace(); &#125; else &#123; // 发送成功的处理逻辑 System.out.println(recordMetadata.topic()); &#125; &#125;); // 同步发送 // kafkaProducer.send(record).get(); // 关闭Producer kafkaProducer.close(); &#125;&#125; 主要流程图 简要说明： 1.new KafkaProducer () 后，创建一个后台线程 KafkaThread (实际运行线程是 Sender，KafkaThread 是对 Sender 的封装) 扫描 RecordAccumulator 中是否有消息； 2.调用 kafkaProducer.send () 发送消息，实际是将消息保存到 RecordAccumulator 中，实际上就是保存到一个 Map 中 (ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)，这条消息会被记录到同一个记录批次 (相同主题相同分区算同一个批次) 里面，这个批次的所有消息会被发送到相同的主题和分区上； 3.后台的独立线程扫描到 RecordAccumulator 中有消息后，会将消息发送到 Kafka 集群中 (不是一有消息就发送，而是要看消息是否 ready)； 4.如果发送成功 (消息成功写入 Kafka)，就返回一个 RecordMetaData 对象，它包换了主题和分区信息，以及记录在分区里的偏移量等信息； 5.如果写入失败，就会返回一个错误，生产者在收到错误之后会尝试重新发送消息 (如果允许的话，此时会将消息在保存到 RecordAccumulator 中)，达到重试次数之后如果还是失败就返回错误消息。 缓存器的创建123456789101112this.accumulator = new RecordAccumulator(logContext, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), this.compressionType, lingerMs(config), retryBackoffMs, deliveryTimeoutMs, metrics, PRODUCER_METRIC_GROUP_NAME, time, apiVersions, transactionManager, new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME)); 后台线程的创建12345678910111213141516171819202122this.sender = newSender(logContext, kafkaClient, this.metadata);String ioThreadName = NETWORK_THREAD_PREFIX + &quot; | &quot; + clientId;this.ioThread = new KafkaThread(ioThreadName, this.sender, true);this.ioThread.start();KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient( new Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), this.metrics, time, &quot;producer&quot;, channelBuilder, logContext), metadata, clientId, maxInflightRequests, producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG), producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG), producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG), producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG), requestTimeoutMs, ClientDnsLookup.forConfig(producerConfig.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)), time, true, apiVersions, throttleTimeSensor, logContext); 上述代码中，构造了一个 KafkaClient 负责和 broker 通信，同时构造一个 Sender 并启动一个异步线程，这个线程会被命名为：kafka-producer-network-thread | $&#123;clientId&#125;，如果你在创建 producer 的时候指定 client.id 的值为 myclient，那么线程名称就是 kafka-producer-network-thread | myclient。 发送消息 (缓存消息)发送消息有同步发送以及异步发送两种方式，我们一般不使用同步发送，毕竟太过于耗时，使用异步发送的时候可以指定回调函数，当消息发送完成的时候 (成功或者失败) 会通过回调通知生产者。 同步 send： 123public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record) &#123; return send(record, null);&#125; 异步 send： 12345public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; // intercept the record, which can be potentially modified; this method does not throw exceptions ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record); return doSend(interceptedRecord, callback);&#125; 可以看出，同步和异步实际上调用的是同一个方法，同步发送时，设置回调函数为 null。 消息发送之前，会先对 key 和 value 进行序列化： 12345678910111213141516byte[] serializedKey;try &#123; serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert key of class &quot; + record.key().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in key.serializer&quot;, cce);&#125;byte[] serializedValue;try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());&#125; catch (ClassCastException cce) &#123; throw new SerializationException(&quot;Can&#x27;t convert value of class &quot; + record.value().getClass().getName() + &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &quot; specified in value.serializer&quot;, cce);&#125; 计算分区： 1int partition = partition(record, serializedKey, serializedValue, cluster); 发送消息，实际上是将消息缓存起来，核心代码如下： 12RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs); RecordAccumulator 的核心数据结构是 ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;，会将相同 topic 相同 partition 的数据放到一个 Deque (双向队列) 中，这也是我们之前提到的同一个记录批次里面的消息会发送到同一个主题和分区的意思。append () 方法的核心源码如下： 123456789101112131415161718192021// 从batchs(ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;)中，根据主题分区获取对应的队列，如果没有则new ArrayDeque&lt;&gt;返回Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);// 计算同一个记录批次占用空间大小，batchSize根据batch.size参数决定int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound( maxUsableMagic, compression, key, value, headers));// 为同一个topic，partition分配buffer，如果同一个记录批次的内存不足，那么会阻塞maxTimeToBlock(max.block.ms参数)这么长时间ByteBuffer buffer = free.allocate(size, maxTimeToBlock);synchronized (dq) &#123; // 创建MemoryRecordBuilder，通过buffer初始化appendStream(DataOutputStream)属性 MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic); ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds()); // 将key，value写入到MemoryRecordsBuilder中的appendStream(DataOutputStream)中 FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds())); // 将需要发送的消息放入到队列中 dq.addLast(batch);&#125; 发送消息到 Kafka上面已经将消息存储 RecordAccumulator 中去了，现在看看怎么发送消息。前面提到创建 KafkaProducer 的时候，会启动一个异步线程去从 RecordAccumulator 中取得消息然后发送到 Kafka，发送消息的核心代码在 Sender 中，它实现了 Runnable 接口并在后台一直运行处理发送请求并将消息发送到合适的节点，直到 KafkaProducer 被关闭。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * The background thread that handles the sending of produce requests to the Kafka cluster. This thread makes metadata * requests to renew its view of the cluster and then sends produce requests to the appropriate nodes. */public class Sender implements Runnable &#123; /** * The main run loop for the sender thread */ public void run() &#123; // main loop, runs until close is called while (running) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // okay we stopped accepting requests but there may still be // requests in the transaction manager, accumulator or waiting for acknowledgment, // wait until these are completed. while (!forceClose &amp;&amp; ((this.accumulator.hasUndrained() || this.client.inFlightRequestCount() &gt; 0) || hasPendingTransactionalRequests())) &#123; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; // Abort the transaction if any commit or abort didn&#x27;t go through the transaction manager&#x27;s queue while (!forceClose &amp;&amp; transactionManager != null &amp;&amp; transactionManager.hasOngoingTransaction()) &#123; if (!transactionManager.isCompleting()) &#123; log.info(&quot;Aborting incomplete transaction due to shutdown&quot;); transactionManager.beginAbort(); &#125; try &#123; runOnce(); &#125; catch (Exception e) &#123; log.error(&quot;Uncaught error in kafka producer I/O thread: &quot;, e); &#125; &#125; if (forceClose) &#123; // We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on // the futures. if (transactionManager != null) &#123; log.debug(&quot;Aborting incomplete transactional requests due to forced shutdown&quot;); transactionManager.close(); &#125; log.debug(&quot;Aborting incomplete batches due to forced shutdown&quot;); this.accumulator.abortIncompleteBatches(); &#125; &#125;&#125; KafkaProducer 的关闭方法有2个：close () 以及 close (Duration timeout)，close (long timeout, TimeUnit timUnit) 已被弃用，其中 timeout 参数的意思是等待生产者完成任何待处理请求的最长时间，第一种方式的 timeout 为 Long.MAX_VALUE 毫秒，如果采用第二种方式关闭，当 timeout = 0 的时候则表示强制关闭，直接关闭 Sender (设置 running = false)。 Send 中，runOnce () 方法，跳过对 transactionManager 的处理，查看发送消息的主要流程： 123456long currentTimeMs = time.milliseconds();// 将记录批次转移到每个节点的生产请求列表中long pollTimeout = sendProducerData(currentTimeMs);// 轮询进行消息发送client.poll(pollTimeout, currentTimeMs); 首先，查看 sendProducerData (currentTimeMs) 方法，它的核心逻辑在 sendProduceRequest (batches, now) 方法中： 123456789101112131415161718192021222324252627282930313233for (ProducerBatch batch : batches) &#123; TopicPartition tp = batch.topicPartition; // 将ProducerBatch中MemoryRecordsBuilder转换为MemoryRecords(发送的数据就在这里面) MemoryRecords records = batch.records(); // down convert if necessary to the minimum magic used. In general, there can be a delay between the time // that the producer starts building the batch and the time that we send the request, and we may have // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use // the new message format, but found that the broker didn&#x27;t support it, so we need to down-convert on the // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may // not all support the same message format version. For example, if a partition migrates from a broker // which is supporting the new magic version to one which doesn&#x27;t, then we will need to convert. if (!records.hasMatchingMagic(minUsedMagic)) records = batch.records().downConvert(minUsedMagic, 0, time).records(); produceRecordsByPartition.put(tp, records); recordsByPartition.put(tp, batch);&#125;ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout, produceRecordsByPartition, transactionalId);// 消息发送完成时的回调(消息发送失败后，在handleProduceResponse中处理)RequestCompletionHandler callback = new RequestCompletionHandler() &#123; public void onComplete(ClientResponse response) &#123; handleProduceResponse(response, recordsByPartition, time.milliseconds()); &#125;&#125;;// 根据参数构造ClientRequest，此时需要发送的消息在requestBuilder中ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, requestTimeoutMs, callback);// 将clientRequest转换成Send对象(Send.java，包含了需要发送数据的buffer)，给KafkaChannel设置该对象，记住这里还没有发送数据client.send(clientRequest, now); 在没有指定 KafkaClient 时，client.send (clientRequest, now) 方法，实际就是 NetworkClient.send (ClientRequest request, long now) 方法，所有的请求 (无论是 producer 发送消息的请求，还是获取 metadata 的请求) 都是通过该方法设置对应的 Send 对象： 1Send send = request.toSend(destination, header); 需要知道的是，上面只是设置了发送消息所需要准备的内容。 接下来，查看 client.poll (pollTimeout, currentTimeMs) 方法，进入到发送消息的主流程，发送消息的核心代码最终可以定位到 Selector 的 pollSelectionKeys (Set&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos) 方法中，代码如下： 12345678910111213141516/* if channel is ready write to any sockets that have space in their buffer and for which we have data */if (channel.ready() &amp;&amp; key.isWritable() &amp;&amp; !channel.maybeBeginClientReauthentication( () -&gt; channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos)) &#123; Send send; try &#123; // 底层实际调用的是java8 GatheringByteChannel的write方法 send = channel.write(); &#125; catch (Exception e) &#123; sendFailed = true; throw e; &#125; if (send != null) &#123; this.completedSends.add(send); this.sensors.recordBytesSent(channel.id(), send.size()); &#125;&#125; 就这样，我们的消息就发送到了 broker 中了，发送流程分析完毕，注意，这个是完美发送的情况。但是总会有发送失败的时候 (消息过大或者没有可用的 leader 等)，那么发送失败后重发又是在哪里完成的呢？还记得上面的回调函数吗，没错，就是在回调函数这里设置的，先来看下回调函数源码： 12345678910111213141516171819202122232425262728293031323334/** * Handle a produce response */private void handleProduceResponse(ClientResponse response, Map&lt;TopicPartition, ProducerBatch&gt; batches, long now) &#123; RequestHeader requestHeader = response.requestHeader(); long receivedTimeMs = response.receivedTimeMs(); int correlationId = requestHeader.correlationId(); if (response.wasDisconnected()) &#123; // 如果是网络断开则构造Errors.NETWORK_EXCEPTION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION), correlationId, now, 0L); &#125; else if (response.versionMismatch() != null) &#123; // 如果是版本不匹配，则构造Errors.UNSUPPORTED_VERSION的响应 for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.UNSUPPORTED_VERSION), correlationId, now, 0L); &#125; else &#123; // if we have a response, parse it(如果存在response就返回正常的response) if (response.hasResponse()) &#123; ProduceResponse produceResponse = (ProduceResponse) response.responseBody(); for (Map.Entry&lt;TopicPartition, ProduceResponse.PartitionResponse&gt; entry : produceResponse.responses().entrySet()) &#123; TopicPartition tp = entry.getKey(); ProduceResponse.PartitionResponse partResp = entry.getValue(); ProducerBatch batch = batches.get(tp); completeBatch(batch, partResp, correlationId, now, receivedTimeMs + produceResponse.throttleTimeMs()); &#125; this.sensors.recordLatency(response.destination(), response.requestLatencyMs()); &#125; else &#123; // this is the acks = 0 case, just complete all requests(如果acks=0，那么则构造Errors.NONE的响应，因为这种情况只需要发送不需要响应结果) for (ProducerBatch batch : batches.values()) &#123; completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NONE), correlationId, now, 0L); &#125; &#125; &#125;&#125; 在 completeBatch () 方法中我们主要关注失败的逻辑处理，核心源码如下： 12345678910111213141516171819202122232425262728293031/** * Complete or retry the given batch of records. * * @param batch The record batch * @param response The produce response * @param correlationId The correlation id for the request * @param now The current POSIX timestamp in milliseconds */private void completeBatch(ProducerBatch batch, ProduceResponse.PartitionResponse response, long correlationId, long now, long throttleUntilTimeMs) &#123; Errors error = response.error; if (error == Errors.MESSAGE_TOO_LARGE &amp;&amp; batch.recordCount &gt; 1 &amp;&amp; !batch.isDone() &amp;&amp; (batch.magic() &gt;= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) &#123; // If the batch is too large, we split the batch and send the split batches again. We do not decrement // the retry attempts in this case.(如果发送的消息太大，需要重新进行分割发送) this.accumulator.splitAndReenqueue(batch); maybeRemoveAndDeallocateBatch(batch); this.sensors.recordBatchSplit(); &#125; else if (error != Errors.NONE) &#123; // 发生了错误，如果此时可以retry(retry次数未达到限制以及产生的异常是RetriableException) if (canRetry(batch, response, now)) &#123; if (transactionManager == null) &#123; // 把需要重试的消息放入队列中，等待重试，实际就是调用deque.addFirst(batch) reenqueueBatch(batch, now); &#125; ... &#125; ... &#125;&#125; 以上，就是 KafkaProducer 发送消息的流程。 补充：分区算法在发送消息前，调用的计算分区方法如下： 123456789101112/** * computes partition for given record. * if the record has partition returns the value otherwise * calls configured partitioner class to compute the partition. */private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition : partitioner.partition( record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; 如果在创建 ProducerRecord 的时候，指定了 partition，则使用指定的，否则调用配置的 partitioner 类来计算分区。 如果没有配置自定义的分区器，Kafka 默认使用 org.apache.kafka.clients.producer.internals.DefaultPartitioner，源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * The default partitioning strategy: * &lt;ul&gt; * &lt;li&gt;If a partition is specified in the record, use it * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key * &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion */public class DefaultPartitioner implements Partitioner &#123; private final ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = new ConcurrentHashMap&lt;&gt;(); public void configure(Map&lt;String, ?&gt; configs) &#123;&#125; /** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; // 如果key为null，则使用Round Robin算法 int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition(根据key进行散列，使用murmur2算法) return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125; private int nextValue(String topic) &#123; AtomicInteger counter = topicCounterMap.get(topic); if (null == counter) &#123; counter = new AtomicInteger(ThreadLocalRandom.current().nextInt()); AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter); if (currentCounter != null) &#123; counter = currentCounter; &#125; &#125; return counter.getAndIncrement(); &#125; public void close() &#123;&#125;&#125; DefaultPartitioner 中对于分区的算法有两种情况： 1.如果键值为 null，那么记录键随机地发送到主题内各个可用的分区上。分区器使用轮询 (Round Robin) 算法键消息均衡地分布到各个分区上。 2.如果键不为 null，那么 Kafka 会对键进行散列 (使用 Kafka 自己的散列算法，即使升级 java 版本，散列值也不会发生变化) ，然后根据散列值把消息映射到特定的分区上。应该注意的是，同一个键总是被映射到同一个分区上 (如果分区数量发生了变化则不能保证)，映射的时候会使用主题所有的分区，而不仅仅是可用分区，所以如果写入数据分区是不可用的，那么就会发生错误，当然这种情况很少发生。 当然，如果你想要实现自定义分区，那么只需要实现 Partitioner 接口即可： 123456789101112131415161718192021222324/** * 将key的hash值，对分区总数取余，以确定消息发送到哪个分区 */public class KeyPartitioner implements Partitioner &#123; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; Integer numPartitions = cluster.partitionCountForTopic(topic); if (keyBytes == null) &#123; throw new InvalidRecordException(&quot;key can not be null&quot;); &#125; return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 然后，使用 partitioner.class 参数，指定你自定义的分区器的路径： 1props.put(&quot;partitioner.class&quot;, &quot;cn.xisun.partitioner.KeyPartitioner&quot;); 本文参考https://generalthink.github.io/2019/03/07/kafka-producer-source-code-analysis/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 &#x77;&#100;&#x73;&#x68;&#102;&#117;&#x74;&#64;&#x31;&#x36;&#51;&#46;&#99;&#111;&#109;","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"什么是 Kafka","slug":"kafka-introduce","date":"2020-10-23T07:58:39.000Z","updated":"2020-11-09T07:48:48.340Z","comments":true,"path":"2020/10/23/kafka-introduce/","link":"","permalink":"http://example.com/2020/10/23/kafka-introduce/","excerpt":"","text":"分布式分布式系统由多个运行的计算机系统组成，所有这些计算机在一个集群中一起工作，对终端用户来讲只是一个单一节点。 Kafka 也是分布式的，因为它在不同的节点 (又被称为 broker) 上存储，接受以及发送消息，这样做的好处是具有很高的可扩展性和容错性。 水平可扩展性在这之前，先看看什么是垂直可扩展，比如你有一个传统的数据库服务器，它开始过度负载，解决这个问题的办法就是给服务器加配置 (cpu，内存，SSD)，这就叫做垂直扩展。但是这种方式存在两个巨大的劣势： 1.硬件存在限制，不可能无限的添加机器配置。 2.它需要停机时间，通常这是很多公司无法容忍的。 水平可扩展就是通过添加更多的机器来解决同样的问题，添加新机器不需要停机，而且集群中也不会对机器的数量有任何的限制。但问题在于并非所有系统都支持水平可伸缩性，因为它们不是设计用于集群中 (在集群中工作会更加复杂)。 容错性非分布式系统中容易最致命的问题就是单点失败，如果你唯一的服务器挂掉了，那么我相信你会很崩溃。 而分布式系统的设计方式就是可以以配置的方式来容许失败。比如在 5 个节点的 Kafka 集群中，即使其中两个节点挂掉了，你仍然可以继续工作。 需要注意的是，容错与性能直接相关，你的系统容错程度越高，性能就越差。 提交日志 (commit log)提交日志 (也被称为预写日志或者事物日志) 是仅支持附加的持久有序数据结构，你无法修改或者删除记录，它从左往右读并且保证日志的顺序。 是不是觉得 Kafka 的数据结构如此简单? 是的，从很多方面来讲，这个数据结构就是 Kafka 的核心。这个数据结构的记录是有序的，而有序的数据可以确保我们的处理流程。这两个在分布式系统中都是极其重要的问题。 Kafka 实际上将所有消息存储到磁盘并在数据结构中对它们进行排序，以便利用顺序磁盘读取。 1.读取和写入都是常量时间 O(1) (当确定了 record id)，与磁盘上其他结构的 O(log N)操作相比是一个巨大的优势，因为每个磁盘搜索都很耗时。 2.读取和写入不会相互影响，写不会锁住读，反之亦然。 这两点有着巨大的优势， 因为数据大小与性能完全分离。无论你的服务器上有 100 KB 还是 100 TB 的数据，Kafka 都具有相同的性能。 如何工作生产者消费者模式：生产者 (producer) 发送消息 (record) 到 Kafka 服务器 (broker)，这些消息存储在主题 (topic) 中，然后消费者 (consumer) 订阅该主题，接受新消息后并进行处理。 随着消息的越来越多，topic 也会越来越大，为了获得更好的性能和可伸缩性，可以在 topic 下建立多个更小的分区 (partition)，在发送消息时，可以根据实际情况，对消息进行分类，同一类的消息发送到同一个 partition (比如存储不同用户发送的消息，可以根据用户名的首字母进行分区匹配)。Kafka 保证 partition 内的所有消息都按照它们的顺序排序，区分特定消息的方式是通过其偏移量 (offset)，你可以将其视为普通数组索引，即为分区中的每个新消息递增的序列号。 Kafka 遵守着愚蠢的 broker 和聪明的 consumer 的准则。这意味着 Kafka 不会跟踪消费者读取了哪些记录并删除它们，而是会将它们存储一定的时间 (比如 1 天，以 log.retention 开头的来决定日志保留时间)，直到达到某个阈值。消费者自己轮询 Kafka 的新消息并且告诉它自己想要读取哪些记录，这允许它们按照自己的意愿递增/递减它们所处的偏移量，从而能够重放和重新处理事件。 需要注意的是消费者是属于消费者组的 (在创建 consumer 时，必须指定其所属的消费者组的 group.id)，消费者组有一个或多个消费者。为了避免两个进程读取同样的消息两次，每个 partition 只能被一个消费者组中的一个消费者访问。 持久化到硬盘正如之前提到的，Kafka 实际上是将所有记录存储到硬盘而不在 RAM 中保存任何内容，这背后有很多优化使得这个方案可行。 1.Kafka 有一个将消息分组的协议，这允许网络请求将消息组合在一起并减少网络开销，服务器反过来一次性保留大量消息，消费者一次获取大量线性块。 2.磁盘上线性读写非常快，现代磁盘非常慢的原因是由于大量磁盘寻址，但是在大量的线性操作中不是问题。 3.操作系统对线性操作进行了大量优化，通过预读 (预取大块多次) 和后写 (将小型逻辑写入组成大型物理写入) 技术。 4.操作系统将磁盘文件缓存在空闲 RAM 中。这称为 page cache，而 Kafka 的读写都大量使用了 page cache： ​ ① 写消息的时候消息先从 java 到 page cache，然后异步线程刷盘，消息从 page cache 刷入磁盘； ​ ② 读消息的时候先从 page cache 找，有就直接转入 socket，没有就先从磁盘 load 到 page cache，然后直接从 socket 发出去。 5.由于 Kafka 在整个流程 (producer → broker → consumer) 中以未经修改的标准化二进制格式存储消息，因此它可以使用零拷贝优化。那时操作系统将数据从 page cache 直接复制到 socket，有效地完全绕过了 Kafka broker。 所有这些优化都使 Kafka 能够以接近网络的速度传递消息。 数据分发和复制下面来谈谈 Kafka 如何实现容错以及它如何在节点之间分配数据。 为了使得一个 broker 挂掉的时候，数据还能得以保留，分区 (partition) 数据在多个 broker 中复制。 在任何时候，一个 broker 拥有一个 partition，应用程序读取/写入都要通过这个节点，这个节点叫做 partition leader。它将收到的数据复制到 N 个其他 broker，这些接收数据的 broker 叫做 follower，follower 也存储数据，一旦 leader 节点死掉的时候，它们就准备竞争上岗成为 leader。 这可以保证你成功发布的消息不会丢失，通过选择更改副本因子，你可以根据数据的重要性来交换性能以获得更强的持久性保证。 这样如果 leader 挂掉了，那么其中一个 follower 就会接替它称为 leader。包括 leader 在内的总副本数就是副本因子 (创建 topic 时，使用 --replication-factor 参数指定)，上图有 1 个 leader，2 个 follower，所以副本因子就是 3。 但是你可能会问：producer 或者 consumer 怎么知道 partition leader 是谁？ 对生产者/消费者对分区的写/读请求，它们需要知道分区的 leader 是哪一个，对吧？这个信息肯定是可以获取到的，Kafka 使用 ZooKeeper 来存储这些元数据。 什么是 ZooKeeperZooKeeper 是一个分布式键值存储。它针对读取进行了高度优化，但写入速度较慢。它最常用于存储元数据和处理集群的机制 (心跳，分发更新/配置等)。 它允许服务的客户 (Kafka broker) 订阅并在发生变更后发送给他们，这就是 Kafka 如何知道何时切换分区领导者。ZooKeeper 本身维护了一个集群，所以它就有很高的容错性，当然它也应该具有，毕竟 Kafka 很大程度上是依赖于它的。 ZooKeeper 用于存储所有的元数据信息，包括但不限于如下几项： 消费者组每个分区的偏移量 (现在客户端在单独的 Kafka topic 上存储偏移量) ACL —— 权限控制 生产者/消费者的流量控制——每秒生产/消费的数据大小。参考：Kafka - 流量控制 Quota 功能 partition leader 以及它们的健康信息 那么 producer/consumer 是如何知道谁是 partition leader 的呢？ 生产者和消费者以前常常直接连接 ZooKeeper 来获取这些信息，但是 Kafka 从 0.8 和 0.9 版本开始移除了这种强耦合关系。客户端直接从 Kafka broker 获取这些元数据，而让 Kafka broker 从 ZooKeeper 那里获取这些元数据。 更多 ZooKeeper 的讲解参考：漫画：什么是 ZooKeeper？ 流式处理 (Streaming)在 Kafka 中，流处理器是指从输入主题获取连续数据流，对此输入执行某些处理并生成数据流以输出到其他主题 (或者外部服务，数据库，容器等等)。 什么是数据流呢？首先，数据流是无边界数据集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的，是因为随着时间推移，新的记录会不断加入进来。比如信用卡交易，股票交易等事件都可以用来表示数据流。 我们可以使用 producer/consumer 的 API 直接进行简单处理，但是对于更加复杂的转换，比如将流连接到一起，Kafka 提供了集成 Stream API 库。 这个 API 是在你自己的代码中使用的，它并不是运行在 broker 上，它的工作原理和 consumer API 类似，可帮助你在多个应用程序 (类似于消费者组) 上扩展流处理工作。 无状态处理流的无状态处理是确定性处理，其不依赖于任何外部条件，对于任何给定的数据，将始终生成与其他任何内容无关的相同输出。举个例子，我们要做一个简单的数据转换—“zhangsan” → “Hello, zhangsan” 流-表二义性重要的是要认识到流和表实质上是一样的，流可以被解释称为表，表也可以被解释称为流。 流作为表流可以解释为数据的一系列更新，聚合后的结果就是表的最终结果，这项技术被称为事件溯源 (Event Sourcing)。 如果你了解数据库备份同步，你就会知道它们的技术实现被称为流式复制—将对表的每个更改都发送报副本服务器。比如 redis 中的 AOF 以及 Mysql 中的 binlog。 Kafka 流可以用相同的方式解释 - 当累积形成最终状态时的事件。此类流聚合保存在本地 RocksDB 中 (默认情况下)，被称为 KTable。 表作为流可以将表视为流中每个键的最新值的快照。与流记录可以生成表一样，表更新可以生成更改日志流。 有状态处理我们在 java 中常用的一些操作比如 map() 或者 filter() 是没有状态的，它不会要求你保留任何原始数据。但是现实中，大多数的操作都是有状态的 (比如 count())，因为这需要你存储当前累计的状态。 在流处理器上维护状态的问题是流处理器可能会失败！你需要在哪里保持这种状态才能容错？ 一种简单的方法是简单地将所有状态存储在远程数据库中，并通过网络连接到该存储，这样做的问题是大量的网络带宽会使得你的应用程序变慢。一个更微妙但重要的问题是你的流处理作业的正常运行时间将与远程数据库紧密耦合，并且作业将不是自包含的 (其他 team 更改数据库可能会破坏你的处理)。 那么什么是更好的办法呢？ 回想一下表和流的二元性。这允许我们将流转换为与我们的处理位于同一位置的表。它还为我们提供了一种处理容错的机制—通过将流存储在 Kafka broker 中。 流处理器可以将其状态保持在本地表 (例如 RocksDB) 中，该表将从输入流 (可能在某些任意转换之后) 更新。当进程失败时，它可以通过重放流来恢复其数据。 你甚至可以将远程数据库作为流的生产者，有效地广播用于在本地重建表的更改日志。 KSQL通常，我们不得不使用 JVM 语言编写流处理，因为这是唯一的官方 Kafka Streams API 客户端。2018 年 4 月，KSQL 作为一项新特性被发布，它允许你使用熟悉的类似 SQL 的语言编写简单的 stream jobs。你安装了 KSQL 服务器并通过 CLI 以交互方式查询以及管理。它使用相同的抽象 (KStream 和 KTable)，保证了 Streams API 的相同优点 (可伸缩性，容错性)，并大大简化了流的工作。 这听起来可能不是很多，但在实践中对于测试内容更有用，甚至允许开发之外的人 (例如产品所有者) 使用流处理，可以看看 Confluent 提供的这篇关于 ksql 的使用。 什么时候使用 kafka正如我们已经介绍的那样，Kafka 允许你通过集中式介质获取大量消息并存储它们，而不必担心性能或数据丢失等问题。 这意味着它非常适合用作系统架构的核心，充当连接不同应用程序的集中式媒体。Kafka 可以成为事件驱动架构的中心部分，使你可以真正地将应用程序彼此分离。 Kafka 允许你轻松地分离不同 (微) 服务之间的通信。使用 Streams API，现在可以比以往更轻松地编写业务逻辑，从而丰富 Kafka 主题数据以供服务使用。可能性很大，我恳请你探讨公司如何使用 Kafka。 总结Apache Kafka 是一个分布式流媒体平台，每天可处理数万亿个事件。Kafka 提供低延迟，高吞吐量，容错的发布和订阅管道，并能够处理事件流。我们回顾了它的基本语义 (producer，broker，consumer，topic)，了解了它的一些优化 (page cache)，通过复制数据了解了它的容错能力，并介绍了它不断增长的强大流媒体功能。Kafka 已经在全球数千家公司中大量采用，其中包括财富 500 强企业中的三分之一。随着 Kafka 的积极开发和最近发布的第一个主要版本 1.0 (2017 年 11 月 1 日)，有预测这个流媒体平台将会与关系数据库一样，是数据平台的重要核心。我希望这篇介绍能帮助你熟悉 Apache Kafka。 本文参考http://generalthink.github.io/2019/02/27/introduction-of-kafka/ 声明：写作本文初衷是个人学习记录，鉴于本人学识有限，如有侵权或不当之处，请联系 &#119;&#x64;&#x73;&#x68;&#x66;&#117;&#x74;&#64;&#49;&#54;&#x33;&#46;&#99;&#111;&#x6d;","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"}]},{"title":"使用 hexo 搭建 github 博客","slug":"hexo-blog","date":"2020-10-23T03:33:51.000Z","updated":"2020-11-11T07:14:05.949Z","comments":true,"path":"2020/10/23/hexo-blog/","link":"","permalink":"http://example.com/2020/10/23/hexo-blog/","excerpt":"","text":"使用工具版本默认已经安装 node.js 和 git。 123git version: git version 2.27.0.windows.1npm version: 6.14.7hexo version: 4.2.0 git 客户端与 github 建立 SSH 连接1Please make sure you have the correct access rights and the repository exists. 当 git 客户端出现以上提示时，说明 SSH 连接过期，需要重新建立连接。参考如下方式： 1.先查看下 name 和 email 123456# 查看user的name和email$ git config user.name$ git config user.email# 如果没设置，按如下命令设置$ git config --global user.name &#123;$yourname&#125;$ git config --global user.email &#123;$youremail&#125; 2.删除 .ssh 文件夹下的 known_hosts，路径为：C:\\Users\\&#123;$userrname&#125;\\.ssh 3.git bash 输入命令 1$ ssh-keygen -t rsa -C &#123;$youremail&#125; 一直按回车，等结束后，.ssh 文件夹下会生成两个文件：id_rsa 和 id_rsa.pub，将 id_rsa.pub 的内容全部复制。 4.登录个人 github 账户，进入 Settings → SSH and GPG keys，点击 New SSH key，将复制的内容粘贴到 Key 里，点击 Add SSH key。 5.git bash 输入命令 1$ ssh -T git@github.com 在弹出的确定对话框输入：yes。 hexo 安装在 git bash 中依次输入以下命令： 123456$ npm install hexo-cli -g$ cd f: # 可以是任何路径$ hexo init blog$ cd blog # 进入blog目录$ npm install$ npm install hexo-deployer-git --save 命令执行完成后，会在 F:\\ 目录下，多一个 blog 文件夹。 修改 _config.yml 文件修改 blog 根目录下的 _config.yml 文件，将 deploy 节点修改为如下内容： 1234deploy: type: git repo: git@github.com:&#123;$yourname&#125;/&#123;$yourname&#125;.github.io.git branch: master 说明：_config.yml 文件的配置均为 [key: value] 形式，value 前面必须要有一个空格。 然后在 git bash 中输入以下命令，发布博客： 1$ hexo deploy 访问自己的博客博客地址：https://&#123;$yourname&#125;.github.io/ 写一个自己的博客hexo 的项目结构是在网站根目录的 source\\_posts 目录下存放你的博客文档，以 .md 文档格式存储，默认已存在一个 hello-world.md 文章。 1.新建文章 1$ hexo new &lt;title&gt; 会在 blog 的 source\\_posts 目录下，新建一个名叫 &lt;title&gt;.md 文章，如： 12INFO Validating configINFO Created: F:\\blog\\source\\_posts\\tesss.md 之后，在文章中添加自己的内容即可，建议使用 Typora 编辑，其语法参考：如何使用 markdown？ 2.发布文章 1234$ hexo clean # 清楚缓存$ hexo generate # 生成静态页面$ hexo server # 本地发布，浏览器输入localhost:4000即可访问博客$ hexo deploy # 将public中的静态页面复制到.deploy_git文件夹中，并提交到github 至此，你的第一个自己的博客发布完成。 说明：以上 hexo 的命令，都要在 F:\\blog 目录下执行。 修改博客的 themes如果想修改自己博客的 themes，可以下载好想要的，然后拷贝到 blog 的 themes 目录下，然后修改 _config.yml 文件，将 theme 节点的值，修改为你下载好的 themes 的名称，如： 1theme: next 之后，再按照你下载的 themes 的使用说明，做相应修改即可。 参考：NexT 的使用 NexT 中 tags 的使用1.修改 NexT 目录下的 _config.yml 文件，取消 menu 菜单下 tags 字段的注释 123menu: home: / || fa fa-home tags: /tags/ || fa fa-tags 2.在 blog 根目录的 source 目录下，新建 tags 目录 1$ hexo new page &quot;tags&quot; 3.修改 tags 目录下的 index.md 文件 1234title: tagsdate: 2020-10-27 16:35:56type: tagslayout: &quot;tags&quot; NexT 中添加字数统计、阅读时长1.安装 hexo-symbols-count-time 插件 1$ npm install hexo-symbols-count-time 或者 1$ yarn add hexo-symbols-count-time 2.hexo 配置，根目录下的 _config.yaml 文件，添加 symbols_count_time 节点 123456# Post wordcount display settingssymbols_count_time: symbols: true # 文章字数 time: true # 阅读时长 total_symbols: true # 所有文章总字数 total_time: true # 所有文章阅读中时长 3.NexT 配置，themes 目录下的 _config.yml 文件，symbols_count_time 节点 123456# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: true # 是否换行显示 字数统计 及 阅读时长 item_text_post: true # 文章 字数统计 阅读时长 使用图标 还是 文本表示 item_text_total: false # 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示 在博客中添加图片md 文件中插入图片的语法为：![]()。 其中，方括号是图片描述，圆括号是图片路径。一般来说有三种图片路径，分别是相对路径，绝对路径和网络路径。 相对而言，使用相对路径会更加方便，设置如下： 1.安装 hexo-renderer-marked 插件 1$ npm install hexo-renderer-marked 2.修改根目录下的 _config.yaml 配置 将： 1post_asset_folder: false 修改为： 1234post_asset_folder: truemarked: prependRoot: true postAsset: true 3.设置 Typora 点击文件 → 偏好设置，设置如下： 这样，在粘贴图片到文件中时，会自动将图片复制到 source\\_posts 目录下，与 .md 文件同名的目录中。 之后，在将博客发布时，会自动上传到文章所生成的静态网页同级目录之下。","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}],"categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"maven","slug":"maven","permalink":"http://example.com/tags/maven/"},{"name":"Poetry and Prose","slug":"Poetry-and-Prose","permalink":"http://example.com/tags/Poetry-and-Prose/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]}